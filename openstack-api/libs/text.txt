 User Guide
Release Version: 15.0.0
OpenStack contributors
May 12, 2017
CONTENTS
Abstract 1
Contents 2 Conventions............................................. 2 HowcanIuseanOpenStackcloud? ................................. 2 OpenStackdashboard ........................................ 3 OpenStackcommand-lineclients................................... 35 OpenStackPythonSDK .......................................157 HOTGuide. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 OpenStackcommand-lineinterfacecheatsheet. . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
Index 225
 i
OpenStack is an open-source cloud computing platform for public and private clouds. A series of interrelated projects deliver a cloud infrastructure solution. This guide shows OpenStack end users how to create and manage resources in an OpenStack cloud with the OpenStack dashboard and OpenStack client commands.
This guide documents OpenStack Ocata, Newton and Mitaka releases.
ABSTRACT
 1
Conventions
The OpenStack documentation uses several typesetting conventions.
Notices
Notices take these forms:
Note: A comment with additional information that explains a part of the text.
Important: Something you must be aware of before proceeding.
Tip: An extra but helpful piece of practical advice.
Caution: Helpful information that prevents the user from making mistakes.
Warning: Critical information about the risk of data loss or security issues.
Command prompts
$ command
Any user, including the root user, can run commands that are prefixed with the $ prompt.
# command
The root user must run commands that are prefixed with the # prompt. You can also prefix these commands
with the sudo command, if available, to run them.
How can I use an OpenStack cloud?
As an OpenStack cloud end user, you can provision your own resources within the limits set by cloud admin- istrators.
The examples in this guide show you how to perform tasks by using the following methods:
CONTENTS
                           2
 • OpenStack dashboard: Use this web-based graphical interface, code named horizon, to view, create, and manage resources.
• OpenStack command-line clients: Each core OpenStack project has a command-line client that you can use to run simple commands to view, create, and manage resources in a cloud and automate tasks by using scripts.
You can modify these examples for your specific use cases.
In addition to these ways of interacting with a cloud, you can access the OpenStack APIs directly or indirectly through cURL commands or open SDKs. You can automate access or build tools to manage resources and services by using the native OpenStack APIs or the EC2 compatibility API.
To use the OpenStack APIs, it helps to be familiar with HTTP/1.1, RESTful web services, the OpenStack services, and JSON or XML data serialization formats.
Who should read this book?
This book is written for anyone who uses virtual machines and cloud resources to develop software or perform research. You should have years of experience with Linux-based tool sets and be comfortable using both GUI and CLI based tools. While this book includes some information about using Python to create and manage cloud resources, Python knowledge is not a pre-requisite for reading this book.
OpenStack dashboard
As a cloud end user, you can use the OpenStack dashboard to provision your own resources within the limits set by administrators. You can modify the examples provided in this section to create other types and sizes of server instances.
Log in to the dashboard
The dashboard is generally installed on the controller node.
1. AskthecloudoperatorforthehostnameorpublicIPaddressfromwhichyoucanaccessthedashboard, and for your user name and password. If the cloud supports multi-domain model, you also need to ask for your domain name.
2. Open a web browser that has JavaScript and cookies enabled.
Note: To use the Virtual Network Computing (VNC) client for the dashboard, your browser must support HTML5 Canvas and HTML5 WebSockets. The VNC client is based on noVNC. For details, see noVNC: HTML5 VNC Client. For a list of supported browsers, see Browser support.
3. In the address bar, enter the host name or IP address for the dashboard, for example, https:// ipAddressOrHostName/.
Note: If a certificate warning appears when you try to access the URL for the first time, a self-signed certificate is in use, which is not considered trustworthy by default. Verify the certificate or add an exception in the browser to bypass the warning.
User Guide (Release Version: 15.0.0)
     OpenStack dashboard 3
 User Guide (Release Version: 15.0.0)
On the Log In page, enter your user name and password, and click Sign In. If the cloud supports multi- domain model, you also need to enter your domain name.
The top of the window displays your user name. You can also access the Settings tab (OpenStack dash- board — Settings tab) or sign out of the dashboard.
The visible tabs and functions in the dashboard depend on the access permissions, or roles, of the user you are logged in as.
• If you are logged in as an end user, the Project tab (OpenStack dashboard — Project tab) and Identity tab (OpenStack dashboard — Identity tab) are displayed.
• Ifyouareloggedinasanadministrator,theProjecttab(OpenStackdashboard—Projecttab)and Admin tab (OpenStack dashboard — Admin tab) and Identity tab (OpenStack dashboard — Identity tab) are displayed.
Some tabs, such as Orchestration and Firewalls, only appear on the dashboard if they are properly configured.
OpenStack dashboard — Project tab
Projects are organizational units in the cloud and are also known as tenants or accounts. Each user is a member of one or more projects. Within a project, a user creates and manages instances.
From the Project tab, you can view and manage the resources in a selected project, including instances and images. You can select the project from the drop-down menu at the top left. If the cloud supports multi-domain model, you can also select the domain from this menu.
Note:
4.
   Fig. 1: Figure: Project tab
 4
OpenStack dashboard
 From the Project tab, you can access the following categories: Compute tab
• Overview: View reports for the project.
• Instances: View, launch, create a snapshot from, stop, pause, or reboot instances, or connect to them
through VNC.
• Volumes: Use the following tabs to complete these tasks:
– Volumes: View, create, edit, and delete volumes.
– Volume Snapshots: View, create, edit, and delete volume snapshots.
• Images: View images and instance snapshots created by project users, plus any images that are publicly
available. Create, edit, and delete images, and launch instances from images and snapshots.
• Access & Security: Use the following tabs to complete these tasks:
– Security Groups: View, create, edit, and delete security groups and security group rules. – Key Pairs: View, create, edit, import, and delete key pairs.
– Floating IPs: Allocate an IP address to or release it from a project.
– API Access: View API endpoints.
• Shares: Use the following tabs to complete these tasks:
– Shares: View, create, manage, and delete shares.
– Snapshots: View, manage, and delete volume snapshots.
– Share Networks: View, manage, and delete share networks.
– Security Services: View, manage, and delete security services.
Network tab
• Network Topology: View the network topology.
• Networks: Create and manage public and private networks. • Routers: Create and manage routers.
• Load Balancers: Create and manage load balancers.
– Pools: Add and manage pools.
– Members: Add and manage members. – Monitors: Add and manage monitors.
• Firewalls: Create and manage firewalls.
– Firewalls: Create and manage firewalls.
– Firewall Policies: Add and manage firewall policies. – Firewall Rules: Add and manage firewall rules.
User Guide (Release Version: 15.0.0)
 OpenStack dashboard 5
 User Guide (Release Version: 15.0.0)
Orchestration tab
• Stacks: Use the REST API to orchestrate multiple composite cloud applications.
• Resource Types: Show a list of all the supported resource types for HOT templates.
Object Store tab
• Containers: Create and manage containers and objects. OpenStack dashboard — Admin tab
Administrative users can use the Admin tab to view usage and to manage instances, volumes, flavors, images, networks, and so on.
Fig. 2: Figure: Admin tab
From the Admin tab, you can access the following category to complete these tasks:
System tab
• Overview: View basic reports.
• Resource Usage: Use the following tabs to view the following usages:
 – Usage Report: View the usage report.
 6
OpenStack dashboard
 – Stats: View the statistics of all resources.
• Hypervisors: View the hypervisor summary.
• Host Aggregates: View, create, and edit host aggregates. View the list of availability zones.
• Instances: View, pause, resume, suspend, migrate, soft or hard reboot, and delete running instances that belong to users of some, but not all, projects. Also, view the log for an instance or access an instance through VNC.
• Volumes: Use the following tabs to complete these tasks:
– Volumes: View, create, manage, and delete volumes.
– Volume Types: View, create, manage, and delete volume types. – Volume Snapshots: View, manage, and delete volume snapshots.
• Flavors: View, create, edit, view extra specifications for, and delete flavors. A flavor is the size of an instance.
• Images: View, create, edit properties for, and delete custom images.
• Networks: View, create, edit properties for, and delete networks.
• Routers: View, create, edit properties for, and delete routers.
• Defaults: View default quota values. Quotas are hard-coded in OpenStack Compute and define the maximum allowable size and number of resources.
• Metadata Definitions: Import namespace and view the metadata information.
• System Information: Use the following tabs to view the service information:
– Services: View a list of the services.
– Compute Services: View a list of all Compute services.
– Block Storage Services: View a list of all Block Storage services. – Network Agents: View the network agents.
– Orchestration Services: View a list of all Orchestration services.
• Shares: Use the following tabs to complete these tasks:
– Shares: View, create, manage, and delete shares.
– Snapshots: View, manage, and delete volume snapshots.
– Share Networks: View, manage, and delete share networks.
– Security Services: View, manage, and delete security services. – Share Types: View, create, manage, and delete share types.
– Share Servers: View, manage, and delete share servers.
User Guide (Release Version: 15.0.0)
 OpenStack dashboard 7
 User Guide (Release Version: 15.0.0)
OpenStack dashboard — Identity tab
 Fig. 3: Figure:Identity tab
• Projects: View, create, assign users to, remove users from, and delete projects. • Users: View, create, enable, disable, and delete users.
 8
OpenStack dashboard
 OpenStack dashboard — Settings tab
Fig. 4: Figure:Settings tab
Click the Settings button from the user drop down menu at the top right of any page, you will see the Settings
tab.
• User Settings: View and manage dashboard settings. • Change Password: Change the password of the user.
Upload and manage images
A virtual machine image, referred to in this document simply as an image, is a single file that contains a virtual disk that has a bootable operating system installed on it. Images are used to create virtual machine instances within the cloud. For information about creating image files, see the OpenStack Virtual Machine Image Guide.
Depending on your role, you may have permission to upload and manage virtual machine images. Operators might restrict the upload and management of images to cloud administrators or operators only. If you have the appropriate privileges, you can use the dashboard to upload and manage images in the admin project.
Note: You can also use the openstack and glance command-line clients or the Image service to manage images. For more information see Manage images.
User Guide (Release Version: 15.0.0)
    OpenStack dashboard 9
 User Guide (Release Version: 15.0.0)
Upload an image
Follow this procedure to upload an image to a project:
1. 2. 3. 4.
Log in to the dashboard.
Select the appropriate project from the drop down menu at the top left. On the Project tab, open the Compute tab and click Images category. Click Create Image.
The Create An Image dialog box appears.
 Fig. 5: Dashboard — Create Image
 10
OpenStack dashboard
 5. Enter the following values:
User Guide (Release Version: 15.0.0)
   Image Name
 Enter a name for the image.
  Image Description
 Enter a brief description of the image.
  Image Source
 Choose the image source from the dropdown list. Your choices are Image Location and Image File.
  Image File or Image Location
 Based on your selection for Image Source, you either enter the location URL of the image in the Image Location field, or browse for the image file on your file system and add it.
  Format
 Select the image format (for example, QCOW2) for the image.
  Architecture
 Specify the architecture. For example, i386 for a 32-bit architecture or x86_64 for a 64-bit architecture.
  Minimum Disk (GB)
 Leave this field empty.
  Minimum RAM (MB)
 Leave this field empty.
  Copy Data
 Specify this option to copy image data to the Image service.
  Visibility
 The access permission for the image. Public or Private.
  Protected
 Select this check box to ensure that only users with permissions can delete the image. Yes or No.
  Image Metadata
 Specify this option to add resource metadata. The glance Metadata Catalog provides a list of metadata image definitions. (Note: Not all cloud providers enable this feature.)
            6. Click Create Image.
The image is queued to be uploaded. It might take some time before the status changes from Queued to
Active.
Update an image
Follow this procedure to update an existing image.
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. Select the image that you want to edit.
4. In the Actions column, click the menu button and then select Edit Image from the list. 5. In the Edit Image dialog box, you can perform various actions. For example:
• Change the name of the image.
• Select the Public check box to make the image public. • Clear the Public check box to make the image private.
6. Click Edit Image. Delete an image
Deletion of images is permanent and cannot be reversed. Only users with the appropriate permissions can delete images.
 OpenStack dashboard 11
 User Guide (Release Version: 15.0.0)
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Images category.
4. Select the images that you want to delete.
5. Click Delete Images.
6. In the Confirm Delete Images dialog box, click Delete Images to confirm the deletion.
Configure access and security for instances
Before you launch an instance, you should add security group rules to enable users to ping and use SSH to connect to the instance. Security groups are sets of IP filter rules that define networking access and are applied to all instances within a project. To do so, you either add rules to the default security group Add a rule to the default security group or add a new security group with rules.
Key pairs are SSH credentials that are injected into an instance when it is launched. To use key pair injection, the image that the instance is based on must contain the cloud-init package. Each project should have at least one key pair. For more information, see the section Add a key pair.
If you have generated a key pair with an external tool, you can import it into OpenStack. The key pair can be used for multiple instances that belong to a project. For more information, see the section Import a key pair.
Note: A key pair belongs to an individual user, not to a project. To share a key pair across multiple users, each user needs to import that key pair.
When an instance is created in OpenStack, it is automatically assigned a fixed IP address in the network to which the instance is assigned. This IP address is permanently associated with the instance until the instance is terminated. However, in addition to the fixed IP address, a floating IP address can also be attached to an instance. Unlike fixed IP addresses, floating IP addresses are able to have their associations modified at any time, regardless of the state of the instances involved.
Add a rule to the default security group
This procedure enables SSH and ICMP (ping) access to instances. The rules apply to all instances within a given project, and should be set for every project unless there is a reason to prohibit SSH or ICMP access to the instances.
This procedure can be adjusted as necessary to add additional security group rules to a project, if your cloud requires them.
   Note:
1. 2. 3.
When adding a rule, you must specify the protocol used with the destination port or source port.
Log in to the dashboard.
Select the appropriate project from the drop down menu at the top left.
On the Project tab, open the Compute tab and click Access & Security category. The Security Groups tab shows the security groups that are available for this project.
  12
OpenStack dashboard
 4. Select the default security group and click Manage Rules.
5. To allow SSH access, click Add Rule.
6. In the Add Rule dialog box, enter the following values:
• Rule:SSH
• Remote:CIDR
• CIDR:0.0.0.0/0
Note: To accept requests from a particular range of IP addresses, specify the IP address block in the CIDR box.
7. Click Add.
Instances will now have SSH port 22 open for requests from any IP address.
8. To add an ICMP rule, click Add Rule.
9. In the Add Rule dialog box, enter the following values:
• Rule:AllICMP
• Direction:Ingress • Remote:CIDR
• CIDR:0.0.0.0/0
10. Click Add.
Instances will now accept all incoming ICMP packets.
Add a key pair
Create at least one key pair for each project.
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Access & Security category.
4. Click the Key Pairs tab, which shows the key pairs that are available for this project.
5. Click Create Key Pair.
6. In the Create Key Pair dialog box, enter a name for your key pair, and click Create Key Pair. 7. Respond to the prompt to download the key pair.
Import a key pair
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Access & Security category.
User Guide (Release Version: 15.0.0)
   OpenStack dashboard 13
 User Guide (Release Version: 15.0.0)
4. Click the Key Pairs tab, which shows the key pairs that are available for this project.
5. Click Import Key Pair.
6. In the Import Key Pair dialog box, enter the name of your key pair, copy the public key into the Public Key box, and then click Import Key Pair.
7. Savethe*.pemfilelocally.
8. To change its permissions so that only you can read and write to the file, run the following command:
$ chmod 0600 yourPrivateKey.pem
Note: If you are using the Dashboard from a Windows computer, use PuTTYgen to load the *.pem file and convert and save it as *.ppk. For more information see the WinSCP web page for PuTTYgen.
9. TomakethekeypairknowntoSSH,runthessh-addcommand. $ ssh-add yourPrivateKey.pem
The Compute database registers the public key of the key pair. The Dashboard lists the key pair on the Access & Security tab.
Allocate a floating IP address to an instance
When an instance is created in OpenStack, it is automatically assigned a fixed IP address in the network to which the instance is assigned. This IP address is permanently associated with the instance until the instance is terminated.
However, in addition to the fixed IP address, a floating IP address can also be attached to an instance. Unlike fixed IP addresses, floating IP addresses can have their associations modified at any time, regardless of the state of the instances involved. This procedure details the reservation of a floating IP address from an existing pool of addresses and the association of that address with a specific instance.
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Access & Security category.
4. Click the Floating IPs tab, which shows the floating IP addresses allocated to instances. 5. Click Allocate IP To Project.
6. Choose the pool from which to pick the IP address.
7. Click Allocate IP.
8. In the Floating IPs list, click Associate.
9. In the Manage Floating IP Associations dialog box, choose the following options:
• The IP Address field is filled automatically, but you can add a new IP address by clicking the + button.
• In the Port to be associated field, select a port from the list. The list shows all the instances with their fixed IP addresses.
             14
OpenStack dashboard
 10. Click Associate.
Note: To disassociate an IP address from an instance, click the Disassociate button.
To release the floating IP address back into the floating IP pool, click the Release Floating IP option in the Actions column.
Launch and manage instances
Instances are virtual machines that run inside the cloud. You can launch an instance from the following sources:
• Images uploaded to the Image service.
• Image that you have copied to a persistent volume. The instance launches from the volume, which is provided by the cinder-volume API through iSCSI.
• Instance snapshot that you took.
Launch an instance
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Instances category.
The dashboard shows the instances with its name, its private and floating IP addresses, size, status, task, power state, and so on.
4. Click Launch Instance.
5. In the Launch Instance dialog box, specify the following values:
Details tab
Instance Name Assign a name to the virtual machine.
Availability Zone By default, this value is set to the availability zone given by the cloud provider (for example, us-west or apac-south). For some cases, it could be nova.
Note: The name you assign here becomes the initial host name of the server. If the name is longer than 63 characters, the Compute service truncates it automatically to ensure dnsmasq works correctly.
After the server is built, if you change the server name in the API or change the host name directly, the names are not updated in the dashboard.
Server names are not guaranteed to be unique when created so you could have two instances with the same host name.
Count To launch multiple instances, enter a value greater than 1. The default is 1. Source tab
Instance Boot Source Your options are:
User Guide (Release Version: 15.0.0)
     OpenStack dashboard 15
 User Guide (Release Version: 15.0.0)
Boot from image If you choose this option, a new field for Image Name displays. You can select the image from the list.
Boot from snapshot If you choose this option, a new field for Instance Snapshot displays. You can select the snapshot from the list.
Boot from volume If you choose this option, a new field for Volume displays. You can select the volume from the list.
Boot from image (creates a new volume) With this option, you can boot from an image and cre- ate a volume by entering the Device Size and Device Name for your volume. Click the Delete Volume on Instance Delete option to delete the volume on deleting the instance.
Bootfromvolumesnapshot(createsanewvolume) Usingthisoption,youcanbootfromavol- ume snapshot and create a new volume by choosing Volume Snapshot from a list and adding a Device Name for your volume. Click the Delete Volume on Instance Delete option to delete the volume on deleting the instance.
Image Name This field changes based on your previous selection. If you have chosen to launch an instance using an image, the Image Name field displays. Select the image name from the dropdown list.
Instance Snapshot This field changes based on your previous selection. If you have chosen to launch an instance using a snapshot, the Instance Snapshot field displays. Select the snapshot name from the dropdown list.
Volume This field changes based on your previous selection. If you have chosen to launch an instance using a volume, the Volume field displays. Select the volume name from the dropdown list. If you want to delete the volume on instance delete, check the Delete Volume on Instance Delete option.
Flavor tab
Flavor Specify the size of the instance to launch.
Note: The flavor is selected based on the size of the image selected for launching an instance. For example, while creating an image, if you have entered the value in the Minimum RAM (MB) field as 2048, then on selecting the image, the default flavor is m1.small.
Networks tab
Selected Networks To add a network to the instance, click the + in the Available field. Network Ports tab
Ports Activate the ports that you want to assign to the instance.
Security Groups tab
Security Groups Activate the security groups that you want to assign to the instance.
Security groups are a kind of cloud firewall that define which incoming network traffic is forwarded to instances.
If you have not created any security groups, you can assign only the default security group to the instance.
Key Pair tab
   16
OpenStack dashboard
 Key Pair Specify a key pair.
If the image uses a static root password or a static key set (neither is recommended), you do not
need to provide a key pair to launch the instance. Configuration tab
Customization Script Source Specify a customization script that runs after your instance launches. Metadata tab
Available Metadata Add Metadata items to your instance.
6. Click Launch Instance.
The instance starts on a compute node in the cloud.
Note: Ifyoudidnotprovideakeypair,securitygroups,orrules,userscanaccesstheinstanceonlyfrominside the cloud through VNC. Even pinging the instance is not possible without an ICMP rule configured.
You can also launch an instance from the Images or Volumes category when you launch an instance from an image or a volume respectively.
When you launch an instance from an image, OpenStack creates a local copy of the image on the compute node where the instance starts.
For details on creating images, see Creating images manually in the OpenStack Virtual Machine Image Guide. When you launch an instance from a volume, note the following steps:
• To select the volume from which to launch, launch an instance from an arbitrary image on the volume. The arbitrary image that you select does not boot. Instead, it is replaced by the image on the volume that you choose in the next steps.
To boot a Xen image from a volume, the image you launch in must be the same type, fully virtualized or paravirtualized, as the one on the volume.
• Select the volume or volume snapshot from which to boot. Enter a device name. Enter vda for KVM images or xvda for Xen images.
Connect to your instance by using SSH
To use SSH to connect to your instance, use the downloaded keypair file.
1. Copy the IP address for your instance.
2. Use the ssh command to make a secure connection to the instance. For example:
$ ssh -i MyKey.pem ubuntu@10.0.0.2
3. Attheprompt,typeyes.
It is also possible to SSH into an instance without an SSH keypair, if the administrator has enabled root password injection. For more information about root password injection, see Injecting the administrator password in the OpenStack Administrator Guide.
User Guide (Release Version: 15.0.0)
   Note:
The user name is ubuntu for the Ubuntu cloud images on TryStack.
       OpenStack dashboard 17
 User Guide (Release Version: 15.0.0)
Track usage for instances
You can track usage for instances for each project. You can track costs per month by showing meters like number of vCPUs, disks, RAM, and uptime for all your instances.
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Overview category.
4. To query the instance usage for a month, select a month and click Submit. 5. To download a summary, click Download CSV Summary.
Create an instance snapshot
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click the Instances category.
4. Select the instance from which to create a snapshot.
5. In the actions column, click Create Snapshot.
6. In the Create Snapshot dialog box, enter a name for the snapshot, and click Create Snapshot.
The Images category shows the instance snapshot.
To launch an instance from the snapshot, select the snapshot and click Launch. Proceed with launching an
instance.
Manage an instance
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Instances category.
4. Select an instance.
5. In the menu list in the actions column, select the state.
You can resize or rebuild an instance. You can also choose to view the instance console log, edit instance or the security groups. Depending on the current state of the instance, you can pause, resume, suspend, soft or hard reboot, or terminate it.
Create and manage networks
The OpenStack Networking service provides a scalable system for managing the network connectivity within an OpenStack cloud deployment. It can easily and quickly react to changing network needs (for example, creating and assigning new IP addresses).
Networking in OpenStack is complex. This section provides the basic instructions for creating a network and a router. For detailed information about managing networks, refer to the OpenStack Administrator Guide.
 18 OpenStack dashboard
 Create a network
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Network tab and click Networks category.
4. Click Create Network.
5. In the Create Network dialog box, specify the following values.
Network tab
Network Name: Specify a name to identify the network.
Shared: Share the network with other projects. Non admin users are not allowed to set shared option.
Admin State: The state to start the network in.
Create Subnet: Select this check box to create a subnet
You do not have to specify a subnet when you create a network, but if you do not specify a subnet, the network can not be attached to an instance.
Subnet tab
Subnet Name: Specify a name for the subnet.
Network Address: Specify the IP address for the subnet.
IP Version: Select IPv4 or IPv6.
Gateway IP: Specify an IP address for a specific gateway. This parameter is optional. Disable Gateway: Select this check box to disable a gateway IP address.
Subnet Details tab
Enable DHCP: Select this check box to enable DHCP.
Allocation Pools: Specify IP address pools.
DNS Name Servers: Specify a name for the DNS server.
Host Routes: Specify the IP address of host routes.
6. Click Create.
The dashboard shows the network on the Networks tab.
Create a router
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Network tab and click Routers category.
4. Click Create Router.
5. In the Create Router dialog box, specify a name for the router and External Network, and click Create Router.
The new router is now displayed in the Routers tab.
OpenStack dashboard 19
User Guide (Release Version: 15.0.0)

 User Guide (Release Version: 15.0.0)
6. To connect a private network to the newly created router, perform the following steps:
(a) On the Routers tab, click the name of the router.
(b) On the Router Details page, click the Interfaces tab, then click Add Interface.
(c) In the Add Interface dialog box, select a Subnet.
Optionally, in the Add Interface dialog box, set an IP Address for the router interface for the selected subnet.
If you choose not to set the IP Address value, then by default OpenStack Networking uses the first host IP address in the subnet.
The Router Name and Router ID fields are automatically updated.
7. Click Add Interface.
You have successfully created the router. You can view the new topology from the Network Topology tab. Create a port
1. Log in to the dashboard.
2. Select the appropriate project from the drop-down menu at the top left.
3. On the Admin tab, click Networks category.
4. Click on the Network Name of the network in which the port has to be created.
5. In the Create Port dialog box, specify the following values.
Name: Specify name to identify the port.
Device ID: Device ID attached to the port.
Device Owner: Device owner attached to the port.
Binding Host: The ID of the host where the port is allocated.
Binding VNIC Type: Select the VNIC type that is bound to the neutron port.
6. Click Create Port.
The new port is now displayed in the Ports list.
Create and manage object containers
OpenStack Object Storage (swift) is used for redundant, scalable data storage using clusters of standardized servers to store petabytes of accessible data. It is a long-term storage system for large amounts of static data which can be retrieved and updated.
OpenStack Object Storage provides a distributed, API-accessible storage platform that can be integrated directly into an application or used to store any type of file, including VM images, backups, archives, or media files. In the OpenStack dashboard, you can only manage containers and objects.
    Warning: Creating and managing ports requires administrator privileges. Contact an administrator before adding or changing ports.
  20 OpenStack dashboard
 In OpenStack Object Storage, containers provide storage for objects in a manner similar to a Windows folder or Linux file directory, though they cannot be nested. An object in OpenStack consists of the file to be stored in the container and any accompanying metadata.
Create a container
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Object Store tab and click Containers category.
4. Click Container.
5. In the Create Container dialog box, enter a name for the container, and then click Create.
You have successfully created a container.
Note: To delete a container, click the More button and select Delete Container.
Upload an object
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Object Store tab and click Containers category.
4. Select the container in which you want to store your object.
5. Click the Upload File icon.
The Upload File To Container: <name> dialog box appears. <name> is the name of the container to which you are uploading the object.
6. Enter a name for the object.
7. Browse to and select the file that you want to upload.
8. Click Upload File.
You have successfully uploaded an object to the container.
Note: To delete an object, click the More button and select Delete Object.
Manage an object
To edit an object
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Object Store tab and click Containers category.
User Guide (Release Version: 15.0.0)
     OpenStack dashboard 21
 User Guide (Release Version: 15.0.0)
4. Select the container in which you want to store your object.
5. Click the menu button and choose Edit from the dropdown list.
The Edit Object dialog box is displayed.
6. Browse to and select the file that you want to upload.
7. Click Update Object.
 Note:
To delete an object, click the menu button and select Delete Object.
 To copy an object from one container to another
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Object Store tab and click Containers category. 4. Select the container in which you want to store your object.
5. Click the menu button and choose Copy from the dropdown list.
6. In the Copy Object launch dialog box, enter the following values:
• Destination Container: Choose the destination container from the list.
• Path: Specify a path in which the new copy should be stored inside of the selected container. • Destination object name: Enter a name for the object in the new container.
7. Click Copy Object.
To create a metadata-only object without a file
You can create a new object in container without a file available and can upload the file later when it is ready. This temporary object acts a place-holder for a new object, and enables the user to share object metadata and URL info in advance.
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Object Store tab and click Containers category.
4. Select the container in which you want to store your object.
5. Click Upload Object.
The Upload Object To Container: <name> dialog box is displayed.
<name> is the name of the container to which you are uploading the object.
6. Enter a name for the object.
7. Click Update Object.
To create a pseudo-folder
Pseudo-folders are similar to folders in your desktop operating system. They are virtual collections defined by a common prefix on the object’s name.
1. Log in to the dashboard.
 22
OpenStack dashboard
 2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Object Store tab and click Containers category.
4. Select the container in which you want to store your object.
5. Click Create Pseudo-folder.
The Create Pseudo-Folder in Container <name> dialog box is displayed. <name> is the name of the container to which you are uploading the object.
6. Enter a name for the pseudo-folder.
A slash (/) character is used as the delimiter for pseudo-folders in Object Storage.
7. Click Create.
Create and manage volumes
Volumes are block storage devices that you attach to instances to enable persistent storage. You can attach a volume to a running instance or detach a volume and attach it to another instance at any time. You can also create a snapshot from or delete a volume. Only administrative users can create volume types.
Create a volume
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Volumes category.
4. Click Create Volume.
In the dialog box that opens, enter or select the following values. Volume Name: Specify a name for the volume.
Description: Optionally, provide a brief description for the volume. Volume Source: Select one of the following options:
• No source, empty volume: Creates an empty volume. An empty volume does not contain a file system or a partition table.
• Snapshot: If you choose this option, a new field for Use snapshot as a source displays. You can select the snapshot from the list.
• Image: If you choose this option, a new field for Use image as a source displays. You can select the image from the list.
• Volume: If you choose this option, a new field for Use volume as a source displays. You can select the volume from the list. Options to use a snapshot or a volume as the source for a volume are displayed only if there are existing snapshots or volumes.
Type: Leave this field blank.
Size (GB): The size of the volume in gibibytes (GiB).
Availability Zone: Select the Availability Zone from the list. By default, this value is set to the availability zone given by the cloud provider (for example, us-west or apac-south). For some cases, it could be nova.
User Guide (Release Version: 15.0.0)
 OpenStack dashboard 23
 User Guide (Release Version: 15.0.0)
5. Click Create Volume.
The dashboard shows the volume on the Volumes tab.
Attach a volume to an instance
After you create one or more volumes, you can attach them to instances. You can attach a volume to one instance at a time.
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Volumes category.
4. Select the volume to add to an instance and click Manage Attachments.
5. In the Manage Volume Attachments dialog box, select an instance.
6. Enter the name of the device from which the volume is accessible by the instance.
Note: The actual device name might differ from the volume name because of hypervisor settings.
7. Click Attach Volume.
The dashboard shows the instance to which the volume is now attached and the device name.
You can view the status of a volume in the Volumes tab of the dashboard. The volume is either Available or In-Use.
Now you can log in to the instance and mount, format, and use the disk.
Detach a volume from an instance
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click the Volumes category. 4. Select the volume and click Manage Attachments.
5. Click Detach Volume and confirm your changes.
A message indicates whether the action was successful.
Create a snapshot from a volume
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left. 3. On the Project tab, open the Compute tab and click Volumes category. 4. Select a volume from which to create a snapshot.
5. In the Actions column, click Create Snapshot.
   24
OpenStack dashboard
 6. In the dialog box that opens, enter a snapshot name and a brief description.
7. Confirm your changes.
The dashboard shows the new volume snapshot in Volume Snapshots tab.
Edit a volume
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Volumes category.
4. Select the volume that you want to edit.
5. In the Actions column, click Edit Volume.
6. In the Edit Volume dialog box, update the name and description of the volume.
7. Click Edit Volume.
Note: You can extend a volume by using the Extend Volume option available in the More dropdown list and entering the new value for volume size.
Delete a volume
When you delete an instance, the data in its attached volumes is not deleted.
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Compute tab and click Volumes category.
4. Select the check boxes for the volumes that you want to delete.
5. Click Delete Volumes and confirm your choice.
A message indicates whether the action was successful.
Create and manage shares
Shares are file storage that you provide access to instances. You can allow access to a share to a running instance or deny access to a share and allow access to it to another instance at any time. You can also delete a share. You can create snapshot from a share if the driver supports it. Only administrative users can create share types.
Create a share
1. Log in to the dashboard, choose a project, and click Shares.
2. Click Create Share.
In the dialog box that opens, enter or select the following values. Share Name: Specify a name for the share.
User Guide (Release Version: 15.0.0)
   OpenStack dashboard 25
 User Guide (Release Version: 15.0.0)
Description: Optionally, provide a brief description for the share. Share Type: Choose a share type.
Size (GB): The size of the share in gibibytes (GiB).
Share Protocol: Select NFS, CIFS, GlusterFS, or HDFS.
Share Network: Choose a share network.
Metadata: Enter metadata for the share creation if needed. 3. Click Create Share.
The dashboard shows the share on the Shares tab. Delete a share
1. Log in to the dashboard, choose a project, and click Shares.
2. Select the check boxes for the shares that you want to delete.
3. Click Delete Shares and confirm your choice.
A message indicates whether the action was successful.
Allow access
1. Log in to the dashboard, choose a project, and click Shares.
2. Go to the share that you want to allow access and choose Manage Rules from Actions.
3. Click Add rule.
Access Type: Choose ip, user, or cert.
Access Level: Choose read-write or read-only. Access To: Fill in Access To field.
4. Click Add Rule.
A message indicates whether the action was successful.
Deny access
1. 2. 3. 4.
Log in to the dashboard, choose a project, and click Shares.
Go to the share that you want to deny access and choose Manage Rules from Actions. Choose the rule you want to delete.
Click Delete rule and confirm your choice.
A message indicates whether the action was successful.
 26
OpenStack dashboard
 Edit share metadata
1. Log in to the dashboard, choose a project, and click Shares.
2. Go to the share that you want to edit and choose Edit Share Metadata from Actions.
3. Metadata: To add share metadata, use key=value. To unset metadata, use key.
4. Click Edit Share Metadata.
A message indicates whether the action was successful.
Edit share
1. Log in to the dashboard, choose a project, and click Shares.
2. Go to the share that you want to edit and choose Edit Share from Actions.
3. Share Name: Enter a new share name.
4. Description: Enter a new description.
5. Click Edit Share.
A message indicates whether the action was successful.
Extend share
1. Log in to the dashboard, choose a project, and click Shares.
2. Go to the share that you want to edit and choose Extend Share from Actions.
3. New Size (GB): Enter new size.
4. Click Extend Share.
A message indicates whether the action was successful.
Create share network
1. Log in to the dashboard, choose a project, click Shares, and click Share Networks.
2. Click Create Share Network.
In the dialog box that opens, enter or select the following values.
Name: Specify a name for the share network.
Description: Optionally, provide a brief description for the share network. Neutron Net: Choose a neutron network.
Neutron Subnet: Choose a neutron subnet.
3. Click Create Share Network.
The dashboard shows the share network on the Share Networks tab.
User Guide (Release Version: 15.0.0)
 OpenStack dashboard 27
 User Guide (Release Version: 15.0.0)
Delete a share network
1. Log in to the dashboard, choose a project, click Shares, and click Share Networks.
2. Select the check boxes for the share networks that you want to delete.
3. Click Delete Share Networks and confirm your choice.
A message indicates whether the action was successful.
Edit share network
1. Log in to the dashboard, choose a project, click Shares, and click Share Networks.
2. Go to the share network that you want to edit and choose Edit Share Network from Actions.
3. Name: Enter a new share network name.
4. Description: Enter a new description.
5. Click Edit Share Network.
A message indicates whether the action was successful.
Create security service
1. Log in to the dashboard, choose a project, click Shares, and click Security Services.
2. Click Create Security Service.
In the dialog box that opens, enter or select the following values. Name: Specify a name for the security service.
DNS IP: Enter the DNS IP address.
Server: Enter the server name.
Domain: Enter the domain name.
User: Enter the user name.
Password: Enter the password.
Confirm Password: Enter the password again to confirm.
Type: Choose the type from Active Directory, LDAP, or Kerberos. Description: Optionally, provide a brief description for the security service.
3. Click Create Security Service.
The dashboard shows the security service on the Security Services tab.
Delete a security service
1. Log in to the dashboard, choose a project, click Shares, and click Security Services. 2. Select the check boxes for the security services that you want to delete.
 28
OpenStack dashboard
 3. Click Delete Security Services and confirm your choice. A message indicates whether the action was successful.
Edit security service
1. Log in to the dashboard, choose a project, click Shares, and click Security Services.
2. Go to the security service that you want to edit and choose Edit Security Service from Actions.
3. Name: Enter a new security service name.
4. Description: Enter a new description.
5. Click Edit Security Service.
A message indicates whether the action was successful.
Launch and manage stacks
OpenStack Orchestration is a service that you can use to orchestrate multiple composite cloud applications. This service supports the use of both the Amazon Web Services (AWS) CloudFormation template format through both a Query API that is compatible with CloudFormation and the native OpenStack Heat Orchestration Tem- plate (HOT) format through a REST API.
These flexible template languages enable application developers to describe and automate the deployment of infrastructure, services, and applications. The templates enable creation of most OpenStack resource types, such as instances, floating IP addresses, volumes, security groups, and users. Once created, the resources are referred to as stacks.
The template languages are described in the Template Guide in the Heat developer documentation. Launch a stack
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left. 3. On the Project tab, open the Orchestration tab and click Stacks category. 4. Click Launch Stack.
5. In the Select Template dialog box, specify the following values:
6. Click Next.
7. In the Launch Stack dialog box, specify the following values:
User Guide (Release Version: 15.0.0)
   Template Source
 Choose the source of the template from the list.
  Template URL/File/Data
 Depending on the source that you select, enter the URL, browse to the file location, or directly include the template.
  Environment Source
 Choose the source of the environment from the list. The environment files contain additional settings for the stack.
  Environment File/Data
 Depending on the source that you select, browse to the file location, directly include the environment
     OpenStack dashboard 29
 User Guide (Release Version: 15.0.0)
   Stack Name
 Enter a name to identify the stack.
  Creation Timeout (minutes)
 Specify the number of minutes that can elapse before the launch of the stack times out.
  Rollback On Failure
 Select this check box if you want the service to roll back changes if the stack fails to launch.
  Password for user “demo”
 Specify the password that the default user uses when the stack is created.
  DBUsername
 Specify the name of the database user.
  LinuxDistribution
 Specify the Linux distribution that is used in the stack.
  DBRootPassword
 Specify the root password for the database.
  KeyName
 Specify the name of the key pair to use to log in to the stack.
  DBName
 Specify the name of the database.
  DBPassword
 Specify the password of the database.
  InstanceType
 Specify the flavor for the instance.
           8. Click Launch to create a stack. The Stacks tab shows the stack.
After the stack is created, click on the stack name to see the following details: Topology The topology of the stack.
Overview The parameters and details of the stack.
Resources The resources used by the stack.
Events The events related to the stack.
Template The template for the stack.
Manage a stack
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
3. On the Project tab, open the Orchestration tab and click Stacks category.
4. Select the stack that you want to update.
5. Click Change Stack Template.
6. In the Select Template dialog box, select the new template source or environment source.
7. Click Next.
The Update Stack Parameters window appears.
8. Enter new values for any parameters that you want to update.
9. Click Update.
Delete a stack
When you delete a stack, you cannot undo this action.
1. Log in to the dashboard.
2. Select the appropriate project from the drop down menu at the top left.
 30
OpenStack dashboard
 3. On the Project tab, open the Orchestration tab and click Stacks category. 4. Select the stack that you want to delete.
5. Click Delete Stack.
6. In the confirmation dialog box, click Delete Stack to confirm the deletion.
Create and manage databases
The Database service provides scalable and reliable cloud provisioning functionality for both relational and non- relational database engines. Users can quickly and easily use database features without the burden of handling complex administrative tasks.
Create a database instance
Prerequisites. Before you create a database instance, you need to configure a default datastore and make sure you have an appropriate flavor for the type of database instance you want.
1. Configure a default datastore.
Because the dashboard does not let you choose a specific datastore to use with an instance, you need to
configure a default datastore. The dashboard then uses the default datastore to create the instance. (a) Addthefollowinglineto/etc/trove/trove.conf:
           default_datastore = DATASTORE_NAME
Replace DATASTORE_NAME with the name that the administrative user set when issuing the trove- manage command to create the datastore. You can use the trove datastore-list command to display the datastores that are available in your environment.
For example, if your MySQL data store name is set to mysql, your entry would look like this: default_datastore = mysql
(b) Restart Database services on the controller node:
2. Verify flavor.
Make sure an appropriate flavor exists for the type of database instance you want.
Create database instance. Once you have configured a default datastore and verified that you have an appro- priate flavor, you can create a database instance.
1. Log in to the dashboard.
2. From the CURRENT PROJECT on the Project tab, select the appropriate project.
3. On the Project tab, open the Database tab and click Instances category. This lists the instances that already exist in your environment.
4. Click Launch Instance.
User Guide (Release Version: 15.0.0)
              # service trove-api restart
# service trove-taskmanager restart # service trove-conductor restart
  OpenStack dashboard 31
 User Guide (Release Version: 15.0.0)
5. In the Launch Database dialog box, specify the following values. Details
Database Name: Specify a name for the database instance.
Flavor: Select an appropriate flavor for the instance.
Volume Size: Select a volume size. Volume size is expressed in GB.
Initialize Databases: Initial Database
Optionally provide a comma separated list of databases to create, for example:
database1, database2, database3
Initial Admin User: Create an initial admin user. This user will have access to all the databases you create.
Password: Specify a password associated with the initial admin user you just named.
Host: Optionally, allow the user to connect only from this host. If you do not specify a host, this user
will be allowed to connect from anywhere.
6. Click the Launch button. The new database instance appears in the databases list.
Backup and restore a database
You can use Database services to backup a database and store the backup artifact in the Object Storage service. Later on, if the original database is damaged, you can use the backup artifact to restore the database. The restore process creates a database instance.
This example shows you how to back up and restore a MySQL database.
To backup the database instance
1. 2. 3.
4. 5.
6.
Log in to the dashboard.
From the CURRENT PROJECT on the Project tab, select the appropriate project.
On the Project tab, open the Database tab and click Instances category. This displays the existing in- stances in your system.
Click Create Backup.
In the Backup Database dialog box, specify the following values: Name
Specify a name for the backup.
Database Instance
Select the instance you want to back up.
Click Backup. The new backup appears in the backup list.
 32
OpenStack dashboard
 To restore a database instance
Now assume that your original database instance is damaged and you need to restore it. You do the restore by using your backup to create a new database instance.
1. Log in to the dashboard.
2. From the CURRENT PROJECT on the Project tab, select the appropriate project.
3. On the Project tab, open the Database tab and click Backups category. This lists the available backups.
4. Check the backup you want to use and click Restore Backup.
5. In the Launch Database dialog box, specify the values you want for the new database instance.
6. ClicktheRestoreFromDatabasetabandmakesurethatthisnewinstanceisbasedonthecorrectbackup.
7. Click Launch.
The new instance appears in the database instances list.
Update a database instance
You can change various characteristics of a database instance, such as its volume size and flavor.
To change the volume size of an instance
1. Log in to the dashboard.
2. From the CURRENT PROJECT on the Project tab, select the appropriate project.
3. On the Project tab, open the Database tab and click Instances category. This displays the existing in- stances in your system.
4. Check the instance you want to work with. In the Actions column, expand the drop down menu and select Resize Volume.
5. In the Resize Database Volume dialog box, fill in the New Size field with an integer indicating the new size you want for the instance. Express the size in GB, and note that the new size must be larger than the current size.
6. Click Resize Database Volume.
To change the flavor of an instance
1. Log in to the dashboard.
2. From the CURRENT PROJECT on the Project tab, select the appropriate project.
3. On the Project tab, open the Database tab and click Instances category. This displays the existing in- stances in your system.
4. Check the instance you want to work with. In the Actions column, expand the drop down menu and select Resize Instance.
5. In the Resize Database Instance dialog box, expand the drop down menu in the New Flavor field. Select the new flavor you want for the instance.
User Guide (Release Version: 15.0.0)
 OpenStack dashboard 33
 User Guide (Release Version: 15.0.0)
6. Click Resize Database Instance. View and manage load balancers v2
Load-Balancer-as-a-Service (LBaaS) enables networking to distribute incoming requests evenly among desig- nated instances. This distribution ensures that the workload is shared predictably among instances and enables more effective use of system resources. Use one of these load-balancing methods to distribute incoming re- quests:
• Round robin: Rotates requests evenly between multiple instances.
• Source IP: Requests from a unique source IP address are consistently directed to the same instance. • Least connections: Allocates requests to the instance with the least number of active connections.
As an end user, you can create and manage load balancers and related objects for users in various projects. You can also delete load balancers and related objects.
LBaaS v2 has several new concepts to understand:
Load balancer The load balancer occupies a neutron network port and has an IP address assigned from a subnet.
Listener Each port that listens for traffic on a particular load balancer is configured separately and tied to the load balancer. Multiple listeners can be associated with the same load balancer.
Pool A pool is a group of hosts that sits behind the load balancer and serves traffic through the load balancer. Member Members are the actual IP addresses that receive traffic from the load balancer. Members are asso-
ciated with pools.
Health monitor Members may go offline from time to time and health monitors diverts traffic away from members that are not responding properly. Health monitors are associated with pools.
View existing load balancers
1. Log in to the OpenStack dashboard.
2. On the Project tab, open the Network tab, and click the Load Balancers category.
This view shows the list of existing load balancers. To view details of any of the load balancers, click on the specific load balancer.
Create a load balancer
1. 2. 3.
Log in to the OpenStack dashboard.
On the Project tab, open the Network tab, and click the Load Balancers category.
Click the Create Load Balancer button.
Use the concepts described in the overview section to fill in the necessary information about the load balancer you want to create.
Keep in mind, the health checks routinely run against each instance within a target load balancer and the result of the health check is used to determine if the instance receives new connections.
 34
OpenStack dashboard
 Note: A message indicates whether the action succeeded.
Delete a load balancer
1. Select the load balancer you want to delete and click the Delete Load Balancer button.
To be deleted successfully, a load balancer must not have any listeners or pools associated with it. The
delete action is also available in the Actions column for the individual load balancers.
OpenStack command-line clients Command-line client overview
OpenStackClient project provides a unified command-line client, which enables you to access the project API through easy-to-use commands. Also, most OpenStack project provides a command-line client for each service. For example, the Compute service provides a nova command-line client.
You can run the commands from the command line, or include the commands within scripts to automate tasks. If you provide OpenStack credentials, such as your user name and password, you can run these commands on any computer.
Internally, each command uses cURL command-line tools, which embed API requests. OpenStack APIs are RESTful APIs, and use the HTTP protocol. They include methods, URIs, media types, and response codes.
OpenStack APIs are open-source Python clients, and can run on Linux or Mac OS X systems. On some client commands, you can specify a debug parameter to show the underlying API request for the command. This is a good way to become familiar with the OpenStack API calls.
As a cloud end user, you can use the OpenStack Dashboard to provision your own resources within the limits set by administrators. You can modify the examples provided in this section to create other types and sizes of server instances.
Unified command-line client
You can use the unified openstack command (python-openstackclient) for the most of OpenStack services. For more information, see OpenStackClient document.
Individual command-line clients
Unless the unified OpenStack Client (python-openstackclient) is used, the following table lists the command- line client for each OpenStack service with its package name and description.
User Guide (Release Version: 15.0.0)
   OpenStack command-line clients 35
 User Guide (Release Version: 15.0.0)
Table 1: OpenStack services and clients
  Service
  Client
 Package
 Description
 Application Catalog service
  murano
 python-muranoclient
 Creates and manages applications.
 Bare Metal service
  ironic
 python-ironicclient
 manages and provisions physical ma- chines.
 Block Storage service
  cinder
 python-cinderclient
 Creates and manages volumes.
 Clustering service
  senlin
 python-senlinclient
 Creates and manages clustering ser- vices.
 Compute service
  nova
 python-novaclient
 Creates and manages images, in- stances, and flavors.
 Container Infrastruc- ture Management ser- vice
  magnum
 python-magnumclient
 Creates and manages containers.
 Database service
  trove
 python-troveclient
 Creates and manages databases.
 Deployment service
  fuel
 python-fuelclient
 Plans deployments.
 DNS service
  designate
 python-designateclient
 Creates and manages self service au- thoritative DNS.
 Image service
  glance
 python-glanceclient
 Creates and manages images.
 Key Manager service
  barbican
 python-barbicanclient
 Creates and manages keys.
 Monitoring
  monasca
 python-monascaclient
 Monitoring solution.
 Networking service
  neutron
 python-neutronclient
 Configures networks for guest servers.
 Object Storage service
  swift
 python-swiftclient
 Gathers statistics, lists items, updates metadata, and uploads, downloads, and deletes files stored by the Object Stor- age service. Gains access to an Object Storage installation for ad hoc process- ing.
 Orchestration service
  heat
 python-heatclient
 Launches stacks from templates, views details of running stacks including events and resources, and updates and deletes stacks.
 Rating service
  cloudkitty
 python-cloudkittyclient
 Rating service.
 Shared File Systems service
  manila
 python-manilaclient
 Creates and manages shared file sys- tems.
 Telemetry service
  ceilometer
 python-ceilometerclient
 Creates and collects measurements across OpenStack.
 Telemetry v3
  gnocchi
 python-gnocchiclient
 Creates and collects measurements across OpenStack.
 Workflow service
  mistral
 python-mistralclient
 Workflow service for OpenStack cloud.
                     Install the OpenStack command-line clients
Install the prerequisite software and the Python package for each OpenStack client.
 36 OpenStack command-line clients
 Install the prerequisite software
Most Linux distributions include packaged versions of the command-line clients. You can directly install the clients from the packages with prerequisites. For more information, see Installing_from_packages.
If you need to install the source package for the command-line package, the following table lists the software needed to run the command-line clients, and provides installation instructions as needed.
Table 2: OpenStack command-line clients prerequisites
User Guide (Release Version: 15.0.0)
   Prerequisite
 Description
  Python 2.7 or later
 Supports Python 2.7, 3.4, and 3.5.
  setuptools package
 Installed by default on Mac OS X.
Many Linux distributions provide packages to make setuptools easy to install. Search your package manager for setuptools to find an installation package. If you cannot find one, download the setuptools package directly from Python Setuptools. The recommended way to install setuptools on Microsoft Windows is to follow the documentation provided on the Python Setuptools website.
Another option is to use the unofficial binary installer maintained by Christoph Gohlke.
    Continued on next page
    OpenStack command-line clients 37
 User Guide (Release Version: 15.0.0)
Table 2 – continued from previous page
   Prerequisite
 Description
  pip package
To install the clients on a Linux, Mac OS X, or Microsoft Windows system, use pip. It is easy to use, ensures that you get the latest version of the clients from the Python Package Index, and lets you update or remove the packages later on.
Since the installation process compiles source files, this requires the related Python development package for your operating system and distribution.
Install pip through the package manager for your system:
MacOS
 # easy_install pip
Microsoft Windows
Ensure that the C:\Python27\Scripts directory is defined in the PATH environ- ment variable, and use the easy_install command from the setuptools package:
 C:\>easy_install pip
Another option is to use the unofficial binary installer provided by Christoph Gohlke.
Ubuntu or Debian
 # apt install python-dev python-pip
Note that extra dependencies may be required, per operating system, depending on the package being installed, such as is the case with Tempest.
Red Hat Enterprise Linux or CentOS
A packaged version enables you to use yum to install the package:
 # yum install python-devel python-pip
On Red Hat Enterprise Linux, this command assumes that you have enabled the OpenStack repository. For more information, see the Installation Tutorial for Red Hat Enterprise Linux and CentOS.
There are also packaged versions of the clients available that enable yum to install the clients as described in Installing_from_packages.
Fedora
A packaged version enables you to use dnf to install the package:
 # dnf install python-devel python-pip
SUSE Linux Enterprise Server
A packaged version available in the Open Build Service enables you to use YaST or zypper to install the package.
First, add the Open Build Service repository as described in the Installation Tutorial. Then install pip and use it to manage client installation:
 # zypper install python-devel python-pip
There are also packaged versions of the clients available that enable zypper to install the clients as described in Installing_from_packages.
openSUSE
You can install pip and use it to manage client installation:
 # zypper install python-devel python-pip
There are also packaged versions of the clients available that enable zypper to install the clients as described in Installing_from_packages.
    38
OpenStack command-line clients
 Install the OpenStack client
The following example shows the command for installing the OpenStack client with pip, which supports mul- tiple services.
# pip install python-openstackclient
The following individual clients are deprecated in favor of a common client. Instead of installing and learning all these clients, we recommend installing and using the OpenStack client. You may need to install an individual project’s client because coverage is not yet sufficient in the OpenStack client. If you need to install an individual client’s project, replace the PROJECT name in this pip install command using the list below.
# pip install python-PROJECTclient
• barbican - Key Manager Service API
• ceilometer - Telemetry API
• cinder - Block Storage API and extensions
• cloudkitty - Rating service API
• designate - DNS service API
• fuel - Deployment service API
• glance - Image service API
• gnocchi - Telemetry API v3
• heat - Orchestration API
• magnum - Container Infrastructure Management service API • manila - Shared file systems API
• mistral - Workflow service API
• monasca - Monitoring API
• murano - Application catalog API
• neutron - Networking API
• nova - Compute API and extensions
• senlin - Clustering service API
• swift - Object Storage API
• trove - Database service API
Installing with pip
Use pip to install the OpenStack clients on a Linux, Mac OS X, or Microsoft Windows system. It is easy to use and ensures that you get the latest version of the client from the Python Package Index. Also, pip enables you to update or remove a package.
Install each client separately by using the following command: • ForMacOSXorLinux:
User Guide (Release Version: 15.0.0)
           OpenStack command-line clients 39
 User Guide (Release Version: 15.0.0)
    # pip install python-PROJECTclient • For Microsoft Windows:
      C:\>pip install python-PROJECTclient
Installing from packages
RDO, openSUSE, SUSE Linux Enterprise, Debian, and Ubuntu have client packages that can be installed without pip.
Note: Thepackagedversionmightinstallolderclients.Ifyouwanttomakesurethelatestclientsareinstalled, you might need to install the clients with pip.
• OnRedHatEnterpriseLinuxorCentOS,useyumtoinstalltheclientsfromthepackagedversions: # yum install python-PROJECTclient
This command assumes that you have enabled the OpenStack repository for your distribution. For more information, see the Installation Tutorial for Red Hat Enterprise Linux and CentOS.
• ForUbuntuorDebian,useapt-gettoinstalltheclientsfromthepackagedversions: # apt-get install python-PROJECTclient
• ForopenSUSE,usezyppertoinstalltheclientsfromthedistributionpackagesservice: # zypper install python-PROJECTclient
• ForSUSELinuxEnterpriseServer,usezyppertoinstalltheclientsfromthepackagedversions: # zypper install python-PROJECTclient
This command assumes that you have enabled the OpenStack repository for your distribution. For more information, see the Installation Tutorial for openSUSE and SUSE Linux Enterprise.
Upgrade or remove clients
To upgrade a client with pip, add the --upgrade option to the pip install command: # pip install --upgrade python-PROJECTclient
To remove the client with pip, run the pip uninstall command: # pip uninstall python-PROJECTclient
What’s next
Before you can run client commands, you must create and source the PROJECT-openrc.sh file to set environ- ment variables. See Set environment variables using the OpenStack RC file.
                                       40 OpenStack command-line clients
 Discover the version number for a client
Run the following command to discover the version number for a client:
$ PROJECT --version
For example, to see the version number for the openstack client, run the following command:
Set environment variables using the OpenStack RC file
To set the required environment variables for the OpenStack command-line clients, you must create an envi- ronment file called an OpenStack rc file, or openrc.sh file. If your OpenStack installation provides it, you can download the file from the OpenStack Dashboard as an administrative user or any other user. This project- specific environment file contains the credentials that all OpenStack services use.
When you source the file, environment variables are set for your current shell. The variables enable the Open- Stack client commands to communicate with the OpenStack services that run in the cloud.
Note: Defining environment variables using an environment file is not a common practice on Microsoft Windows. Environment variables are usually defined in the Advanced > System Properties dialog box. One method for using these scripts as-is on Windows is to install Git for Windows and using Git Bash to source the environment variables and to run all CLI commands.
Download and source the OpenStack RC file
1. Log in to the dashboard and from the drop-down list select the project for which you want to download the OpenStack RC file.
2. On the Project tab, open the Compute tab and click Access & Security.
3. On the API Access tab, click Download OpenStack RC File and save the file. The filename will be of the form PROJECT-openrc.sh where PROJECT is the name of the project for which you downloaded the file.
4. CopythePROJECT-openrc.shfiletothecomputerfromwhichyouwanttorunOpenStackcommands. For example, copy the file to the computer from which you want to upload an image with a glance client
command.
5. On any shell from which you want to run OpenStack commands, source the PROJECT-openrc.sh file for the respective project.
In the following example, the demo-openrc.sh file is sourced for the demo project: $ . demo-openrc.sh
6. When you are prompted for an OpenStack password, enter the password for the user who downloaded the PROJECT-openrc.sh file.
User Guide (Release Version: 15.0.0)
         $ openstack --version openstack 3.2.0
         OpenStack command-line clients 41
 User Guide (Release Version: 15.0.0)
Create and source the OpenStack RC file
Alternatively, you can create the PROJECT-openrc.sh file from scratch, if you cannot download the file from the dashboard.
1.
Inatexteditor,createafilenamedPROJECT-openrc.shandaddthefollowingauthenticationinforma- tion:
    export OS_USERNAME=username
export OS_PASSWORD=password
export OS_TENANT_NAME=projectName
export OS_AUTH_URL=https://identityHost:portNumber/v2.0 # The following lines can be omitted
export OS_TENANT_ID=tenantIDString
export OS_REGION_NAME=regionName
export OS_CACERT=/path/to/cacertFile
     2.
On any shell from which you want to run OpenStack commands, source the PROJECT-openrc.sh file for the respective project. In this example, you source the admin-openrc.sh file for the admin project:
$ . admin-openrc.sh
Warning: Saving OS_PASSWORD in plain text may bring a security risk. You should protect the file or not save OS_PASSWORD into the file in the production environment.
       You are not prompted for the password with this method. The password lives in clear text format in the PROJECT-openrc.sh file. Restrict the permissions on this file to avoid security problems. You can also remove the OS_PASSWORD variable from the file, and use the --password parameter with OpenStack client commands instead.
Note: You must set the OS_CACERT environment variable when using the https protocol in the OS_AUTH_URL environment setting because the verification process for the TLS (HTTPS) server certificate uses the one indi- cated in the environment. This certificate will be used when verifying the TLS (HTTPS) server certificate.
Override environment variable values
When you run OpenStack client commands, you can override some environment variable settings by using the options that are listed at the end of the help output of the various client commands. For example, you can override the OS_PASSWORD setting in the PROJECT-openrc.sh file by specifying a password on a openstack command, as follows:
$ openstack --os-password PASSWORD server list Where PASSWORD is your password.
A user specifies their username and password credentials to interact with OpenStack, using any client command. These credentials can be specified using various mechanisms, namely, the environment variable or command- line argument. It is not safe to specify the password using either of these methods.
Note:
         42 OpenStack command-line clients
 For example, when you specify your password using the command-line client with the --os-password argu- ment, anyone with access to your computer can view it in plain text with the ps field.
To avoid storing the password in plain text, you can prompt for the OpenStack password interactively.
Manage images
The cloud operator assigns roles to users. Roles determine who can upload and manage images. The operator might restrict image upload and management to only cloud administrators or operators.
You can upload images through the openstack image create command or the Image service API. You can use the openstack client for the image management. It provides mechanisms to list and delete images, set and delete image metadata, and create images of a running instance or snapshot and backup types.
After you upload an image, you cannot change it.
For details about image creation, see the Virtual Machine Image Guide.
List or get details for images (glance)
To get a list of images and to get further details about a single image, use openstack image list and open- stack image show commands.
User Guide (Release Version: 15.0.0)
    $ openstack image list +--------------------------------------+---------------------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------------------+--------+ | dfc1dfb0-d7bf-4fff-8994-319dd6f703d7 | cirros-0.3.5-x86_64-uec | active | | a3867e29-c7a1-44b0-9e7f-10db587cad20 | cirros-0.3.5-x86_64-uec-kernel | active | | 4b916fba-6775-4092-92df-f41df7246a6b | cirros-0.3.5-x86_64-uec-ramdisk | active | | d07831df-edc3-4817-9881-89141f9134c3 | myCirrosImage | active | +--------------------------------------+---------------------------------+--------+
     $ openstack image show myCirrosImage +------------------+------------------------------------------------------+ | Field | Value | +------------------+------------------------------------------------------+ | checksum | ee1eca47dc88f4879d8a229cc70a07c6 | | container_format | ami |
| created_at
| disk_format
| file
| id
| min_disk
| min_ram
| name
| owner
| protected
| schema
| size
| status
| 2016-08-11T15:07:26Z                                 |
| ami                                                  |
| /v2/images/d07831df-edc3-4817-9881-89141f9134c3/file |
| d07831df-edc3-4817-9881-89141f9134c3                 |
| 0                                                    |
| 0                                                    |
| myCirrosImage                                        |
| d88310717a8e4ebcae84ed075f82c51e                     |
| False                                                |
| /v2/schemas/image                                    |
| 13287936                                             |
| active                                               |
|                                                      |
| 2016-08-11T15:20:02Z                                 |
| None                                                 |
| private                                              |
| tags
| updated_at
| virtual_size
| visibility
+------------------+------------------------------------------------------+
  OpenStack command-line clients 43
 User Guide (Release Version: 15.0.0)
     When viewing a list of images, you can also use grep to filter the list, as follows:
Note: To store location metadata for images, which enables direct file access for a client, update the /etc/ glance/glance-api.conf file with the following statements:
• show_multiple_locations = True
• filesystem_store_metadata_file = filePath
where filePath points to a JSON file that defines the mount point for OpenStack images on your system and a unique ID. For example:
After you restart the Image service, you can use the following syntax to view the image’s location information:
$ openstack --os-image-api-version 2 image show imageID
For example, using the image ID shown above, you would issue the command as follows:
$ openstack --os-image-api-version 2 image show 2d9bb53f-70ea-4066-a68b-67960eaae673
Create or update an image (glance)
To create an image, use openstack image create: $ openstack image create imageName
To update an image by name or ID, use openstack image set: $ openstack image set imageName
The following list explains the optional arguments that you can use with the create and set commands to modify image properties. For more information, refer to the OpenStack Image command reference.
The following example shows the command that you would use to upload a CentOS 6.3 image in qcow2 format and configure it for public access:
The following example shows how to update an existing image with a properties that describe the disk bus, the CD-ROM bus, and the VIF model:
    $ openstack image list | grep 'cirros'
| dfc1dfb0-d7bf-4fff-8994-319dd6f703d7 | cirros-0.3.5-x86_64-uec | active | | a3867e29-c7a1-44b0-9e7f-10db587cad20 | cirros-0.3.5-x86_64-uec-kernel | active | | 4b916fba-6775-4092-92df-f41df7246a6b | cirros-0.3.5-x86_64-uec-ramdisk | active |
      [{
"id": "2d9bb53f-70ea-4066-a68b-67960eaae673", "mountpoint": "/var/lib/glance/images/"
}]
                          $ openstack image create --disk-format qcow2 --container-format bare \ --public --file ./centos63.qcow2 centos63-image
  44 OpenStack command-line clients
 Note: When you use OpenStack with VMware vCenter Server, you need to specify the vmware_disktype and vmware_adaptertype properties with openstack image create. Also, we recommend that you set the hypervisor_type="vmware" property. For more information, see Images with VMware vSphere in the OpenStack Configuration Reference.
Currently the libvirt virtualization tool determines the disk, CD-ROM, and VIF device models based on the configured hypervisor type (libvirt_type in /etc/nova/nova.conf file). For the sake of optimal perfor- mance, libvirt defaults to using virtio for both disk and VIF (NIC) models. The disadvantage of this approach is that it is not possible to run operating systems that lack virtio drivers, for example, BSD, Solaris, and older versions of Linux and Windows.
If you specify a disk or CD-ROM bus model that is not supported, see the Disk_and_CD- ROM_bus_model_values_table. If you specify a VIF model that is not supported, the instance fails to launch. See the VIF_model_values_table.
The valid model values depend on the libvirt_type setting, as shown in the following tables. Disk and CD-ROM bus model values
User Guide (Release Version: 15.0.0)
      $ openstack image set \
--property hw_disk_bus=scsi \ --property hw_cdrom_bus=ide \ --property hw_vif_model=e1000 \ f16-x86_64-openstack-sda
    libvirt_type setting
 Supported model values
  qemu or kvm
 • ide
• scsi
• virtio
  xen
 • ide • xen
   VIF model values
 OpenStack command-line clients 45
 User Guide (Release Version: 15.0.0)
   libvirt_type setting
 Supported model values
  qemu or kvm
 • e1000
• ne2k_pci • pcnet
• rtl8139
• virtio
  xen
 • e1000
• netfront • ne2k_pci • pcnet
• rtl8139
  vmware
 • VirtualE1000
• VirtualPCNet32 • VirtualVmxnet
     Note: By default, hardware properties are retrieved from the image properties. However, if this information is not available, the libosinfo database provides an alternative source for these values.
If the guest operating system is not in the database, or if the use of libosinfo is disabled, the default system values are used.
Users can set the operating system ID or a short-id in image properties. For example:
Alternatively, users can set id to a URL:
Create an image from ISO image
You can upload ISO images to the Image service (glance). You can subsequently boot an ISO image using Compute.
In the Image service, run the following command:
Optionally, to confirm the upload in Image service, run:
$ openstack image list
    $ openstack image set --property short-id=fedora23 \ name-of-my-fedora-image
     $ openstack image set \
--property id=http://fedoraproject.org/fedora/23 \ ID-of-my-fedora-image
      $ openstack image create ISO_IMAGE --file IMAGE.iso \ --disk-format iso --container-format bare
       46 OpenStack command-line clients
 Troubleshoot image creation
If you encounter problems in creating an image in the Image service or Compute, the following information may help you troubleshoot the creation process.
• Ensure that the version of qemu you are using is version 0.14 or later. Earlier versions of qemu result in an unknown option -s error message in the /var/log/nova/nova-compute.log file.
• Examinethe/var/log/nova/nova-api.logand/var/log/nova/nova-compute.loglogfilesfor error messages.
Manage images using cURL
This section is intended to provide a series of commands a typical client of the API might use to create and modify an image.
These commands assume the implementation of the v2 Image API using the Identity Service for authentication and authorization. The X-Auth-Token header is used to provide the authentication token issued by the Identity Service.
The strings $OS_IMAGE_URL and $OS_AUTH_TOKEN represent variables defined in the client’s environ- ment. $OS_IMAGE_URL is the full path to your image service endpoint, for example, http://example. com. $OS_AUTH_TOKEN represents an auth token generated by the Identity Service, for example, 6583fb17c27b48b4b4a6033fe9cc0fe0.
Create an image
User Guide (Release Version: 15.0.0)
    $ curl -i -X POST -H "X-Auth-Token: $OS_AUTH_TOKEN" \ -H "Content-Type: application/json" \
-d '{"name": "Ubuntu 14.04", \
"tags": ["ubuntu", "14.04", "trusty"]}' \ $OS_IMAGE_URL/v2/images
HTTP/1.1 201 Created
Content-Length: 451
Content-Type: application/json; charset=UTF-8
Location: http://example.com:9292/v2/images
          /7b97f37c-899d-44e8-aaa0-543edbc4eaad
Date: Fri, 11 Mar 2016 12:25:32 GMT
{
    "id": "7b97f37c-899d-44e8-aaa0-543edbc4eaad",
    "name": "Ubuntu 14.04",
    "status": "queued",
    "visibility": "private",
    "protected": false,
    "tags": ["ubuntu", "14.04", "trusty"],
    "created_at": "2016-03-11T12:25:32Z",
    "updated_at": "2016-03-11T12:25:32Z",
    "file": "/v2/images/7b97f37c-899d-44e8-aaa0-543edbc4eaad/file",
    "self": "/v2/images/7b97f37c-899d-44e8-aaa0-543edbc4eaad",
    "schema": "/v2/schemas/image"
}
  OpenStack command-line clients 47
 User Guide (Release Version: 15.0.0)
Update the image
    $ curl -i -X PATCH -H "X-Auth-Token: $OS_AUTH_TOKEN" \
-H "Content-Type: application/json" \
-d '[{"op": "add", "path": "/login-user", "value": "root"}]' \ $OS_IMAGE_URL/v2/images/7b97f37c-899d-44e8-aaa0-543edbc4eaad
HTTP/1.1 200 OK
Content-Length: 477
Content-Type: application/json; charset=UTF-8
Date: Fri, 11 Mar 2016 12:44:56 GMT
{
    "id": "7b97f37c-899d-44e8-aaa0-543edbc4eaad",
    "name": "Ubuntu 14.04",
    "status": "queued",
    "visibility": "private",
    "protected": false,
    "tags": ["ubuntu", "14.04", "trusty"],
    "login_user": "root",
    "created_at": "2016-03-11T12:25:32Z",
    "updated_at": "2016-03-11T12:44:56Z",
    "file": "/v2/images/7b97f37c-899d-44e8-aaa0-543edbc4eaad/file",
    "self": "/v2/images/7b97f37c-899d-44e8-aaa0-543edbc4eaad",
    "schema": "/v2/schemas/image"
}
 Upload binary image data
    $ curl -i -X PUT -H "X-Auth-Token: $OS_AUTH_TOKEN" \
-H "Content-Type: application/octet-stream" \
--data-binary @/home/glance/ubuntu-14.04.qcow2 \ $OS_IMAGE_URL/v2/images/7b97f37c-899d-44e8-aaa0-543edbc4eaad/file
HTTP/1.1 100 Continue
HTTP/1.1 201 Created
Content-Length: 0
Date: Fri, 11 Mar 2016 12:51:02 GMT
 Download binary image data
    $ curl -i -X GET -H "X-Auth-Token: $OS_AUTH_TOKEN" \ $OS_IMAGE_URL/v2/images/7b97f37c-899d-44e8-aaa0-543edbc4eaad/file
HTTP/1.1 200 OK
Content-Type: application/octet-stream
Content-Md5: 912ec803b2ce49e4a541068d495ab570
Transfer-Encoding: chunked
Date: Fri, 11 Mar 2016 12:57:41 GMT
  48 OpenStack command-line clients
 Delete an image
Manage volumes
A volume is a detachable block storage device, similar to a USB hard drive. You can attach a volume to only one instance. Use the openstack client commands to create and manage volumes.
Migrate a volume
As an administrator, you can migrate a volume with its data from one location to another in a manner that is transparent to users and workloads. You can migrate only detached volumes with no snapshots.
Possible use cases for data migration include:
• Bring down a physical storage device for maintenance without disrupting workloads. • Modify the properties of a volume.
• Free up space in a thinly-provisioned back end.
Migrate a volume with the openstack volume migrate command, as shown in the following example:
In this example, --force-host-copy forces the generic host-based migration mechanism and bypasses any driver optimizations. --lock-volume | --unlock-volume applies to the available volume. To determine whether the termination of volume migration caused by other commands. --lock-volume locks the volume state and does not allow the migration to be aborted.
Note: If the volume has snapshots, the specified host destination cannot accept the volume. If the user is not an administrator, the migration fails.
Create a volume
This example creates a my-new-volume volume based on an image.
1. List images, and note the ID of the image that you want to use for your volume:
User Guide (Release Version: 15.0.0)
    $ curl -i -X DELETE -H "X-Auth-Token: $OS_AUTH_TOKEN" \ $OS_IMAGE_URL/v2/images/7b97f37c-899d-44e8-aaa0-543edbc4eaad
HTTP/1.1 204 No Content
Content-Length: 0
Date: Fri, 11 Mar 2016 12:59:11 GMT
     $ openstack volume migrate [-h] --host <host> [--force-host-copy] [--lock-volume | --unlock-volume]
<volume>
       $ openstack image list +--------------------------------------+---------------------------------+ | ID | Name | +--------------------------------------+---------------------------------+ | 8bf4dc2a-bf78-4dd1-aefa-f3347cf638c8 | cirros-0.3.5-x86_64-uec |
  OpenStack command-line clients 49
 User Guide (Release Version: 15.0.0)
    | 9ff9bb2e-3a1d-4d98-acb5-b1d3225aca6c | cirros-0.3.5-x86_64-uec-kernel  |
| 4b227119-68a1-4b28-8505-f94c6ea4c6dc | cirros-0.3.5-x86_64-uec-ramdisk |
+--------------------------------------+---------------------------------+
 2. List the availability zones, and note the ID of the availability zone in which you want to create your volume:
3. Create a volume with 8 gibibytes (GiB) of space, and specify the availability zone and image:
    $ openstack availability zone list +------+-----------+
| Name | Status | +------+-----------+
| nova | available |
+------+-----------+
     $ openstack volume create --image 8bf4dc2a-bf78-4dd1-aefa-f3347cf638c8 \ --size 8 --availability-zone nova my-new-volume
+------------------------------+--------------------------------------+
| Property                     | Value                                |
+------------------------------+--------------------------------------+
| replication_status
| size
| snapshot_id
| source_volid
| disabled | |8 | | None | | None | | creating | | None | | fe19e3a9f63f4a14bd4697789247bbc5 | | lvmdriver-1 |
| [] | | nova | | false | | None | | 2016-09-23T07:52:42.000000 | | None | | False | | bab4b0e0-ce3d-4d57-bf57-3c51319f5202 | |{} | | False | | my-new-volume |
| attachments
| availability_zone
| bootable
| consistencygroup_id
| created_at
| description
| encrypted
| id
| metadata
| multiattach
| name
| os-vol-tenant-attr:tenant_id | 3f670abbe9b34ca5b81db6e7b540b8d8     |
| status
| updated_at
| user_id
| volume_type
+------------------------------+--------------------------------------+
 4. To verify that your volume was created successfully, list the available volumes:
    $ openstack volume list +--------------------------------------+---------------+-----------+------+-----------
 →--+
| ID | DisplayName | Status | Size | Attached
 →to | +--------------------------------------+---------------+-----------+------+-----------
 →--+
| bab4b0e0-ce3d-4d57-bf57-3c51319f5202 | my-new-volume | available | 8 |
 → | +--------------------------------------+---------------+-----------+------+-----------
 →--+
  50 OpenStack command-line clients
 If your volume was created successfully, its status is available. If its status is error, you might have exceeded your quota.
Create a volume from specified volume type
Cinder supports these three ways to specify volume type during volume creation. 1. volume_type
2. cinder_img_volume_type (via glance image metadata)
3. default_volume_type (via cinder.conf)
volume_type
User can specify volume type when creating a volume.
cinder_img_volume_type
If glance image has cinder_img_volume_type property, Cinder uses this parameter to specify volume type when creating a volume.
Choose glance image which has cinder_img_volume_type property and create a volume from the image.
User Guide (Release Version: 15.0.0)
    $ openstack volume create -h -f {json,shell,table,value,yaml} -c COLUMN --max-width <integer>
                         --noindent --prefix PREFIX --size <size>
                         --type <volume-type> --image <image>
                         --snapshot <snapshot> --source <volume>
                         --description <description> --user <user>
                         --project <project>
                         --availability-zone <availability-zone>
                         --property <key=value>
                         <name>
     $ openstack image list +----------------------------------+---------------------------------+--------+ | ID | Name | Status | +----------------------------------+---------------------------------+--------+ | 376bd633-c9c9-4c5d-a588-342f4f66 | cirros-0.3.5-x86_64-uec | active |
|d086
| 2c20fce7-2e68-45ee-ba8d-
| beba27a91ab5
| a5752de4-9faf-4c47-acbc-
| 78a5efa7cc6e | || +----------------------------------+---------------------------------+--------+
| || | cirros-0.3.5-x86_64-uec-ramdisk | active | | || | cirros-0.3.5-x86_64-uec-kernel | active |
$ openstack image show 376bd633-c9c9-4c5d-a588-342f4f66d086 +------------------+-----------------------------------------------------------+ | Field | Value | +------------------+-----------------------------------------------------------+ | checksum | eb9139e4942121f22bbc2afc0400b2a4 | | container_format | ami |
  OpenStack command-line clients 51
 User Guide (Release Version: 15.0.0)
    | created_at
| disk_format
| file
| id
| min_disk
| min_ram
| name
| owner
| properties
|
| protected
| schema
| 2016-10-13T03:28:55Z                                      |
| ami                                                       |
| /v2/images/376bd633-c9c9-4c5d-a588-342f4f66d086/file      |
| 376bd633-c9c9-4c5d-a588-342f4f66d086                      |
| 0                                                         |
| 0                                                         |
| cirros-0.3.5-x86_64-uec                                   |
| 88ba456e3a884c318394737765e0ef4d                          |
| kernel_id='a5752de4-9faf-4c47-acbc-78a5efa7cc6e',         |
| ramdisk_id='2c20fce7-2e68-45ee-ba8d-beba27a91ab5'         |
| False                                                     |
| /v2/schemas/image                                         |
| 25165824                                                  |
| active                                                    |
|                                                           |
| 2016-10-13T03:28:55Z                                      |
| None                                                      |
| public                                                    |
| size
| status
| tags
| updated_at
| virtual_size
| visibility
+------------------+-----------------------------------------------------------+
$ openstack volume create --image 376bd633-c9c9-4c5d-a588-342f4f66d086 \ --size 1 --availability-zone nova test
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| []                                   |
| nova                                 |
| false                                |
| attachments
| availability_zone
| bootable
| consistencygroup_id | None                                 |
| created_at
| description
| encrypted
| id
| 2016-10-13T06:29:53.688599           |
| None                                 |
| False                                |
| e6e6a72d-cda7-442c-830f-f306ea6a03d5 |
| False                                |
| test                                 |
|                                      |
| multiattach
| name
| properties
| replication_status  | disabled                             |
| size
| snapshot_id
| source_volid
| status
| type
| updated_at
| user_id
+---------------------+--------------------------------------+
| 1                                    |
| None                                 |
| None                                 |
| creating                             |
| lvmdriver-1                          |
| None                                 |
| 33fdc37314914796883706b33e587d51     |
 default_volume_type
If above parameters are not set, Cinder uses default_volume_type which is defined in cinder.conf during volume creation.
Example cinder.conf file configuration.
    [default]
default_volume_type = lvmdriver-1
  52 OpenStack command-line clients
 Attach a volume to an instance
1. Attach your volume to a server, specifying the server ID and the volume ID:
2. Show information for your volume:
$ openstack volume show 573e024d-5235-49ce-8332-be1576d323f8
The output shows that the volume is attached to the server with ID 84c6e57d-a6b1-44b6-81eb-
fcb36afd31b5, is in the nova availability zone, and is bootable.
User Guide (Release Version: 15.0.0)
    $ openstack server add volume 84c6e57d-a6b1-44b6-81eb-fcb36afd31b5 \ 573e024d-5235-49ce-8332-be1576d323f8 --device /dev/vdb
          +------------------------------+-----------------------------------------------+
| Field                        | Value                                         |
+------------------------------+-----------------------------------------------+
| attachments
|
|
|
| availability_zone
| bootable
| consistencygroup_id
| created_at
| description
| encrypted
| id
| multiattach
| name
| os-vol-tenant-attr:tenant_id | 7ef070d3fee24bdfae054c17ad742e28              |
| [{u'device': u'/dev/vdb',                     |
|
|
|
| nova
| true
| None
| 2016-10-13T06:08:07.000000                    |
| None                                          |
| False                                         |
| 573e024d-5235-49ce-8332-be1576d323f8          |
| False                                         |
| my-new-volume                                 |
| properties
| replication_status
| size
| snapshot_id
| source_volid
| status
| type
| updated_at
| user_id
| volume_image_metadata
|
|
|
+------------------------------+-----------------------------------------------+
u'server_id': u'84c6e57d-a             |
   u'id': u'573e024d-...               |
u'volume_id': u'573e024d...            |
                                       |
                                       |
                                       |
|                                               |
| disabled                                      |
| 8                                             |
| None                                          |
| None                                          |
| in-use                                        |
| lvmdriver-1                                   |
| 2016-10-13T06:08:11.000000                    |
| 33fdc37314914796883706b33e587d51              |
|{u'kernel_id': u'df430cc2...,                  |
|        u'image_id': u'397e713c...,            |
|        u'ramdisk_id': u'3cf852bd...,          |
|u'image_name': u'cirros-0.3.5-x86_64-uec'}     |
 Resize a volume
1. To resize your volume, you must first detach it from the server. To detach the volume from your server, pass the server ID and volume ID to the following command:
This command does not provide any output.
    $ openstack server remove volume 84c6e57d-a6b1-44b6-81eb-fcb36afd31b5 573e024d-5235-  →49ce-8332-be1576d323f8
  OpenStack command-line clients 53
 User Guide (Release Version: 15.0.0)
2. List volumes:
Note that the volume is now available.
3. Resize the volume by passing the volume ID and the new size (a value greater than the old one) as parameters:
$ openstack volume set 573e024d-5235-49ce-8332-be1576d323f8 --size 10 This command does not provide any output.
Note: When extending an LVM volume with a snapshot, the volume will be deactivated. The reactiva- tion is automatic unless auto_activation_volume_list is defined in lvm.conf. See lvm.conf for more information.
Delete a volume
    $ openstack volume list +----------------+-----------------+-----------+------+-------------+ | ID | Display Name | Status | Size | Attached to | +----------------+-----------------+-----------+------+-------------+ | 573e024d-52... | my-new-volume | available | 8 | | | bd7cf584-45... | my-bootable-vol | available | 8 | | +----------------+-----------------+-----------+------+-------------+
        1.
2.
To delete your volume, you must first detach it from the server. To detach the volume from your server and check for the list of existing volumes, see steps 1 and 2 in Resize_a_volume.
Delete the volume using either the volume name or ID:
$ openstack volume delete my-new-volume This command does not provide any output.
List the volumes again, and note that the status of your volume is deleting:
When the volume is fully deleted, it disappears from the list of volumes:
         $ openstack volume list +----------------+-----------------+-----------+------+-------------+ | ID | Display Name | Status | Size | Attached to | +----------------+-----------------+-----------+------+-------------+ | 573e024d-52... | my-new-volume | deleting | 8 | | | bd7cf584-45... | my-bootable-vol | available | 8 | | +----------------+-----------------+-----------+------+-------------+
     $ openstack volume list +----------------+-----------------+-----------+------+-------------+ | ID | Display Name | Status | Size | Attached to | +----------------+-----------------+-----------+------+-------------+ | bd7cf584-45... | my-bootable-vol | available | 8 | | +----------------+-----------------+-----------+------+-------------+
  54
OpenStack command-line clients
 Transfer a volume
You can transfer a volume from one owner to another by using the openstack volume transfer request create command. The volume donor, or original owner, creates a transfer request and sends the created transfer ID and authorization key to the volume recipient. The volume recipient, or new owner, accepts the transfer by using the ID and key.
Note: The procedure for volume transfer is intended for projects (both the volume donor and recipient) within the same cloud.
Use cases include:
• Create a custom bootable volume or a volume with a large data set and transfer it to a customer.
• Forbulkimportofdatatothecloud,thedataingresssystemcreatesanewBlockStoragevolume,copies data from the physical device, and transfers device ownership to the end user.
Create a volume transfer request
1. While logged in as the volume donor, list the available volumes:
2. As the volume donor, request a volume transfer authorization code for a specific volume:
The volume must be in an available state or the request will be denied. If the transfer request is valid in the database (that is, it has not expired or been deleted), the volume is placed in an awaiting-transfer state. For example:
$ openstack volume transfer request create a1cdace0-08e4-4dc7-b9dc-457e9bcfe25f The output shows the volume transfer ID in the id row and the authorization key.
User Guide (Release Version: 15.0.0)
      $ openstack volume list +-----------------+-----------------+-----------+------+-------------+ | ID | Display Name | Status | Size | Attached to | +-----------------+-----------------+-----------+------+-------------+ | 72bfce9f-cac... | None | error | 1 | | | a1cdace0-08e... | None | available | 1 | | +-----------------+-----------------+-----------+------+-------------+
     $ openstack volume transfer request create <volume>
<volume>
   Name or ID of volume to transfer.
          +------------+--------------------------------------+
| Field      | Value                                |
+------------+--------------------------------------+
| auth_key   | 0a59e53630f051e2                     |
| created_at | 2016-11-03T11:49:40.346181           |
| id         | 34e29364-142b-4c7b-8d98-88f765bf176f |
| name       | None                                 |
| volume_id  | a1cdace0-08e4-4dc7-b9dc-457e9bcfe25f |
+------------+--------------------------------------+
  OpenStack command-line clients 55
 User Guide (Release Version: 15.0.0)
 Note: Optionally, you can specify a name for the transfer by using the --name transferName param- eter.
Note: While the auth_key property is visible in the output of openstack volume transfer re- quest create VOLUME_ID, it will not be available in subsequent openstack volume transfer re- quest show TRANSFER_ID command.
3. Send the volume transfer ID and authorization key to the new owner (for example, by email).
4. View pending transfers:
5. After the volume recipient, or new owner, accepts the transfer, you can see that the transfer is no longer available:
Accept a volume transfer request
       $ openstack volume transfer request list +--------------------------------------+--------------------------------------+------+ | ID | Volume | Name | +--------------------------------------+--------------------------------------+------+ | 6e4e9aa4-bed5-4f94-8f76-df43232f44dc | a1cdace0-08e4-4dc7-b9dc-457e9bcfe25f | None | +--------------------------------------+--------------------------------------+------+
     $ openstack volume transfer request list +----+-----------+------+
| ID | Volume ID | Name | +----+-----------+------+ +----+-----------+------+
 1. 2.
As the volume recipient, you must first obtain the transfer ID and authorization key from the original owner.
Accept the request:
$ openstack volume transfer request accept transferID authKey For example:
         $ openstack volume transfer request accept 6e4e9aa4-bed5-4f94-8f76-df43232f44dc   →b2c8e585cbc68a80
+-----------+--------------------------------------+
|  Property |                Value                 |
+-----------+--------------------------------------+
|     id    | 6e4e9aa4-bed5-4f94-8f76-df43232f44dc |
|    name   |                 None                 |
| volume_id | a1cdace0-08e4-4dc7-b9dc-457e9bcfe25f |
+-----------+--------------------------------------+
  Note: If you do not have a sufficient quota for the transfer, the transfer is refused.
  56
OpenStack command-line clients
 Delete a volume transfer
1. List available volumes and their statuses:
2. Find the matching transfer ID:
3. Delete the volume:
$ openstack volume transfer request delete <transfer> <transfer> Name or ID of transfer to delete.
For example:
$ openstack volume transfer request delete a6da6888-7cdf-4291-9c08-8c1f22426b8a 4. Verify that transfer list is now empty and that the volume is again available for transfer:
User Guide (Release Version: 15.0.0)
    $ openstack volume list +-----------------+-----------------+-----------------+------+-------------+ | ID | Display Name | Status | Size | Attached to | +-----------------+-----------------+-----------------+------+-------------+ | 72bfce9f-cac... | None | error | 1 | | | a1cdace0-08e... | None |awaiting-transfer| 1 | | +-----------------+-----------------+-----------------+------+-------------+
     $ openstack volume transfer request list +--------------------------------------+--------------------------------------+------+ | ID | VolumeID | Name | +--------------------------------------+--------------------------------------+------+ | a6da6888-7cdf-4291-9c08-8c1f22426b8a | a1cdace0-08e4-4dc7-b9dc-457e9bcfe25f | None | +--------------------------------------+--------------------------------------+------+
               $ openstack volume transfer request list +----+-----------+------+
| ID | Volume ID | Name | +----+-----------+------+ +----+-----------+------+
     $ openstack volume list +-----------------+-----------+--------------+------+-------------+----------+--------
 →-----+
| ID | Status | Display Name | Size | Volume Type | Bootable |
 →Attached to | +-----------------+-----------+--------------+------+-------------+----------+--------
 →-----+
| 72bfce9f-ca... | error | None | 1 | None | false |
 → |
| a1cdace0-08... | available | None | 1 | None | false |
 → | +-----------------+-----------+--------------+------+-------------+----------+--------
 →-----+
  OpenStack command-line clients 57
 User Guide (Release Version: 15.0.0)
Manage and unmanage a snapshot
A snapshot is a point in time version of a volume. As an administrator, you can manage and unmanage snapshots.
Manage a snapshot
Manage a snapshot with the openstack volume snapshot set command:
The arguments to be passed are:
--name <name> New snapshot name
--description <description> New snapshot description
--no-property Remove all properties from <snapshot> (specify both –no-property and –property to remove the current properties before setting new properties.)
--property <key=value> Property to add or modify for this snapshot (repeat option to set multiple proper- ties)
--state <state> New snapshot state. (“available”, “error”, “creating”, “deleting”, or “error_deleting”) (ad- min only) (This option simply changes the state of the snapshot in the database with no regard to actual status, exercise caution when using)
<snapshot> Snapshot to modify (name or ID)
$ openstack volume snapshot set my-snapshot-id
Unmanage a snapshot
Unmanage a snapshot with the openstack volume snapshot unset command:
The arguments to be passed are:
--property <key> Property to remove from snapshot (repeat option to remove multiple properties) <snapshot> Snapshot to modify (name or ID).
The following example unmanages the my-snapshot-id image:
$ openstack volume snapshot unset my-snapshot-id
    $ openstack volume snapshot set [-h]
[--name <name>]
                                [--description <description>]
                                [--no-property]
                                [--property <key=value>]
                                [--state <state>]
<snapshot>
          $ openstack volume snapshot unset [-h]
[--property <key>]
<snapshot>
       58 OpenStack command-line clients
 Manage shares
A share is provided by file storage. You can give access to a share to instances. To create and manage shares, you use manila client commands.
Create a share network
1. Create a share network.
2. List share networks.
Create a share
1. Create a share.
User Guide (Release Version: 15.0.0)
    $ manila share-network-create \
--name mysharenetwork \
--description "My Manila network" \
--neutron-net-id dca0efc7-523d-43ef-9ded-af404a02b055 \ --neutron-subnet-id 29ecfbd5-a9be-467e-8b4a-3415d1f82888
+-------------------+--------------------------------------+
| Property          | Value                                |
+-------------------+--------------------------------------+
| name
| segmentation_id
| created_at
| neutron_subnet_id | 29ecfbd5-a9be-467e-8b4a-3415d1f82888 |
| mysharenetwork                       |
| None                                 |
| 2016-03-24T14:13:02.888816           |
| updated_at
| network_type
| neutron_net_id
| ip_version
| nova_net_id
| cidr
| project_id
| id
| description
+-------------------+--------------------------------------+
| None                                 |
| None                                 |
| dca0efc7-523d-43ef-9ded-af404a02b055 |
| None                                 |
| None                                 |
| None                                 |
| 907004508ef4447397ce6741a8f037c1     |
| c895fe26-92be-4152-9e6c-f2ad230efb13 |
| My Manila network                    |
     $ manila share-network-list +--------------------------------------+----------------+ | id | name | +--------------------------------------+----------------+ | c895fe26-92be-4152-9e6c-f2ad230efb13 | mysharenetwork | +--------------------------------------+----------------+
     $ manila create NFS 1 \ --name myshare \
--description "My Manila share" \ --share-network mysharenetwork \ --share-type default
+-----------------------------+--------------------------------------+
| Property                    | Value                                |
+-----------------------------+--------------------------------------+
| status                      | creating                             |
  OpenStack command-line clients 59
 User Guide (Release Version: 15.0.0)
    | share_type_name
| description
| availability_zone
| share_network_id
| share_server_id
| host
| access_rules_status
| snapshot_id
| is_public
| task_state
| snapshot_support
| id
| size
| name
| default                              |
| My Manila share                      |
| None                                 |
| c895fe26-92be-4152-9e6c-f2ad230efb13 |
| None                                 |
|                                      |
| active                               |
| None                                 |
| False                                |
| None                                 |
| True                                 |
| 8d8b854b-ec32-43f1-acc0-1b2efa7c3400 |
| 1                                    |
| myshare                              |
| bf6ada49-990a-47c3-88bc-c0cb31d5c9bf |
| False                                |
| None                                 |
| 2016-03-24T14:15:34.000000           |
| NFS                                  |
| None                                 |
| share_type
| has_replicas
| replication_type
| created_at
| share_proto
| consistency_group_id
| source_cgsnapshot_member_id | None                                 |
| project_id                  | 907004508ef4447397ce6741a8f037c1     |
| metadata                    | {}                                   |
+-----------------------------+--------------------------------------+
 2. Show a share.
$ manila show myshare +-----------------------------+-------------------------------------------------------
 →--------+
| Property | Value
 → | +-----------------------------+-------------------------------------------------------
     →--------+ | status
 → |
| share_type_name
 → | | description
 → |
| availability_zone
 → |
| share_network_id
 → |
| export_locations
 → | |
 →45f668732b1d | |
 → | |
 → | |
 → | |
 →45f668732b1d | |
| available
| default
| My Manila share
| nova
| c895fe26-92be-4152-9e6c-f2ad230efb13
|
| path = 10.254.0.3:/share-e1c2d35e-fe67-4028-ad7a-
| preferred = False
| is_admin_only = False
| id = b6bd76ce-12a2-42a9-a30a-8a43b503867d
| share_instance_id = e1c2d35e-fe67-4028-ad7a-
| path = 10.0.0.3:/share-e1c2d35e-fe67-4028-ad7a-
 →45f668732b1d |
  60
OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    |
 → |
|
 → |
|
 → |
|
 →45f668732b1d |
| share_server_id  → |
| host
 → |
| access_rules_status  → |
| snapshot_id  → | | is_public  → |
| task_state  → |
| snapshot_support  → |
| id
 → |
| size
 → |
| name
 → |
| share_type  → |
| has_replicas  → |
| replication_type  → |
| created_at  → |
| share_proto  → |
| consistency_group_id  → |
| preferred = False
| is_admin_only = True
| id = 6921e862-88bc-49a5-a2df-efeed9acd583
| share_instance_id = e1c2d35e-fe67-4028-ad7a-
| 2e9d2d02-883f-47b5-bb98-e053b8d1e683   | nosb-devstack@london#LONDON   | active   | None   | False   | None   | True   | 8d8b854b-ec32-43f1-acc0-1b2efa7c3400   |1   | myshare   | bf6ada49-990a-47c3-88bc-c0cb31d5c9bf   | False   | None   | 2016-03-24T14:15:34.000000   | NFS   | None
| source_cgsnapshot_member_id | None    → |
| project_id | 907004508ef4447397ce6741a8f037c1    → |
| metadata | {}    → |
+-----------------------------+-------------------------------------------------------  →--------+
 3. List shares.
$ manila list +--------------------------------------+---------+------+-------------+-----------+---
 →--------+-----------------+-----------------------------+-------------------+
| ID | Name | Size | Share Proto | Status |
 →Is Public | Share Type Name | Host | Availability Zone | +--------------------------------------+---------+------+-------------+-----------+---
 →--------+-----------------+-----------------------------+-------------------+
      OpenStack command-line clients 61
 User Guide (Release Version: 15.0.0)
    | 8d8b854b-ec32-43f1-acc0-1b2efa7c3400 | myshare | 1 | NFS | available |   →False | default | nosb-devstack@london#LONDON | nova |
+--------------------------------------+---------+------+-------------+-----------+---  →--------+-----------------+-----------------------------+-------------------+
 4. List share export locations.
    $ manila share-export-location-list myshare +--------------------------------------+----------------------------------------------
 →----------+-----------+
| ID | Path
 → | Preferred | +--------------------------------------+----------------------------------------------
 →----------+-----------+
| 6921e862-88bc-49a5-a2df-efeed9acd583 | 10.0.0.3:/share-e1c2d35e-fe67-4028-ad7a-
 →45f668732b1d |False |
| b6bd76ce-12a2-42a9-a30a-8a43b503867d | 10.254.0.3:/share-e1c2d35e-fe67-4028-ad7a-
 →45f668732b1d | False | +--------------------------------------+----------------------------------------------
 →----------+-----------+
 Allow read-write access
1. Allow access.
    $ manila access-allow myshare ip 10.0.0.0/24 +--------------+--------------------------------------+ | Property | Value | +--------------+--------------------------------------+ | share_id | 8d8b854b-ec32-43f1-acc0-1b2efa7c3400 | | access_type | ip | | access_to | 10.0.0.0/24 | | access_level | rw | | state | new | | id | 0c8470ca-0d77-490c-9e71-29e1f453bf97 | +--------------+--------------------------------------+
 2. List access.
    $ manila access-list myshare +--------------------------------------+-------------+-------------+--------------+---
 →-----+
| id | access_type | access_to | access_level |
 →state | +--------------------------------------+-------------+-------------+--------------+---
 →-----+
| 0c8470ca-0d77-490c-9e71-29e1f453bf97 | ip | 10.0.0.0/24 | rw |
 →active | +--------------------------------------+-------------+-------------+--------------+---
 →-----+
 The access is created.
 62
OpenStack command-line clients
 Allow read-only access
1. Allow access.
2. List access.
User Guide (Release Version: 15.0.0)
    $ manila access-allow myshare ip 20.0.0.0/24 --access-level ro +--------------+--------------------------------------+
| Property | Value | +--------------+--------------------------------------+
| share_id     | 8d8b854b-ec32-43f1-acc0-1b2efa7c3400 |
| access_type  | ip                                   |
| access_to    | 20.0.0.0/24                          |
| access_level | ro                                   |
| state        | new                                  |
| id           | f151ad17-654d-40ce-ba5d-98a5df67aadc |
+--------------+--------------------------------------+
     $ manila access-list myshare +--------------------------------------+-------------+-------------+--------------+---
 →-----+
| id | access_type | access_to | access_level |
 →state | +--------------------------------------+-------------+-------------+--------------+---
 →-----+
| 0c8470ca-0d77-490c-9e71-29e1f453bf97 | ip | 10.0.0.0/24 | rw |
 →active |
| f151ad17-654d-40ce-ba5d-98a5df67aadc | ip | 20.0.0.0/24 | ro |
 →active | +--------------------------------------+-------------+-------------+--------------+---
 →-----+
 The access is created.
Deny access
1. Deny access.
2. List access.
The access is removed.
Create snapshot
1. Create a snapshot.
    $ manila access-deny myshare 0c8470ca-0d77-490c-9e71-29e1f453bf97 $ manila access-deny myshare f151ad17-654d-40ce-ba5d-98a5df67aadc
     $ manila access-list myshare +----+-------------+-----------+--------------+-------+ | id | access type | access to | access level | state | +----+-------------+-----------+--------------+-------+ +----+-------------+-----------+--------------+-------+
  OpenStack command-line clients 63
 User Guide (Release Version: 15.0.0)
    $ manila snapshot-create --name mysnapshot --description "My Manila snapshot" myshare +-------------------+--------------------------------------+
| Property | Value | +-------------------+--------------------------------------+
| status
| share_id
| description
| created_at
| share_proto
| provider_location | None                                 |
| creating                             |
| 8d8b854b-ec32-43f1-acc0-1b2efa7c3400 |
| My Manila snapshot                   |
| 2016-03-24T14:39:58.232844           |
| NFS                                  |
| id
| size
| share_size
| name
+-------------------+--------------------------------------+
| e744ca47-0931-4e81-9d9f-2ead7d7c1640 | |1 | |1 | | mysnapshot |
 2. List snapshots.
    $ manila snapshot-list +--------------------------------------+--------------------------------------+-------
 →----+------------+------------+
| ID | Share ID |
 →Status | Name | Share Size | +--------------------------------------+--------------------------------------+-------
 →----+------------+------------+
| e744ca47-0931-4e81-9d9f-2ead7d7c1640 | 8d8b854b-ec32-43f1-acc0-1b2efa7c3400 |
 →available | mysnapshot | 1 | +--------------------------------------+--------------------------------------+-------
 →----+------------+------------+
 Create share from snapshot
1. Create a share from a snapshot.
    $ manila create NFS 1 \
--snapshot-id e744ca47-0931-4e81-9d9f-2ead7d7c1640 \ --share-network mysharenetwork \
--name mysharefromsnap
+-----------------------------+--------------------------------------+
| Property                    | Value                                |
+-----------------------------+--------------------------------------+
| status
| share_type_name
| description
| availability_zone
| share_network_id
| share_server_id
| host
| access_rules_status
| snapshot_id
| is_public
| task_state
| snapshot_support
| id
| size
| creating | | default | | None | | nova | | c895fe26-92be-4152-9e6c-f2ad230efb13 | | None | | nosb-devstack@london#LONDON | | active | | e744ca47-0931-4e81-9d9f-2ead7d7c1640 | | False | | None | | True | | e73ebcd3-4764-44f0-9b42-fab5cf34a58b | |1 |
  64
OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    | name
| share_type
| has_replicas
| replication_type
| created_at
| share_proto
| consistency_group_id
| source_cgsnapshot_member_id | None                                 |
| project_id                  | 907004508ef4447397ce6741a8f037c1     |
| metadata                    | {}                                   |
+-----------------------------+--------------------------------------+
| mysharefromsnap                      |
| bf6ada49-990a-47c3-88bc-c0cb31d5c9bf |
| False                                |
| None                                 |
| 2016-03-24T14:41:36.000000           |
| NFS                                  |
| None                                 |
 2. List shares.
    $ manila list +--------------------------------------+-----------------+------+-------------+-------
 →----+-----------+-----------------+-----------------------------+-------------------  →+
| ID | Name | Size | Share Proto |   →Status |IsPublic|ShareTypeName|Host |   →Availability Zone |
+--------------------------------------+-----------------+------+-------------+-------  →----+-----------+-----------------+-----------------------------+-------------------  →+
| 8d8b854b-ec32-43f1-acc0-1b2efa7c3400 | myshare | 1 | NFS  →available | False | default | nosb-devstack@london#LONDON | nova  → |
| e73ebcd3-4764-44f0-9b42-fab5cf34a58b | mysharefromsnap | 1 | NFS  →available | False | default | nosb-devstack@london#LONDON | nova  → |
|

|

+--------------------------------------+-----------------+------+-------------+-------  →----+-----------+-----------------+-----------------------------+-------------------  →+
 3. Show the share created from snapshot.
    $ manila show mysharefromsnap +-----------------------------+-------------------------------------------------------
 →--------+
| Property | Value
 → | +-----------------------------+-------------------------------------------------------
 →--------+ | status
 → |
| share_type_name
 → | | description
 → |
| availability_zone
 → |
| share_network_id
 → |
| export_locations
 → | |
| available
| default
| None
| nova
| c895fe26-92be-4152-9e6c-f2ad230efb13
|
| path = 10.254.0.3:/share-4c00cb49-51d9-478e-abc1-
 →d1853efaf6d3 |
  OpenStack command-line clients 65
 User Guide (Release Version: 15.0.0)
    |
 → |
|
 → |
|
 → |
|
 →d1853efaf6d3 |
|
 →d1853efaf6d3 |
|
 → |
|
 → |
|
 → |
|
 →d1853efaf6d3 |
| share_server_id  → |
| host
 → |
| access_rules_status  → |
| snapshot_id  → | | is_public  → |
| task_state  → |
| snapshot_support  → |
| id
 → |
| size
 → |
| name
 → |
| share_type  → |
| has_replicas  → |
| replication_type  → |
| created_at  → |
| share_proto  → |
| consistency_group_id  → |
| preferred = False
| is_admin_only = False
| id = 5419fb40-04b9-4a52-b08e-19aa1ce13a5c
| share_instance_id = 4c00cb49-51d9-478e-abc1-
| path = 10.0.0.3:/share-4c00cb49-51d9-478e-abc1-
| preferred = False   | is_admin_only = True   | id = 26f55e4c-6edc-4e55-8c55-c62b7db1aa9f   | share_instance_id = 4c00cb49-51d9-478e-abc1-
| 2e9d2d02-883f-47b5-bb98-e053b8d1e683   | nosb-devstack@london#LONDON   | active   | e744ca47-0931-4e81-9d9f-2ead7d7c1640   | False   | None   | True   | e73ebcd3-4764-44f0-9b42-fab5cf34a58b   |1   | mysharefromsnap   | bf6ada49-990a-47c3-88bc-c0cb31d5c9bf   | False   | None   | 2016-03-24T14:41:36.000000   | NFS   | None
| source_cgsnapshot_member_id | None    → |
| project_id | 907004508ef4447397ce6741a8f037c1    → |
| metadata | {}    → |
+-----------------------------+-------------------------------------------------------  →--------+
  66
OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
     Delete share
1. Delete a share.
$ manila delete mysharefromsnap
2. List shares.
         $ manila list +--------------------------------------+-----------------+------+-------------+-------
 →----+-----------+-----------------+-----------------------------+-------------------  →+
| ID | Name | Size | Share Proto |   →Status |IsPublic|ShareTypeName|Host |   →Availability Zone |
+--------------------------------------+-----------------+------+-------------+-------  →----+-----------+-----------------+-----------------------------+-------------------  →+
| 8d8b854b-ec32-43f1-acc0-1b2efa7c3400 | myshare | 1 | NFS  →available | False | default | nosb-devstack@london#LONDON | nova  → |
| e73ebcd3-4764-44f0-9b42-fab5cf34a58b | mysharefromsnap | 1 | NFS  →deleting |False |default |nosb-devstack@london#LONDON|nova  → |
|

|

+--------------------------------------+-----------------+------+-------------+-------  →----+-----------+-----------------+-----------------------------+-------------------  →+
 The share is being deleted.
Delete snapshot
1. List snapshots before deleting.
    $ manila snapshot-list +--------------------------------------+--------------------------------------+-------
 →----+------------+------------+
| ID | Share ID |
 →Status | Name | Share Size | +--------------------------------------+--------------------------------------+-------
 →----+------------+------------+
| e744ca47-0931-4e81-9d9f-2ead7d7c1640 | 8d8b854b-ec32-43f1-acc0-1b2efa7c3400 |
 →available | mysnapshot | 1 | +--------------------------------------+--------------------------------------+-------
 →----+------------+------------+
 2. Delete a snapshot.
$ manila snapshot-delete mysnapshot
3. List snapshots after deleting.
      OpenStack command-line clients 67
 User Guide (Release Version: 15.0.0)
    $ manila snapshot-list
+----+----------+--------+------+------------+
| ID | Share ID | Status | Name | Share Size |
+----+----------+--------+------+------------+
+----+----------+--------+------+------------+
 The snapshot is deleted.
Extend share
1. Extend share.
$ manila extend myshare 2
2. Show the share while it is being extended.
         $ manila show myshare +-----------------------------+-------------------------------------------------------
 →--------+
| Property | Value
 → | +-----------------------------+-------------------------------------------------------
 →--------+ | status
 → |
| share_type_name
 → | | description
 → |
| availability_zone
 → |
| share_network_id
 → |
| export_locations
 → | |
 →45f668732b1d | |
 → | |
 → | |
 → | |
 →45f668732b1d |
 →45f668732b1d | |
 → | |
 → | |
 → | |
|
| extending
| default
| My Manila share
| nova
| c895fe26-92be-4152-9e6c-f2ad230efb13
|
| path = 10.254.0.3:/share-e1c2d35e-fe67-4028-ad7a-
| preferred = False
| is_admin_only = False
| id = b6bd76ce-12a2-42a9-a30a-8a43b503867d
| share_instance_id = e1c2d35e-fe67-4028-ad7a-
| path = 10.0.0.3:/share-e1c2d35e-fe67-4028-ad7a-
| preferred = False
| is_admin_only = True
| id = 6921e862-88bc-49a5-a2df-efeed9acd583
| share_instance_id = e1c2d35e-fe67-4028-ad7a-
 →45f668732b1d
|
  68
OpenStack command-line clients
 3. Show the share after it is extended.
$ manila show myshare +-----------------------------+-------------------------------------------------------
 →--------+
| Property | Value
 → | +-----------------------------+-------------------------------------------------------
User Guide (Release Version: 15.0.0)
    | share_server_id  → |
| host
 → |
| access_rules_status  → |
| snapshot_id  → | | is_public  → |
| task_state  → |
| snapshot_support  → |
| id
 → |
| size
 → |
| name
 → |
| share_type  → |
| has_replicas  → |
| replication_type  → |
| created_at  → |
| share_proto  → |
| consistency_group_id  → |
| 2e9d2d02-883f-47b5-bb98-e053b8d1e683   | nosb-devstack@london#LONDON   | active   | None   | False   | None   | True   | 8d8b854b-ec32-43f1-acc0-1b2efa7c3400   |1   | myshare   | bf6ada49-990a-47c3-88bc-c0cb31d5c9bf   | False   | None   | 2016-03-24T14:15:34.000000   | NFS   | None
| source_cgsnapshot_member_id | None    → |
| project_id | 907004508ef4447397ce6741a8f037c1    → |
| metadata | {}    → |
+-----------------------------+-------------------------------------------------------  →--------+
      →--------+ | status
 → |
| share_type_name
 → | | description
 → |
| availability_zone
 → |
| available
| default
| My Manila share
| nova
  OpenStack command-line clients
69
 User Guide (Release Version: 15.0.0)
    | share_network_id  → |
| export_locations  → |
|
 →45f668732b1d |
|
 → |
|
 → |
|
 → |
|
 →45f668732b1d |
|
 →45f668732b1d |
|
 → |
|
 → |
|
 → |
|
 →45f668732b1d |
| share_server_id  → |
| host
 → |
| access_rules_status  → |
| snapshot_id  → | | is_public  → |
| task_state  → |
| snapshot_support  → |
| id
 → |
| size
 → |
| name
 → |
| share_type  → |
| has_replicas  → |
| replication_type  → |
| created_at  → |
| share_proto  → |
| consistency_group_id  → |
| c895fe26-92be-4152-9e6c-f2ad230efb13
|
| path = 10.254.0.3:/share-e1c2d35e-fe67-4028-ad7a-
| preferred = False
| is_admin_only = False
| id = b6bd76ce-12a2-42a9-a30a-8a43b503867d
| share_instance_id = e1c2d35e-fe67-4028-ad7a-
| path = 10.0.0.3:/share-e1c2d35e-fe67-4028-ad7a-
| preferred = False
| is_admin_only = True
| id = 6921e862-88bc-49a5-a2df-efeed9acd583
| share_instance_id = e1c2d35e-fe67-4028-ad7a-
| 2e9d2d02-883f-47b5-bb98-e053b8d1e683
| nosb-devstack@london#LONDON
| active
| None
| False
| None
| True
| 8d8b854b-ec32-43f1-acc0-1b2efa7c3400
| 2
| myshare
| bf6ada49-990a-47c3-88bc-c0cb31d5c9bf
| source_cgsnapshot_member_id | None  → |
| False
| None
| 2016-03-24T14:15:34.000000
| NFS
| None

  70
OpenStack command-line clients
 Shrink share
1. Shrink a share.
$ manila shrink myshare 1
2. Show the share while it is being shrunk.
User Guide (Release Version: 15.0.0)
    | project_id | 907004508ef4447397ce6741a8f037c1    → |
| metadata | {}    → |
+-----------------------------+-------------------------------------------------------  →--------+
          $ manila show myshare +-----------------------------+-------------------------------------------------------
 →--------+
| Property | Value
 → | +-----------------------------+-------------------------------------------------------
 →--------+ | status
 → |
| share_type_name
 → | | description
 → |
| availability_zone
 → |
| share_network_id
 → |
| export_locations
 → | |
 →45f668732b1d | |
 → | |
 → | |
 → | |
 →45f668732b1d | |
 →45f668732b1d | |
 → | |
 → | |
 → | |
 →45f668732b1d | | share_server_id
| shrinking
| default
| My Manila share
| nova
| c895fe26-92be-4152-9e6c-f2ad230efb13
|
| path = 10.254.0.3:/share-e1c2d35e-fe67-4028-ad7a-
| preferred = False
| is_admin_only = False
| id = b6bd76ce-12a2-42a9-a30a-8a43b503867d
| share_instance_id = e1c2d35e-fe67-4028-ad7a-
| path = 10.0.0.3:/share-e1c2d35e-fe67-4028-ad7a-
| preferred = False
| is_admin_only = True
| id = 6921e862-88bc-49a5-a2df-efeed9acd583
| share_instance_id = e1c2d35e-fe67-4028-ad7a-
| 2e9d2d02-883f-47b5-bb98-e053b8d1e683
 → |
  OpenStack command-line clients 71
 User Guide (Release Version: 15.0.0)
    | host
 → |
| access_rules_status  → |
| snapshot_id  → | | is_public  → |
| task_state  → |
| snapshot_support  → |
| id
 → |
| size
 → |
| name
 → |
| share_type  → |
| has_replicas  → |
| replication_type  → |
| created_at  → |
| share_proto  → |
| consistency_group_id  → |
| nosb-devstack@london#LONDON   | active   | None   | False   | None   | True   | 8d8b854b-ec32-43f1-acc0-1b2efa7c3400   |2   | myshare   | bf6ada49-990a-47c3-88bc-c0cb31d5c9bf   | False   | None   | 2016-03-24T14:15:34.000000   | NFS   | None
| source_cgsnapshot_member_id | None    → |
| project_id | 907004508ef4447397ce6741a8f037c1    → |
| metadata | {}    → |
+-----------------------------+-------------------------------------------------------  →--------+
 3. Show the share after it is being shrunk.
$ manila show myshare +-----------------------------+-------------------------------------------------------
 →--------+
| Property | Value
 → | +-----------------------------+-------------------------------------------------------
     →--------+ | status
 → |
| share_type_name
 → | | description
 → |
| availability_zone
 → |
| share_network_id
| available
| default
| My Manila share
| nova
| c895fe26-92be-4152-9e6c-f2ad230efb13
 → |
  72
OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    | export_locations  → |
|
 →45f668732b1d |
|
 → |
|
 → |
|
 → |
|
 →45f668732b1d |
|
 →45f668732b1d |
|
 → |
|
 → |
|
 → |
|
 →45f668732b1d |
| share_server_id  → |
| host
 → |
| access_rules_status  → |
| snapshot_id  → | | is_public  → |
| task_state  → |
| snapshot_support  → |
| id
 → |
| size
 → |
| name
 → |
| share_type  → |
| has_replicas  → |
| replication_type  → |
| created_at  → |
| share_proto  → |
| consistency_group_id  → |
|
| path = 10.254.0.3:/share-e1c2d35e-fe67-4028-ad7a-
| preferred = False
| is_admin_only = False
| id = b6bd76ce-12a2-42a9-a30a-8a43b503867d
| share_instance_id = e1c2d35e-fe67-4028-ad7a-
| path = 10.0.0.3:/share-e1c2d35e-fe67-4028-ad7a-
| preferred = False
| is_admin_only = True
| id = 6921e862-88bc-49a5-a2df-efeed9acd583
| share_instance_id = e1c2d35e-fe67-4028-ad7a-
| 2e9d2d02-883f-47b5-bb98-e053b8d1e683
| nosb-devstack@london#LONDON
| active
| None
| False
| None
| True
| 8d8b854b-ec32-43f1-acc0-1b2efa7c3400
| 1
| myshare
| bf6ada49-990a-47c3-88bc-c0cb31d5c9bf
| False
| None
| 2016-03-24T14:15:34.000000
| NFS
| None
| source_cgsnapshot_member_id | None    → |
| project_id | 907004508ef4447397ce6741a8f037c1    → |
  OpenStack command-line clients
73
 User Guide (Release Version: 15.0.0)
    | metadata | {}    → |
+-----------------------------+-------------------------------------------------------  →--------+
 Configure access and security for instances
When you launch a virtual machine, you can inject a key pair, which provides SSH access to your instance. For this to work, the image must contain the cloud-init package.
You can create at least one key pair for each project. You can use the key pair for multiple instances that belong to that project. If you generate a key pair with an external tool, you can import it into OpenStack.
Note: A key pair belongs to an individual user, not to a project. To share a key pair across multiple users, each user needs to import that key pair.
If an image uses a static root password or a static key set (neither is recommended), you must not provide a key pair when you launch the instance.
A security group is a named collection of network access rules that are use to limit the types of traffic that have access to instances. When you launch an instance, you can assign one or more security groups to it. If you do not create security groups, new instances are automatically assigned to the default security group, unless you explicitly specify a different security group.
The associated rules in each security group control the traffic to instances in the group. Any incoming traffic that is not matched by a rule is denied access by default. You can add rules to or remove rules from a security group, and you can modify rules for the default and any other security group.
You can modify the rules in a security group to allow access to instances through different ports and protocols. For example, you can modify rules to allow access to instances through SSH, to ping instances, or to allow UDP traffic; for example, for a DNS server running on an instance. You specify the following parameters for rules:
• Source of traffic. Enable traffic to instances from either IP addresses inside the cloud from other group members or from all IP addresses.
• Protocol. Choose TCP for SSH, ICMP for pings, or UDP.
• Destination port on virtual machine. Define a port range. To open a single port only, enter the same value twice. ICMP does not support ports; instead, you enter values to define the codes and types of ICMP traffic to be allowed.
Rules are automatically enforced as soon as you create or modify them.
Note: Instances that use the default security group cannot, by default, be accessed from any IP address outside of the cloud. If you want those IP addresses to access the instances, you must modify the rules for the default security group. Additionally, security groups will automatically drop DHCP responses coming from instances.
You can also assign a floating IP address to a running instance to make it accessible from outside the cloud. See Manage IP addresses.
     74 OpenStack command-line clients
 Add a key pair
You can generate a key pair or upload an existing public key.
1. To generate a key pair, run the following command.
$ openstack keypair create KEY_NAME > MY_KEY.pem
This command generates a key pair with the name that you specify for KEY_NAME, writes the private
key to the .pem file that you specify, and registers the public key to the Nova database.
2. To set the permissions of the .pem file so that only you can read and write to it, run the following
command.
$ chmod 600 MY_KEY.pem
Import a key pair
1. If you have already generated a key pair and the public key is located at ~/.ssh/id_rsa.pub, run the following command to upload the public key.
$ openstack keypair create --public-key ~/.ssh/id_rsa.pub KEY_NAME
This command registers the public key at the Nova database and names the key pair the name that you
specify for KEY_NAME.
2. To ensure that the key pair has been successfully imported, list key pairs as follows:
$ openstack keypair list
Create and manage security groups
1. To list the security groups for the current project, including descriptions, enter the following command:
$ openstack security group list
2. To create a security group with a specified name and description, enter the following command:
$ openstack security group create SECURITY_GROUP_NAME --description GROUP_DESCRIPTION
3. To delete a specified group, enter the following command:
$ openstack security group delete SECURITY_GROUP_NAME
You cannot delete the default security group for a project. Also, you cannot delete a security group that
is assigned to a running instance.
Create and manage security group rules
Modify security group rules with the openstack security group rule commands. Before you begin, source the OpenStack RC file. For details, see Set environment variables using the OpenStack RC file.
User Guide (Release Version: 15.0.0)
                                    Note:
  OpenStack command-line clients 75
 User Guide (Release Version: 15.0.0)
1. To list the rules for a security group, run the following command:
$ openstack security group rule list SECURITY_GROUP_NAME
2. To allow SSH access to the instances, choose one of the following options:
• AllowaccessfromallIPaddresses,specifiedasIPsubnet0.0.0.0/0inCIDRnotation:
• Allow access only from IP addresses from other security groups (source groups) to access the specified port:
3. To allow pinging of the instances, choose one of the following options:
• AllowpingingfromallIPaddresses,specifiedasIPsubnet0.0.0.0/0inCIDRnotation.
This allows access to all codes and all types of ICMP traffic.
• Allow only members of other security groups (source groups) to ping instances.
4. ToallowaccessthroughaUDPport,suchasallowingaccesstoaDNSserverthatrunsonaVM,choose one of the following options:
• AllowUDPaccessfromIPaddresses,specifiedasIPsubnet0.0.0.0/0inCIDRnotation.
• Allow only IP addresses from other security groups (source groups) to access the specified port.
Delete a security group rule
To delete a security group rule, specify the ID of the rule.
$ openstack security group rule delete RULE_ID
Launch instances
Instances are virtual machines that run inside the cloud.
Before you can launch an instance, gather the following parameters:
         $ openstack security group rule create SECURITY_GROUP_NAME \ --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0
     $ openstack security group rule create SECURITY_GROUP_NAME \ --protocol tcp --dst-port 22:22 --remote-group SOURCE_GROUP_NAME
     $ openstack security group rule create --protocol icmp \ SECURITY_GROUP_NAME
     $ openstack security group rule create --protocol icmp \ --remote-group SOURCE_GROUP_NAME SECURITY_GROUP
     $ openstack security group rule create --protocol udp \ --dst-port 53:53 SECURITY_GROUP
     $ openstack security group rule create --protocol udp \ --dst-port 53:53 --remote-group SOURCE_GROUP_NAME SECURITY_GROUP
       76 OpenStack command-line clients
 • The instance source can be an image, snapshot, or block storage volume that contains an image or snapshot.
• A name for your instance.
• The flavor for your instance, which defines the compute, memory, and storage capacity of nova com- puting instances. A flavor is an available hardware configuration for a server. It defines the size of a virtual server that can be launched.
• Any user data files. A user data file is a special key in the metadata service that holds a file that cloud- aware applications in the guest instance can access. For example, one application that uses user data is the cloud-init system, which is an open-source package from Ubuntu that is available on various Linux distributions and that handles early initialization of a cloud instance.
• Access and security credentials, which include one or both of the following credentials:
– A key pair for your instance, which are SSH credentials that are injected into images when they are launched. For the key pair to be successfully injected, the image must contain the cloud-init package. Create at least one key pair for each project. If you already have generated a key pair with an external tool, you can import it into OpenStack. You can use the key pair for multiple instances that belong to that project.
– A security group that defines which incoming network traffic is forwarded to instances. Security groups hold a set of firewall policies, known as security group rules.
• If needed, you can assign a floating (public) IP address to a running instance.
• You can also attach a block storage device, or volume, for persistent storage.
Instances that use the default security group cannot, by default, be accessed from any IP address outside of the cloud. If you want those IP addresses to access the instances, you must modify the rules for the default security group.
You can also assign a floating IP address to a running instance to make it accessible from outside the cloud. See Manage IP addresses.
After you gather the parameters that you need to launch an instance, you can launch it from an image or a volume. You can launch an instance directly from one of the available OpenStack images or from an image that you have copied to a persistent volume. The OpenStack Image service provides a pool of images that are accessible to members of different projects.
Gather parameters to launch an instance
Before you begin, source the OpenStack RC file.
1. Create a flavor.
$ openstack flavor create --ram 512 --disk 1 --vcpus 1 m1.tiny
2. List the available flavors.
$ openstack flavor list
Note the ID of the flavor that you want to use for your instance:
Note:
User Guide (Release Version: 15.0.0)
             OpenStack command-line clients 77
 User Guide (Release Version: 15.0.0)
    +-----+-----------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is_Public | +-----+-----------+-------+------+-----------+-------+-----------+ |1 |m1.tiny | 512| 1| 0| 1|True | |2 |m1.small | 2048| 20| 0| 1|True | |3 |m1.medium| 4096| 40| 0| 2|True | |4 |m1.large | 8192| 80| 0| 4|True | |5 |m1.xlarge|16384| 160| 0| 8|True | +-----+-----------+-------+------+-----------+-------+-----------+
 3. List the available images.
$ openstack image list
Note the ID of the image from which you want to boot your instance:
You can also filter the image list by using grep to find a specific image, as follows:
4. List the available security groups.
$ openstack security group list
Note: If you are an admin user, this command will list groups for all tenants. Note the ID of the security group that you want to use for your instance:
         +--------------------------------------+---------------------------------+--------+
| ID                                   | Name                            | Status |
+--------------------------------------+---------------------------------+--------+
| 397e713c-b95b-4186-ad46-6126863ea0a9 | cirros-0.3.5-x86_64-uec         | active |
| df430cc2-3406-4061-b635-a51c16e488ac | cirros-0.3.5-x86_64-uec-kernel  | active |
| 3cf852bd-2332-48f4-9ae4-7d926d50945e | cirros-0.3.5-x86_64-uec-ramdisk | active |
+--------------------------------------+---------------------------------+--------+
                +--------------------------------------+---------+------------------------+-----------  →-----------------------+
| ID | Name | Description | Project    → |
+--------------------------------------+---------+------------------------+-----------  →-----------------------+
| b0d78827-0981-45ef-8561-93aee39bbd9f | default | Default security group |   →5669caad86a04256994cdf755df4d3c1 |
| ec02e79e-83e1-48a5-86ad-14ab9a8c375f | default | Default security group |   →1eaaf6ede7a24e78859591444abf314a |
+--------------------------------------+---------+------------------------+-----------  →-----------------------+
 78
OpenStack command-line clients
$ openstack image list | grep 'kernel'
| df430cc2-3406-4061-b635-a51c16e488ac | cirros-0.3.5-x86_64-uec-kernel | active |
 If you have not created any security groups, you can assign the instance to only the default security group. You can view rules for a specified security group:

 User Guide (Release Version: 15.0.0)
    $ openstack security group rule list default
5. List the available key pairs, and note the key pair name that you use for SSH access.
$ openstack keypair list
Launch an instance
You can launch an instance from various sources.
Launch an instance from an image
Follow the steps below to launch an instance from an image.
1. After you gather required parameters, run the following command to launch an instance. Specify the server name, flavor ID, and image ID.
Optionally, you can provide a key name for access control and a security group for security. You can also include metadata key and value pairs. For example, you can add a description for your server by providing the --property description="My Server" parameter.
You can pass user data in a local file at instance launch by using the --user-data USER-DATA-FILE parameter.
Important: If you boot an instance with an INSTANCE_NAME greater than 63 characters, Compute truncates it automatically when turning it into a host name to ensure the correct work of dnsmasq. The corresponding warning is written into the neutron-dnsmasq.log file.
The following command launches the MyCirrosServer instance with the m1.small flavor (ID of 1), cirros-0.3.2-x86_64-uec image (ID of 397e713c-b95b-4186-ad46-6126863ea0a9), default security group, KeyPair01 key, and a user data file called cloudinit.file:
Depending on the parameters that you provide, the command returns a list of server properties.
          $ openstack server create --flavor FLAVOR_ID --image IMAGE_ID --key-name KEY_NAME \ --user-data USER_DATA_FILE --security-group SEC_GROUP_NAME --property KEY=VALUE \ INSTANCE_NAME
       $ openstack server create --flavor 1 --image 397e713c-b95b-4186-ad46-6126863ea0a9 \ --security-group default --key-name KeyPair01 --user-data cloudinit.file \ myCirrosServer
     +--------------------------------------+----------------------------------------------  →-+
| Field | Value    → |
+--------------------------------------+----------------------------------------------  →-+
| OS-DCF:diskConfig | MANUAL    → |
| OS-EXT-AZ:availability_zone |    → |
  OpenStack command-line clients 79
 User Guide (Release Version: 15.0.0)
    | OS-EXT-SRV-ATTR:host | None    → |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None    → |
| OS-EXT-SRV-ATTR:instance_name  → |
| OS-EXT-STS:power_state  → |
| OS-EXT-STS:task_state  → |
| OS-EXT-STS:vm_state  → |
| OS-SRV-USG:launched_at  → |
| OS-SRV-USG:terminated_at  → |
| accessIPv4  → |
| accessIPv6  → |
| addresses  → |
| adminPass  → |
| config_drive  → |
| created  → |
| flavor  → |
| hostId  → |
| id  → |
| image  →6126863ea0a9) |
| key_name  → |
| name  → |
|
| NOSTATE
| scheduling
| building
| None
| None
|
|
|
| E4Ksozt4Efi8
|
| 2016-11-30T14:48:05Z
| m1.tiny
|
| 89015cc9-bdf1-458a-8518-fdca2b4a5785
| cirros (397e713c-b95b-4186-ad46-
| KeyPair01
| myCirrosServer
| os-extended-volumes:volumes_attached | []    → |
| progress  → |
| project_id  → |
| properties  → |
| security_groups  → |
| status  → |
| updated  → |
| user_id  → |
| metadata  → |
| 0
| 5669caad86a04256994cdf755df4d3c1
|
| [{u'name': u'default'}]
| BUILD
| 2016-11-30T14:48:05Z
| c36cec73b0e44876a4478b1e6cd749bb
| {u'KEY': u'VALUE'}
  80
OpenStack command-line clients
     +--------------------------------------+----------------------------------------------  →-+
 A status of BUILD indicates that the instance has started, but is not yet online.
A status of ACTIVE indicates that the instance is active.
2. CopytheserverIDvaluefromtheidfieldintheoutput.UsetheIDtogetserverdetailsortodeleteyour
server.
3. Copy the administrative password value from the adminPass field. Use the password to log in to your server.
Note: You can also place arbitrary local files into the instance file system at creation time by using the --file <dst-path=src-path> option. You can store up to five files. For example, if you have a special authorized keys file named special_authorized_keysfile that you want to put on the instance rather than using the regular SSH key injection, you can use the --file option as shown in the following example.
4. Check if the instance is online.
$ openstack server list
The list shows the ID, name, status, and private (and if assigned, public) IP addresses for all instances in
the project to which you belong:
User Guide (Release Version: 15.0.0)
      $ openstack server create --image ubuntu-cloudimage --flavor 1 vm-name \ --file /root/.ssh/authorized_keys=special_authorized_keysfile
          +-------------+----------------------+--------+------------+-------------+------------  →------+------------+
| ID | Name | Status | Task State | Power State | Networks    → | Image Name |
+-------------+----------------------+--------+------------+-------------+------------  →------+------------+
| 84c6e57d... | myCirrosServer | ACTIVE | None | Running | private=10.  →0.0.3 | cirros |
| 8a99547e... | myInstanceFromVolume | ACTIVE | None | Running | private=10.  →0.0.4 | centos |
+-------------+----------------------+--------+------------+-------------+------------  →------+------------+
 If the status for the instance is ACTIVE, the instance is online.
5. Toviewtheavailableoptionsfortheopenstackserverlistcommand,runthefollowingcommand:
$ openstack help server list
Note: If you did not provide a key pair, security groups, or rules, you can access the instance only from inside the cloud through VNC. Even pinging the instance is not possible.
        OpenStack command-line clients 81
 User Guide (Release Version: 15.0.0)
Launch an instance from a volume
You can boot instances from a volume instead of an image.
To complete these tasks, use these parameters on the openstack server create command:
  Task
  openstack server create pa- rameter
 Information
 Boot an instance from an image and attach a non-bootable vol- ume.
  --block-device
 Boot instance from image and attach non- bootable volume
 Create a volume from an image and boot an instance from that volume.
  --block-device
 Create volume from image and boot instance
 Boot from an existing source im- age, volume, or snapshot.
  --block-device
 Create volume from image and boot instance
     Note: To attach a volume to a running instance, see Attach a volume to an instance.
Boot instance from image and attach non-bootable volume
Create a non-bootable volume and attach that volume to an instance that you boot from an image.
To create a non-bootable volume, do not create it from an image. The volume must be entirely empty with no partition table and no file system.
1. Create a non-bootable volume.
     $ openstack volume create --size 8 my-volume +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+
| []                                   |
| nova                                 |
| false                                |
| attachments
| availability_zone
| bootable
| consistencygroup_id | None                                 |
| created_at
| description
| encrypted
| id
| migration_status
| multiattach
| name
| properties
| 2016-11-25T10:37:08.850997           |
| None                                 |
| False                                |
| b8f7bbec-6274-4cd7-90e7-60916a5e75d4 |
| None                                 |
| False                                |
| my-volume                            |
|                                      |
| replication_status  | disabled                             |
| size
| snapshot_id
| source_volid
| status
| type
| updated_at
| user_id
+---------------------+--------------------------------------+
| 8                                    |
| None                                 |
| None                                 |
| creating                             |
| None                                 |
| None                                 |
| 0678735e449149b0a42076e12dd54e28     |
  82
OpenStack command-line clients
 2. List volumes.
3. Boot an instance from an image and attach the empty volume to the instance.
User Guide (Release Version: 15.0.0)
    $ openstack volume list +--------------------------------------+--------------+-----------+------+------------
 →-+
| ID | Display Name | Status | Size | Attached
 →to | +--------------------------------------+--------------+-----------+------+------------
 →-+
| b8f7bbec-6274-4cd7-90e7-60916a5e75d4 | my-volume | available | 8 |
 → | +--------------------------------------+--------------+-----------+------+------------
 →-+
     $ openstack server create --flavor 2 --image 98901246-af91-43d8-b5e6-a4506aa8f369 \ --block-device-mapping \ myVolumeAttach=b8f7bbec-6274-4cd7-90e7-60916a5e75d4:volume:8:false \ myInstanceWithVolume
+--------------------------------------+--------------------------------------------+
| Field                                | Value                                      |
+--------------------------------------+--------------------------------------------+
| MANUAL                                     |
| nova                                       |
| -                                          |
| OS-DCF:diskConfig
| OS-EXT-AZ:availability_zone
| OS-EXT-SRV-ATTR:host
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                          |
| OS-EXT-SRV-ATTR:instance_name
| OS-EXT-STS:power_state
| OS-EXT-STS:task_state
| OS-EXT-STS:vm_state
| instance-00000004                          |
| 0                                          |
| scheduling                                 |
| building                                   |
| -                                          |
| -                                          |
|                                            |
|                                            |
| ZaiYeC8iucgU                               |
|                                            |
| 2014-05-09T16:34:50Z                       |
| m1.small (2)                               |
|                                            |
| 1e1797f3-1662-49ff-ae8c-a77e82ee1571       |
| cirros-0.3.5-x86_64-uec (98901246-af91-... |
| -                                          |
| {}                                         |
| myInstanceWithVolume                       |
| OS-SRV-USG:launched_at
| OS-SRV-USG:terminated_at
| accessIPv4
| accessIPv6
| adminPass
| config_drive
| created
| flavor
| hostId
| id
| image
| key_name
| metadata
| name
| os-extended-volumes:volumes_attached | [{"id": "b8f7bbec-6274-4cd7-90e7-60916a... |
| progress
| security_groups
| status
| tenant_id
| updated
| user_id
+--------------------------------------+--------------------------------------------+
| 0                                          |
| default                                    |
| BUILD                                      |
| ccef9e62b1e645df98728fb2b3076f27           |
| 2014-05-09T16:34:51Z                       |
| fef060ae7bfd4024b3edb97dff59017a           |
  OpenStack command-line clients 83
 User Guide (Release Version: 15.0.0)
Create volume from image and boot instance
You can create a volume from an existing image, volume, or snapshot. This procedure shows you how to create a volume from an image, and use the volume to boot an instance.
1.
List the available images.
    $ openstack image list +-----------------+---------------------------------+--------+ | ID | Name | Status | +-----------------+---------------------------------+--------+ | 484e05af-a14... | Fedora-x86_64-20-20131211.1-sda | active | | 98901246-af9... | cirros-0.3.5-x86_64-uec | active | | b6e95589-7eb... | cirros-0.3.5-x86_64-uec-kernel | active | | c90893ea-e73... | cirros-0.3.5-x86_64-uec-ramdisk | active | +-----------------+---------------------------------+--------+
 Note the ID of the image that you want to use to create a volume.
If you want to create a volume to a specific storage backend, you need to use an image which has cin- der_img_volume_type property. In this case, a new volume will be created as storage_backend1 volume type.
    $ openstack image show 98901246-af9d-4b61-bea8-09cc6dc41829 +------------------+------------------------------------------------------+ | Field | Value | +------------------+------------------------------------------------------+ | checksum | ee1eca47dc88f4879d8a229cc70a07c6 | | container_format | bare |
| created_at
| disk_format
| file
| id
| min_disk
| min_ram
| name
| owner
| protected
| schema
| size
| status
| 2016-10-08T14:59:05Z                                 |
| qcow2                                                |
| /v2/images/9fef3b2d-c35d-4b61-bea8-09cc6dc41829/file |
| 98901246-af9d-4b61-bea8-09cc6dc41829                 |
| 0                                                    |
| 0                                                    |
| cirros-0.3.5-x86_64-uec                              |
| 8d8ef3cdf2b54c25831cbb409ad9ae86                     |
| False                                                |
| /v2/schemas/image                                    |
| 13287936                                             |
| active                                               |
|                                                      |
| 2016-10-19T09:12:52Z                                 |
| None                                                 |
| public                                               |
| tags
| updated_at
| virtual_size
| visibility
+------------------+------------------------------------------------------+
 2.
List the available flavors.
    $ openstack flavor list +-----+-----------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is_Public | +-----+-----------+-------+------+-----------+-------+-----------+
|1 |m1.tiny | 512| 1| |2 |m1.small | 2048| 20| |3 |m1.medium| 4096| 40| |4 |m1.large | 8192| 80|
0| 1|True | 0| 1|True | 0| 2|True | 0| 4|True |
  84
OpenStack command-line clients
     |5 |m1.xlarge|16384| 160| 0| 8|True | +-----+-----------+-------+------+-----------+-------+-----------+
 Note the ID of the flavor that you want to use to create a volume.
3. Tocreateabootablevolumefromanimageandlaunchaninstancefromthisvolume,usethe--block-
device parameter. For example:
The parameters are:
• --flavor The flavor ID or name.
• --block-devicesource=SOURCE,id=ID,dest=DEST,size=SIZE,shutdown=PRESERVE,bootindex=INDEX
source=SOURCE The type of object used to create the block device. Valid values are volume, snapshot, image, and blank.
id=ID The ID of the source object.
dest=DEST The type of the target virtual device. Valid values are volume and local.
size=SIZE The size of the volume that is created.
shutdown={preserve|remove} What to do with the volume when the instance is deleted. pre- serve does not delete the volume. remove deletes the volume.
bootindex=INDEX Orders the boot disks. Use 0 to boot from this volume.
• NAME. The name for the server.
4. Createabootablevolumefromanimage.Cindermakesavolumebootablewhen--imageparameteris passed.
$ openstack volume create --image IMAGE_ID --size SIZE_IN_GB bootable_volume
5. Create a VM from previously created bootable volume. The volume is not deleted when the instance is
terminated.
User Guide (Release Version: 15.0.0)
    $ openstack server create --flavor FLAVOR --block-device \ source=SOURCE,id=ID,dest=DEST,size=SIZE,shutdown=PRESERVE,bootindex=INDEX \ NAME
          $ openstack server create --flavor 2 --volume VOLUME_ID \
--block-device source=volume,id=$VOLUME_ID,dest=volume,size=10,shutdown=preserve,
 →bootindex=0 \ myInstanceFromVolume
+--------------------------------------+--------------------------------+
| Field                                | Value                          |
+--------------------------------------+--------------------------------+
| OS-EXT-STS:task_state
| image
|
| OS-EXT-STS:vm_state
| OS-EXT-SRV-ATTR:instance_name
| OS-SRV-USG:launched_at
| flavor
| id
| security_groups
| scheduling                     |
| Attempt to boot from volume    |
| - no image supplied            |
| building                       |
| instance-00000003              |
| None                           |
| m1.small                       |
| 2e65c854-dba9-4f68-8f08-fe3... |
| [{u'name': u'default'}]        |
  OpenStack command-line clients 85
 User Guide (Release Version: 15.0.0)
    | user_id
| OS-DCF:diskConfig
| accessIPv4
| accessIPv6
| progress
| OS-EXT-STS:power_state
| OS-EXT-AZ:availability_zone
| config_drive
| status
| updated
| hostId
| OS-EXT-SRV-ATTR:host
| OS-SRV-USG:terminated_at
| key_name
| OS-EXT-SRV-ATTR:hypervisor_hostname  | None                           |
| name
| adminPass
| tenant_id
| created
| os-extended-volumes:volumes_attached | [{"id": "2fff50ab..."}]        |
| metadata                             | {}                             |
+--------------------------------------+--------------------------------+
| 352b37f5c89144d4ad053413926... |
| MANUAL                         |
|                                |
|                                |
| 0                              |
| 0                              |
| nova                           |
|                                |
| BUILD                          |
| 2014-02-02T13:29:54Z           |
|                                |
| None                           |
| None                           |
| None                           |
| myInstanceFromVolume           |
| TzjqyGsRcJo9                   |
| f7ac731cc11f40efbc03a9f9e1d... |
| 2014-02-02T13:29:53Z           |
 6. ListvolumestoseethebootablevolumeanditsattachedmyInstanceFromVolumeinstance.
    $ openstack volume list +---------------------+-----------------+--------+------+-----------------------------
 →----+
| ID | Display Name | Status | Size | Attached to
 → | +---------------------+-----------------+--------+------+-----------------------------
 →----+
| c612f739-8592-44c4- | bootable_volume | in-use | 10 | Attached to
 →myInstanceFromVolume|
| b7d4-0fee2fe1da0c | | | | on /dev/vda
 → | +---------------------+-----------------+--------+------+-----------------------------
 →----+
 Attach swap or ephemeral disk to an instance
To attach swap or ephemeral disk to an instance, you need create new flavor first. This procedure shows you how to boot an instance with a 512 MB swap disk and 2 GB ephemeral disk.
1. Create a new flavor.
Note: The flavor defines the maximum swap and ephemeral disk size. You cannot exceed these maxi- mum values.
      86
OpenStack command-line clients
$ openstack flavor create --vcpus 1 --ram 64 --disk 1 \ --swap 512 --ephemeral 2 my_flavor
 2. Create a server with 512 MB swap disk and 2 GB ephemeral disk.

     $ openstack server create --image IMAGE_ID --flavor \ my_flavor NAME
 Launch an instance using ISO image
Boot an instance from an ISO image
OpenStack supports booting instances using ISO images. But before you make such instances functional, use the openstack server create command with the following parameters to boot an instance:
OpenStack command-line clients 87
User Guide (Release Version: 15.0.0)
    $ openstack server create --image ubuntu-14.04.2-server-amd64.iso \ --nic net-id = NETWORK_UUID \
--flavor 2 INSTANCE_NAME
+--------------------------------------+--------------------------------------------+
| Field                                | Value                                      |
+--------------------------------------+--------------------------------------------+
| MANUAL                                     |
| nova                                       |
| -                                          |
| OS-DCF:diskConfig
| OS-EXT-AZ:availability_zone
| OS-EXT-SRV-ATTR:host
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                          |
| OS-EXT-SRV-ATTR:instance_name
| OS-EXT-STS:power_state
| OS-EXT-STS:task_state
| OS-EXT-STS:vm_state
| instance-00000004                          |
| 0                                          |
| scheduling                                 |
| building                                   |
| -                                          |
| -                                          |
|                                            |
|                                            |
| ZaiYeC8iucgU                               |
|                                            |
| 2015-06-01T16:34:50Z                       |
| m1.small (2)                               |
|                                            |
| 1e1797f3-1662-49ff-ae8c-a77e82ee1571       |
| ubuntu-14.04.2-server-amd64.iso            |
| -                                          |
| {}                                         |
| INSTANCE_NAME                              |
| OS-SRV-USG:launched_at
| OS-SRV-USG:terminated_at
| accessIPv4
| accessIPv6
| adminPass
| config_drive
| created
| flavor
| hostId
| id
| image
| key_name
| metadata
| name
| os-extended-volumes:volumes_attached | []                                         |
| progress
| security_groups
| status
| tenant_id
| updated
| user_id
+--------------------------------------+--------------------------------------------+
| 0                                          |
| default                                    |
| BUILD                                      |
| ccef9e62b1e645df98728fb2b3076f27           |
| 2014-05-09T16:34:51Z                       |
| fef060ae7bfd4024b3edb97dff59017a           |
 In this command, ubuntu-14.04.2-server-amd64.iso is the ISO image, and INSTANCE_NAME is the name of the new instance. NETWORK_UUID is a valid network id in your system.
Create a bootable volume for the instance to reside on after shutdown. 1. Create the volume:

 User Guide (Release Version: 15.0.0)
    $ openstack volume create \ --size <SIZE_IN_GB> \ --bootable VOLUME_NAME
 2. Attach the instance to the volume:
Note: You need the Block Storage service to preserve the instance after shutdown. The --block-device argument, used with the legacy nova boot, will not work with the OpenStack openstack server create command. Instead, the openstack volume create and openstack server add volume commands cre- ate persistent storage.
After the instance is successfully launched, connect to the instance using a remote console and follow the instructions to install the system as using ISO images on regular computers. When the installation is finished and system is rebooted, the instance asks you again to install the operating system, which means your instance is not usable. If you have problems with image creation, please check the Virtual Machine Image Guide for reference.
Make the instances booted from ISO image functional
Now complete the following steps to make your instances created using ISO image actually functional.
    $ openstack server add volume INSTANCE_NAME \ VOLUME_NAME \
--device /dev/vda
   1.
2.
Delete the instance using the following command.
$ openstack server delete INSTANCE_NAME Afteryoudeletetheinstance,thesystemyouhavejustinstalledusingyourISOimageremains,because
the parameter shutdown=preserve was set, so run the following command.
         $ openstack volume list +--------------------------+-------------------------+-----------+------+-------------
 →+
| ID | Display Name | Status | Size | Attached to
 →| +--------------------------+-------------------------+-----------+------+-------------
 →+
| 8edd7c97-1276-47a5-9563- |dc01d873-d0f1-40b6-bfcc- | available | 10 |
 →|
| 1025f4264e4f | 26a8d955a1d9-blank-vol | | |
 →| +--------------------------+-------------------------+-----------+------+-------------
 →+
 3.
You get a list with all the volumes in your system. In this list, you can find the volume that is attached to your ISO created instance, with the false bootable property.
Upload the volume to glance.
 88
OpenStack command-line clients
 The SOURCE_VOLUME is the UUID or a name of the volume that is attached to your ISO created instance, and the IMAGE_NAME is the name that you give to your new image.
4. After the image is successfully uploaded, you can use the new image to boot instances.
The instances launched using this image contain the system that you have just installed using the ISO image.
Manage instances and hosts
Instances are virtual machines that run inside the cloud on physical compute nodes. The Compute service manages instances. A host is the node on which a group of instances resides.
This section describes how to perform the different tasks involved in instance management, such as adding floating IP addresses, stopping and starting instances, and terminating instances. This section also discusses node management tasks.
Manage IP addresses
Each instance has a private, fixed IP address and can also have a public, or floating IP address. Private IP addresses are used for communication between instances, and public addresses are used for communication with networks outside the cloud, including the Internet.
When you launch an instance, it is automatically assigned a private IP address that stays the same until you explicitly terminate the instance. Rebooting an instance has no effect on the private IP address.
A pool of floating IP addresses, configured by the cloud administrator, is available in OpenStack Compute. The project quota defines the maximum number of floating IP addresses that you can allocate to the project. After you allocate a floating IP address to a project, you can:
• Associate the floating IP address with an instance of the project. Only one floating IP address can be allocated to an instance at any given time.
• Disassociate a floating IP address from an instance in the project.
• Delete a floating IP from the project which automatically deletes that IP’s associations. Use the openstack commands to manage floating IP addresses.
List floating IP address information
To list all pools that provide floating IP addresses, run:
User Guide (Release Version: 15.0.0)
    $ openstack image create --volume SOURCE_VOLUME IMAGE_NAME $ openstack image list +-------------------+------------+--------+
| ID | Name | Status | +-------------------+------------+--------+
| 74303284-f802-... | IMAGE_NAME | active |
+-------------------+------------+--------+
     $ openstack floating ip pool list +--------+
|name |
  OpenStack command-line clients 89
 User Guide (Release Version: 15.0.0)
    +--------+ | public | |test | +--------+
  Note: If this list is empty, the cloud administrator must configure a pool of floating IP addresses. To list all floating IP addresses that are allocated to the current project, run:
For each floating IP address that is allocated to the current project, the command outputs the floating IP address, the ID for the instance to which the floating IP address is assigned, the associated fixed IP address, and the pool from which the floating IP address was allocated.
Associate floating IP addresses
You can assign a floating IP address to a project and to an instance.
1. Run the following command to allocate a floating IP address to the current project. By default, the floating IP address is allocated from the public pool. The command outputs the allocated IP address:
     $ openstack floating ip list +--------------------------------------+---------------------+------------------+------+ | ID | Floating IP Address | Fixed IP Address | Port | +--------------------------------------+---------------------+------------------+------+
| 760963b2-779c-4a49-a50d-f073c1ca5b9e | 172.24.4.228
| 89532684-13e1-4af3-bd79-f434c9920cc3 | 172.24.4.235
| ea3ebc6d-a146-47cd-aaa8-35f06e1e8c3d | 172.24.4.229
+--------------------------------------+---------------------+------------------+------+
| None
| None
| None
| None |
| None |
| None |
     $ openstack floating ip create public +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+
| created_at
| description
| fixed_ip_address
| floating_ip_address | 172.24.4.230                        |
| floating_network_id | c213f520-aade-42eb-8bf1-6826505d74bb |
| 2017-03-30T12:35:25Z                 |
|                                      |
| None                                 |
| id
| name
| port_id
| project_id
| revision_number
| router_id
| status
| updated_at
+---------------------+--------------------------------------+
| 1e777f9e-4fc8-4df8-be6f-89f5caba3c0f |
| None                                 |
| None                                 |
| b3abf186ac64462e85741315376e9ca7     |
| 1                                    |
| None                                 |
| DOWN                                 |
| 2017-03-30T12:35:25Z                 |
 2. List all project instances with which a floating IP address could be associated.
    $ openstack server list +---------------------+------+---------+------------+-------------+------------------
 →+------------+
  90 OpenStack command-line clients
 3. Associate an IP address with an instance in the project, as follows:
$ openstack server add floating ip INSTANCE_NAME_OR_ID FLOATING_IP_ADDRESS For example:
$ openstack server add floating ip VM1 172.24.4.225 The instance is now associated with two IP addresses:
User Guide (Release Version: 15.0.0)
    | ID | Name | Status | Task State | Power State | Networks    →| Image Name |
+---------------------+------+---------+------------+-------------+------------------  →+------------+
| d5c854f9-d3e5-4f... | VM1 | ACTIVE | - | Running | private=10.0.0.3   →| cirros |
| 42290b01-0968-43... | VM2 | SHUTOFF | - | Shutdown | private=10.0.0.4   →| centos |
+---------------------+------+---------+------------+-------------+------------------  →+------------+
               $ openstack server list +------------------+------+--------+------------+-------------+-----------------------
 →--------+------------+
| ID | Name | Status | Task State | Power State | Networks
 → | Image Name | +------------------+------+--------+------------+-------------+-----------------------
 →--------+------------+
| d5c854f9-d3e5... | VM1 | ACTIVE | - | Running | private=10.0.0.3, 172.
 →24.4.225| cirros |
| 42290b01-0968... | VM2 | SHUTOFF| - | Shutdown | private=10.0.0.4
 → | centos | +------------------+------+--------+------------+-------------+-----------------------
 →--------+------------+
 After you associate the IP address and configure security group rules for the instance, the instance is publicly available at the floating IP address.
Note: The openstack server command does not allow users to associate a floating IP address with a specific fixed IP address using the optional --fixed-address parameter, which legacy commands required as an argument.
Disassociate floating IP addresses
To disassociate a floating IP address from an instance:
$ openstack server remove floating ip INSTANCE_NAME_OR_ID FLOATING_IP_ADDRESS To remove the floating IP address from a project:
$ openstack floating ip delete FLOATING_IP_ADDRESS
             OpenStack command-line clients 91
 User Guide (Release Version: 15.0.0)
The IP address is returned to the pool of IP addresses that is available for all projects. If the IP address is still associated with a running instance, it is automatically disassociated from that instance.
Change the size of your server
Change the size of a server by changing its flavor.
1. Showinformationaboutyourserver,includingitssize,whichisshownasthevalueoftheflavorproperty:
    $ openstack server show myCirrosServer +--------------------------------------+----------------------------------------------
 →------------+
| Field | Value
 → | +--------------------------------------+----------------------------------------------
 →------------+
| OS-DCF:diskConfig
 → |
| OS-EXT-AZ:availability_zone
 → |
| OS-EXT-SRV-ATTR:host
| AUTO
| nova
| node-7.domain.tld
 → |
| OS-EXT-SRV-ATTR:hypervisor_hostname | node-7.domain.tld
 → |
| OS-EXT-SRV-ATTR:instance_name
 → |
| OS-EXT-STS:power_state
 → |
| OS-EXT-STS:task_state
 → |
| OS-EXT-STS:vm_state
 → |
| OS-SRV-USG:launched_at
 → |
| OS-SRV-USG:terminated_at
 → | | accessIPv4
 → | | accessIPv6
 → | | addresses
 → | | config_drive
 → | | created
 → | | flavor
 → | | hostId
| instance-000000f3
| 1
| None
| active
| 2016-10-26T01:13:15.000000
| None
|
|
| admin_internal_net=192.168.111.139
| True
| 2016-10-26T01:12:38Z
| m1.small (2)
|   →d815539ce1a8fad3d597c3438c13f1229d3a2ed66d1a75447845a2f3 |
| id
 → |
| image
 →df7a9b8f538f) |
| key_name
 → |
| name
| 67bc9a9a-5928-47c4-852c-3631fef2a7e8
| cirros-test (dc5ec4b8-5851-4be8-98aa-
| None
| myCirrosServer
  → |
 92
OpenStack command-line clients
 3. Toresizetheserver,usetheopenstackserverresizecommandandaddtheserverIDornameand the new flavor. For example:
$ openstack server resize --flavor 4 myCirrosServer
Note: Bydefault,theopenstackserverresizecommandgivestheguestoperatingsystemachance to perform a controlled shutdown before the instance is powered off and the instance is resized. The shutdown behavior is configured by the shutdown_timeout parameter that can be set in the nova. conf file. Its value stands for the overall period (in seconds) a guest operating system is allowed to complete the shutdown. The default timeout is 60 seconds. See Description of Compute configuration options for details.
The timeout value can be overridden on a per image basis by means of os_shutdown_timeout that is an image metadata setting allowing different types of operating systems to specify how much time they need to shut down cleanly.
4. Show the status for your server.
User Guide (Release Version: 15.0.0)
    | os-extended-volumes:volumes_attached | []    → |
| progress
 → |
| project_id
 → |
| properties
 → |
| security_groups  → |
| status
 → |
| updated
 → |
| user_id
 → |
| 0
| c08367f25666480f9860c6a0122dfcc4
|
| [{u'name': u'default'}]
| ACTIVE
| 2016-10-26T01:13:00Z
| 0209430e30924bf9b5d8869990234e44
+--------------------------------------+----------------------------------------------  →------------+
 The size (flavor) of the server is m1.small (2).
2. List the available flavors with the following command:
    $ openstack flavor list +-----+-----------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is_Public | +-----+-----------+-------+------+-----------+-------+-----------+
|1 |m1.tiny | 512| 1|
|2 |m1.small | 2048| 20|
|3 |m1.medium| 4096| 40|
|4 |m1.large | 8192| 80|
|5 |m1.xlarge|16384| 160| +-----+-----------+-------+------+-----------+-------+-----------+
0| 1|True | 0| 1|True | 0| 2|True | 0| 4|True | 0| 8|True |
            $ openstack server list +----------------------+----------------+--------+------------------------------------
 →-----+
  OpenStack command-line clients 93
 User Guide (Release Version: 15.0.0)
    | ID | Name | Status | Networks    → |
+----------------------+----------------+--------+------------------------------------  →-----+
| 67bc9a9a-5928-47c... | myCirrosServer | RESIZE | admin_internal_net=192.168.111.139   → |
+----------------------+----------------+--------+------------------------------------  →-----+
 When the resize completes, the status becomes VERIFY_RESIZE. 5. Confirm the resize,for example:
$ openstack server resize --confirm 67bc9a9a-5928-47c4-852c-3631fef2a7e8 The server status becomes ACTIVE.
6. If the resize fails or does not work as expected, you can revert the resize. For example:
$ openstack server resize --revert 67bc9a9a-5928-47c4-852c-3631fef2a7e8 The server status becomes ACTIVE.
Stop and start an instance
Use one of the following methods to stop and start an instance.
Pause and unpause an instance
To pause an instance, run the following command:
$ openstack server pause INSTANCE_NAME
This command stores the state of the VM in RAM. A paused instance continues to run in a frozen state.
To unpause an instance, run the following command:
$ openstack server unpause INSTANCE_NAME
Suspend and resume an instance
To initiate a hypervisor-level suspend operation, run the following command:
$ openstack server suspend INSTANCE_NAME
To resume a suspended instance, run the following command:
$ openstack server resume INSTANCE_NAME
                               94 OpenStack command-line clients
 Shelve and unshelve an instance
Shelving is useful if you have an instance that you are not using, but would like retain in your list of servers. For example, you can stop an instance at the end of a work week, and resume work again at the start of the next week. All associated data and resources are kept; however, anything still in memory is not retained. If a shelved instance is no longer needed, it can also be entirely removed.
You can run the following shelving tasks:
• Shelve an instance - Shuts down the instance, and stores it together with associated data and resources (a snapshot is taken if not volume backed). Anything in memory is lost.
$ openstack server shelve SERVERNAME
Note: By default, the openstack server shelve command gives the guest operating system a chance to perform a controlled shutdown before the instance is powered off. The shutdown behavior is configured by the shutdown_timeout parameter that can be set in the nova.conf file. Its value stands for the overall period (in seconds) a guest operating system is allowed to complete the shutdown. The default timeout is 60 seconds. See Description of Compute configuration options for details.
The timeout value can be overridden on a per image basis by means of os_shutdown_timeout that is an image metadata setting allowing different types of operating systems to specify how much time they need to shut down cleanly.
• Unshelve an instance - Restores the instance.
$ openstack server unshelve SERVERNAME
• Remove a shelved instance - Removes the instance from the server; data and resource associations are deleted. If an instance is no longer needed, you can move the instance off the hypervisor in order to minimize resource usage.
$ nova shelve-offload SERVERNAME
Search for an instance using IP address
You can search for an instance using the IP address parameter, --ip, with the openstack server list com- mand.
$ openstack server list --ip IP_ADDRESS
The following example shows the results of a search on 10.0.0.4.
User Guide (Release Version: 15.0.0)
                          $ openstack server list --ip 10.0.0.4 +------------------+----------------------+--------+------------+-------------+-------------
 →-----+------------+
| ID | Name | Status | Task State | Power State | Networks
 → | Image Name | +------------------+----------------------+--------+------------+-------------+-------------
 →-----+------------+
| 8a99547e-7385... | myInstanceFromVolume | ACTIVE | None | Running | private=10.
 →0.0.4 | cirros | +------------------+----------------------+--------+------------+-------------+-------------
  →-----+------------+
OpenStack command-line clients 95

 User Guide (Release Version: 15.0.0)
     Reboot an instance
You can soft or hard reboot a running instance. A soft reboot attempts a graceful shut down and restart of the instance. A hard reboot power cycles the instance.
By default, when you reboot an instance, it is a soft reboot.
$ openstack server reboot SERVER
To perform a hard reboot, pass the --hard parameter, as follows:
$ openstack server reboot --hard SERVER
It is also possible to reboot a running instance into rescue mode. For example, this operation may be required,
if a filesystem of an instance becomes corrupted with prolonged use.
Note: Pause, suspend, and stop operations are not allowed when an instance is running in rescue mode, as triggering these actions causes the loss of the original instance state, and makes it impossible to unrescue the instance.
Rescue mode provides a mechanism for access, even if an image renders the instance inaccessible. By default, it starts an instance from the initial image attaching the current boot disk as a secondary one.
To perform an instance reboot into rescue mode, run the following command:
$ openstack server rescue SERVER
Note: On running the openstack server rescue command, an instance performs a soft shutdown first. This means that the guest operating system has a chance to perform a controlled shutdown before the instance is powered off. The shutdown behavior is configured by the shutdown_timeout parameter that can be set in the nova.conf file. Its value stands for the overall period (in seconds) a guest operating system is allowed to complete the shutdown. The default timeout is 60 seconds. See Description of Compute configuration options for details.
The timeout value can be overridden on a per image basis by means of os_shutdown_timeout that is an image metadata setting allowing different types of operating systems to specify how much time they need to shut down cleanly.
To restart the instance from the normal boot disk, run the following command:
$ openstack server unrescue SERVER
If you want to rescue an instance with a specific image, rather than the default one, use the --image parameter:
$ nova rescue --image IMAGE_ID SERVER
Delete an instance
When you no longer need an instance, you can delete it.
                              96 OpenStack command-line clients
 1. List all instances:
User Guide (Release Version: 15.0.0)
    $ openstack server list +-------------+----------------------+--------+------------+-------------+------------
 →------+------------+
| ID | Name | Status | Task State | Power State | Networks
 → | Image Name | +-------------+----------------------+--------+------------+-------------+------------
 →------+------------+
| 84c6e57d... | myCirrosServer | ACTIVE | None
 →0.0.3 | cirros |
| 8a99547e... | myInstanceFromVolume | ACTIVE | None
 →0.0.4 | ubuntu |
| d7efd3e4... | newServer | ERROR | None
| Running
| Running
| NOSTATE
| private=10.
| private=10.
|
 → | centos | +-------------+----------------------+--------+------------+-------------+------------
 →------+------------+
 2. Run the openstack server delete command to delete the instance. The following example shows deletion of the newServer instance, which is in ERROR state:
$ openstack server delete newServer
The command does not notify that your server was deleted.
3. Toverifythattheserverwasdeleted,runtheopenstackserverlistcommand:
         $ openstack server list +-------------+----------------------+--------+------------+-------------+------------
 →------+------------+
| ID | Name | Status | Task State | Power State | Networks
 → | Image Name | +-------------+----------------------+--------+------------+-------------+------------
 →------+------------+
| 84c6e57d... | myCirrosServer | ACTIVE | None | Running | private=10.
 →0.0.3 | cirros |
| 8a99547e... | myInstanceFromVolume | ACTIVE | None | Running | private=10.
 →0.0.4 | ubuntu | +-------------+----------------------+--------+------------+-------------+------------
 →------+------------+
 The deleted instance does not appear in the list.
Access an instance through a console
VNC or SPICE is used to view the console output of an instance, regardless of whether or not the console log has output. This allows relaying keyboard and mouse activity to and from an instance.
There are three remote console access methods commonly used with OpenStack:
novnc An in-browser VNC client implemented using HTML5 Canvas and WebSockets spice A complete in-browser client solution for interaction with virtualized instances xvpvnc A Java client offering console access to an instance
Example:
 OpenStack command-line clients 97
 User Guide (Release Version: 15.0.0)
To access an instance through a remote console, run the following command:
$ openstack console url show INSTANCE_NAME --xvpvnc
The command returns a URL from which you can access your instance:
--xvpvnc can be replaced by any of the above values as connection types.
When using SPICE to view the console of an instance, a browser plugin can be used directly on the instance page, or the openstack console url show command can be used with it, as well, by returning a token- authenticated address, as in the example above.
For further information and comparisons (including security considerations), see the Security Guide. Manage bare-metal nodes
The bare-metal driver for OpenStack Compute manages provisioning of physical hardware by using common cloud APIs and tools such as Orchestration (Heat). The use case for this driver is for single project clouds such as a high-performance computing cluster, or for deploying OpenStack itself.
If you use the bare-metal driver, you must create a network interface and add it to a bare-metal node. Then, you can launch an instance from a bare-metal image.
You can list and delete bare-metal nodes. When you delete a node, any associated network interfaces are removed. You can list and remove network interfaces that are associated with a bare-metal node.
Commands
The following commands can be used to manage bare-metal nodes. baremetal-interface-add Adds a network interface to a bare-metal node. baremetal-interface-list Lists network interfaces associated with a bare-metal node. baremetal-interface-remove Removes a network interface from a bare-metal node. baremetal-node-create Creates a bare-metal node.
baremetal-node-delete Removes a bare-metal node and any associated interfaces. baremetal-node-list Lists available bare-metal nodes.
baremetal-node-show Shows information about a bare-metal node.
Create a bare-metal node
When you create a bare-metal node, your PM address, user name, and password should match the information in your hardware’s BIOS/IPMI configuration.
         +--------+------------------------------------------------------------------------------+ |Type |Url | +--------+------------------------------------------------------------------------------+ | xvpvnc | http://192.168.5.96:6081/console?token=c83ae3a3-15c4-4890-8d45-aefb494a8d6c | +--------+------------------------------------------------------------------------------+
  98 OpenStack command-line clients
     $ nova baremetal-node-create --pm_address PM_ADDRESS --pm_user PM_USERNAME \ --pm_password PM_PASSWORD $(hostname -f) 1 512 10 aa:bb:cc:dd:ee:ff
 The following example shows the command and results from creating a node with the PM address 1.2.3.4, the PM user name ipmi, and password ipmi.
Add a network interface to the node
For each NIC on the node, you must create an interface, specifying the interface’s MAC address.
User Guide (Release Version: 15.0.0)
    $ nova baremetal-node-create --pm_address 1.2.3.4 --pm_user ipmi \ --pm_password ipmi $(hostname -f) 1 512 10 aa:bb:cc:dd:ee:ff
+------------------+-------------------+
| Property         | Value             |
+------------------+-------------------+
| instance_uuid
| pm_address
| interfaces
| prov_vlan_id
| cpus
| None | | 1.2.3.4 | | [] | | None | |1 | | 512 |
| memory_mb
| prov_mac_address | aa:bb:cc:dd:ee:ff |
| service_host | local_gb |id
| pm_user
| ubuntu | | 10 | |1 | | ipmi | | None |
| terminal_port
+------------------+-------------------+
     $ nova baremetal-interface-add 1 aa:bb:cc:dd:ee:ff +-------------+-------------------+
| Property | Value | +-------------+-------------------+
| datapath_id | 0 | |id |1 | | port_no | 0 | | address | aa:bb:cc:dd:ee:ff | +-------------+-------------------+
 Launch an instance from a bare-metal image
A bare-metal instance is an instance created directly on a physical machine, without any virtualization layer running underneath it. Nova retains power control via IPMI. In some situations, Nova may retain network control via Neutron and OpenFlow.
    $ openstack server create --image my-baremetal-image --flavor \ my-baremetal-flavor test
+-----------------------------+--------------------------------------+
| Property                    | Value                                |
+-----------------------------+--------------------------------------+
| status                      | BUILD                                |
| id                          | cc302a8f-cd81-484b-89a8-b75eb3911b1b |
+-----------------------------+--------------------------------------+
  OpenStack command-line clients 99
 User Guide (Release Version: 15.0.0)
    ... wait for instance to become active ...
  Note: Set the --availability-zone parameter to specify which zone or node to use to start the server. Separate the zone from the host name with a comma. For example:
$ openstack server create --availability-zone zone:HOST,NODE
host is optional for the --availability-zone parameter. You can simply specify zone:,node, still includ-
ing the comma.
List bare-metal nodes and interfaces
Use the nova baremetal-node-list command to view all bare-metal nodes and interfaces. When a node is in use, its status includes the UUID of the instance that runs on it:
          $ nova baremetal-node-list +----+--------+------+-----------+---------+-------------------+------+------------+--------
 →-----+-------------+---------------+
| ID | Host | CPUs | Memory_MB | Disk_GB | MAC Address | VLAN | PM Address | PM
 →Username | PM Password | Terminal Port | +----+--------+------+-----------+---------+-------------------+------+------------+--------
 →-----+-------------+---------------+
| 1 | ubuntu | 1 | 512 | 10 | aa:bb:cc:dd:ee:ff | None | 1.2.3.4 | ipmi
 → | |None | +----+--------+------+-----------+---------+-------------------+------+------------+--------
 →-----+-------------+---------------+
 Show details for a bare-metal node
Use the nova baremetal-node-show command to view the details for a bare-metal node:
    $ nova baremetal-node-show 1 +------------------+--------------------------------------+ | Property | Value | +------------------+--------------------------------------+
| instance_uuid
| pm_address
| interfaces
|
| cc302a8f-cd81-484b-89a8-b75eb3911b1b | | 1.2.3.4 | | [{u'datapath_id': u'0', u'id': 1, | | u'port_no': 0, | | u'address': u'aa:bb:cc:dd:ee:ff'}] | | None | |1 | | 512 |
|
| prov_vlan_id
| cpus
| memory_mb
| prov_mac_address | aa:bb:cc:dd:ee:ff | | service_host | ubuntu | | local_gb | 10 | |id|1 | | pm_user | ipmi | | terminal_port | None | +------------------+--------------------------------------+
  100 OpenStack command-line clients
 Provide user data to instances
A user data file is a special key in the metadata service that holds a file that cloud-aware applications in the guest instance can access. For example, one application that uses user data is the cloud-init system, which is an open-source package from Ubuntu that is available on various Linux distributions and which handles early initialization of a cloud instance.
You can place user data in a local file and pass it through the --user-data <user-data-file> parameter at instance creation.
Use snapshots to migrate instances
To use snapshots to migrate instances from OpenStack projects to clouds, complete these steps. In the source project:
1. Create a snapshot of the instance
2. Download the snapshot as an image In the destination project:
1. Import the snapshot to the new environment 2. Boot a new instance from the snapshot
Note: Some cloud providers allow only administrators to perform this task.
Create a snapshot of the instance
1. Shut down the source VM before you take the snapshot to ensure that all data is flushed to disk. If necessary, list the instances to view the instance name:
User Guide (Release Version: 15.0.0)
    $ openstack server create --image ubuntu-cloudimage --flavor 1 \ --user-data mydata.file VM_INSTANCE
       $ openstack server list +--------------------------------------+------------+--------+------------------------
 →------+------------+
| ID | Name | Status | Networks
 → | Image Name | +--------------------------------------+------------+--------+------------------------
 →------+------------+
| c41f3074-c82a-4837-8673-fa7e9fea7e11 | myInstance | ACTIVE | private=10.0.0.3
 → | cirros | +--------------------------------------+------------+--------+------------------------
 →------+------------+
 2. Usetheopenstackserverstopcommandtoshutdowntheinstance: $ openstack server stop myInstance
3. UsetheopenstackserverlistcommandtoconfirmthattheinstanceshowsaSHUTOFFstatus:
      OpenStack command-line clients 101
 User Guide (Release Version: 15.0.0)
    $ openstack server list +--------------------------------------+------------+---------+------------------+----
 →--------+
| ID | Name | Status | Networks |
 →Image Name | +--------------------------------------+------------+---------+------------------+----
 →--------+
| c41f3074-c82a-4837-8673-fa7e9fea7e11 | myInstance | SHUTOFF | private=10.0.0.3 |
 →cirros | +--------------------------------------+------------+---------+------------------+----
 →--------+
 4. Usetheopenstackserverimagecreatecommandtotakeasnapshot:
$ openstack server image create myInstance --name myInstanceSnapshot
The above command creates the image myInstance by taking a snapshot of a running server.
5. Usetheopenstackimagelistcommandtocheckthestatusuntilthestatusisactive:
         $ openstack image list +--------------------------------------+---------------------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------------------+--------+ | 657ebb01-6fae-47dc-986a-e49c4dd8c433 | cirros-0.3.5-x86_64-uec | active | | 72074c6d-bf52-4a56-a61c-02a17bf3819b | cirros-0.3.5-x86_64-uec-kernel | active | | 3c5e5f06-637b-413e-90f6-ca7ed015ec9e | cirros-0.3.5-x86_64-uec-ramdisk | active | | f30b204e-1ce6-40e7-b8d9-b353d4d84e7d | myInstanceSnapshot | active | +--------------------------------------+---------------------------------+--------+
 Download the snapshot as an image
1. Get the image ID:
2. Download the snapshot by using the image ID that was returned in the previous step:
$ openstack image save --file snapshot.raw f30b204e-1ce6-40e7-b8d9-b353d4d84e7d
Note: The openstack image save command requires the image ID or the image name. Check there is sufficient space on the destination file system for the image file.
3. Make the image available to the new environment, either through HTTP or direct upload to a machine (scp).
    $ openstack image list +-------------------+-------------------+--------+ | ID | Name | Status | +-------------------+-------------------+--------+ | f30b204e-1ce6... | myInstanceSnapshot| active | +-------------------+-------------------+--------+
         102
OpenStack command-line clients
 Import the snapshot to the new environment
In the new project or cloud environment, import the snapshot:
Boot a new instance from the snapshot
In the new project or cloud environment, use the snapshot to create the new instance:
$ openstack server create --flavor m1.tiny --image myInstanceSnapshot myNewInstance
Store metadata on a configuration drive
You can configure OpenStack to write metadata to a special configuration drive that attaches to the instance when it boots. The instance can mount this drive and read files from it to get information that is normally available through the metadata service. This metadata is different from the user data.
One use case for using the configuration drive is to pass a networking configuration when you do not use DHCP to assign IP addresses to instances. For example, you might pass the IP address configuration for the instance through the configuration drive, which the instance can mount and access before you configure the network settings for the instance.
Any modern guest operating system that is capable of mounting an ISO 9660 or VFAT file system can use the configuration drive.
Requirements and guidelines
To use the configuration drive, you must follow the following requirements for the compute host and image.
Compute host requirements
• The following hypervisors support the configuration drive: libvirt, XenServer, Hyper-V, and VMware. Also, the Bare Metal service supports the configuration drive.
• To use configuration drive with libvirt, XenServer, or VMware, you must first install the genisoimage package on each compute host. Otherwise, instances do not boot properly.
Use the mkisofs_cmd flag to set the path where you install the genisoimage program. If genisoimage is in same path as the nova-compute service, you do not need to set this flag.
• To use configuration drive with Hyper-V, you must set the mkisofs_cmd value to the full path to an mkisofs.exe installation. Additionally, you must set the qemu_img_cmd value in the hyperv configu- ration section to the full path to an qemu-img command installation.
• TouseconfigurationdrivewiththeBareMetalservice,youdonotneedtoprepareanythingbecausethe Bare Metal service treats the configuration drive properly.
Image requirements
• Animagebuiltwitharecentversionofthecloud-initpackagecanautomaticallyaccessmetadatapassed through the configuration drive. The following table lists the references for cloud-init versions mapped to a particular operating system:
User Guide (Release Version: 15.0.0)
    $ openstack image create NEW_IMAGE_NAME \
--container-format bare --disk-format qcow2 --file IMAGE_URL
       OpenStack command-line clients 103
 User Guide (Release Version: 15.0.0)
   Operating system
 Reference for cloud-init version
  Ubuntu
 http://packages.ubuntu.com/search?keywords=cloud-init
  Fedora (RHEL)
 https://www.rpmfind.net/linux/rpm2html/search.php?query=cloud-init
  openSUSE (SLE)
 http://software.opensuse.org/download.html?project=Cloud%3ATools& package=cloud-init
    • If an image does not have the cloud-init package installed, you must customize the image to run a script that mounts the configuration drive on boot, reads the data from the drive, and takes appropriate action such as adding the public key to an account. You can read more details about how data is organized on the configuration drive.
• IfyouuseXenwithaconfigurationdrive,usethexenapi_disable_agentconfigurationparameterto disable the agent.
Guidelines
• Do not rely on the presence of the EC2 metadata in the configuration drive, because this content might be removed in a future release. For example, do not rely on files in the ec2 directory.
• Whenyoucreateimagesthataccessconfigurationdrivedataandmultipledirectoriesareundertheopen- stack directory, always select the highest API version by date that your consumer supports. For exam- ple, if your guest image supports the 2012-03-05, 2012-08-05, and 2013-04-13 versions, try 2013-04-13 first and fall back to a previous version if 2013-04-13 is not present.
Enable and access the configuration drive
1.
To enable the configuration drive, pass the --config-drive true parameter to the openstack server create command.
The following example enables the configuration drive and passes user data, two files, and two key/value metadata pairs, all of which are accessible from the configuration drive:
You can also configure the Compute service to always create a configuration drive by setting the follow- ing option in the /etc/nova/nova.conf file:
force_config_drive = true
Note: If a user passes the --config-drive true flag to the nova boot command, an administrator cannot disable the configuration drive.
If your guest operating system supports accessing disk by label, you can mount the configuration drive as the /dev/disk/by-label/configurationDriveVolumeLabel device. In the following example, the configuration drive has the config-2 volume label:
    $ openstack server create --config-drive true --image my-image-name \ --flavor 1 --key-name mykey --user-data ./my-user-data.txt \
--file /etc/network/interfaces=/home/myuser/instance-interfaces \ --file known_hosts=/home/myuser/.ssh/known_hosts \
  --property role=webservers --property essential=false MYINSTANCE
        2.
     104
OpenStack command-line clients
# mkdir -p /mnt/config
# mount /dev/disk/by-label/config-2 /mnt/config

 Note: Ensure that you use at least version 0.3.1 of CirrOS for configuration drive support.
If your guest operating system does not use udev, the /dev/disk/by-label directory is not present.
You can use the blkid command to identify the block device that corresponds to the configuration drive. For example, when you boot the CirrOS image with the m1.tiny flavor, the device is /dev/vdb:
# blkid -t LABEL="config-2" -odevice
/dev/vdb
Once identified, you can mount the device:
Configuration drive contents
In this example, the contents of the configuration drive are as follows:
User Guide (Release Version: 15.0.0)
               # mkdir -p /mnt/config
# mount /dev/vdb /mnt/config
      ec2/2009-04-04/meta-data.json
ec2/2009-04-04/user-data
ec2/latest/meta-data.json
ec2/latest/user-data
openstack/2012-08-10/meta_data.json
openstack/2012-08-10/user_data
openstack/content
openstack/content/0000
openstack/content/0001
openstack/latest/meta_data.json
openstack/latest/user_data
 The files that appear on the configuration drive depend on the arguments that you pass to the openstack server create command.
OpenStack metadata format
The following example shows the contents of the openstack/2012-08-10/meta_data.json and openstack/latest/meta_data.json files. These files are identical. The file contents are formatted for readability.
    {
"availability_zone": "nova", "files": [
{
"content_path": "/content/0000", "path": "/etc/network/interfaces"
}, {
"content_path": "/content/0001", "path": "known_hosts"
  OpenStack command-line clients 105
 User Guide (Release Version: 15.0.0)
    } ],
"hostname": "test.novalocal", "launch_index": 0,
"name": "test",
"meta": {
"role": "webservers",
"essential": "false" },
"public_keys": {
"mykey": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDBqUfVvCSez0/
 →Wfpd8dLLgZXV9GtXQ7hnMN+Z0OWQUyebVEHey1CXuin0uY1cAJMhUq8j98SiW+cU0sU4J3x5l2+xi1bodDm1BtFWVeLIOQINpfV1n8fKjH  →Generated by Nova\n"
},
"uuid": "83679162-1378-4288-a2d4-70e13ec132aa" }
Note the effect of the --file /etc/network/interfaces=/home/myuser/instance-interfaces argu- ment that was passed to the openstack server create command. The contents of this file are contained in the openstack/content/0000 file on the configuration drive, and the path is specified as /etc/network/ interfaces in the meta_data.json file.
EC2 metadata format
The following example shows the contents of the ec2/2009-04-04/meta-data.json and the ec2/latest/ meta-data.json files. These files are identical. The file contents are formatted to improve readability.
{
"ami-id": "ami-00000001", "ami-launch-index": 0, "ami-manifest-path": "FIXME", "block-device-mapping": {
"ami": "sda1", "ephemeral0": "sda2", "root": "/dev/sda1", "swap": "sda3"
},
"hostname": "test.novalocal", "instance-action": "none", "instance-id": "i-00000001", "instance-type": "m1.tiny", "kernel-id": "aki-00000002", "local-hostname": "test.novalocal", "local-ipv4": null,
"placement": {
"availability-zone": "nova" },
"public-hostname": "test.novalocal", "public-ipv4": "",
"public-keys": {
"0": {
"openssh-key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDBqUfVvCSez0/
 →Wfpd8dLLgZXV9GtXQ7hnMN+Z0OWQUyebVEHey1CXuin0uY1cAJMhUq8j98SiW+cU0sU4J3x5l2+xi1bodDm1BtFWVeLIOQINpfV1n8fKjH  →Generated by Nova\n"
     }
  106
OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    ] }
},
"ramdisk-id": "ari-00000003", "reservation-id": "r-7lfps8wj", "security-groups": [
"default"
 User data
The openstack/2012-08-10/user_data, openstack/latest/user_data, ec2/2009-04-04/user- data, and ec2/latest/user-data file are present only if the --user-data flag and the contents of the user data file are passed to the openstack server create command.
Configuration drive format
The default format of the configuration drive as an ISO 9660 file system. To explicitly specify the ISO 9660 format, add the following line to the /etc/nova/nova.conf file:
config_drive_format=iso9660
By default, you cannot attach the configuration drive image as a CD drive instead of as a disk drive. To attach a CD drive, add the following line to the /etc/nova/nova.conf file:
config_drive_cdrom=true
For legacy reasons, you can configure the configuration drive to use VFAT format instead of ISO 9660. It is unlikely that you would require VFAT format because ISO 9660 is widely supported across operating systems. However, to use the VFAT format, add the following line to the /etc/nova/nova.conf file:
config_drive_format=vfat
If you choose VFAT, the configuration drive is 64 MB.
Note: In current version (Liberty) of OpenStack Compute, live migration with config_drive on local disk is forbidden due to the bug in libvirt of copying a read-only disk. However, if we use VFAT as the format of config_drive, the function of live migration works well.
Create and manage networks
Before you run commands, set environment variables using the OpenStack RC file. Create networks
1. List the extensions of the system:
                  OpenStack command-line clients 107
 User Guide (Release Version: 15.0.0)
    $ openstack extension list -c Alias -c Name --network +------------------------------------------+---------------------------+ | Name | Alias | +------------------------------------------+---------------------------+
| Default Subnetpools
| Network IP Availability
| Auto Allocated Topology Services
| Neutron L3 Configurable external gateway | ext-gw-mode               |
| Address scope                            | address-scope             |
| Neutron Extra Route                      | extraroute                |
+------------------------------------------+---------------------------+
| default-subnetpools       |
| network-ip-availability   |
| auto-allocated-topology   |
 2. Create a network:
    $ openstack network create net1
Created a new network: +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+
| revision_number
| router:external
| shared
| status
| 3                                    |
| Internal                             |
| False                                |
| ACTIVE                               |
|                                      |
| []                                   |
| 2016-12-21T08:32:54Z                 |
| UP                                   |
|                                      |
|                                      |
| 2016-12-21T08:32:54Z                 |
|                                      |
|                                      |
| 180620e3-9eae-4ba7-9739-c5847966e1f0 |
| None                                 |
| None                                 |
| 1450                                 |
| net1                                 |
| True                                 |
| c961a8f6d3654657885226378ade8220     |
| vxlan                                |
| admin_state_up
| availability_zone_hints
| availability_zones
| created_at
| description
| headers
| id
| ipv4_address_scope
| ipv6_address_scope
| mtu
| name
| port_security_enabled
| project_id
| provider:network_type
| provider:physical_network | None                                 |
| provider:segmentation_id  | 14                                   |
| subnets
| tags
| updated_at
+---------------------------+--------------------------------------+
  Note: Some fields of the created network are invisible to non-admin users. 3. Create a network with specified provider network type.
     $ openstack network create net2 --provider-network-type vxlan Created a new network: +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | |
  108 OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    | availability_zones
| created_at
| description
| headers
|                                      |
| 2016-12-21T08:33:34Z                 |
|                                      |
|                                      |
| c0a563d5-ef7d-46b3-b30d-6b9d4138b6cf |
| None                                 |
| None                                 |
| 1450                                 |
| net2                                 |
| True                                 |
| c961a8f6d3654657885226378ade8220     |
| vxlan                                |
| id
| ipv4_address_scope
| ipv6_address_scope
| mtu
| name
| port_security_enabled
| project_id
| provider:network_type
| provider:physical_network | None                                 |
| provider:segmentation_id  | 87                                   |
| revision_number
| router:external
| shared
| status
| 3                                    |
| Internal                             |
| False                                |
| ACTIVE                               |
|                                      |
| []                                   |
| 2016-12-21T08:33:34Z                 |
| subnets
| tags
| updated_at
+---------------------------+--------------------------------------+
 Create subnets
Create a subnet:
    $ openstack subnet create subnet1 --network net1 --subnet-range 192.0.2.0/24
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| allocation_pools  | 192.0.2.2-192.0.2.254                |
| cidr
| created_at
| description
| dns_nameservers
| enable_dhcp
| gateway_ip
| headers
| host_routes
| id
| ip_version
| ipv6_address_mode | None                                 |
| 192.0.2.0/24                         |
| 2016-12-22T18:47:52Z                 |
|                                      |
|                                      |
| True                                 |
| 192.0.2.1                            |
|                                      |
|                                      |
| a394689c-f547-4834-9778-3e0bb22130dc |
| 4                                    |
| ipv6_ra_mode
| name
| network_id
| project_id
| revision_number
| service_types
| subnetpool_id
| updated_at
+-------------------+--------------------------------------+
| None                                 |
| subnet1                              |
| 180620e3-9eae-4ba7-9739-c5847966e1f0 |
| c961a8f6d3654657885226378ade8220     |
| 2                                    |
|                                      |
| None                                 |
| 2016-12-22T18:47:52Z                 |
 The subnet-create command has the following positional and optional parameters:
 OpenStack command-line clients 109
 User Guide (Release Version: 15.0.0)
• The name or ID of the network to which the subnet belongs.
In this example, net1 is a positional argument that specifies the network name.
• The CIDR of the subnet.
In this example, 192.0.2.0/24 is a positional argument that specifies the CIDR.
• The subnet name, which is optional.
In this example, --name subnet1 specifies the name of the subnet.
For information and examples on more advanced use of neutron’s subnet subcommand, see the OpenStack Administrator Guide.
Create routers
1. Create a router:
    $ openstack router create router1 +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | |
| availability_zones
| created_at
| description
| distributed
| external_gateway_info
| flavor_id
| ha
| headers
|                                      |
| 2016-12-22T18:48:57Z                 |
|                                      |
| True                                 |
| null                                 |
| None                                 |
| False                                |
|                                      |
| e25a24ee-3458-45c7-b16e-edf49092aab7 |
| router1                              |
| e17431afc0524e0690484889a04b7fa0     |
| 1                                    |
|                                      |
| ACTIVE                               |
| 2016-12-22T18:48:57Z                 |
| id
| name
| project_id
| revision_number
| routes
| status
| updated_at
+-------------------------+--------------------------------------+
 Take note of the unique router identifier returned, this will be required in subsequent steps.
2. Link the router to the external provider network:
$ openstack router set ROUTER --external-gateway NETWORK
Replace ROUTER with the unique identifier of the router, replace NETWORK with the unique identifier
of the external provider network.
3. Link the router to the subnet:
$ openstack router add subnet ROUTER SUBNET
Replace ROUTER with the unique identifier of the router, replace SUBNET with the unique identifier of the subnet.
           110
OpenStack command-line clients
 Create ports
1. Create a port with specified IP address:
User Guide (Release Version: 15.0.0)
    $ openstack port create --network net1 --fixed-ip subnet=subnet1,ip-address=192.0.2.  →40 port1
+-----------------------+-----------------------------------------+
| Field                 | Value                                   |
+-----------------------+-----------------------------------------+
| admin_state_up        | UP                                      |
| allowed_address_pairs |                                         |
| binding_host_id
| binding_profile
| binding_vif_details
| binding_vif_type
| binding_vnic_type
| created_at
| description
| device_id
| device_owner
| extra_dhcp_opts
| fixed_ips
|
| headers
| id
| mac_address
| name
| network_id
| port_security_enabled | True                                    |
|                                         |
|                                         |
|                                         |
| unbound                                 |
| normal                                  |
| 2016-12-22T18:54:43Z                    |
|                                         |
|                                         |
|                                         |
|                                         |
| ip_address='192.0.2.40', subnet_id='a   |
| 394689c-f547-4834-9778-3e0bb22130dc'    |
|                                         |
| 031ddba8-3e3f-4c3c-ae26-7776905eb24f    |
| fa:16:3e:df:3d:c7                       |
| port1                                   |
| 180620e3-9eae-4ba7-9739-c5847966e1f0    |
| project_id
| revision_number
| security_groups
| status
| updated_at
+-----------------------+-----------------------------------------+
| c961a8f6d3654657885226378ade8220        |
| 5                                       |
| 84abb9eb-dc59-40c1-802c-4e173c345b6a    |
| DOWN                                    |
| 2016-12-22T18:54:44Z                    |
 In the previous command, net1 is the network name, which is a positional argument. --fixed-ip subnet<subnet>,ip-address=192.0.2.40 is an option which specifies the port’s fixed IP address we wanted.
Note: When creating a port, you can specify any unallocated IP in the subnet even if the address is not in a pre-defined pool of allocated IP addresses (set by your cloud provider).
2. Create a port without specified IP address:
      $ openstack port create port2 --network net1 +-----------------------+-----------------------------------------+ | Field | Value | +-----------------------+-----------------------------------------+ | admin_state_up | UP | | allowed_address_pairs | |
| binding_host_id
| binding_profile
| binding_vif_details
| binding_vif_type
|                                         |
|                                         |
|                                         |
| unbound                                 |
  OpenStack command-line clients 111
 User Guide (Release Version: 15.0.0)
    | binding_vnic_type
| created_at
| description
| device_id
| normal                                  |
| 2016-12-22T18:56:06Z                    |
|                                         |
|                                         |
|                                         |
|                                         |
| ip_address='192.0.2.10', subnet_id='a   |
| 394689c-f547-4834-9778-3e0bb22130dc'    |
|                                         |
| eac47fcd-07ac-42dd-9993-5b36ac1f201b    |
| fa:16:3e:96:ae:6e                       |
| port2                                   |
| 180620e3-9eae-4ba7-9739-c5847966e1f0    |
| device_owner
| extra_dhcp_opts
| fixed_ips
|
| headers
| id
| mac_address
| name
| network_id
| port_security_enabled | True                                    |
| project_id
| revision_number
| security_groups
| status
| updated_at
+-----------------------+-----------------------------------------+
| c961a8f6d3654657885226378ade8220        |
| 5                                       |
| 84abb9eb-dc59-40c1-802c-4e173c345b6a    |
| DOWN                                    |
| 2016-12-22T18:56:06Z                    |
  Note: NotethatthesystemallocatesoneIPaddressifyoudonotspecifyanIPaddressintheopenstack port create command.
Note: You can specify a MAC address with --mac-address MAC_ADDRESS. If you specify an invalid MAC address, including 00:00:00:00:00:00 or ff:ff:ff:ff:ff:ff, you will get an error.
3. Query ports with specified fixed IP addresses:
       $ neutron port-list --fixed-ips ip_address=192.0.2.2 \ ip_address=192.0.2.40
+----------------+------+-------------------+-----------------------------------------  →--------+
| id | name | mac_address | fixed_ips    → |
+----------------+------+-------------------+-----------------------------------------  →--------+
| baf13412-26... | | fa:16:3e:f6:ec:c7 | {"subnet_id"... ..."ip_address": "192.0.  →2.2"} |
| f7a08fe4-e7... | | fa:16:3e:97:e0:fc | {"subnet_id"... ..."ip_address": "192.0.  →2.40"} |
+----------------+------+-------------------+-----------------------------------------  →--------+
 Manage objects and containers
The OpenStack Object Storage service provides the swift client, which is a command-line interface (CLI). Use this client to list objects and containers, upload objects to containers, and download or delete objects from containers. You can also gather statistics and update metadata for accounts, containers, and objects.
This client is based on the native swift client library, client.py, which seamlessly re-authenticates if the cur-
 112 OpenStack command-line clients
 rent token expires during processing, retries operations multiple times, and provides a processing concurrency of 10.
Create and manage containers
• To create a container, run the following command and replace CONTAINER with the name of your con- tainer.
$ swift post CONTAINER
• To list all containers, run the following command:
$ swift list
• To check the status of containers, run the following command:
$ swift stat
You can also use the swift stat command with the ACCOUNT or CONTAINER names as parameters. $ swift stat CONTAINER
User Guide (Release Version: 15.0.0)
                   Account: AUTH_7b5970fbe7724bf9b74c245e77c03bcg
Containers: 2
Objects: 3
Bytes: 268826
Accept-Ranges: bytes
X-Timestamp: 1392683866.17952
Content-Type: text/plain; charset=utf-8
          Account: AUTH_7b5970fbe7724bf9b74c245e77c03bcg
Container: storage1
Objects: 2
Bytes: 240221
Read ACL:
Write ACL:
Sync To:
Sync Key:
Accept-Ranges: bytes
X-Timestamp: 1392683866.20180
Content-Type: text/plain; charset=utf-8
 Manage access
• Users have roles on accounts. For example, a user with the admin role has full access to all containers and objects in an account. You can set access control lists (ACLs) at the container level and support lists for read and write access, which you set with the X-Container-Read and X-Container-Write headers.
To give a user read access, use the swift post command with the -r parameter. To give a user write access, use the -w parameter.
• The following are examples of read ACLs for containers:
 OpenStack command-line clients 113
 User Guide (Release Version: 15.0.0)
              $ swift post CONTAINER -r \ ".r:openstack.example.com,.r:swift.example.com,.r:storage.example.com"
     $ swift post CONTAINER -r \ ".r:*,.r:-openstack.example.com,.r:-swift.example.com,.r:-storage.example.com"
               •
A request with any HTTP referer header can read container contents:
$ swift post CONTAINER -r ".r:*"
A request with any HTTP referer header can read and list container contents:
$ swift post CONTAINER -r ".r:*,.rlistings"
A list of specific HTTP referer headers permitted to read container contents:
A list of specific HTTP referer headers denied read access:
All users residing in project1 can read container contents:
$ swift post CONTAINER -r "project1:*" User1 from project1 can read container contents:
$ swift post CONTAINER -r "project1:user1"
A list of specific users and projects permitted to read container contents:
The following are examples of write ACLs for containers: All users residing in project1 can write to the container:
$ swift post CONTAINER -w "project1:*"
User1 from project1 can write to the container:
$ swift post CONTAINER -w "project1:user1"
A list of specific users and projects permitted to write to the container:
$ swift post CONTAINER -r \ "project1:user1,project1:user2,project3:*,project4:user1"
               $ swift post CONTAINER -w \ "project1:user1,project1:user2,project3:*,project4:user1"
  To successfully write to a container, a user must have read privileges (in addition to write) on the container. For all aforementioned read/write ACL examples, one can replace the project/user name with project/user UUID, i.e. <project_uuid>:<user_uuid>. If using multiple keystone domains, UUID format is required.
Manage objects
Note:
 • To upload an object to a container, run the following command:
 114
OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    $ swift upload CONTAINER OBJECT_FILENAME
To upload an object in chunks, for larger than 5GB files, run the following command:
$ swift upload -S CHUNK_SIZE CONTAINER OBJECT_FILENAME
Important: Uploading objects in chunks is mandatory if uploading an object larger than 5GB. • To check the status of the object, run the following command:
$ swift stat CONTAINER OBJECT_FILENAME
                 Account: AUTH_7b5970fbe7724bf9b74c245e77c03bcg
Container: storage1
Object: images
Content Type: application/octet-stream
Content Length: 211616
Last Modified: Tue, 18 Feb 2014 00:40:36 GMT
ETag: 82169623d55158f70a0d720f238ec3ef
Meta Orig-Filename: images.jpg
Accept-Ranges: bytes
X-Timestamp: 1392684036.33306
 • To list the objects in a container, run the following command:
$ swift list CONTAINER
• To download an object from a container, run the following command:
$ swift download CONTAINER OBJECT_FILENAME
Environment variables required to run examples
To run the cURL command examples for the Object Storage API requests, set these environment variables:
publicURL The public URL that is the HTTP endpoint from where you can access Object Storage. It includes the Object Storage API version number and your account name. For example, https://23.253.72. 207/v1/my_account.
token The authentication token for Object Storage.
To obtain these values, run the swift stat -v command.
As shown in this example, the public URL appears in the StorageURL field, and the token appears in the Auth Token field:
              StorageURL: https://23.253.72.207/v1/my_account
Auth Token: {token}
Account: my_account
Containers: 2
Objects: 3
Bytes: 47
Meta Book: MobyDick
  OpenStack command-line clients 115
 User Guide (Release Version: 15.0.0)
    X-Timestamp: 1389453423.35964
X-Trans-Id: txee55498935404a2caad89-0052dd3b77
Content-Type: text/plain; charset=utf-8
Accept-Ranges: bytes
 Object versioning
You can store multiple versions of your content so that you can recover from unintended overwrites. Object versioning is an easy way to implement version control, which you can use with any type of content.
Note: You cannot version a large-object manifest file, but the large-object manifest file can point to versioned segments.
We strongly recommend that you put non-current objects in a different container than the container where current object versions reside.
To enable and use object versioning
1. Toenableobjectversioning,askyourcloudprovidertosettheallow_versionsoptiontoTRUEinthe container configuration file.
2. Createanarchivecontainertostoreolderversionsofobjects:
$ curl -i $publicURL/archive -X PUT -H "Content-Length: 0" -H "X-Auth-Token: $token"
3. Createacurrentcontainertostorecurrentversionsofobjects.
Include the X-Versions-Location header. This header defines the container that holds the non-current versions of your objects. You must UTF-8-encode and then URL-encode the container name before you include it in the X-Versions-Location header. This header enables object versioning for all objects in the current container. Changes to objects in the current container automatically create non-current versions in the archive container.
           HTTP/1.1 201 Created
Content-Length: 0
Content-Type: text/html; charset=UTF-8
X-Trans-Id: tx46f8c29050834d88b8d7e-0052e1859d
Date: Thu, 23 Jan 2014 21:11:57 GMT
     $ curl -i $publicURL/current -X PUT -H "Content-Length: 0" -H \ ”X-Auth-Token: $token" -H "X-Versions-Location: archive"
     HTTP/1.1 201 Created
Content-Length: 0
Content-Type: text/html; charset=UTF-8
X-Trans-Id: txb91810fb717347d09eec8-0052e18997
Date: Thu, 23 Jan 2014 21:28:55 GMT
 4. Createthefirstversionofanobjectinthecurrentcontainer:
 116
OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    $ curl -i $publicURL/current/my_object --data-binary 1 -X PUT -H \ ”Content-Length: 0" -H "X-Auth-Token: $token"
     HTTP/1.1 201 Created
Last-Modified: Thu, 23 Jan 2014 21:31:22 GMT
Content-Length: 0
Etag: d41d8cd98f00b204e9800998ecf8427e
Content-Type: text/html; charset=UTF-8
X-Trans-Id: tx5992d536a4bd4fec973aa-0052e18a2a
Date: Thu, 23 Jan 2014 21:31:22 GMT
 Nothing is written to the non-current version container when you initially PUT an object in the current container. However, subsequent PUT requests that edit an object trigger the creation of a version of that object in the archive container.
These non-current versions are named as follows:
   <length><object_name><timestamp>
Where length is the 3-character, zero-padded hexadecimal character length of the object, <ob- ject_name> is the object name, and <timestamp> is the time when the object was initially created as a current version.
5. Createasecondversionoftheobjectinthecurrentcontainer:
         $ curl -i $publicURL/current/my_object --data-binary 2 -X PUT -H \ “Content-Length: 0" -H "X-Auth-Token: $token"
     HTTP/1.1 201 Created
Last-Modified: Thu, 23 Jan 2014 21:41:32 GMT
Content-Length: 0
Etag: d41d8cd98f00b204e9800998ecf8427e
Content-Type: text/html; charset=UTF-8
X-Trans-Id: tx468287ce4fc94eada96ec-0052e18c8c
Date: Thu, 23 Jan 2014 21:41:32 GMT
 6. Issue a GET request to a versioned object to get the current version of the object. You do not have to do any request redirects or metadata lookups.
List older versions of the object in the archive container:
    $ curl -i $publicURL/archive?prefix=009my_object -X GET -H \ "X-Auth-Token: $token"
     HTTP/1.1 200 OK
Content-Length: 30
X-Container-Object-Count: 1
Accept-Ranges: bytes
X-Timestamp: 1390513280.79684
X-Container-Bytes-Used: 0
Content-Type: text/plain; charset=utf-8
X-Trans-Id: tx9a441884997542d3a5868-0052e18d8e
Date: Thu, 23 Jan 2014 21:45:50 GMT
009my_object/1390512682.92052
  OpenStack command-line clients 117
 User Guide (Release Version: 15.0.0)
 Note: A POST request to a versioned object updates only the metadata for the object and does not create a new version of the object. New versions are created only when the content of the object changes.
7. Issue a DELETE request to a versioned object to remove the current version of the object and replace it with the next-most current version in the non-current container.
     $ curl -i $publicURL/current/my_object -X DELETE -H \ "X-Auth-Token: $token"
     HTTP/1.1 204 No Content
Content-Length: 0
Content-Type: text/html; charset=UTF-8
X-Trans-Id: tx006d944e02494e229b8ee-0052e18edd
Date: Thu, 23 Jan 2014 21:51:25 GMT
 List objects in the archive container to show that the archived object was moved back to the current container:
    $ curl -i $publicURL/archive?prefix=009my_object -X GET -H \ "X-Auth-Token: $token"
     HTTP/1.1 204 No Content
Content-Length: 0
X-Container-Object-Count: 0
Accept-Ranges: bytes
X-Timestamp: 1390513280.79684
X-Container-Bytes-Used: 0
Content-Type: text/html; charset=UTF-8
X-Trans-Id: tx044f2a05f56f4997af737-0052e18eed
Date: Thu, 23 Jan 2014 21:51:41 GMT
 This next-most current version carries with it any metadata last set on it. If you want to completely remove an object and you have five versions of it, you must DELETE it five times.
8. To disable object versioning for the current container, remove its X-Versions-Location metadata header by sending an empty key value.
    $ curl -i $publicURL/current -X PUT -H "Content-Length: 0" -H \ "X-Auth-Token: $token" -H "X-Versions-Location: "
       HTTP/1.1 202 Accepted
  Content-Length: 76
  Content-Type: text/html; charset=UTF-8
  X-Trans-Id: txe2476de217134549996d0-0052e19038
  Date: Thu, 23 Jan 2014 21:57:12 GMT
<html><h1>Accepted</h1><p>The request is accepted for processing.</p></html>
 Versioning with python-swiftclient
You can utilize python-swiftclient to enable object versioning.
 118 OpenStack command-line clients
 • Create an additional container to hold previous versions:
$ swift post CONTAINER_versions
• Enable object versioning on your desired container:
$ swift post CONTAINER -H "X-Versions-Location:CONTAINER-versions"
Object expiration
You can schedule Object Storage (swift) objects to expire by setting the X-Delete-At or X-Delete-After header. Once the object is deleted, swift will no longer serve the object and it will be deleted from the cluster shortly thereafter.
User Guide (Release Version: 15.0.0)
          •
•
Set an object to expire at an absolute time (in Unix time). You can get the current Unix time by running date +'%s'.
$ swift post CONTAINER OBJECT_FILENAME -H "X-Delete-At:UNIX_TIME" Verify the X-Delete-At header has posted to the object:
$ swift stat CONTAINER OBJECT_FILENAME
Set an object to expire after a relative amount of time (in seconds):
$ swift post CONTAINER OBJECT_FILENAME -H "X-Delete-After:SECONDS"
The X-Delete-After header will be converted to X-Delete-At. Verify the X-Delete-At header has
posted to the object:
$ swift stat CONTAINER OBJECT_FILENAME
If you no longer want to expire the object, you can remove the X-Delete-At header:
$ swift post CONTAINER OBJECT_FILENAME -H "X-Remove-Delete-At:"
                          In order for object expiration to work properly, the swift-object-expirer daemon will need access to all backend servers in the cluster. The daemon does not need access to the proxy-server or public network.
Serialized response formats
By default, the Object Storage API uses a text/plain response format. In addition, both JSON and XML data serialization response formats are supported.
Note: To run the cURL command examples, you must export environment variables. For more information, see the section Environment variables required to run examples.
To define the response format, use one of these methods:
Note:
    OpenStack command-line clients 119
 User Guide (Release Version: 15.0.0)
   Method
 Description
  format= format query parameter
 Append this parameter to the URL for a GET request, where format is json or xml.
  Accept request header
 Include this header in the GET request. The valid header values are:
text/plain Plain text response format. The default. application/jsontext JSON data serialization re-
sponse format.
application/xml XML data serialization response
format.
text/xml XML data serialization response format.
   Example 1. JSON example with format query parameter
For example, this request uses the format query parameter to ask for a JSON response: $ curl -i $publicURL?format=json -X GET -H "X-Auth-Token: $token"
         HTTP/1.1 200 OK
Content-Length: 96
X-Account-Object-Count: 1
X-Timestamp: 1389453423.35964
X-Account-Meta-Subject: Literature
X-Account-Bytes-Used: 14
X-Account-Container-Count: 2
Content-Type: application/json; charset=utf-8
Accept-Ranges: bytes
X-Trans-Id: tx274a77a8975c4a66aeb24-0052d95365
Date: Fri, 17 Jan 2014 15:59:33 GMT
 Object Storage lists container names with additional information in JSON format:
    [
{
}, {
} ]
"count":0, "bytes":0, "name":"janeausten"
"count":1, "bytes":14, "name":"marktwain"
 Example 2. XML example with Accept header
This request uses the Accept request header to ask for an XML response:
    $ curl -i $publicURL -X GET -H "X-Auth-Token: $token" -H \ ”Accept: application/xml; charset=utf-8"
  120 OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    HTTP/1.1 200 OK
Content-Length: 263
X-Account-Object-Count: 3
X-Account-Meta-Book: MobyDick
X-Timestamp: 1389453423.35964
X-Account-Bytes-Used: 47
X-Account-Container-Count: 2
Content-Type: application/xml; charset=utf-8
Accept-Ranges: bytes
X-Trans-Id: txf0b4c9727c3e491694019-0052e03420
Date: Wed, 22 Jan 2014 21:12:00 GMT
 Object Storage lists container names with additional information in XML format:
    <?xml version="1.0" encoding="UTF-8"?>
<account name="AUTH_73f0aa26640f4971864919d0eb0f0880"> <container>
<name>janeausten</name> <count>2</count> <bytes>33</bytes>
    </container>
    <container>
<name>marktwain</name> <count>1</count> <bytes>14</bytes>
    </container>
</account>
 The remainder of the examples in this guide use standard, non-serialized responses. However, all GET requests that perform list operations accept the format query parameter or Accept request header.
Page through large lists of containers or objects
If you have a large number of containers or objects, you can use the marker, limit, and end_marker param- eters to control how many items are returned in a list and where the list starts or ends.
• marker When you request a list of containers or objects, Object Storage returns a maximum of 10,000 names for each request. To get subsequent names, you must make another request with the marker parameter. Set the marker parameter to the name of the last item returned in the previous list. You must URL-encode the marker value before you send the HTTP request. Object Storage returns a maximum of 10,000 names starting after the last item returned.
• limit To return fewer than 10,000 names, use the limit parameter. If the number of names returned equals the specified limit (or 10,000 if you omit the limit parameter), you can assume there are more names to list. If the number of names in the list is exactly divisible by the limit value, the last request has no content.
• end_marker Limits the result set to names that are less than the end_marker parameter value. You must URL-encode the end_marker value before you send the HTTP request.
To page through a large list of containers
Assume the following list of container names:
 OpenStack command-line clients 121
 User Guide (Release Version: 15.0.0)
    apples
bananas
kiwis
oranges
pears
 1. Usealimitoftwo:
# curl -i $publicURL/?limit=2 -X GET -H "X-Auth-Token: $token"
Because two container names are returned, there are more names to list.
2. Makeanotherrequestwithamarkerparametersettothenameofthelastitemreturned:
Again, two items are returned, and there might be more.
3. Makeanotherrequestwithamarkerofthelastitemreturned:
pears
You receive a one-item response, which is fewer than the limit number of names. This indicates that this is the end of the list.
4. Usetheend_markerparametertolimittheresultsettoobjectnamesthatarelessthantheend_marker parameter value:
You receive a result set of all container names before the end-marker value.
Pseudo-hierarchical folders and directories
Although you cannot nest directories in OpenStack Object Storage, you can simulate a hierarchical structure within a single container by adding forward slash characters (/) in the object name. To navigate the pseudo- directory structure, you can use the delimiter query parameter. This example shows you how to use pseudo- hierarchical folders and directories.
         apples
bananas
     # curl -i $publicURL/?limit=2&amp;marker=bananas -X GET -H \ “X-Auth-Token: $token"
     kiwis
oranges
     # curl -i $publicURL/?limit=2&amp;marker=oranges -X GET -H \” X-Auth-Token: $token"
          # curl -i $publicURL/?end_marker=oranges -X GET -H \” X-Auth-Token: $token"
     apples
bananas
kiwis
  122 OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
 Note: In this example, the objects reside in a container called backups. Within that container, the objects are organized in a pseudo-directory called photos. The container name is not displayed in the example, but it is a part of the object URLs. For instance, the URL of the picture me.jpg is https://storage.swiftdrive. com/v1/CF_xer7_343/backups/photos/me.jpg.
List pseudo-hierarchical folders request: HTTP
To display a list of all the objects in the storage container, use GET without a delimiter or prefix.
The system returns status code 2xx (between 200 and 299, inclusive) and the requested list of the objects.
Use the delimiter parameter to limit the displayed results. To use delimiter with pseudo-directories, you must use the parameter slash (/).
The system returns status code 2xx (between 200 and 299, inclusive) and the requested matching objects. Be- cause you use the slash, only the pseudo-directory photos/ displays. The returned values from a slash de- limiter query are not real objects. The value will refer to a real object if it does not end with a slash. The pseudo-directories have no content-type, rather, each pseudo-directory has its own subdir entry in the response of JSON and XML results. For example:
     $ curl -X GET -i -H "X-Auth-Token: $token" \ $publicurl/v1/AccountString/backups
     photos/animals/cats/persian.jpg
photos/animals/cats/siamese.jpg
photos/animals/dogs/corgi.jpg
photos/animals/dogs/poodle.jpg
photos/animals/dogs/terrier.jpg
photos/me.jpg
photos/plants/fern.jpg
photos/plants/rose.jpg
     $ curl -X GET -i -H "X-Auth-Token: $token" \ $publicurl/v1/AccountString/backups?delimiter=/
     [
{
"subdir": "photos/" }
]
[ {
"subdir": "photos/animals/" },
{
"hash": "b249a153f8f38b51e92916bbc6ea57ad", "last_modified": "2015-12-03T17:31:28.187370", "bytes": 2906,
"name": "photos/me.jpg",
"content_type": "image/jpeg"
}, {
  OpenStack command-line clients 123
 User Guide (Release Version: 15.0.0)
    "subdir": "photos/plants/" }
]
     <?xml version="1.0" encoding="UTF-8"?>
<container name="backups"> <subdir name="photos/">
<name>photos/</name> </subdir>
 </container>
 <?xml version="1.0" encoding="UTF-8"?>
<container name="backups"> <subdir name="photos/animals/">
<name>photos/animals/</name> </subdir>
<object>
<name>photos/me.jpg</name> <hash>b249a153f8f38b51e92916bbc6ea57ad</hash> <bytes>2906</bytes> <content_type>image/jpeg</content_type> <last_modified>2015-12-03T17:31:28.187370</last_modified>
</object>
<subdir name="photos/plants/">
<name>photos/plants/</name> </subdir>
 </container>
 Use the prefix and delimiter parameters to view the objects inside a pseudo-directory, including further nested pseudo-directories.
The system returns status code 2xx (between 200 and 299, inclusive) and the objects and pseudo-directories within the top level pseudo-directory.
You can create an unlimited number of nested pseudo-directories. To navigate through them, use a longer prefix parameter coupled with the delimiter parameter. In this sample output, there is a pseudo-directory called dogs within the pseudo-directory animals. To navigate directly to the files contained within dogs, enter the following command:
The system returns status code 2xx (between 200 and 299, inclusive) and the objects and pseudo-directories within the nested pseudo-directory.
    $ curl -X GET -i -H "X-Auth-Token: $token" \ $publicurl/v1/AccountString/backups?prefix=photos/&delimiter=/
     photos/animals/
photos/me.jpg
photos/plants/
     $ curl -X GET -i -H "X-Auth-Token: $token" \ $publicurl/v1/AccountString/backups?prefix=photos/animals/dogs/&delimiter=/
     photos/animals/dogs/corgi.jpg
photos/animals/dogs/poodle.jpg
photos/animals/dogs/terrier.jpg
  124 OpenStack command-line clients
 Discoverability
Your Object Storage system might not enable all features that this document describes. These features are: • Large objects
• Auto-extract archive files • Bulk delete
• Create static website
To discover which features are enabled in your Object Storage system, use the /info request.
To use the /info request, send a GET request using the /info path to the Object Store endpoint as shown in
this example:
$ curl https://storage.example.com/info This example shows a truncated response body:
User Guide (Release Version: 15.0.0)
         {
"swift":{
"version":"1.11.0" },
"staticweb":{ },
"tempurl":{ }
}
 This output shows that the Object Storage system has enabled the static website and temporary URL features. Note: In some cases, the /info request will return an error. This could be because your service provider has
disabled the /info request function, or because you are using an older version that does not support it.
Large objects
To discover whether your Object Storage system supports this feature, see Discoverability or check with your service provider.
By default, the content of an object cannot be greater than 5 GB. However, you can use a number of smaller objects to construct a large object. The large object is comprised of two types of objects:
• Segment objects store the object content. You can divide your content into segments and upload each segment into its own segment object. Segment objects do not have any special features. You create, update, download, and delete segment objects just as you do with normal objects.
• A manifest object links the segment objects into one logical large object. When you download a manifest object, Object Storage concatenates and returns the contents of the segment objects in the re- sponse body. This behavior extends to the response headers returned by GET and HEAD requests. The Content-Length response header contains the total size of all segment objects.
   OpenStack command-line clients 125
 User Guide (Release Version: 15.0.0)
Note:
Object Storage takes the ETag value of each segment, concatenates them together, and returns the MD5 checksum of the result to calculate the ETag response header value. The manifest object types are:
Static large objects The manifest object content is an ordered list of the names of the segment objects in JSON format. See Static large objects.
Dynamic large objects The manifest object has no content but it has a X-Object-Manifest metadata header. The value of this header is CONTAINER/PREFIX, where CONTAINER is the name of the container where the segment objects are stored, and PREFIX is a string that all segment objects have in common. See Dynamic large objects.
 If you use a manifest object as the source of a COPY request, the new object is a normal, and not a segment, object. If the total size of the source segment objects exceeds 5 GB, the COPY request fails. However, you can make a duplicate of the manifest object and this new object can be larger than 5 GB.
Static large objects
To create a static large object, divide your content into pieces and create (upload) a segment object to contain each piece.
You must record the ETag response header value that the PUT operation returns. Alternatively, you can calculate the MD5 checksum of the segment before you perform the upload and include this value in the ETag request header. This action ensures that the upload cannot corrupt your data.
List the name of each segment object along with its size and MD5 checksum in order.
Create a manifest object. Include the ?multipart-manifest=put query string at the end of the manifest object name to indicate that this is a manifest object.
The body of the PUT request on the manifest object comprises a JSON list where each element contains these attributes:
path The container and object name in the format: CONTAINER_NAME/OBJECT_NAME.
etag The MD5 checksum of the content of the segment object. This value must match the ETag of that object. size_bytes The size of the segment object. This value must match the Content-Length of that object.
Static large object manifest list
This example shows three segment objects. You can use several containers and the object names do not have to conform to a specific pattern, in contrast to dynamic large objects.
     [
{
}, {
"path": "mycontainer/objseg1",
"etag": "0228c7926b8b642dfb29554cd1f00963", "size_bytes": 1468006
},
"path": "mycontainer/pseudodir/seg-obj2", "etag": "5bfc9ea51a00b790717eeb934fb77b9b", "size_bytes": 1572864
  126 OpenStack command-line clients
 The Content-Length request header must contain the length of the JSON content and not the length of the segment objects. However, after the PUT operation completes, the Content-Length metadata is set to the total length of all the object segments. A similar situation applies to the ETag. If used in the PUT operation, it must contain the MD5 checksum of the JSON content. The ETag metadata value is then set to be the MD5 checksum of the concatenated ETag values of the object segments. You can also set the Content-Type request header and custom object metadata.
When the PUT operation sees the ?multipart-manifest=put query parameter, it reads the request body and verifies that each segment object exists and that the sizes and ETags match. If there is a mismatch, the PUT operation fails.
If everything matches, the API creates the manifest object and sets the X-Static-Large-Object metadata to true to indicate that the manifest is a static object manifest.
Normally when you perform a GET operation on the manifest object, the response body contains the concatenated content of the segment objects. To download the manifest list, use the ?multipart-manifest=get query parameter. The list in the response is not formatted the same as the manifest that you originally used in the PUT operation.
If you use the DELETE operation on a manifest object, the manifest object is deleted. The segment objects are not affected. However, if you add the ?multipart-manifest=delete query parameter, the segment objects are deleted and if all are successfully deleted, the manifest object is also deleted.
To change the manifest, use a PUT operation with the ?multipart-manifest=put query parameter. This request creates a manifest object. You can also update the object metadata in the usual way.
Dynamic large objects
Before you can upload objects that are larger than 5 GB, you must segment them. You upload the segment objects like you do with any other object and create a dynamic large manifest object. The manifest object tells Object Storage how to find the segment objects that comprise the large object. You can still access each segment individually, but when you retrieve the manifest object, the API concatenates the segments. You can include any number of segments in a single large object.
To ensure the download works correctly, you must upload all the object segments to the same container and prefix each object name so that the segments sort in correct concatenation order.
You also create and upload a manifest file. The manifest file is a zero-byte file with the extra X-Object- Manifest CONTAINER/PREFIX header. The CONTAINER is the container the object segments are in and PREFIX is the common prefix for all the segments. You must UTF-8-encode and then URL-encode the container and common prefix in the X-Object-Manifest header.
User Guide (Release Version: 15.0.0)
    {
"path": "other-container/seg-final", "etag": "b9c3da507d2557c1ddc51f27c54bae51", "size_bytes": 256
} ]
  OpenStack command-line clients 127
 User Guide (Release Version: 15.0.0)
It is best to upload all the segments first and then create or update the manifest. With this method, the full object is not available for downloading until the upload is complete. Also, you can upload a new set of segments to a second location and update the manifest to point to this new location. During the upload of the new segments, the original manifest is still available to download the first set of segments.
Upload segment of large object request: HTTP
No response body is returned.
The 2‘‘nn‘‘ response code indicates a successful write. nn is a value from 00 to 99.
The Length Required (411) response code indicates that the request does not include a required Content- Length or Content-Type header.
The Unprocessable Entity (422) response code indicates that the MD5 checksum of the data written to the storage system does NOT match the optional ETag value.
You can continue to upload segments, like this example shows, before you upload the manifest.
Upload next segment of large object request: HTTP
Next, upload the manifest. This manifest specifies the container where the object segments reside. Note that if you upload additional segments after you create the manifest, the concatenated object becomes that much larger but you do not need to recreate the manifest file for subsequent additional segments.
Upload manifest request: HTTP
    PUT /API_VERSION/ACCOUNT/CONTAINER/OBJECT HTTP/1.1
Host: storage.example.com
X-Auth-Token: eaaafd18-0fed-4b3a-81b4-663c99ec1cbb
ETag: 8a964ee2a5e88be344f36c22562a6486
Content-Length: 1
X-Object-Meta-PIN: 1234
     PUT /API_VERSION/ACCOUNT/CONTAINER/OBJECT HTTP/1.1
Host: storage.example.com
X-Auth-Token: eaaafd18-0fed-4b3a-81b4-663c99ec1cbb
ETag: 8a964ee2a5e88be344f36c22562a6486
Content-Length: 1
X-Object-Meta-PIN: 1234
     PUT /API_VERSION/ACCOUNT/CONTAINER/OBJECT HTTP/1.1
Host: storage.clouddrive.com
X-Auth-Token: eaaafd18-0fed-4b3a-81b4-663c99ec1cbb
Content-Length: 0
X-Object-Meta-PIN: 1234
X-Object-Manifest: CONTAINER/PREFIX
  128 OpenStack command-line clients
 Upload manifest response: HTTP
[...]
A GET or HEAD request on the manifest returns a Content-Type response header value that is the same as the Content-Type request header value in the PUT request that created the manifest. To change the Content- Type, reissue the PUT request.
Extra transaction information
You can use the X-Trans-Id-Extra request header to include extra information to help you debug any errors that might occur with large object upload and other Object Storage transactions.
The Object Storage API appends the first 32 characters of the X-Trans-Id-Extra request header value to the transaction ID value in the generated X-Trans-Id response header. You must UTF-8-encode and then URL-encode the extra transaction information before you include it in the X-Trans-Id-Extra request header.
For example, you can include extra transaction information when you upload large objects such as images.
When you upload each segment and the manifest, include the same value in the X-Trans-Id-Extra request header. If an error occurs, you can find all requests that are related to the large object upload in the Object Storage logs.
You can also use X-Trans-Id-Extra strings to help operators debug requests that fail to receive responses. The operator can search for the extra information in the logs.
Comparison of static and dynamic large objects
While static and dynamic objects have similar behavior, this table describes their differences:
User Guide (Release Version: 15.0.0)
      OpenStack command-line clients 129
 User Guide (Release Version: 15.0.0)
  Description
 Static large object
 Dynamic large object
  End-to-end integrity
 Assured. The list of segments includes the MD5 checksum (ETag) of each seg- ment. You cannot upload the mani- fest object if the ETag in the list differs from the uploaded segment object. If a segment is somehow lost, an attempt to download the manifest object results in an error.
 Not guaranteed. The eventual consis- tency model means that although you have uploaded a segment object, it might not appear in the container list- ing until later. If you download the manifest before it appears in the con- tainer, it does not form part of the con- tent returned in response to a GET re- quest.
  Upload order
 You must upload the segment objects before upload the manifest object.
 You can upload manifest and segment objects in any order. You are recom- mended to upload the manifest object after the segments in case a prema- ture download of the manifest occurs. However, this is not enforced.
  Removal or addition of segment objects
 You cannot add or remove segment ob- jects from the manifest. However, you can create a completely new manifest object of the same name with a differ- ent manifest list.
 You can upload new segment objects or remove existing segments. The names must simply match the PREFIX sup- plied in X-Object-Manifest.
  Segment object size and number
 Segment objects must be at least 1 MB in size (by default). The final segment object can be any size. At most, 1000 segments are supported (by default).
 Segment objects can be any size.
  Segment object con- tainer name
 The manifest list includes the container name of each object. Segment objects can be in different containers.
 All segment objects must be in the same container.
  Manifest object meta- data
 The object has X-Static-Large- Object set to true. You do not set this metadata directly. Instead the system sets it when you PUT a static manifest object.
 The X-Object-Manifest value is the CONTAINER/PREFIX, which indicates where the segment objects are located. You supply this request header in the PUT operation.
  Copying the manifest object
 Include the ?multipart- manifest=get query string in the COPY request. The new object contains the same manifest as the original. The segment objects are not copied. Instead, both the original and new manifest objects share the same set of segment objects.
 The COPY operation does not create a manifest object. To duplicate a manifest object, use the GET opera- tion to read the value of X-Object- Manifest and use this value in the X- Object-Manifest request header in a PUT operation. This creates a new manifest object that shares the same set of segment objects as the original man- ifest object.
         Upload large objects with python-swiftclient
You can use python-swiftclient to easily upload large objects.
• Uploadalargefilebyspecifyingthesegmentsizewiththe--segment-sizeor-Sarguments:
 130 OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    $ swift upload CONTAINER OBJECT_FILENAME --segment-size <bytes>
This will automatically break the file into the desired segment size and upload segments to a container
named <container>_segments.
• After upload has completed, you can download the large object as a single file:
$ swift download CONTAINER OBJECT_FILENAME
• Additionallargeobjectargumentscanbefoundbyusing--help:
$ swift upload --help
Auto-extract archive files
To discover whether your Object Storage system supports this feature, see Discoverability. Alternatively, check with your service provider.
Use the auto-extract archive feature to upload a tar archive file.
The Object Storage system extracts files from the archive file and creates an object.
Auto-extract archive request
To upload an archive file, make a PUT request. Add the extract-archive=format query parameter to indicate that you are uploading a tar archive file instead of normal content.
Valid values for the format variable are tar, tar.gz, or tar.bz2.
The path you specify in the PUT request is used for the location of the object and the prefix for the resulting
object names.
In the PUT request, you can specify the path for:
• An account
• Optionally, a specific container
• Optionally, a specific object prefix
For example, if the first object in the tar archive is /home/file1.txt and you specify the /v1/ 12345678912345/mybackup/castor/ path, the operation creates the castor/home/file1.txt object in the mybackup container in the 12345678912345 account.
Create an archive for auto-extract
You must use the tar utility to create the tar archive file.
You can upload regular files but you cannot upload other items (for example, empty directories or symbolic links).
You must UTF-8-encode the member names.
The archive auto-extract feature supports these formats:
• The POSIX.1-1988 Ustar format.
            OpenStack command-line clients 131
 User Guide (Release Version: 15.0.0)
• The GNU tar format. Includes the long name, long link, and sparse extensions.
• The POSIX.1-2001 pax format.
Use gzip or bzip2 to compress the archive.
Use the extract-archive query parameter to specify the format. Valid values for this parameter are
tar, tar.gz, or tar.bz2. Auto-extract archive response
When Object Storage processes the request, it performs multiple sub-operations. Even if all sub-operations fail, the operation returns a 201 Created status. Some sub-operations might succeed while others fail. Examine the response body to determine the results of each auto-extract archive sub-operation.
You can set the Accept request header to one of these values to define the response format:
text/plain Formats response as plain text. If you omit the Accept header, text/plain is the default.
application/json Formats response as JSON.
application/xml Formats response as XML.
text/xml Formats response as XML.
The following auto-extract archive files example shows a text/plain response body where no failures oc- curred:
The following auto-extract archive files example shows a text/plain response where some failures occurred. In this example, the Object Storage system is configured to reject certain character strings so that the 400 Bad Request error occurs for any objects that use the restricted strings.
The following example shows the failure response in application/json format.
    Number Files Created: 10
Errors:
     Number Files Created: 8
Errors:
/v1/12345678912345/mycontainer/home/xx%3Cyy, 400 Bad Request
/v1/12345678912345/mycontainer/../image.gif, 400 Bad Request
     {
"Number Files Created":1, "Errors":[
      [
         "/v1/12345678912345/mycontainer/home/xx%3Cyy",
         "400 Bad Request"
], [
] ]
}
"/v1/12345678912345/mycontainer/../image.gif",
"400 Bad Request"
  132 OpenStack command-line clients
 Bulk delete
To discover whether your Object Storage system supports this feature, see Discoverability. Alternatively, check with your service provider.
With bulk delete, you can delete up to 10,000 objects or containers (configurable) in one request.
Bulk delete request
To perform a bulk delete operation, add the bulk-delete query parameter to the path of a POST or DELETE operation.
Note: The DELETE operation is supported for backwards compatibility.
The path is the account, such as /v1/12345678912345, that contains the objects and containers.
In the request body of the POST or DELETE operation, list the objects or containers to be deleted. Separate each name with a newline character. You can include a maximum of 10,000 items (configurable) in the list.
In addition, you must:
• UTF-8-encode and then URL-encode the names.
• To indicate an object, specify the container and object name as: CONTAINER_NAME/OBJECT_NAME.
• To indicate a container, specify the container name as: CONTAINER_NAME. Make sure that the container is empty. If it contains objects, Object Storage cannot delete the container.
• SettheContent-Typerequestheadertotext/plain. Bulk delete response
When Object Storage processes the request, it performs multiple sub-operations. Even if all sub-operations fail, the operation returns a 200 status. The bulk operation returns a response body that contains details that indicate which sub-operations have succeeded and failed. Some sub-operations might succeed while others fail. Examine the response body to determine the results of each delete sub-operation.
You can set the Accept request header to one of the following values to define the response format: text/plain Formats response as plain text. If you omit the Accept header, text/plain is the default. application/json Formats response as JSON.
application/xml or text/xml Formats response as XML.
The response body contains the following information:
• The number of files actually deleted.
• The number of not found objects.
• Errors. A list of object names and associated error statuses for the objects that failed to delete. The format depends on the value that you set in the Accept header.
The following bulk delete response is in application/xml format. In this example, the mycontainer con- tainer is not empty, so it cannot be deleted.
User Guide (Release Version: 15.0.0)
   OpenStack command-line clients 133
 User Guide (Release Version: 15.0.0)
    <delete> <number_deleted>2</number_deleted> <number_not_found>4</number_not_found> <errors>
<object> <name>/v1/12345678912345/mycontainer</name> <status>409 Conflict</status>
        </object>
    </errors>
</delete>
 Create static website
To discover whether your Object Storage system supports this feature, see Discoverability. Alternatively, check with your service provider.
You can use your Object Storage account to create a static website. This static website is created with Static Web middleware and serves container data with a specified index file, error file resolution, and optional file listings. This mode is normally active only for anonymous requests, which provide no authentication token. To use it with authenticated requests, set the header X-Web-Mode to TRUE on the request.
The Static Web filter must be added to the pipeline in your /etc/swift/proxy-server.conf file below any authentication middleware. You must also add a Static Web middleware configuration section.
See the Cloud Administrator Guide for an example of the static web configuration syntax.
See the Cloud Administrator Guide for a complete example of the /etc/swift/proxy-server.conf file (including
static web).
Your publicly readable containers are checked for two headers, X-Container-Meta-Web-Index and X- Container-Meta-Web-Error. The X-Container-Meta-Web-Error header is discussed below, in the sec- tion called Set error pages for static website.
Use X-Container-Meta-Web-Index to determine the index file (or default page served, such as index.html) for your website. When someone initially enters your site, the index.html file displays automatically. If you create sub-directories for your site by creating pseudo-directories in your container, the index page for each sub-directory is displayed by default. If your pseudo-directory does not have a file with the same name as your index file, visits to the sub-directory return a 404 error.
You also have the option of displaying a list of files in your pseudo-directory instead of a web page. To do this, set the X-Container-Meta-Web-Listings header to TRUE. You may add styles to your file listing by setting X-Container-Meta-Web-Listings-CSS to a style sheet (for example, lists.css).
Static Web middleware through Object Storage
The following sections show how to use Static Web middleware through Object Storage.
Make container publicly readable
Make the container publicly readable. Once the container is publicly readable, you can access your objects directly, but you must set the index file to browse the main site URL and its sub-directories.
 134 OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    $ swift post -r '.r:*,.rlistings' container
Set site index file
Set the index file. In this case, index.html is the default file displayed when the site appears. $ swift post -m 'web-index:index.html' container
Enable file listing
Turn on file listing. If you do not set the index file, the URL displays a list of the objects in the container. Instructions on styling the list with a CSS follow.
$ swift post -m 'web-listings: true' container
Enable CSS for file listing
Style the file listing using a CSS.
$ swift post -m 'web-listings-css:listings.css' container
Set error pages for static website
You can create and set custom error pages for visitors to your website; currently, only 401 (Unauthorized) and 404 (Not Found) errors are supported. To do this, set the metadata header, X-Container-Meta-Web-Error.
Error pages are served with the status code pre-pended to the name of the error page you set. For instance, if you set X-Container-Meta-Web-Error to error.html, 401 errors will display the page 401error.html. Similarly, 404 errors will display 404error.html. You must have both of these pages created in your container when you set the X-Container-Meta-Web-Error metadata, or your site will display generic error pages.
You only have to set the X-Container-Meta-Web-Error metadata once for your entire static website. Set error pages for static website request
$ swift post -m 'web-error:error.html' container Any 2nn response indicates success.
Create and manage stacks
The Orchestration service enables you to orchestrate multiple composite cloud applications. This service sup- ports use of both the Amazon Web Services (AWS) CloudFormation template format through both a Query API that is compatible with CloudFormation and the native OpenStack Heat Orchestration Template (HOT) format through a REST API.
                      OpenStack command-line clients 135
 User Guide (Release Version: 15.0.0)
These flexible template languages enable application developers to describe and automate the deployment of infrastructure, services, and applications. The templates enable creation of most OpenStack resource types, such as instances, floating IP addresses, volumes, security groups, and users. The resources, once created, are referred to as stacks.
The template languages are described in the Template Guide in the Heat developer documentation. Create a stack from an example template file
• To create a stack, or template, from an example template file, run the following command:
The --parameter values that you specify depend on the parameters that are defined in the template. If a website hosts the template file, you can also specify the URL with the --template parameter.
The command returns the following output:
    $ openstack stack create --template server_console.yaml \ --parameter "image=cirros" MYSTACK
     +---------------------+---------------------------------------------------------------  →-+
| Field | Value    → |
+---------------------+---------------------------------------------------------------  →-+
| id  → |
| stack_name  → |
| description  →attribute |
|
 → |
| 70b9feca-8f99-418e-b2f1-cc38d61b3ffb
| MYSTACK
| The heat template is used to demo the 'console_urls'
| of OS::Nova::Server.
||   → |
| creation_time  → |
| updated_time  → |
| stack_status  → |
| 2016-06-08T09:54:15
| None
| CREATE_IN_PROGRESS
| stack_status_reason |    → |
+---------------------+---------------------------------------------------------------  →-+
 • You can also use the --dry-run option with the openstack stack create command to validate a template file without creating a stack from it.
If validation fails, the response returns an error message.
Get information about stacks
To explore the state and history of a particular stack, you can run a number of commands. • To see which stacks are visible to the current user, run the following command:
 136 OpenStack command-line clients
 • To show the details of a stack, run the following command:
$ openstack stack show MYSTACK
• A stack consists of a collection of resources. To list the resources and their status, run the following
command:
User Guide (Release Version: 15.0.0)
    $ openstack stack list +--------------------------------------+------------+-----------------+---------------
 →------+--------------+
| ID | Stack Name | Stack Status | Creation Time
 → | Updated Time | +--------------------------------------+------------+-----------------+---------------
 →------+--------------+
| 70b9feca-8f99-418e-b2f1-cc38d61b3ffb | MYSTACK | CREATE_COMPLETE | 2016-06-
 →08T09:54:15 | None | +--------------------------------------+------------+-----------------+---------------
 →------+--------------+
          $ openstack stack resource list MYSTACK +---------------+--------------------------------------+------------------+-----------
 →------+---------------------+
| resource_name | physical_resource_id | resource_type | resource_
 →status | updated_time | +---------------+--------------------------------------+------------------+-----------
 →------+---------------------+
| server | 1b3a7c13-42be-4999-a2a1-8fbefd00062b | OS::Nova::Server | CREATE_
 →COMPLETE | 2016-06-08T09:54:15 | +---------------+--------------------------------------+------------------+-----------
 →------+---------------------+
 • To show the details for a specific resource in a stack, run the following command:
$ openstack stack resource show MYSTACK server
• Someresourceshaveassociatedmetadatawhichcanchangethroughoutthelifecycleofaresource.Show
the metadata by running the following command:
$ openstack stack resource metadata MYSTACK server
• A series of events is generated during the lifecycle of a stack. To display lifecycle events, run the fol-
lowing command:
• To show the details for a particular event, run the following command:
$ openstack stack event show MYSTACK server EVENT
              $ openstack stack event list MYSTACK
2016-06-08 09:54:15 [MYSTACK]: CREATE_IN_PROGRESS Stack CREATE started
2016-06-08 09:54:15 [server]: CREATE_IN_PROGRESS state changed
2016-06-08 09:54:41 [server]: CREATE_COMPLETE state changed
2016-06-08 09:54:41 [MYSTACK]: CREATE_COMPLETE Stack CREATE completed successfully
       OpenStack command-line clients 137
 User Guide (Release Version: 15.0.0)
Update a stack
To update an existing stack from a modified template file, run a command like the following command:
    $ openstack stack update --template server_console.yaml \ --parameter "image=ubuntu" MYSTACK
+---------------------+----------------------------------------------------------------+
| Field               | Value                                                          |
+---------------------+----------------------------------------------------------------+
| id
| stack_name
| description
| |||
| 267a459a-a8cd-4d3e-b5a1-8c08e945764f                           |
| mystack                                                        |
| The heat template is used to demo the 'console_urls' attribute |
| of OS::Nova::Server.                                           |
| creation_time
| updated_time
| stack_status
| stack_status_reason | Stack UPDATE started                                           |
+---------------------+----------------------------------------------------------------+
| 2016-06-08T09:54:15                                            |
| 2016-06-08T10:41:18                                            |
| UPDATE_IN_PROGRESS                                             |
 Some resources are updated in-place, while others are replaced with new resources.
Measure cloud resources
Telemetry measures cloud resources in OpenStack. It collects data related to billing. Currently, this metering service is available through only the ceilometer command-line client.
To model data, Telemetry uses the following abstractions:
Meter Measures a specific aspect of resource usage, such as the existence of a running instance, or ongoing performance, such as the CPU utilization for an instance. Meters exist for each type of resource. For example, a separate cpu_util meter exists for each instance. The lifecycle of a meter is decoupled from the existence of its related resource. The meter persists after the resource goes away.
A meter has the following attributes:
• String name
• A unit of measurement
• A type, which indicates whether values increase monotonically (cumulative), are interpreted as a change from the previous value (delta), or are stand-alone and relate only to the current duration (gauge)
Sample An individual data point that is associated with a specific meter. A sample has the same attributes as the associated meter, with the addition of time stamp and value attributes. The value attribute is also known as the sample volume.
Statistic A set of data point aggregates over a time duration. (In contrast, a sample represents a single data point.) The Telemetry service employs the following aggregation functions:
• count. The number of samples in each period.
• max. The maximum number of sample volumes in each period. • min. The minimum number of sample volumes in each period. • avg. The average of sample volumes over each period.
 138
OpenStack command-line clients
 • sum. The sum of sample volumes over each period.
Alarm A set of rules that define a monitor and a current state, with edge-triggered actions associated with target states. Alarms provide user-oriented Monitoring-as-a-Service and a general purpose utility for OpenStack. Orchestration auto scaling is a typical use case. Alarms follow a tristate model of ok, alarm, and insufficient data. For conventional threshold-oriented alarms, a static threshold value and comparison operator govern state transitions. The comparison operator compares a selected meter statistic against an evaluation window of configurable length into the recent past.
This example uses the openstack client to create an auto-scaling stack and the ceilometer client to measure resources.
1. Create an auto-scaling stack by running the following command. The -f option specifies the name of the stack template file, and the -P option specifies the KeyName parameter as heat_key:
2. List the heat resources that were created:
User Guide (Release Version: 15.0.0)
    $ openstack stack create --template cfn/F17/AutoScalingCeilometer.yaml \ --parameter "KeyName=heat_key" mystack
     $ openstack stack resource list mystack +---------------+--------------------------------------+------------------+-----------
 →------+---------------------+
| resource_name | physical_resource_id | resource_type | resource_
 →status | updated_time | +---------------+--------------------------------------+------------------+-----------
 →------+---------------------+
| server | 1b3a7c13-42be-4999-a2a1-8fbefd00062b | OS::Nova::Server | CREATE_
 →COMPLETE | 2013-10-02T05:53:41Z |
| ... | ... | ... | ...
 → | ... | +---------------+--------------------------------------+------------------+-----------
 →------+---------------------+
 3. List the alarms that are set:
    $ ceilometer alarm-list +--------------------------------------+------------------------------+---------------
 →----+---------+------------+----------------------------------+
| Alarm ID | Name | State
 → | Enabled | Continuous | Alarm condition | +--------------------------------------+------------------------------+---------------
 →----+---------+------------+----------------------------------+
| 4f896b40-0859-460b-9c6a-b0d329814496 | as-CPUAlarmLow-i6qqgkf2fubs | insufficient
 →data | True | False | cpu_util &lt; 15.0 during 1x 60s |
| 75d8ecf7-afc5-4bdc-95ff-19ed9ba22920 | as-CPUAlarmHigh-sf4muyfruy5m | insufficient
 →data | True | False | cpu_util &gt; 50.0 during 1x 60s | +--------------------------------------+------------------------------+---------------
 →----+---------+------------+----------------------------------+
 4. List the meters that are set:
 →--------------------------+----------------------------------+
OpenStack command-line clients 139
    $ ceilometer meter-list +-------------+------------+----------+--------------------------------------+--------
 →--------------------------+----------------------------------+
| Name | Type | Unit | Resource ID | User
 →ID | Project ID | +-------------+------------+----------+--------------------------------------+--------

 User Guide (Release Version: 15.0.0)
    | cpu | cumulative | ns | 3965b41b-81b0-4386-bea5-6ec37c8841c1 |   →d1a2996d3b1f4e0e8645ba9650308011 | bf03bf32e3884d489004ac995ff7a61c |
| cpu | cumulative | ns | 62520a83-73c7-4084-be54-275fe770ef2c |   →d1a2996d3b1f4e0e8645ba9650308011 | bf03bf32e3884d489004ac995ff7a61c |
| cpu_util | gauge | % | 3965b41b-81b0-4386-bea5-6ec37c8841c1 |   →d1a2996d3b1f4e0e8645ba9650308011 | bf03bf32e3884d489004ac995ff7a61c |
+-------------+------------+----------+--------------------------------------+--------  →--------------------------+----------------------------------+
 5. List samples:
    $ ceilometer sample-list -m cpu_util +--------------------------------------+----------+-------+---------------+------+----
 →-----------------+
| Resource ID | Name | Type | Volume | Unit |
 →Timestamp | +--------------------------------------+----------+-------+---------------+------+----
 →-----------------+
| 3965b41b-81b0-4386-bea5-6ec37c8841c1 | cpu_util | gauge | 3.98333333333 | % |
 →2013-10-02T10:50:12 | +--------------------------------------+----------+-------+---------------+------+----
 →-----------------+
 6. View statistics:
    $ ceilometer statistics -m cpu_util +--------+---------------------+---------------------+-------+---------------+--------
 →-------+---------------+---------------+----------+---------------------+-----------  →----------+
| Period | Period Start | Period End | Count | Min | Max    → | Sum | Avg | Duration | Duration Start | Duration   →End |
+--------+---------------------+---------------------+-------+---------------+--------  →-------+---------------+---------------+----------+---------------------+-----------  →----------+
| 0 | 2013-10-02T10:50:12 | 2013-10-02T10:50:12 | 1 | 3.98333333333 | 3.  →98333333333 | 3.98333333333 | 3.98333333333 | 0.0 | 2013-10-02T10:50:12 | 2013-  →10-02T10:50:12 |
+--------+---------------------+---------------------+-------+---------------+--------  →-------+---------------+---------------+----------+---------------------+-----------  →----------+
 Create and manage databases
The Database service provides scalable and reliable cloud provisioning functionality for both relational and non- relational database engines. Users can quickly and easily use database features without the burden of handling complex administrative tasks.
Create and access a database
Assume that you have installed the Database service and populated your data store with images for the type and versions of databases that you want, and that you can create and access a database.
This example shows you how to create and access a MySQL 5.5 database.
 140 OpenStack command-line clients
 Create and access a database
1. Determine which flavor to use for your database
When you create a database instance, you must specify a nova flavor. The flavor indicates various characteristics of the instance, such as RAM and root volume size. You will need to create or obtain new nova flavors that work for databases.
The first step is to list flavors by using the openstack flavor list command. $ openstack flavor list
Now take a look at the minimum requirements for various database instances:
• If you have a custom flavor that meets the needs of the database that you want to create, proceed to Step 2 and use that flavor.
• If your environment does not have a suitable flavor, an administrative user must create a custom flavor by using the openstack flavor create command.
MySQL example. This example creates a flavor that you can use with a MySQL database. This example has the following attributes:
• Flavorname:mysql_minimum
• Flavor ID: You must use an ID that is not already in use. In this example, IDs 1 through 5 are in
use, so use ID 6.
• RAM:512
• RootvolumesizeinGB:5
• VirtualCPUs:1
User Guide (Release Version: 15.0.0)
       Database
 RAM (MB)
 Disk (GB)
  VCPUs
 MySQL
 512
 5
  1
 Cassandra
 2048
 5
  1
 MongoDB
 1024
 5
  1
 Redis
 512
 5
  1
         $ openstack flavor create mysql-minimum --id 6 --ram 512 --disk 5 --vcpus 1 +----------------------------+---------------+
| Field | Value | +----------------------------+---------------+
| OS-FLV-DISABLED:disabled | OS-FLV-EXT-DATA:ephemeral | disk
|id
| False | |0 | |5 | |6 | | mysql-minimum |
| name
| os-flavor-access:is_public | True          |
| properties
| ram
| rxtx_factor
| swap
|               |
| 512           |
| 1.0           |
|               |
| 1             |
| vcpus
+----------------------------+---------------+
 2. Create a database instance
 OpenStack command-line clients 141
 User Guide (Release Version: 15.0.0)
This example creates a database instance with the following characteristics: • Nameoftheinstance:mysql_instance_1
• Databaseflavor:6
In addition, this command specifies these options for the instance:
• Avolumesizeof5(5GB).
• ThemyDBdatabase.
• Thedatabaseisbasedonthemysqldatastoreandthemysql-5.5datastore_version. • TheuserAuserwiththepasswordpassword.
    $ trove create mysql_instance_1 6 --size 5 --databases myDB \ --users userA:password --datastore_version mysql-5.5 \ --datastore mysql
+-------------------+-----------------------------------------------------------------  →----------------------t-------------------------------------------------------------  →----------------------------------------------------+
| Property |
  Value
 →
 → |
+-------------------+-----------------------------------------------------------------  →------------------------------------------------------------------------------------  →----------------------------------------------------+
| created |

2014-05-29T21:26:21
 →
 → |
| datastore |    → {u'version': u'mysql-5.5', u'type': u'mysql'}    → |
| datastore_version |    → mysql-5.5    → |
| flavor | {u'id': u'6', u'links': [{u'href': u'https://controller:8779/v1.  →0/46d0bc4fc32e4b9e8520f8fc62199f58/flavors/6', u'rel': u'self'}, {u'href': u'https:/  →/controller:8779/flavors/6', u'rel': u'bookmark'}]} |
| id |
5599dad6-731e-44df-bb60-488da3da9cfe
                                  |
 →  →
|
 →  →
|
 →  →
|
 →  →
|
 →  →
 →------------------------------------------------------------------------------------  →----------------------------------------------------+
name |




status
updated
volume
|
|
|
  mysql_instance_1
BUILD
2014-05-29T21:26:21
{u'size': 5}
|
|
|
                                                      |
+-------------------+-----------------------------------------------------------------
 3. Get the IP address of the database instance
142 OpenStack command-line clients

 First, use the trove list command to list all instances and their IDs:
User Guide (Release Version: 15.0.0)
    $ trove list +--------------------------------------+------------------+-----------+---------------
 →----+--------+-----------+------+
| id | name | datastore | datastore_
 →version | status | flavor_id | size | +--------------------------------------+------------------+-----------+---------------
 →----+--------+-----------+------+
| 5599dad6-731e-44df-bb60-488da3da9cfe | mysql_instance_1 | mysql | mysql-5.5
 → |BUILD|6|5| +--------------------------------------+------------------+-----------+---------------
 →----+--------+-----------+------+
 This command returns the instance ID of your new instance.
You can now pass in the instance ID with the trove show command to get the IP address of the instance.
In this example, replace INSTANCE_ID with 5599dad6-731e-44df-bb60-488da3da9cfe.
    $ trove show INSTANCE_ID
+-------------------+--------------------------------------+
|      Property     |                Value                 |
+-------------------+--------------------------------------+
|      created      |
|     datastore     |
| datastore_version |
|       flavor      |
          2014-05-29T21:26:21          |
                 mysql                 |
               mysql-5.5               |
                   6                   |
| 5599dad6-731e-44df-bb60-488da3da9cfe |
|         id
|         ip
|        name
|       status
|      updated
|       volume      |
+-------------------+--------------------------------------+
| | | |
    172.16.200.2             |
  mysql_instance_1           |
       BUILD                 |
2014-05-29T21:26:54          |
         5                   |
 This command returns the IP address of the database instance. 4. Access the new database
You can now access the new database you just created (myDB) by using typical database access com- mands. In this MySQL example, replace IP_ADDRESS with 172.16.200.2.
$ mysql -u userA -p password -h IP_ADDRESS myDB
Backup and restore a database
You can use Database services to backup a database and store the backup artifact in the Object Storage service. Later on, if the original database is damaged, you can use the backup artifact to restore the database. The restore process creates a database instance.
This example shows you how to back up and restore a MySQL database. 1. Backup the database instance
As background, assume that you have created a database instance with the following characteristics: • Nameofthedatabaseinstance:guest1
      OpenStack command-line clients 143
 User Guide (Release Version: 15.0.0)
• FlavorID:10
• Rootvolumesize:2
• Databases:db1anddb2
• Users:Theuser1userwiththepasswordpassword
First, get the ID of the guest1 database instance by using the trove list command:
    $ trove list
+--------------------------------------+--------+-----------+-------------------+-----  →---+-----------+------+
| id | name | datastore | datastore_version |   →status | flavor_id | size |
+--------------------------------------+--------+-----------+-------------------+-----  →---+-----------+------+
| 97b4b853-80f6-414f-ba6f-c6f455a79ae6 | guest1 | mysql | mysql-5.5 |   →ACTIVE| 10|2|
+--------------------------------------+--------+-----------+-------------------+-----  →---+-----------+------+
 Back up the database instance by using the trove backup-create command. In this example, the backup is called backup1. In this example, replace INSTANCE_ID with 97b4b853-80f6-414f-ba6f- c6f455a79ae6:
Note: Thiscommandsyntaxpertainsonlytopython-troveclientversion1.0.6andlater.Earlierversions require you to pass in the backup name as the first argument.
      $ trove backup-create INSTANCE_ID backup1
+-------------+--------------------------------------+
|   Property  |                Value                 |
+-------------+--------------------------------------+
|   created   |         2014-03-18T17:09:07          |
| description |                 None                 |
|      id     | 8af30763-61fd-4aab-8fe8-57d528911138 |
| instance_id | 97b4b853-80f6-414f-ba6f-c6f455a79ae6 |
| locationRef |                 None                 |
|     name    |               backup1                |
|  parent_id  |                 None                 |
|     size    |                 None                 |
|    status   |                 NEW                  |
|   updated   |
+-------------+--------------------------------------+
2014-03-18T17:09:07          |
 Note that the command returns both the ID of the original instance (instance_id) and the ID of the backup artifact (id).
Later on, use the trove backup-list command to get this information:
    $ trove backup-list +--------------------------------------+--------------------------------------+-------
 →name| status|parent_id|
 →--+-----------+-----------+---------------------+ | id |
             instance_id              |
updated       |
  144
OpenStack command-line clients
 2. Restore a database instance
Now assume that your guest1 database instance is damaged and you need to restore it. In this example,
you use the trove create command to create a new database instance called guest2.
• Youspecifythatthenewguest2instancehasthesameflavor(10)andthesamerootvolumesize
(2) as the original guest1 instance.
• You use the --backup argument to indicate that this new instance is based on the backup arti- fact identified by BACKUP_ID. In this example, replace BACKUP_ID with 8af30763-61fd-4aab- 8fe8-57d528911138.
User Guide (Release Version: 15.0.0)
    +--------------------------------------+--------------------------------------+-------  →--+-----------+-----------+---------------------+
| 8af30763-61fd-4aab-8fe8-57d528911138 | 97b4b853-80f6-414f-ba6f-c6f455a79ae6 |   →backup1|COMPLETED| None |2014-03-18T17:09:11|
+--------------------------------------+--------------------------------------+-------  →--+-----------+-----------+---------------------+
 You can get additional information about the backup by using the trove backup-show command and passing in the BACKUP_ID, which is 8af30763-61fd-4aab-8fe8-57d528911138.
    $ trove backup-show BACKUP_ID
+-------------+----------------------------------------------------+
|   Property  |                   Value                            |
+-------------+----------------------------------------------------+
|   created   |
| description |
|      id     |
| instance_id |
| locationRef | http://10.0.0.1:.../.../8af...138.xbstream.gz.enc  |
2014-03-18T17:09:07                   |
     None                             |
   8af...138                          |
   97b...ae6                          |
|     name    |
|  parent_id  |
|     size    |
|    status   |
|   updated   |
+-------------+----------------------------------------------------+
backup1                            |
 None                              |
 0.17                              |
    COMPLETED                            |
2014-03-18T17:09:11                      |
     $ trove create guest2 10 --size 2 --backup BACKUP_ID
+-------------------+----------------------------------------------+
|      Property     |                Value                         |
+-------------------+----------------------------------------------+
|      created      |         2014-03-18T17:12:03                  |
|     datastore     | {u'version': u'mysql-5.5', u'type': u'mysql'}|
|datastore_version  |                mysql-5.5                     |
|
|
|
|
|
|
+-------------------+----------------------------------------------+
flavor
  id
 name
status
| {u'id': u'10', u'links': [{u'href': ...]}    |
|  ac7a2b35-a9b4-4ff6-beac-a1bcee86d04b        |
updated
 volume
| | | |
      guest2                        |
       BUILD                        |
2014-03-18T17:12:03                 |
   {u'size': 2}                     |
 3. Verify backup
Now check that the new guest2 instance has the same characteristics as the original guest1 instance.
 OpenStack command-line clients 145
 User Guide (Release Version: 15.0.0)
Start by getting the ID of the new guest2 instance.
Use the trove show command to display information about the new guest2 instance. Pass in guest2’s INSTANCE_ID, which is ac7a2b35-a9b4-4ff6-beac-a1bcee86d04b.
    $ trove list
+-----------+--------+-----------+-------------------+--------+-----------+------+
|     id    |  name  | datastore | datastore_version | status | flavor_id | size |
+-----------+--------+-----------+-------------------+--------+-----------+------+
| 97b...ae6 | guest1 |   mysql   |     mysql-5.5     | ACTIVE |     10    |  2   |
| ac7...04b | guest2 |   mysql   |     mysql-5.5     | ACTIVE |     10    |  2   |
+-----------+--------+-----------+-------------------+--------+-----------+------+
     $ trove show INSTANCE_ID
+-------------------+--------------------------------------+
|      Property     |                Value                 |
+-------------------+--------------------------------------+
|      created      |
|     datastore     |
| datastore_version |
          2014-03-18T17:12:03          |
                 mysql                 |
               mysql-5.5               |
                   10                  |
| ac7a2b35-a9b4-4ff6-beac-a1bcee86d04b |
| | | |
|
|       flavor
|         id
|         ip
|        name
|       status
|      updated
|       volume      |
|    volume_used    |
+-------------------+--------------------------------------+
10.0.0.3               |
 guest2                |
 ACTIVE                |
2014-03-18T17:12:06          |
         2                   |
        0.18                 |
 Note that the data store, flavor ID, and volume size have the same values as in the original guest1 instance.
Use the trove database-list command to check that the original databases (db1 and db2) are present on the restored instance.
    $ trove database-list INSTANCE_ID
+--------------------+
|        name        |
+--------------------+
|        db1         |
|        db2         |
| performance_schema |
|        test        |
+--------------------+
 Use the trove user-list command to check that the original user (user1) is present on the restored instance.
    $ trove user-list INSTANCE_ID
+--------+------+-----------+
|  name  | host | databases |
+--------+------+-----------+
  146
OpenStack command-line clients
     |user1 | % | db1,db2| +--------+------+-----------+
 4. Notify users
Tell the users who were accessing the now-disabled guest1 database instance that they can now access guest2. Provide them with guest2‘s name, IP address, and any other information they might need. (You can get this information by using the trove show command.)
5. Clean up
At this point, you might want to delete the disabled guest1 instance, by using the trove delete com-
mand.
$ trove delete INSTANCE_ID
Use incremental backups
Incremental backups let you chain together a series of backups. You start with a regular backup. Then, when you want to create a subsequent incremental backup, you specify the parent backup.
Restoring a database instance from an incremental backup is the same as creating a database instance from a regular backup—the Database service handles the complexities of applying the chain of incremental backups.
This example shows you how to use incremental backups with a MySQL database.
Assumptions. Assume that you have created a regular backup for the following database instance:
• Instancename:guest1
• IDoftheinstance(INSTANCE_ID):792a6a56-278f-4a01-9997-d997fa126370
• IDoftheregularbackupartifact(BACKUP_ID):6dc3a9b7-1f3e-4954-8582-3f2e4942cddd
Create and use incremental backups
1. Create your first incremental backup
Use the trove backup-create command and specify:
• The INSTANCE_ID of the database instance you are doing the incremental backup for (in this ex- ample, 792a6a56-278f-4a01-9997-d997fa126370)
• The name of the incremental backup you are creating: backup1.1
• The BACKUP_ID of the parent backup. In this case, the parent is the regular backup, with an ID of
          6dc3a9b7-1f3e-4954-8582-3f2e4942cddd
User Guide (Release Version: 15.0.0)
         $ trove backup-create INSTANCE_ID backup1.1 --parent BACKUP_ID
+-------------+--------------------------------------+
|   Property  |                Value                 |
+-------------+--------------------------------------+
|   created   |         2014-03-19T14:09:13          |
| description |                 None                 |
|      id     | 1d474981-a006-4f62-b25f-43d7b8a7097e |
| instance_id | 792a6a56-278f-4a01-9997-d997fa126370 |
  OpenStack command-line clients 147
 User Guide (Release Version: 15.0.0)
    | locationRef | None | |name| backup1.1 | | parent_id | 6dc3a9b7-1f3e-4954-8582-3f2e4942cddd | |size| None | | status | NEW |
|   updated   |
+-------------+--------------------------------------+
2014-03-19T14:09:13          |
 Note that this command returns both the ID of the database instance you are incrementally backing up (instance_id) and a new ID for the new incremental backup artifact you just created (id).
2. Create your second incremental backup
The name of your second incremental backup is backup1.2. This time, when you specify the parent, pass in the ID of the incremental backup you just created in the previous step (backup1.1). In this example, it is 1d474981-a006-4f62-b25f-43d7b8a7097e.
    $ trove backup-create INSTANCE_ID backup1.2 --parent BACKUP_ID
+-------------+--------------------------------------+
|   Property  |                Value                 |
+-------------+--------------------------------------+
|   created   |         2014-03-19T14:09:13          |
| description |                 None                 |
|      id     | bb84a240-668e-49b5-861e-6a98b67e7a1f |
| instance_id | 792a6a56-278f-4a01-9997-d997fa126370 |
| locationRef |                 None                 |
|     name    |              backup1.2               |
|  parent_id  | 1d474981-a006-4f62-b25f-43d7b8a7097e |
|     size    |                 None                 |
|    status   |                 NEW                  |
|   updated   |
+-------------+--------------------------------------+
2014-03-19T14:09:13          |
 3. Restore using incremental backups
Now assume that your guest1 database instance is damaged and you need to restore it from your incre- mental backups. In this example, you use the trove create command to create a new database instance called guest2.
To incorporate your incremental backups, you simply use the –backup‘ parameter to pass in the BACKUP_ID of your most recent incremental backup. The Database service handles the complexities of applying the chain of all previous incremental backups.
    $ trove create guest2 10 --size 1 --backup BACKUP_ID
+-------------------+-----------------------------------------------------------+
|      Property     |                       Value                               |
+-------------------+-----------------------------------------------------------+
|      created      |
|     datastore     |
| datastore_version |
| flavor |
|
|
|
                   2014-03-19T14:10:56                      |
          {u'version': u'mysql-5.5', u'type': u'mysql'}     |
                       mysql-5.5                            |
| {u'id': u'10', u'links':                                  |
| [{u'href': u'https://10.125.1.135:8779/v1.0/              |
|  626734041baa4254ae316de52a20b390/flavors/10', u'rel':    |
|  u'self'}, {u'href': u'https://10.125.1.135:8779/         |
|  flavors/10', u'rel': u'bookmark'}]}                      |
  148
OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
       id        |
  name       |
 status      |
updated      |
 volume      |
a3680953-eea9-4cf2-918b-5b8e49d7e1b3              |
             guest2                               |
             BUILD                                |
         2014-03-19T14:10:56                      |
          {u'size': 1}                            |
|
|
|
|
|
+-------------------+-----------------------------------------------------------+
 Manage database configuration
You can manage database configuration tasks by using configuration groups. Configuration groups let you set configuration options, in bulk, on one or more databases.
This example assumes you have created a MySQL database and shows you how to use a configuration group to configure it. Although this example sets just one option on one database, you can use these same procedures to set multiple options on multiple database instances throughout your environment. This can provide significant time savings in managing your cloud.
Bulk-configure a database or databases
1. List available options
First, determine which configuration options you can set. Different data store versions have different
configuration options.
List the names and IDs of all available versions of the mysql data store:
Pass in the data store version ID with the trove configuration-parameter-list command to get the available options:
$ trove configuration-parameter-list DATASTORE_VERSION_ID
+--------------------------------+---------+---------+----------------------+---------  →---------+
| name | type | min | max | restart_  →required |
+--------------------------------+---------+---------+----------------------+---------  →---------+
    $ trove datastore-version-list mysql
+--------------------------------------+-----------+
|                  id                  |    name   |
+--------------------------------------+-----------+
| eeb574ce-f49a-48b6-820d-b2959fcd38bb | mysql-5.5 |
+--------------------------------------+-----------+
     | auto_increment_increment  →False |
| auto_increment_offset  →False |
| integer |    1    |
| integer |    1    |
| integer |    0    |
| integer |
|  string |
65535 65535 1
|   |   |
|
 →False |
            autocommit
|    bulk_insert_buffer_size
  |
 →False |
| character_set_client
 →False |
|
0    | 18446744073709547520 |
  OpenStack command-line clients
149
 User Guide (Release Version: 15.0.0)
    | character_set_connection  →False |
| character_set_database  →False |
| character_set_filesystem  →False |
| character_set_results  →False |
| character_set_server  →False |
| collation_connection  →False |
                                 |  string |         |
                                 |  string |         |
                                 |  string |         |
                                 |  string |         |
                                 |  string |         |
                                 |  string |         |
                                 |  string |         |
                                 |  string |         |
                                 | integer |    1    |
                                 | integer |    1    |
                                 | integer |    0    |
                                 | integer |    0    |
| innodb_flush_log_at_trx_commit | integer | 0 |  →False |
                                 | integer | 1048576 |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
 →False |
|
 →False |
|
 →False |
65535
   65535
68719476736
     1
     2
 4294967296
 4294967296
    1000
   65535
 4294967296
 4294967296
1 1073741824
|
 →False |
collation_database
collation_server
connect_timeout
expire_logs_days
| innodb_buffer_pool_size  →True |
|
 →True |
innodb_file_per_table
|
 →True |
innodb_log_buffer_size
  innodb_open_files
| integer |
| integer |
| integer |
| integer |
| integer |
| integer |
| integer |   1024  |
|
 →True |
10   |
0    |
1    |
0    |
0    |
0    |
| innodb_thread_concurrency  →False |
| interactive_timeout  →False |
|
 →False |
|
 →False |
|
 →False |
|
 →False |
|
 →False |
1    | 18446744073709547520 |
1    |        65535         |
1    |        100000        |
4    | 18446744073709547520 |
1    |        100000        |
|
 →False |
|
 →False |
|
 →False |
|
 →False |
|
31536000 |
join_buffer_size
key_buffer_size
local_infile
max_allowed_packet
max_connect_errors
| integer |
| integer |
| integer |
| integer |
| integer |
| integer |  32768  | 18446744073709547520 |



max_connections
| max_user_connections  →False |
| myisam_sort_buffer_size  →False |
|
 →True |
server_id
sort_buffer_size
  sync_binlog
| integer |
| integer |
0 1
| 18446744073709547520 |
wait_timeout
  150
OpenStack command-line clients
     +--------------------------------+---------+---------+----------------------+---------  →---------+
 In this example, the trove configuration-parameter-list command returns a list of options that work with MySQL 5.5.
2. Create a configuration group
A configuration group contains a comma-separated list of key-value pairs. Each pair consists of a con-
figuration option and its value.
You can create a configuration group by using the trove configuration-create command. The general syntax for this command is:
$ trove configuration-create NAME VALUES --datastore DATASTORE_NAME
• NAME. The name you want to use for this group.
• VALUES. The list of key-value pairs.
• DATASTORE_NAME. The name of the associated data store.
Set VALUES as a JSON dictionary, for example: {"myFirstKey" : "someString", "mySecondKey" : 1}
This example creates a configuration group called group1. group1 contains just one key and value pair, and this pair sets the sync_binlog option to 1.
User Guide (Release Version: 15.0.0)
              $ trove configuration-create group1 '{"sync_binlog" : 1}' --datastore mysql
+----------------------+--------------------------------------+
|       Property       |                Value                 |
+----------------------+--------------------------------------+
| datastore_version_id | eeb574ce-f49a-48b6-820d-b2959fcd38bb |
|     description      |                 None                 |
|          id
|         name
|        values
+----------------------+--------------------------------------+
| 9a9ef3bc-079b-476a-9cbf-85aa64f898a5 |
|                group1                |
|          {"sync_binlog": 1}          |
 3. Examine your existing configuration
Before you use the newly-created configuration group, look at how the sync_binlog option is config- ured on your database. Replace the following sample connection values with values that connect to your database:
    $ mysql -u user7 -ppassword -h 172.16.200.2 myDB7 Welcome to the MySQL monitor. Commands end with ; or \g. ...
mysql> show variables like 'sync_binlog'; +---------------+-------+
| Variable_name | Value |
+---------------+-------+
| sync_binlog | 0 |
+---------------+-------+
 As you can see, the sync_binlog option is currently set to 0 for the myDB7 database.
OpenStack command-line clients 151

 User Guide (Release Version: 15.0.0)
4. Change the database configuration using a configuration group
You can change a database’s configuration by attaching a configuration group to a database instance. You do this by using the trove configuration-attach command and passing in the ID of the database instance and the ID of the configuration group.
Get the ID of the database instance:
    $ trove list
+-------------+------------------+-----------+-------------------+--------+-----------  →+------+
| id | name | datastore | datastore_version | status | flavor_id   →| size |
+-------------+------------------+-----------+-------------------+--------+-----------  →+------+
| 26a265dd... | mysql_instance_7 | mysql | mysql-5.5 | ACTIVE | 6    →| 5 |
+-------------+------------------+-----------+-------------------+--------+-----------  →+------+
 Get the ID of the configuration group:
Attach the configuration group to the database instance:
Note: Thiscommandsyntaxpertainsonlytopython-troveclientversion1.0.6andlater.Earlierversions require you to pass in the configuration group ID as the first argument.
$ trove configuration-attach DB_INSTANCE_ID CONFIG_GROUP_ID 5. Re-examine the database configuration
Display the sync_binlog setting again:
As you can see, the sync_binlog option is now set to 1, as specified in the group1 configuration group. Conclusion. Using a configuration group to set a single option on a single database is obviously a trivial
example. However, configuration groups can provide major efficiencies when you consider that:
• A configuration group can specify a large number of option values.
• Youcanapplyaconfigurationgrouptohundredsorthousandsofdatabaseinstancesinyourenvironment.
    $ trove configuration-list
+-------------+--------+-------------+---------------------+
|    id       |  name  | description |datastore_version_id |
+-------------+--------+-------------+---------------------+
| 9a9ef3bc... | group1 |     None    |      eeb574ce...    |
+-------------+--------+-------------+---------------------+
            mysql> show variables like 'sync_binlog';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| sync_binlog   | 1     |
+---------------+-------+
  152 OpenStack command-line clients
 Used in this way, configuration groups let you modify your database cloud configuration, on the fly, on a massive scale.
Maintenance. There are also a number of useful maintenance features for working with configuration groups. You can:
• Disassociateaconfigurationgroupfromadatabaseinstance,usingthetroveconfiguration-detach command.
• Modifyaconfigurationgrouponthefly,usingthetroveconfiguration-patchcommand.
• Findoutwhatinstancesareusingaconfigurationgroup,usingthetroveconfiguration-instances
command.
• Deleteaconfigurationgroup,usingthetroveconfiguration-deletecommand.Youmightwantto do this if no instances use a group.
Set up database replication
You can create a replica of an existing database instance. When you make subsequent changes to the original instance, the system automatically applies those changes to the replica.
• Replicas are read-only.
• Whenyoucreateareplica,donotspecifythe--usersor--databasesoptions.
• You can choose a smaller volume or flavor for a replica than for the original, but the replica’s volume must be big enough to hold the data snapshot from the original.
This example shows you how to replicate a MySQL database instance.
Set up replication
1. Get the instance ID
Get the ID of the original instance you want to replicate:
2. Create the replica
Create a new instance that will be a replica of the original instance. You do this by passing in the --replica_of option with the trove create command. This example creates a replica called replica_1. replica_1 is a replica of the original instance, base_1:
3. Verify replication status
User Guide (Release Version: 15.0.0)
    $ trove list +-----------+------------+-----------+-------------------+--------+-----------+------+ | id | name | datastore | datastore_version | status | flavor_id | size | +-----------+------------+-----------+-------------------+--------+-----------+------+ | 97b...ae6 | base_1 | mysql | mysql-5.5 | ACTIVE | 10 | 2 | +-----------+------------+-----------+-------------------+--------+-----------+------+
     $ trove create replica_1 6 --size=5 --datastore_version mysql-5.5 \ --datastore mysql --replica_of ID_OF_ORIGINAL_INSTANCE
  OpenStack command-line clients 153
 User Guide (Release Version: 15.0.0)
Pass in replica_1‘s instance ID with the trove show command to verify that the newly created replica_1 instance is a replica of the original base_1. Note that the replica_of property is set to the ID of base_1.
    $ trove show INSTANCE_ID_OF_REPLICA_1 +-------------------+--------------------------------------+ | Property | Value | +-------------------+--------------------------------------+ | created | 2014-09-16T11:16:49 | | datastore | mysql | | datastore_version | mysql-5.5 |
| flavor
| id
| name
| replica_of
| status
|6 | | 49c6eff6-ef91-4eff-91c0-efbda7e83c38 | | replica_1 | | 97b4b853-80f6-414f-ba6f-c6f455a79ae6 | | BUILD | | 2014-09-16T11:16:49 | |5 |
| updated
| volume
+-------------------+--------------------------------------+
 Now pass in base_1‘s instance ID with the trove show command to list the replica(s) associated with the original instance. Note that the replicas property is set to the ID of replica_1. If there are multiple replicas, they appear as a comma-separated list.
    $ trove show INSTANCE_ID_OF_BASE_1 +-------------------+--------------------------------------+ | Property | Value | +-------------------+--------------------------------------+ | created | 2014-09-16T11:04:56 | | datastore | mysql | | datastore_version | mysql-5.5 |
| flavor
| id
| ip
| name
| replicas
| status
| updated
| volume
| volume_used
+-------------------+--------------------------------------+
|6 | | 97b4b853-80f6-414f-ba6f-c6f455a79ae6 | | 172.16.200.2 | | base_1 | | 49c6eff6-ef91-4eff-91c0-efbda7e83c38 | | ACTIVE | | 2014-09-16T11:05:06 | |5 | | 0.11 |
 4. Detach the replica
If the original instance goes down, you can detach the replica. The replica becomes a standalone database
instance. You can then take the new standalone instance and create a new replica of that instance. You detach a replica using the trove detach-replica command:
$ trove detach-replica INSTANCE_ID_OF_REPLICA
Set up database clustering
You can store data across multiple machines by setting up MongoDB sharded clusters. Each cluster includes:
      154 OpenStack command-line clients
 • One or more shards. Each shard consists of a three member replica set (three instances organized as a replica set).
• One or more query routers. A query router is the machine that your application actually connects to. This machine is responsible for communicating with the config server to figure out where the requested data is stored. It then accesses and returns the data from the appropriate shard(s).
• One or more config servers. Config servers store the metadata that links requested data with the shard that contains it.
This example shows you how to set up a MongoDB sharded cluster.
User Guide (Release Version: 15.0.0)
 Note:
• •
Before you begin. Make sure that:
The administrative user has registered a MongoDB datastore type and version.
The administrative user has created an appropriate flavor that meets the MongoDB minimum require- ments.
 Set up clustering
1. Create a cluster
Create a cluster by using the trove cluster-create command. This command creates a one-shard
cluster. Pass in:
• The name of the cluster.
• The name and version of the datastore you want to use.
• The three instances you want to include in the replication set for the first shard. Specify each instance by using the --instance argument and the associated flavor ID and volume size. Use the same flavor ID and volume size for each instance. In this example, flavor 7 is a custom flavor that meets the MongoDB minimum requirements.
    $ trove cluster-create cluster1 mongodb "2.4" \
--instance flavor=7,volume=2 --instance flavor=7,volume=2 \ --instance flavor=7,volume=2
 +-------------------+--------------------------------------+
 | Property          | Value                                |
 +-------------------+--------------------------------------+
 | created           | 2014-08-16T01:46:51                  |
 | datastore         | mongodb                              |
 | datastore_version | 2.4                                  |
 | id                | aa6ef0f5-dbef-48cd-8952-573ad881e717 |
 | name              | cluster1                             |
 | task_description  | Building the initial cluster.        |
 | task_name         | BUILDING                             |
 | updated           | 2014-08-16T01:46:51                  |
 +-------------------+--------------------------------------+
 2. Display cluster information
Display information about a cluster by using the trove cluster-show command. Pass in the ID of the
cluster.
 OpenStack command-line clients 155
 User Guide (Release Version: 15.0.0)
The cluster ID displays when you first create a cluster. (If you need to find it later on, use the trove cluster-list command to list the names and IDs of all the clusters in your system.)
    $ trove cluster-show CLUSTER_ID +-------------------+--------------------------------------+ | Property | Value | +-------------------+--------------------------------------+ | created | 2014-08-16T01:46:51 | | datastore | mongodb | | datastore_version | 2.4 |
| id
| ip
| name
| task_description  | No tasks for the cluster.            |
| task_name         | NONE                                 |
| updated           | 2014-08-16T01:59:33                  |
+-------------------+--------------------------------------+
| aa6ef0f5-dbef-48cd-8952-573ad881e717 |
| 10.0.0.2                             |
| cluster1                             |
  Note: Your application connects to this IP address. The trove cluster-show command displays the IP address of the query router. This is the IP address your application uses to retrieve data from the database.
3. List cluster instances
List the instances in a cluster by using the trove cluster-instances command.
Naming conventions for replication sets and instances. Note that the Name column displays an in- stance name that includes the replication set name. The replication set names and instance names are automatically generated, following these rules:
• Replication set name. This name consists of the cluster name, followed by the string -rsn, where n is 1 for the first replication set you create, 2 for the second replication set, and so on. In this example, the cluster name is cluster1, and there is only one replication set, so the replication set name is cluster1-rs1.
• Instance name. This name consists of the replication set name followed by the string -n, where n is 1 for the first instance in a replication set, 2 for the second instance, and so on. In this example, the instance names are cluster1-rs1-1, cluster1-rs1-2, and cluster1-rs1-3.
4. List clusters
List all the clusters in your system, using the trove cluster-list command.
     $ trove cluster-instances CLUSTER_ID +--------------------------------------+----------------+-----------+------+ | ID | Name | Flavor ID | Size | +--------------------------------------+----------------+-----------+------+
| 45532fc4-661c-4030-8ca4-18f02aa2b337 | cluster1-rs1-1 | 7
| 7458a98d-6f89-4dfd-bb61-5cf1dd65c121 | cluster1-rs1-2 | 7
| b37634fb-e33c-4846-8fe8-cf2b2c95e731 | cluster1-rs1-3 | 7
+--------------------------------------+----------------+-----------+------+
| 2 | | 2 | | 2 |
     $ trove cluster-list +--------------------------------------+----------+-----------+-------------------+---
 →--------+
| ID | Name | Datastore | Datastore Version |
  →Task Name |
 156
OpenStack command-line clients
 User Guide (Release Version: 15.0.0)
    +--------------------------------------+----------+-----------+-------------------+---  →--------+
| aa6ef0f5-dbef-48cd-8952-573ad881e717 | cluster1 | mongodb | 2.4 |   →NONE |
| b8829c2a-b03a-49d3-a5b1-21ec974223ee | cluster2 | mongodb | 2.4 |   →BUILDING |
+--------------------------------------+----------+-----------+-------------------+---  →--------+
 5. Delete a cluster
Delete a cluster, using the trove cluster-delete command. $ trove cluster-delete CLUSTER_ID
Query routers and config servers
Each cluster includes at least one query router and one config server. Query routers and config servers count against your quota. When you delete a cluster, the system deletes the associated query router(s) and config server(s).
OpenStack Python SDK Overview
OpenStack provides four different options for interacting with its APIs from Python, each targeting a slightly different user:
• OpenStack SDK
• shade
• Per-project client libraries
• Direct REST calls via keystoneauth
You should also be familiar with:
• RESTful web services
• HTTP/1.1
• JSON and data serialization formats
OpenStack SDK
The OpenStack Python Software Development Kit (SDK) is used to write Python automation scripts that create and manage resources in your OpenStack cloud. The SDK implements Python bindings to the OpenStack API, which enables you to perform automation tasks in Python by making calls on Python objects, rather than making REST calls directly.
New users should default to coding against the OpenStack SDK.
      OpenStack Python SDK 157
 User Guide (Release Version: 15.0.0)
shade
shade is an abstraction library focused on hiding implementation differences between OpenStack clouds. While the OpenStack SDK presents a clean object interface to the underlying REST APIs, shade hides them if doing so is advantageous. If you plan on running the same Python program against many OpenStack clouds, you may want to use shade - but if you need to access any features of a cloud that do not have a cloud-neutral abstraction mapping, you will be unable to do so with shade.
Per-project client libraries
Each OpenStack project produces a client library that wraps its own REST API. Unless there is no other choice for some reason, the per-project libraries should be avoided.
Direct REST calls via keystoneauth
All of OpenStack’s APIs are actually REST APIs. The keystoneauth library provides an object that looks very much like a Session object from the Python requests library that handles all of the authentication for you. If you are more comfortable just dealing with REST or if there is a feature implemented in your cloud that has not seen support in any of the libraries yet, this option is for you.
Installing OpenStack SDK
Each OpenStack project has its own Python library. These libraries are bundled with the command-line clients. For example, the Python bindings for the Compute API are bundled with the python-novaclient package.
For details about how to install the clients, see Install the OpenStack command-line clients. Authenticate
When using the SDK, you must authenticate against an OpenStack endpoint before you can use OpenStack services. Because all projects use Keystone for authentication, the process is the same no matter which service or library you have decided to use. Each library also has more advanced and complicated ways to do things, should those be needed.
There are two basic ways to deal with your cloud config and credentials: • Environment variables via an openrc.sh file
• clouds.yaml config file
The environment variables have been around the longest and are the form you are most likely to receive from your cloud provider. If you have one and only one cloud account, they are the most convenient way.
clouds.yaml is a bit newer and was designed to help folks who have more than one OpenStack cloud that they are using.
Create a Legacy Client Object
All of the legacy client objects can be constructed the same way - the only difference is the first argument to make_client. The examples will use compute to get a nova client, but neutron can be accessed instead by replacing compute with network.
 158 OpenStack Python SDK
 To use the legacy python-novaclient with a Compute endpoint, instantiate a novaclient.v2.client.Client ob- ject using os-client-config:
If you desire a specific micro-version of the Nova API, you can pass that as the version parameter:
User Guide (Release Version: 15.0.0)
    import os_client_config
nova = os_client_config.make_client(
    'compute',
    auth_url='https://example.com',
    username='example-openstack-user',
    password='example-password',
    project_name='example-project-name',
    region_name='example-region-name')
     import os_client_config
nova = os_client_config.make_client(
    'compute',
    version='2.10',
    auth_url='https://example.com',
    username='example-openstack-user',
    password='example-password',
    project_name='example-project-name',
    region_name='example-region-name')
 If you authenticate against an endpoint that uses a custom authentication back end, you must provide the name of the plugin in the auth_type parameter.
For instance, the Rackspace public cloud is an OpenStack deployment that has an optional custom authentication back end. While normal keystone password authentication works perfectly well, you may want to use the custom Rackspace keystoneauth API Key plugin found in rackspace-keystoneauth-plugin.
    nova = os_client_config.make_client(
    'compute',
    auth_type='rackspace_apikey',
    auth_url='https://example.com',
    username='example-openstack-user',
    api_key='example-apikey',
    project_name='example-project-name',
    region_name='example-region-name')
 Manage images
When working with images in the SDK, you will call both glance and nova methods. List images
To list the available images, call the glanceclient.v2.images.Controller.list method:
    import glanceclient.v2.client as glclient glance = glclient.Client(...)
images = glance.images.list()
  OpenStack Python SDK 159
 User Guide (Release Version: 15.0.0)
The images method returns a Python generator, as shown in the following interaction with the Python interpreter:
    >>> images = glance.images.list()
>>> images
<generator object list at 0x105e9c2d0> >>> list(images)
[{u'checksum': u'f8a2eeee2dc65b3d9b6e63678955bd83',
  u'container_format': u'ami',
  u'created_at': u'2013-10-20T14:28:10Z',
  u'disk_format': u'ami',
  u'file': u'/v2/images/dbc9b2db-51d7-403d-b680-3f576380b00c/file',
  u'id': u'dbc9b2db-51d7-403d-b680-3f576380b00c',
  u'kernel_id': u'c002c82e-2cfa-4952-8461-2095b69c18a6',
  u'min_disk': 0,
  u'min_ram': 0,
  u'name': u'cirros-0.3.5-x86_64-uec',
  u'protected': False,
  u'ramdisk_id': u'4c1c9b4f-3fe9-425a-a1ec-1d8fd90b4db3',
  u'schema': u'/v2/schemas/image',
  u'size': 25165824,
  u'status': u'active',
  u'tags': [],
  u'updated_at': u'2013-10-20T14:28:11Z',
  u'visibility': u'public'},
 {u'checksum': u'69c33642f44ca552ba4bb8b66ad97e85',
  u'container_format': u'ari',
  u'created_at': u'2013-10-20T14:28:09Z',
  u'disk_format': u'ari',
  u'file': u'/v2/images/4c1c9b4f-3fe9-425a-a1ec-1d8fd90b4db3/file',
  u'id': u'4c1c9b4f-3fe9-425a-a1ec-1d8fd90b4db3',
  u'min_disk': 0,
  u'min_ram': 0,
  u'name': u'cirros-0.3.5-x86_64-uec-ramdisk',
  u'protected': False,
  u'schema': u'/v2/schemas/image',
  u'size': 3714968,
  u'status': u'active',
  u'tags': [],
  u'updated_at': u'2013-10-20T14:28:10Z',
  u'visibility': u'public'},
 {u'checksum': u'c352f4e7121c6eae958bc1570324f17e',
  u'container_format': u'aki',
  u'created_at': u'2013-10-20T14:28:08Z',
  u'disk_format': u'aki',
  u'file': u'/v2/images/c002c82e-2cfa-4952-8461-2095b69c18a6/file',
  u'id': u'c002c82e-2cfa-4952-8461-2095b69c18a6',
  u'min_disk': 0,
  u'min_ram': 0,
  u'name': u'cirros-0.3.5-x86_64-uec-kernel',
  u'protected': False,
  u'schema': u'/v2/schemas/image',
  u'size': 4955792,
  u'status': u'active',
  u'tags': [],
  u'updated_at': u'2013-10-20T14:28:09Z',
  u'visibility': u'public'}]
  160 OpenStack Python SDK
 Get image by ID
To retrieve an image object from its ID, call the glanceclient.v2.images.Controller.get method:
Get image by name
The Image service Python bindings do not support the retrieval of an image object by name. However, the Compute Python bindings enable you to get an image object by name. To get an image object by name, call the novaclient.v2.images.ImageManager.find method:
Upload an image
To upload an image, call the glanceclient.v2.images.ImageManager.create method:
Assign CORS headers to requests
Cross-Origin Resource Sharing (CORS) is a specification that defines how browsers and servers communicate across origins by using HTTP headers, such as those assigned by Object Storage API requests. The Object Storage API supports the following headers:
• Access-Control-Allow-Credentials • Access-Control-Allow-Methods
• Access-Control-Allow-Origin
• Access-Control-Expose-Headers
• Access-Control-Max-Age
• Access-Control-Request-Headers • Access-Control-Request-Method • Origin
User Guide (Release Version: 15.0.0)
    import glanceclient.v2.client as glclient image_id = 'c002c82e-2cfa-4952-8461-2095b69c18a6' glance = glclient.Client(...)
image = glance.images.get(image_id)
     import novaclient.v2.client as nvclient name = "cirros"
nova = nvclient.Client(...)
image = nova.images.find(name=name)
     import glanceclient.v2.client as glclient imagefile = "/tmp/myimage.img"
glance = glclient.Client(...)
with open(imagefile) as fimage:
  glance.images.create(name="myimage", is_public=False, disk_format="qcow2",
                       container_format="bare", data=fimage)
  OpenStack Python SDK 161
 User Guide (Release Version: 15.0.0)
You can only assign these headers to objects. For more information, see www.w3.org/TR/access-control/. This example assigns the file origin to the Origin header, which ensures that the file originated from a reputable
source.
    $ curl -i -X POST -H "Origin: example.com" -H "X-Auth-Token: 48e17715dfce47bb90dc2a336f63493a" https://storage.example.com/v1/MossoCloudFS_c31366f1-9f1c-40dc-a b92-6b3f0b5a8c45/ephotos
HTTP/1.1 204 No Content
Content-Length: 0
Content-Type: text/html; charset=UTF-8
Access-Control-Allow-Origin: example.com
Access-Control-Expose-Headers: cache-control, content-language,
content-type, expires, last-modified, pragma, etag, x-timestamp, x-trans-id
X-Trans-Id: tx979bfe26be6649c489ada-0054cba1d9ord1
Date: Fri, 30 Jan 2015 15:23:05 GMT
 Schedule objects for deletion
To determine whether your Object Storage system supports this feature, see Manage objects and containers. Alternatively, check with your service provider.
Scheduling an object for deletion is helpful for managing objects that you do not want to permanently store, such as log files, recurring full backups of a dataset, or documents or images that become outdated at a specified time.
To schedule an object for deletion, include one of these headers with the PUT or POST request on the object:
X-Delete-At AUNIXepochtimestamp,inintegerform.Forexample,1348691905representsWed,26Sept 2012 20:38:25 GMT. It specifies the time you want the object to expire, no longer be served, and be deleted completely from the object store.
X-Delete-After An integer value which specifies the number of seconds from the time of the request to when you want to delete the object. This header is converted to a X-Delete-At header that is set to the sum of the X-Delete-After value plus the current time, in seconds.
Note: Use EpochConverter to convert dates to and from epoch timestamps and for batch conversions. Use the POST method to assign expiration headers to existing objects that you want to expire.
In this example, the X-Delete-At header is assigned a UNIX epoch timestamp in integer form for Mon, 11 Jun 2012 15:38:25 GMT.
In this example, the X-Delete-After header is set to 864000 seconds. The object expires after this time.
      $ curl -i publicURL/marktwain/goodbye -X PUT -H "X-Auth-Token: token" \ -H "X-Delete-At: 1390581073" -H "Content-Length: 14" -H \ "Content-Type: application/octet-stream"
     PUT /<api version>/<account>/<container>/<object> HTTP/1.1
Host: storage.example.com
X-Auth-Token: eaaafd18-0fed-4b3a-81b4-663c99ec1cbb
Content-Type: image/jpeg
X-Delete-After: 864000
  162 OpenStack Python SDK
 Configure access and security for instances
When working with images in the SDK, you will call novaclient methods. Add a keypair
To generate a keypair, call the novaclient.v1_1.keypairs.KeypairManager.create method:
The Python script output looks something like this:
You typically write the private key to a file to use it later. The file must be readable and writeable by only the file owner; otherwise, the SSH client will refuse to read the private key file. The safest way is to create the file with the appropriate permissions, as shown in the following example:
User Guide (Release Version: 15.0.0)
    import novaclient.v2.client as nvclient
nova = nvclient.Client(...)
keypair_name = "staging"
keypair = nova.keypairs.create(name=keypair_name) print keypair.private_key
     -----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEA8XkaMqInSPfy0hMfWO+OZRtIgrQAbQkNcaNHmv2GN2G6xZlb\nuBRux5Xk/6SZ
ABaNPm1nRWm/ZDHnxCsFTcAl2LYOQXx3Cl2qKNY4r2di4G48GAkd\n7k5lDP2RgQatUM8npO0CD9PU
...
mmrceYYK08/lQ7JKLmVkdzdQKt77+v1oBBuHiykLfI6h1m77NRDw9r8cV\nzczYeoALifpjTPMkKS8
ECfDCuDn/vc9K1He8CRaJHf8AMLQLM3MN
-----END RSA PRIVATE KEY-----
     import novaclient.v2.client as nvclient
import os
nova = nvclient.Client(...)
keypair_name = "staging"
private_key_filename = "/home/alice/id-staging" keypair = nova.keypairs.create(name=keypair_name)
# Create a file for writing that can only be read and written by
owner
fp = os.open(private_key_filename, os.O_WRONLY | os.O_CREAT, 0o600) with os.fdopen(fp, 'w') as f:
    f.write(keypair.private_key)
 Import a keypair
If you have already generated a keypair with the public key located at ~/.ssh/id_rsa.pub, pass the contents of the file to the novaclient.v1_1.keypairs.KeypairManager.create method to import the public key to Compute:
    import novaclient.v2.client as nvclient
import os.path
with open(os.path.expanduser('~/.ssh/id_rsa.pub')) as f:
    public_key = f.read()
nova = nvclient.Client(...)
nova.keypairs.create('mykey', public_key)
  OpenStack Python SDK 163
 User Guide (Release Version: 15.0.0)
List keypairs
To list keypairs, call the novaclient.v1_1.keypairs.KeypairManager.list method:
Create and manage security groups
To list security groups for the current project, call the novaclient.v_1.security_groups.SecurityGroupManager.list method:
To create a security group with a specified name and description, call the nova- client.v_1.security_groups.SecurityGroupManager.create method:
To delete a security group, call the novaclient.v_1.security_groups.SecurityGroupManager.delete method, pass- ing either a novaclient.v1_1.security_groups.SecurityGroup object or group ID as an argument:
    import novaclient.v2.client as nvclient nova = nvclient.Client(...)
keypairs = nova.keypairs.list()
     import novaclient.v2.client as nvclient
nova = nvclient.Client(...)
security_groups = nova.security_groups.list()
     import novaclient.v2.client as nvclient
nova = nvclient.Client(...) nova.security_groups.create(name="web", description="Web servers")
     import novaclient.v2.client as nvclient
nova = nvclient.Client(...)
group = nova.security_groups.find(name="web") nova.security_groups.delete(group)
# The following lines would also delete the group: # nova.security_groups.delete(group.id)
# group.delete()
 Create and manage security group rules
Access the security group rules from the rules attribute of a novaclient.v1_1.security_groups.SecurityGroup object:
To add a rule to a security group, call the novaclient.v1_1.security_group_rules.SecurityGroupRuleManager.create method:
    import novaclient.v2.client as nvclient
nova = nvclient.Client(...)
group = nova.security_groups.find(name="web") print group.rules
     import novaclient.v2.client as nvclient
nova = nvclient.Client(...)
group = nova.security_groups.find(name="web")
# Add rules for ICMP, tcp/80 and tcp/443 nova.security_group_rules.create(group.id, ip_protocol="icmp",
  164 OpenStack Python SDK
 Networking
To use the information in this section, you should have a general understanding of OpenStack Networking, OpenStack Compute, and the integration between the two. You should also have access to a plug-in that im- plements the Networking API v2.0.
Set environment variables
Make sure that you set the relevant environment variables.
As an example, see the sample shell file that sets these variables to get credentials:
Get credentials
The examples in this section use the get_credentials method:
This code resides in the credentials.py file, which all samples import. Use the get_credentials() method to populate and get a dictionary: credentials = get_credentials()
Get Nova credentials
The examples in this section use the get_nova_credentials method:
User Guide (Release Version: 15.0.0)
                                     from_port=-1, to_port=-1)
nova.security_group_rules.create(group.id, ip_protocol="tcp",
                                 from_port=80, to_port=80)
nova.security_group_rules.create(group.id, ip_protocol="tcp",
                                 from_port=443, to_port=443)
     export OS_USERNAME="admin"
export OS_PASSWORD="password"
export OS_TENANT_NAME="admin"
export OS_AUTH_URL="http://IPADDRESS/v2.0"
     def get_credentials(): d = {}
d['username'] = os.environ['OS_USERNAME'] d['password'] = os.environ['OS_PASSWORD'] d['auth_url'] = os.environ['OS_AUTH_URL'] d['tenant_name'] = os.environ['OS_TENANT_NAME'] return d
          def get_nova_credentials(): d = {}
    d['username'] = os.environ['OS_USERNAME']
    d['api_key'] = os.environ['OS_PASSWORD']
    d['auth_url'] = os.environ['OS_AUTH_URL']
  OpenStack Python SDK 165
 User Guide (Release Version: 15.0.0)
    d['project_id'] = os.environ['OS_TENANT_NAME'] return d
 This code resides in the credentials.py file, which all samples import. Use the get_nova_credentials() method to populate and get a dictionary: nova_credentials = get_nova_credentials()
Print values
The examples in this section use the print_values and print_values_server methods:
         def print_values(val, type): if type == 'ports':
val_list = val['ports'] if type == 'networks':
val_list = val['networks'] if type == 'routers':
val_list = val['routers'] for p in val_list:
for k, v in p.items(): print("%s : %s" % (k, v))
print('\n')
def print_values_server(val, server_id, type): if type == 'ports':
        val_list = val['ports']
if type == 'networks': val_list = val['networks']
for p in val_list: bool = False
for k, v in p.items():
if k == 'device_id' and v == server_id:
bool = True if bool:
for k, v in p.items(): print("%s : %s" % (k, v))
print('\n')
 This code resides in the utils.py file, which all samples import. Create network
The following program creates a network:
    #!/usr/bin/env python
from neutronclient.v2_0 import client from credentials import get_credentials
network_name = 'sample_network'
credentials = get_credentials()
  166 OpenStack Python SDK
 User Guide (Release Version: 15.0.0)
    neutron = client.Client(**credentials) try:
    body_sample = {'network': {'name': network_name,
                   'admin_state_up': True}}
netw = neutron.create_network(body=body_sample) net_dict = netw['network']
network_id = net_dict['id']
print('Network %s created' % network_id)
    body_create_subnet = {'subnets': [{'cidr': '192.168.199.0/24',
                          'ip_version': 4, 'network_id': network_id}]}
    subnet = neutron.create_subnet(body=body_create_subnet)
print('Created subnet %s' % subnet) finally:
print("Execution completed")
 List networks
The following program lists networks:
    #!/usr/bin/env python
from neutronclient.v2_0 import client from credentials import get_credentials from utils import print_values
credentials = get_credentials()
neutron = client.Client(**credentials)
netw = neutron.list_networks()
print_values(netw, 'networks')
 For print_values, see Print values. Create ports
The following program creates a port:
    #!/usr/bin/env python
from neutronclient.v2_0 import client import novaclient.v2.client as nvclient from credentials import get_credentials from credentials import get_nova_credentials
credentials = get_nova_credentials()
nova_client = nvclient.Client(**credentials)
# Replace with server_id and network_id from your environment
server_id = '9a52795a-a70d-49a8-a5d0-5b38d78bd12d' network_id = 'ce5d204a-93f5-43ef-bd89-3ab99ad09a9a' server_detail = nova_client.servers.get(server_id) print(server_detail.id)
  OpenStack Python SDK 167
 User Guide (Release Version: 15.0.0)
    if server_detail != None:
credentials = get_credentials() neutron = client.Client(**credentials)
body_value = {
} }
"port": {
        "admin_state_up": True,
        "device_id": server_id,
        "name": "port1",
        "network_id": network_id
response = neutron.create_port(body=body_value) print(response)
 For get_nova_credentials, see Get Nova credentials. For get_credentials, see Get credentials.
List ports
The following program lists ports:
    #!/usr/bin/env python
from neutronclient.v2_0 import client from credentials import get_credentials from utils import print_values
credentials = get_credentials()
neutron = client.Client(**credentials)
ports = neutron.list_ports()
print_values(ports, 'ports')
 For get_credentials see Get credentials. For print_values, see Print values.
List server ports
The following program lists the ports for a server:
    #!/usr/bin/env python
from neutronclient.v2_0 import client import novaclient.v2.client as nvclient from credentials import get_credentials from credentials import get_nova_credentials from utils import print_values_server
credentials = get_nova_credentials()
nova_client = nvclient.Client(**credentials)
# change these values according to your environment
server_id = '9a52795a-a70d-49a8-a5d0-5b38d78bd12d'
  168 OpenStack Python SDK
 Create router and add port to subnet
This example queries OpenStack Networking to create a router and add a port to a subnet. 1. Import the following modules:
2. Get Nova Credentials. See :ref:’Get Nova credentials <get-nova-credentials>’.
3. Instantiatethenova_clientclientobjectbyusingthecredentialsdictionaryobject:
      nova_client = nvclient.Client(**credentials)
4. Create a router and add a port to the subnet:
User Guide (Release Version: 15.0.0)
    network_id = 'ce5d204a-93f5-43ef-bd89-3ab99ad09a9a' server_detail = nova_client.servers.get(server_id) print(server_detail.id)
if server_detail is not None:
credentials = get_credentials() neutron = client.Client(**credentials) ports = neutron.list_ports()
    print_values_server(ports, server_id, 'ports')
    body_value = {'port': {
        'admin_state_up': True,
        'device_id': server_id,
        'name': 'port1',
        'network_id': network_id,
        }}
response = neutron.create_port(body=body_value) print(response)
     from neutronclient.v2_0 import client import novaclient.v2.client as nvclient from credentials import get_credentials from credentials import get_nova_credentials from utils import print_values_server
          # Replace with network_id from your environment
network_id = '81bf592a-9e3f-4f84-a839-ae87df188dc1'
credentials = get_credentials()
neutron = client.Client(**credentials)
neutron.format = json
request = {'router': {'name': 'router name',
                      'admin_state_up': True}}
router = neutron.create_router(request)
router_id = router['router']['id']
# for example: '72cf1682-60a8-4890-b0ed-6bad7d9f5466' router = neutron.show_router(router_id) print(router)
body_value = {'port': {
  OpenStack Python SDK 169
 User Guide (Release Version: 15.0.0)
        'admin_state_up': True,
    'device_id': router_id,
    'name': 'port1',
    'network_id': network_id,
    }}
response = neutron.create_port(body=body_value) print(response)
print("Execution Completed")
 Create router: complete code listing example
    #!/usr/bin/env python
from neutronclient.v2_0 import client import novaclient.v2.client as nvclient from credentials import get_credentials from credentials import get_nova_credentials from utils import print_values_server
credentials = get_nova_credentials()
nova_client = nvclient.Client(**credentials)
# Replace with network_id from your environment
network_id = '81bf592a-9e3f-4f84-a839-ae87df188dc1' try:
    credentials = get_credentials()
    neutron = client.Client(**credentials)
    neutron.format = 'json'
    request = {'router': {'name': 'router name',
                          'admin_state_up': True}}
    router = neutron.create_router(request)
router_id = router['router']['id']
# for example: '72cf1682-60a8-4890-b0ed-6bad7d9f5466' router = neutron.show_router(router_id) print(router)
body_value = {'port': {
        'admin_state_up': True,
        'device_id': router_id,
        'name': 'port1',
        'network_id': network_id,
        }}
    response = neutron.create_port(body=body_value)
print(response) finally:
print("Execution completed")
 Delete a network
This example queries OpenStack Networking to delete a network. To delete a network:
 170 OpenStack Python SDK
 1. Import the following modules:
2. Get credentials. See Get Nova credentials.
3. Instantiatetheneutronclientobjectbyusingthecredentialsdictionaryobject:
   neutron = client.Client(**credentials)
4. Delete the network:
User Guide (Release Version: 15.0.0)
    from neutronclient.v2_0 import client from credentials import get_credentials
          body_sample = {'network': {'name': network_name,
               'admin_state_up': True}}
netw = neutron.create_network(body=body_sample) net_dict = netw['network']
network_id = net_dict['id']
print('Network %s created' % network_id)
body_create_subnet = {'subnets': [{'cidr': '192.168.199.0/24',
                      'ip_version': 4, 'network_id': network_id}]}
subnet = neutron.create_subnet(body=body_create_subnet) print('Created subnet %s' % subnet)
neutron.delete_network(network_id) print('Deleted Network %s' % network_id)
print("Execution completed")
 Delete network: complete code listing example
    #!/usr/bin/env python
from neutronclient.v2_0 import client from credentials import get_credentials
network_name = 'temp_network' credentials = get_credentials() neutron = client.Client(**credentials) try:
    body_sample = {'network': {'name': network_name,
                   'admin_state_up': True}}
netw = neutron.create_network(body=body_sample) net_dict = netw['network']
network_id = net_dict['id']
print('Network %s created' % network_id)
    body_create_subnet = {'subnets': [{'cidr': '192.168.199.0/24',
                          'ip_version': 4, 'network_id': network_id}]}
subnet = neutron.create_subnet(body=body_create_subnet) print('Created subnet %s' % subnet)
  OpenStack Python SDK 171
 User Guide (Release Version: 15.0.0)
        neutron.delete_network(network_id)
print('Deleted Network %s' % network_id) finally:
print("Execution Completed")
 List routers
This example queries OpenStack Networking to list all routers.
1. Import the following modules:
2. Get credentials. See Get Nova credentials.
3. Instantiatetheneutronclientobjectbyusingthecredentialsdictionaryobject:
      neutron = client.Client(**credentials)
4. List the routers:
For print_values, see Print values.
List routers: complete code listing example
    from neutronclient.v2_0 import client from credentials import get_credentials from utils import print_values
          routers_list = neutron.list_routers(retrieve_all=True) print_values(routers_list, 'routers')
print("Execution completed")
     #!/usr/bin/env python
from neutronclient.v2_0 import client from credentials import get_credentials from utils import print_values
try:
credentials = get_credentials()
neutron = client.Client(**credentials)
routers_list = neutron.list_routers(retrieve_all=True) print_values(routers_list, 'routers')
finally:
print("Execution completed")
 List security groups
This example queries OpenStack Networking to list security groups. 1. Import the following modules:
    from neutronclient.v2_0 import client from credentials import get_credentials from utils import print_values
  172 OpenStack Python SDK
 2. Get credentials. See Get credentials.
3. Instantiatetheneutronclientobjectbyusingthecredentialsdictionaryobject:
      neutron = client.Client(**credentials)
4. List Security groups
List security groups: complete code listing example
User Guide (Release Version: 15.0.0)
         sg = neutron.list_security_groups() print(sg)
     #!/usr/bin/env python
from neutronclient.v2_0 import client from credentials import get_credentials from utils import print_values
credentials = get_credentials() neutron = client.Client(**credentials) sg = neutron.list_security_groups() print(sg)
  Note: OpenStack Networking security groups are case-sensitive while the nova-network security groups are case-insensitive.
List subnets
This example queries OpenStack Networking to list subnets.
1. Import the following modules:
2. Get credentials. See :ref:’Get credentials <get-credentials>’.
3. Instantiatetheneutronclientobjectbyusingthecredentialsdictionaryobject:
      neutron = client.Client(**credentials)
4. List subnets:
     from neutronclient.v2_0 import client from credentials import get_credentials from utils import print_values
          subnets = neutron.list_subnets() print(subnets)
  OpenStack Python SDK 173
 User Guide (Release Version: 15.0.0)
List subnets: complete code listing example
    #!/usr/bin/env python
from neutronclient.v2_0 import client from credentials import get_credentials from utils import print_values
credentials = get_credentials() neutron = client.Client(**credentials) subnets = neutron.list_subnets() print(subnets)
 Compute
To use the information in this section, you must be familiar with OpenStack Compute.
Set environment variables
To set up environmental variables and authenticate against Compute API endpoints, see Authenticate. Get OpenStack credentials (API v2)
This example uses the get_nova_credentials_v2 method:
This code resides in the credentials.py file, which all samples import.
Use the get_nova_credentials_v2() method to populate and get a dictionary:
credentials = get_nova_credentials_v2()
List servers (API v2)
The following program lists servers by using the Compute API v2. 1. Import the following modules:
2. Get Nova credentials. See Get OpenStack credentials (API v2).
3. Instantiatethenova_clientclientobjectbyusingthecredentialsdictionaryobject:
    def get_nova_credentials_v2(): d = {}
d['version'] = '2'
d['username'] = os.environ['OS_USERNAME'] d['api_key'] = os.environ['OS_PASSWORD'] d['auth_url'] = os.environ['OS_AUTH_URL'] d['project_id'] = os.environ['OS_TENANT_NAME'] return d
          from credentials import get_nova_credentials_v2 from novaclient.client import Client
  174 OpenStack Python SDK
 User Guide (Release Version: 15.0.0)
          nova_client = Client(**credentials)
4. Listserversbycallingservers.listonnova_clientobject: print(nova_client.servers.list())
List server code listing example
          #!/usr/bin/env python
from credentials import get_nova_credentials_v2 from novaclient.client import Client
credentials = get_nova_credentials_v2()
nova_client = Client(**credentials)
print(nova_client.servers.list())
 Create server (API v2)
The following program creates a server (VM) by using the Compute API v2.
1. Import the following modules:
2. Get OpenStack credentials. See Get OpenStack credentials (API v2).
3. Instantiatethenova_clientclientobjectbyusingthecredentialsdictionaryobject:
      nova_client = Client(**credentials)
4. Gettheflavorandimagetousetocreateaserver.Thiscodeusesthecirrosimage,them1.tinyflavor, and the private network:
5. To create the server, use the network, image, and flavor:
6. Runthe“Sleepforfiveseconds”command,anddeterminewhethertheserver/vmwascreatedbycalling nova_client.servers.list():
    import time
from credentials import get_nova_credentials_v2 from novaclient.client import Client
          image = nova_client.images.find(name="cirros")
flavor = nova_client.flavors.find(name="m1.tiny")
net = nova_client.networks.find(label="private")
     nics = [{'net-id': net.id}]
instance = nova_client.servers.create(name="vm2", image=image,
flavor=flavor, key_name="keypair-1", nics=nics)
     print("Sleeping for 5s after create command") time.sleep(5)
print("List of VMs") print(nova_client.servers.list())
  OpenStack Python SDK 175
 User Guide (Release Version: 15.0.0)
Create server code listing example
    #!/usr/bin/env python
import time
from credentials import get_nova_credentials_v2 from novaclient.client import Client
try:
credentials = get_nova_credentials_v2() nova_client = Client(**credentials)
    image = nova_client.images.find(name="cirros")
    flavor = nova_client.flavors.find(name="m1.tiny")
    net = nova_client.networks.find(label="private")
    nics = [{'net-id': net.id}]
    instance = nova_client.servers.create(name="vm2", image=image,
                                      flavor=flavor, key_name="keypair-1", nics=nics)
print("Sleeping for 5s after create command") time.sleep(5)
print("List of VMs") print(nova_client.servers.list())
finally:
print("Execution Completed")
 Delete server (API v2)
The following program deletes a server (VM) by using the Compute API v2.
1. Import the following modules:
2. Get Nova credentials. See Get OpenStack credentials (API v2).
3. Instantiatethenova_clientclientobjectbyusingthecredentialsdictionaryobject:
      nova_client = Client(**credentials)
4. Determinewhetherthevm1serverexists:
(a) Listservers:servers_list.
(b) Iterateoverservers_listandcomparenamewithvm1.
(c) Iftrue,setthevariablenameserver_existstoTrueandbreakfromtheforloop:
    import time
from credentials import get_nova_credentials_v2 from novaclient.client import Client
          servers_list = nova_client.servers.list()
server_del = "vm1"
server_exists = False
for s in servers_list:
if s.name == server_del:
print("This server %s exists" % server_del) server_exists = True
break
  176 OpenStack Python SDK
 User Guide (Release Version: 15.0.0)
     5. Iftheserverexists,runthedeletemethodofthenova_client.serversobject: nova_client.servers.delete(s)
Delete server code example
         #!/usr/bin/env python
from credentials import get_nova_credentials_v2 from novaclient.client import Client
credentials = get_nova_credentials_v2()
nova_client = Client(**credentials)
servers_list = nova_client.servers.list()
server_del = "vm1"
server_exists = False
for s in servers_list:
if s.name == server_del:
print("This server %s exists" % server_del) server_exists = True
break
if not server_exists:
print("server %s does not exist" % server_del)
else:
print("deleting server..........") nova_client.servers.delete(s) print("server %s deleted" % server_del)
 Update server (API v2)
The following program updates the name of a server (VM) by using the Compute API v2. 1. Import the following modules:
print_server is a method defined in utils.py and prints the server details as shown in the code listing below:
    from credentials import get_nova_credentials_v2 from novaclient.client import Client
from utils import print_server
     def print_server(server): print("-"*35)
print("server id: %s" % server.id) print("server name: %s" % server.name) print("server image: %s" % server.image) print("server flavor: %s" % server.flavor) print("server key name: %s" % server.key_name) print("user_id: %s" % server.user_id) print("-"*35)
  OpenStack Python SDK 177
 User Guide (Release Version: 15.0.0)
2. Get OpenStack Credentials. See Get OpenStack credentials (API v2).
3. Instantiatethenova_clientclientobjectbyusingthecredentialsdictionaryobject:
      nova_client = Client(**credentials)
4. Gettheserverinstanceusingserver_idandprintthedetailsbycallingprint_servermethod:
5. Callserver.updateontheserverobjectwiththenewvaluefornamevariable: server.update(name = n + '1')
6. Get the updated instance of the server:
      server_updated = nova_client.servers.get(server_id)
7. Callprint_serveragaintochecktheupdateserverdetails: print_server(server_updated)
Update server code listing example
         server_id = '99889c8d-113f-4a7e-970c-77f1916bfe14'
server = nova_client.servers.get(server_id)
n = server.name
print_server(server)
                    #!/usr/bin/env python
from credentials import get_nova_credentials_v2 from novaclient.client import Client
from utils import print_server
credentials = get_nova_credentials_v2()
nova_client = Client(**credentials)
# Change the server_id specific to your environment
server_id = '99889c8d-113f-4a7e-970c-77f1916bfe14'
server = nova_client.servers.get(server_id)
n = server.name
print_server(server)
server.update(name=n +'1')
server_updated = nova_client.servers.get(server_id)
print_server(server_updated)
 List flavors (API v2)
The following program lists flavors and their details by using the Compute API v2. 1. Import the following modules:
 178 OpenStack Python SDK
     from credentials import get_nova_credentials_v2 from novaclient.client import Client
from utils import print_flavors
 The print_flavors method is defined in utils.py and prints the flavor details:
2. Get OpenStack credentials. Get OpenStack credentials (API v2).
3. Instantiatethenova_clientclientobjectbyusingthecredentialsdictionaryobject:
      nova_client = Client(**credentials)
4. Listflavorsbycallinglist()onnova_client.flavorsobject: flavors_list = nova_client.flavors.list()
5. Printtheflavordetails,idandnamebycallingprint_flavors: print_flavors(flavors_list)
List flavors code listing example
User Guide (Release Version: 15.0.0)
    def print_flavors(flavor_list): for flavor in flavor_list:
print("-"*35)
print("flavor id : %s" % flavor.id) print("flavor name : %s" % flavor.name) print("-"*35)
                    #!/usr/bin/env python
from credentials import get_nova_credentials_v2 from novaclient.client import Client
from utils import print_flavors
credentials = get_nova_credentials_v2()
nova_client = Client(**credentials)
flavors_list = nova_client.flavors.list()
print_flavors(flavors_list)
 List floating IPs (API v2)
The following program lists the floating IPs and their details by using the Compute API v2. 1. Import the following modules:
The print_values_ip method is defined in utils.py and prints the floating_ip object details:
    from credentials import get_nova_credentials_v2 from novaclient.client import Client
from utils import print_values_ip
  OpenStack Python SDK 179
 User Guide (Release Version: 15.0.0)
    def print_values_ip(ip_list): ip_dict_lisl = []
for ip in ip_list:
print("-"*35)
print("fixed_ip : %s" % ip.fixed_ip) print("id : %s" % ip.id) print("instance_id : %s" % ip.instance_id) print("ip : %s" % ip.ip)
print("pool : %s" % ip.pool)
 2. Get OpenStack credentials. See Get OpenStack credentials (API v2).
3. Instantiatethenova_clientclientobjectbyusingthecredentialsdictionaryobject:
      nova_client = Client(**credentials)
4. ListfloatingIPsbycallinglist()onnova_client.floating_ipsobject: ip_list = nova_client.floating_ips.list()
5. PrintthefloatingIPobjectdetailsbycallingprint_values_ip: print_values_ip(ip_list)
List floating IPs code listing example
                   #!/usr/bin/env python
from credentials import get_nova_credentials_v2 from novaclient.client import Client
from utils import print_values_ip
credentials = get_nova_credentials_v2()
nova_client = Client(**credentials)
ip_list = nova_client.floating_ips.list()
print_values_ip(ip_list)
 List hosts (API v2)
The following program lists the hosts by using the Compute API v2. 1. Import the following modules:
The print_hosts method is defined in utils.py and prints the host object details:
    from credentials import get_nova_credentials_v2 from novaclient.client import Client
from utils import print_hosts
     def print_hosts(host_list): for host in host_list:
print("-"*35)
print("host_name : %s" % host.host_name)
  180 OpenStack Python SDK
     print("service : %s" % host.service) print("zone : %s" % host.zone) print("-"*35)
 2. Get OpenStack credentials. See Get OpenStack credentials (API v2).
3. Instantiatethenova_clientclientobjectbyusingthecredentialsdictionaryobject:
      nova_client = Client(**credentials)
4. Listhostsbycallinglist()onnova_client.hostsobject: host_list = nova_client.hosts.list()
5. Printthehostobjectdetailsbycallingprint_hosts(host_list): print_hosts(host_list)
List hosts code listing example
User Guide (Release Version: 15.0.0)
                   #!/usr/bin/env python
from credentials import get_nova_credentials_v2 from novaclient.client import Client
from utils import print_hosts
credentials = get_nova_credentials_v2()
nova_client = Client(**credentials)
host_list = nova_client.hosts.list()
print_hosts(host_list)
 HOT Guide
Orchestration is compatible with the CloudFormation template, but you can also write heat templates to orches- trate cloud resources.
To learn how, refer to the Template Guide on the OpenStack developer documentation website.
OpenStack command-line interface cheat sheet
Here is a list of common commands for reference.
Identity (keystone)
List all users
$ openstack user list
      HOT Guide 181
 User Guide (Release Version: 15.0.0)
List Identity service catalog
$ openstack catalog list
Images (glance)
List images you can access
$ openstack image list Delete specified image
$ openstack image delete IMAGE Describe a specific image
$ openstack image show IMAGE Update image
$ openstack image set IMAGE Upload kernel image
Upload RAM image
Upload three-part image
Register raw image
Compute (nova)
List instances, check status of instance
$ openstack server list
                             $ openstack image create "cirros-threepart-kernel" \ --disk-format aki --container-format aki --public \ --file ~/images/cirros-0.3.5-x86_64-kernel
     $ openstack image create "cirros-threepart-ramdisk" \ --disk-format ari --container-format ari --public \ --file ~/images/cirros-0.3.5-x86_64-initramfs
     $ openstack image create "cirros-threepart" --disk-format ami \ --container-format ami --public \
--property kernel_id=$KID-property ramdisk_id=$RID \
--file ~/images/cirros-0.3.5-x86_64-rootfs.img
     $ openstack image create "cirros-raw" --disk-format raw \ --container-format bare --public \
--file ~/images/cirros-0.3.5-x86_64-disk.img
       182 OpenStack command-line interface cheat sheet
 List images
$ openstack image list Create a flavor named m1.tiny
$ openstack flavor create --ram 512 --disk 1 --vcpus 1 m1.tiny List flavors
$ openstack flavor list
Boot an instance using flavor and image names (if names are unique)
Log in to the instance (from Linux)
Note: The ip command is available only on Linux. Using ip netns provides your environment a copy of the network stack with its own routes, firewall rules, and network devices for better troubleshooting.
Note: In CirrOS, the password for user cirros is cubswin:). For any other operating system, use SSH keys. Log in to the instance with a public IP address (from Mac)
$ ssh cloud-user@128.107.37.150 Show details of instance
View console log of instance
$ openstack console log show MyFirstInstance Set metadata on an instance
$ nova meta volumeTwoImage set newmeta='my meta data' Create an instance snapshot
User Guide (Release Version: 15.0.0)
                   $ openstack server create --image IMAGE --flavor FLAVOR INSTANCE_NAME
$ openstack server create --image cirros-0.3.5-x86_64-uec --flavor m1.tiny \
  MyFirstInstance
       # ip netns
# ip netns exec NETNS_NAME ssh USER@SERVER
# ip netns exec qdhcp-6021a3b4-8587-4f9c-8064-0103885dfba2 \
  ssh cirros@10.0.0.2
            $ openstack server show NAME
$ openstack server show MyFirstInstance
               $ openstack image create volumeTwoImage snapshotOfVolumeImage $ openstack image show snapshotOfVolumeImage
  OpenStack command-line interface cheat sheet 183
 User Guide (Release Version: 15.0.0)
Pause, suspend, stop, rescue, resize, rebuild, reboot an instance
Pause
Unpause
$ openstack server unpause NAME Suspend
$ openstack server suspend NAME Unsuspend
$ openstack server resume NAME Stop
$ openstack server stop NAME Start
$ openstack server start NAME Rescue
Resize
Rebuild
Reboot
Inject user data and files into an instance
To validate that the file was injected, use ssh to connect to the instance, and look in /var/lib/cloud for the file.
    $ openstack server pause NAME
$ openstack server pause volumeTwoImage
                              $ openstack server rescue NAME
$ openstack server rescue NAME --rescue_image_ref RESCUE_IMAGE
     $ openstack server resize NAME FLAVOR
$ openstack server resize my-pem-server m1.small $ openstack server resize --confirm my-pem-server1
     $ openstack server rebuild NAME IMAGE
$ openstack server rebuild newtinny cirros-qcow2
     $ openstack server reboot NAME
$ openstack server reboot newtinny
     $ openstack server create --user-data FILE INSTANCE
$ openstack server create --user-data userdata.txt --image cirros-qcow2 \
  --flavor m1.tiny MyUserdataInstance2
  184 OpenStack command-line interface cheat sheet
 Inject a keypair into an instance and access the instance with that keypair Create keypair
Start an instance (boot)
Use ssh to connect to the instance
Manage security groups
Add rules to default security group allowing ping and SSH between instances in the default security group
Networking (neutron)
Create network
$ openstack network create NETWORK_NAME Create a subnet
Block Storage (cinder)
Used to manage volumes and volume snapshots that attach to instances. Create a new volume
Boot an instance and attach to volume
$ openstack server create --image cirros-qcow2 --flavor m1.tiny MyVolumeInstance List all volumes, noticing the volume status
$ openstack volume list
$ openstack keypair create test > test.pem $ chmod 600 test.pem
User Guide (Release Version: 15.0.0)
         $ openstack server create --image cirros-0.3.5-x86_64 --flavor m1.small \ --key-name test MyFirstServer
     # ip netns exec qdhcp-98f09f1e-64c4-4301-a897-5067ee6d544f \ ssh -i test.pem cirros@10.0.0.4
     $ openstack security group rule create default \ --remote-group default --protocol icmp
$ openstack security group rule create default \ --remote-group default --dst-port 22
          $ openstack subnet create --subnet-pool SUBNET --network NETWORK SUBNET_NAME $ openstack subnet create --subnet-pool 10.0.0.0/29 --network net1 subnet1
     $ openstack volume create --size SIZE_IN_GB NAME $ openstack volume create --size 1 MyFirstVolume
            OpenStack command-line interface cheat sheet 185
 User Guide (Release Version: 15.0.0)
Attach a volume to an instance after the instance is active, and the volume is available
Note: On the Xen Hypervisor it is possible to provide a specific device name instead of automatic allocation. For example:
Manage volumes after login into the instance List storage devices
# fdisk -l
Make filesystem on volume
# mkfs.ext3 /dev/vdb Create a mountpoint
# mkdir /myspace
Mount the volume at the mountpoint
# mount /dev/vdb /myspace Create a file on the volume
Unmount the volume
# umount /myspace
Object Storage (swift)
Display information for the account, container, or object
List containers
$ swift list
    $ openstack server add volume INSTANCE_ID VOLUME_ID
$ openstack server add volume MyVolumeInstance 573e024d-5235-49ce-8332-be1576d323f8
       $ openstack server add volume --device /dev/vdb MyVolumeInstance 573e024d..1576d323f8 This is not currently possible when using non-Xen hypervisors with OpenStack.
                         # touch /myspace/helloworld.txt # ls /myspace
          $ swift stat
$ swift stat ACCOUNT $ swift stat CONTAINER $ swift stat OBJECT
       186 OpenStack command-line interface cheat sheet
 Appendix Community support
The following resources are available to help you run and use OpenStack. The OpenStack community constantly improves and adds to the main features of OpenStack, but if you have any questions, do not hesitate to ask. Use the following resources to get OpenStack support and troubleshoot your installations.
Documentation
For the available OpenStack documentation, see docs.openstack.org.
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at OpenStack Documentation Mailing List, join our IRC channel #openstack-doc on the freenode IRC network, or report a bug.
The following books explain how to install an OpenStack cloud and its associated components: • Installation Tutorial for openSUSE Leap 42.2 and SUSE Linux Enterprise Server 12 SP2 • Installation Tutorial for Red Hat Enterprise Linux 7 and CentOS 7
• Installation Tutorial for Ubuntu 16.04 (LTS)
The following books explain how to configure and run an OpenStack cloud: • Architecture Design Guide
• Administrator Guide
• Configuration Reference
• Operations Guide
• Networking Guide
• High Availability Guide
• Security Guide
• Virtual Machine Image Guide
The following books explain how to use the OpenStack Dashboard and command-line clients: • End User Guide
• Command-Line Interface Reference
The following documentation provides reference and guidance information for the OpenStack APIs: • API Guide
The following guide provides how to contribute to OpenStack documentation: • Documentation Contributor Guide
User Guide (Release Version: 15.0.0)
 Appendix 187
 User Guide (Release Version: 15.0.0)
ask.openstack.org
During the set up or testing of OpenStack, you might have questions about how a specific task is completed or be in a situation where a feature does not work correctly. Use the ask.openstack.org site to ask questions and get answers. When you visit the Ask OpenStack site, scan the recently asked questions to see whether your question has already been answered. If not, ask a new question. Be sure to give a clear, concise summary in the title and provide as much detail as possible in the description. Paste in your command output or stack traces, links to screen shots, and any other information which might be useful.
OpenStack mailing lists
A great way to get answers and insights is to post your question or problematic scenario to the OpenStack mailing list. You can learn from and help others who might have similar issues. To subscribe or view the archives, go to the general OpenStack mailing list. If you are interested in the other mailing lists for specific projects or development, refer to Mailing Lists.
The OpenStack wiki
The OpenStack wiki contains a broad range of topics but some of the information can be difficult to find or is a few pages deep. Fortunately, the wiki search feature enables you to search by title or content. If you search for specific information, such as about networking or OpenStack Compute, you can find a large amount of relevant material. More is being added all the time, so be sure to check back often. You can find the search box in the upper-right corner of any OpenStack wiki page.
The Launchpad Bugs area
The OpenStack community values your set up and testing efforts and wants your feedback. To log a bug, you must sign up for a Launchpad account. You can view existing bugs and report bugs in the Launchpad Bugs area. Use the search feature to determine whether the bug has already been reported or already been fixed. If it still seems like your bug is unreported, fill out a bug report.
Some tips:
• Give a clear, concise summary.
• Provideasmuchdetailaspossibleinthedescription.Pasteinyourcommandoutputorstacktraces,links to screen shots, and any other information which might be useful.
• Be sure to include the software and package versions that you are using, especially
if you are using a development branch, such as, "Kilo release" vs git commit bc79c3ecc55929bac585d04a03475b72e06a3208.
• Any deployment-specific information is helpful, such as whether you are using Ubuntu 14.04 or are performing a multi-node installation.
The following Launchpad Bugs areas are available: • Bugs: OpenStack Block Storage (cinder)
• Bugs: OpenStack Compute (nova)
• Bugs: OpenStack Dashboard (horizon)
• Bugs: OpenStack Identity (keystone)
 188
Appendix
 • Bugs: OpenStack Image service (glance) • Bugs: OpenStack Networking (neutron) • Bugs: OpenStack Object Storage (swift) • Bugs: Application catalog (murano)
• Bugs: Bare metal service (ironic)
• Bugs: Clustering service (senlin)
• Bugs: Container Infrastructure Management service (magnum)
• Bugs: Data processing service (sahara)
• Bugs: Database service (trove)
• Bugs: Deployment service (fuel)
• Bugs: DNS service (designate)
• Bugs: Key Manager Service (barbican)
• Bugs: Monitoring (monasca)
• Bugs: Orchestration (heat)
• Bugs: Rating (cloudkitty)
• Bugs: Shared file systems (manila)
• Bugs: Telemetry (ceilometer)
• Bugs: Telemetry v3 (gnocchi)
• Bugs: Workflow service (mistral)
• Bugs: Messaging service (zaqar)
• Bugs: OpenStack API Documentation (developer.openstack.org) • Bugs: OpenStack Documentation (docs.openstack.org)
The OpenStack IRC channel
The OpenStack community lives in the #openstack IRC channel on the Freenode network. You can hang out, ask questions, or get immediate feedback for urgent and pressing issues. To install an IRC client or use a browser- based client, go to https://webchat.freenode.net/. You can also use Colloquy (Mac OS X), mIRC (Windows), or XChat (Linux). When you are in the IRC channel and want to share code or command output, the generally accepted method is to use a Paste Bin. The OpenStack project has one at Paste. Just paste your longer amounts of text or logs in the web form and you get a URL that you can paste into the channel. The OpenStack IRC channel is #openstack on irc.freenode.net. You can find a list of all OpenStack IRC channels on the IRC page on the wiki.
Documentation feedback
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at OpenStack Documentation Mailing List, or report a bug.
User Guide (Release Version: 15.0.0)
 Appendix 189
 User Guide (Release Version: 15.0.0)
OpenStack distribution packages
The following Linux distributions provide community-supported packages for OpenStack:
• Debian: https://wiki.debian.org/OpenStack
• CentOS, Fedora, and Red Hat Enterprise Linux: https://www.rdoproject.org/
• openSUSE and SUSE Linux Enterprise Server: https://en.opensuse.org/Portal:OpenStack • Ubuntu: https://wiki.ubuntu.com/ServerTeam/CloudArchive
Glossary
This glossary offers a list of terms and definitions to define a vocabulary for OpenStack-related concepts.
To add to OpenStack glossary, clone the openstack/openstack-manuals repository and update the source file
doc/common/glossary.rst through the OpenStack contribution process. 0-9
6to4 A mechanism that allows IPv6 packets to be transmitted over an IPv4 network, providing a strategy for migrating to IPv6.
A
absolute limit Impassable limits for guest VMs. Settings include total RAM size, maximum number of vC- PUs, and maximum disk size.
access control list (ACL) A list of permissions attached to an object. An ACL specifies which users or system processes have access to objects. It also defines which operations can be performed on specified objects. Each entry in a typical ACL specifies a subject and an operation. For instance, the ACL entry (Alice, delete) for a file gives Alice permission to delete the file.
access key Alternative term for an Amazon EC2 access key. See EC2 access key.
account The Object Storage context of an account. Do not confuse with a user account from an authentication
service, such as Active Directory, /etc/passwd, OpenLDAP, OpenStack Identity, and so on.
account auditor Checks for missing replicas and incorrect or corrupted objects in a specified Object Storage account by running queries against the back-end SQLite database.
account database A SQLite database that contains Object Storage accounts and related metadata and that the accounts server accesses.
account reaper An Object Storage worker that scans for and deletes account databases and that the account server has marked for deletion.
account server Lists containers in Object Storage and stores container information in the account database. account service An Object Storage component that provides account services such as list, create, modify, and
audit. Do not confuse with OpenStack Identity service, OpenLDAP, or similar user-account services.
accounting The Compute service provides accounting information through the event notification and system usage data facilities.
Active Directory Authentication and identity service by Microsoft, based on LDAP. Supported in OpenStack. 190 Appendix

 active/active configuration In a high-availability setup with an active/active configuration, several systems share the load together and if one fails, the load is distributed to the remaining systems.
active/passive configuration In a high-availability setup with an active/passive configuration, systems are set up to bring additional resources online to replace those that have failed.
address pool A group of fixed and/or floating IP addresses that are assigned to a project and can be used by or assigned to the VM instances in a project.
Address Resolution Protocol (ARP) The protocol by which layer-3 IP addresses are resolved into layer-2 link local addresses.
admin API A subset of API calls that are accessible to authorized administrators and are generally not ac- cessible to end users or the public Internet. They can exist as a separate service (keystone) or can be a subset of another API (nova).
admin server In the context of the Identity service, the worker process that provides access to the admin API.
administrator The person responsible for installing, configuring, and managing an OpenStack cloud.
Advanced Message Queuing Protocol (AMQP) The open standard messaging protocol used by OpenStack components for intra-service communications, provided by RabbitMQ, Qpid, or ZeroMQ.
Advanced RISC Machine (ARM) Lower power consumption CPU often found in mobile and embedded devices. Supported by OpenStack.
alert The Compute service can send alerts through its notification system, which includes a facility to create custom notification drivers. Alerts can be sent to and displayed on the dashboard.
allocate The process of taking a floating IP address from the address pool so it can be associated with a fixed IP on a guest VM instance.
Amazon Kernel Image (AKI) Both a VM container format and disk format. Supported by Image service. Amazon Machine Image (AMI) Both a VM container format and disk format. Supported by Image service. Amazon Ramdisk Image (ARI) Both a VM container format and disk format. Supported by Image service. Anvil A project that ports the shell script-based project named DevStack to Python.
aodh Part of the OpenStack Telemetry service; provides alarming functionality.
Apache The Apache Software Foundation supports the Apache community of open-source software projects.
These projects provide software products for the public good.
Apache License 2.0 All OpenStack core projects are provided under the terms of the Apache License 2.0 license.
Apache Web Server The most common web server software currently used on the Internet.
API endpoint The daemon, worker, or service that a client communicates with to access an API. API endpoints can provide any number of services, such as authentication, sales data, performance meters, Compute VM commands, census data, and so on.
API extension Custom modules that extend some OpenStack core APIs.
API extension plug-in Alternative term for a Networking plug-in or Networking API extension. API key Alternative term for an API token.
API server Any node running a daemon or worker that provides an API endpoint.
User Guide (Release Version: 15.0.0)
 Appendix
191
 User Guide (Release Version: 15.0.0)
API token Passed to API requests and used by OpenStack to verify that the client is authorized to run the requested operation.
API version In OpenStack, the API version for a project is part of the URL. For example, example.com/ nova/v1/foobar.
applet A Java program that can be embedded into a web page.
Application Catalog service (murano) The project that provides an application catalog service so that users can compose and deploy composite environments on an application abstraction level while managing the application lifecycle.
Application Programming Interface (API) A collection of specifications used to access a service, appli- cation, or program. Includes service calls, required parameters for each call, and the expected return values.
application server A piece of software that makes available another piece of software over a network. Application Service Provider (ASP) Companies that rent specialized applications that help businesses and
organizations provide additional services with lower cost.
arptables Tool used for maintaining Address Resolution Protocol packet filter rules in the Linux kernel fire- wall modules. Used along with iptables, ebtables, and ip6tables in Compute to provide firewall services for VMs.
associate The process associating a Compute floating IP address with a fixed IP address.
Asynchronous JavaScript and XML (AJAX) A group of interrelated web development techniques used on
the client-side to create asynchronous web applications. Used extensively in horizon.
ATA over Ethernet (AoE) A disk storage protocol tunneled within Ethernet.
attach The process of connecting a VIF or vNIC to a L2 network in Networking. In the context of Compute, this process connects a storage volume to an instance.
attachment (network) Association of an interface ID to a logical port. Plugs an interface into a port.
auditing Provided in Compute through the system usage data facility.
auditor A worker process that verifies the integrity of Object Storage objects, containers, and accounts. Au- ditors is the collective term for the Object Storage account auditor, container auditor, and object auditor.
Austin The code name for the initial release of OpenStack. The first design summit took place in Austin, Texas, US.
auth node Alternative term for an Object Storage authorization node.
authentication The process that confirms that the user, process, or client is really who they say they are
through private key, secret token, password, fingerprint, or similar method.
authentication token A string of text provided to the client after authentication. Must be provided by the user or process in subsequent requests to the API endpoint.
AuthN The Identity service component that provides authentication services.
authorization The act of verifying that a user, process, or client is authorized to perform an action. authorization node An Object Storage node that provides authorization services.
AuthZ The Identity component that provides high-level authorization services.
Auto ACK Configuration setting within RabbitMQ that enables or disables message acknowledgment. En- abled by default.
 192
Appendix
 auto declare A Compute RabbitMQ setting that determines whether a message exchange is automatically created when the program starts.
availability zone An Amazon EC2 concept of an isolated area that is used for fault tolerance. Do not confuse with an OpenStack Compute zone or cell.
AWS CloudFormation template AWS CloudFormation allows Amazon Web Services (AWS) users to create and manage a collection of related resources. The Orchestration service supports a CloudFormation- compatible format (CFN).
B
back end Interactions and processes that are obfuscated from the user, such as Compute volume mount, data transmission to an iSCSI target by a daemon, or Object Storage object integrity checks.
back-end catalog The storage method used by the Identity service catalog service to store and retrieve infor- mation about API endpoints that are available to the client. Examples include an SQL database, LDAP database, or KVS back end.
back-end store The persistent data store used to save and retrieve information for a service, such as lists of Object Storage objects, current state of guest VMs, lists of user names, and so on. Also, the method that the Image service uses to get and store VM images. Options include Object Storage, locally mounted file system, RADOS block devices, VMware datastore, and HTTP.
Backup, Restore, and Disaster Recovery service (freezer) The project that provides integrated tooling for backing up, restoring, and recovering file systems, instances, or database backups.
bandwidth The amount of available data used by communication resources, such as the Internet. Represents the amount of data that is used to download things or the amount of data available to download.
barbican Code name of the Key Manager service.
bare An Image service container format that indicates that no container exists for the VM image.
Bare Metal service (ironic) The OpenStack service that provides a service and associated libraries capable of managing and provisioning physical machines in a security-aware and fault-tolerant manner.
base image An OpenStack-provided image.
Bell-LaPadula model A security model that focuses on data confidentiality and controlled access to classified information. This model divides the entities into subjects and objects. The clearance of a subject is compared to the classification of the object to determine if the subject is authorized for the specific access mode. The clearance or classification scheme is expressed in terms of a lattice.
Benchmark service (rally) OpenStack project that provides a framework for performance analysis and bench- marking of individual OpenStack components as well as full production OpenStack cloud deployments.
Bexar A grouped release of projects related to OpenStack that came out in February of 2011. It included only Compute (nova) and Object Storage (swift). Bexar is the code name for the second release of OpenStack. The design summit took place in San Antonio, Texas, US, which is the county seat for Bexar county.
binary Information that consists solely of ones and zeroes, which is the language of computers.
bit A bit is a single digit number that is in base of 2 (either a zero or one). Bandwidth usage is measured in
bits per second.
bits per second (BPS) The universal measurement of how quickly data is transferred from place to place.
User Guide (Release Version: 15.0.0)
 Appendix 193
 User Guide (Release Version: 15.0.0)
block device A device that moves data in the form of blocks. These device nodes interface the devices, such as hard disks, CD-ROM drives, flash drives, and other addressable regions of memory.
block migration A method of VM live migration used by KVM to evacuate instances from one host to another with very little downtime during a user-initiated switchover. Does not require shared storage. Supported by Compute.
Block Storage API An API on a separate endpoint for attaching, detaching, and creating block storage for compute VMs.
Block Storage service (cinder) The OpenStack service that implement services and libraries to provide on- demand, self-service access to Block Storage resources via abstraction and automation on top of other block storage devices.
BMC (Baseboard Management Controller) The intelligence in the IPMI architecture, which is a specialized micro-controller that is embedded on the motherboard of a computer and acts as a server. Manages the interface between system management software and platform hardware.
bootable disk image A type of VM image that exists as a single, bootable file.
Bootstrap Protocol (BOOTP) A network protocol used by a network client to obtain an IP address from a configuration server. Provided in Compute through the dnsmasq daemon when using either the FlatD- HCP manager or VLAN manager network manager.
Border Gateway Protocol (BGP) The Border Gateway Protocol is a dynamic routing protocol that connects autonomous systems. Considered the backbone of the Internet, this protocol connects disparate networks to form a larger network.
browser Any client software that enables a computer or device to access the Internet.
builder file Contains configuration information that Object Storage uses to reconfigure a ring or to re-create
it from scratch after a serious failure.
bursting The practice of utilizing a secondary environment to elastically build instances on-demand when the primary environment is resource constrained.
button class A group of related button types within horizon. Buttons to start, stop, and suspend VMs are in one class. Buttons to associate and disassociate floating IP addresses are in another class, and so on.
byte Set of bits that make up a single character; there are usually 8 bits to a byte. C
cache pruner A program that keeps the Image service VM image cache at or below its configured maximum size.
Cactus An OpenStack grouped release of projects that came out in the spring of 2011. It included Compute (nova), Object Storage (swift), and the Image service (glance). Cactus is a city in Texas, US and is the code name for the third release of OpenStack. When OpenStack releases went from three to six months long, the code name of the release changed to match a geography nearest the previous summit.
CALL One of the RPC primitives used by the OpenStack message queue software. Sends a message and waits for a response.
capability Defines resources for a cell, including CPU, storage, and networking. Can apply to the specific services within a cell or a whole cell.
capacity cache A Compute back-end database table that contains the current workload, amount of free RAM, and number of VMs running on each host. Used to determine on which host a VM starts.
 194
Appendix
 capacity updater A notification driver that monitors VM instances and updates the capacity cache as needed. CAST One of the RPC primitives used by the OpenStack message queue software. Sends a message and does
not wait for a response.
catalog A list of API endpoints that are available to a user after authentication with the Identity service.
catalog service An Identity service that lists API endpoints that are available to a user after authentication with the Identity service.
ceilometer Part of the OpenStack Telemetry service; gathers and stores metrics from other OpenStack services. cell Provides logical partitioning of Compute resources in a child and parent relationship. Requests are passed
from parent cells to child cells if the parent cannot provide the requested resource.
cell forwarding A Compute option that enables parent cells to pass resource requests to child cells if the parent cannot provide the requested resource.
cell manager The Compute component that contains a list of the current capabilities of each host within the cell and routes requests as appropriate.
CentOS A Linux distribution that is compatible with OpenStack.
Ceph Massively scalable distributed storage system that consists of an object store, block store, and POSIX-
compatible distributed file system. Compatible with OpenStack.
CephFS The POSIX-compliant file system provided by Ceph.
certificate authority (CA) In cryptography, an entity that issues digital certificates. The digital certificate certifies the ownership of a public key by the named subject of the certificate. This enables others (relying parties) to rely upon signatures or assertions made by the private key that corresponds to the certified public key. In this model of trust relationships, a CA is a trusted third party for both the subject (owner) of the certificate and the party relying upon the certificate. CAs are characteristic of many public key infrastructure (PKI) schemes. In OpenStack, a simple certificate authority is provided by Compute for cloudpipe VPNs and VM image decryption.
Challenge-Handshake Authentication Protocol (CHAP) An iSCSI authentication method supported by Compute.
chance scheduler A scheduling method used by Compute that randomly chooses an available host from the pool.
changes since A Compute API parameter that downloads changes to the requested item since your last request, instead of downloading a new, fresh set of data and comparing it against the old data.
Chef An operating system configuration management tool supporting OpenStack deployments.
child cell If a requested resource such as CPU time, disk storage, or memory is not available in the parent cell, the request is forwarded to its associated child cells. If the child cell can fulfill the request, it does. Otherwise, it attempts to pass the request to any of its children.
cinder Codename for Block Storage service.
CirrOS A minimal Linux distribution designed for use as a test image on clouds such as OpenStack.
Cisco neutron plug-in A Networking plug-in for Cisco devices and technologies, including UCS and Nexus.
cloud architect A person who plans, designs, and oversees the creation of clouds.
Cloud Auditing Data Federation (CADF) Cloud Auditing Data Federation (CADF) is a specification for audit event data. CADF is supported by OpenStack Identity.
User Guide (Release Version: 15.0.0)
 Appendix 195
 User Guide (Release Version: 15.0.0)
cloud computing A model that enables access to a shared pool of configurable computing resources, such as networks, servers, storage, applications, and services, that can be rapidly provisioned and released with minimal management effort or service provider interaction.
cloud controller Collection of Compute components that represent the global state of the cloud; talks to ser- vices, such as Identity authentication, Object Storage, and node/storage workers through a queue.
cloud controller node A node that runs network, volume, API, scheduler, and image services. Each service may be broken out into separate nodes for scalability or availability.
Cloud Data Management Interface (CDMI) SINA standard that defines a RESTful API for managing ob- jects in the cloud, currently unsupported in OpenStack.
Cloud Infrastructure Management Interface (CIMI) An in-progress specification for cloud management. Currently unsupported in OpenStack.
cloud-init A package commonly installed in VM images that performs initialization of an instance after boot using information that it retrieves from the metadata service, such as the SSH public key and user data.
cloudadmin One of the default roles in the Compute RBAC system. Grants complete system access.
Cloudbase-Init A Windows project providing guest initialization features, similar to cloud-init.
cloudpipe A compute service that creates VPNs on a per-project basis.
cloudpipe image A pre-made VM image that serves as a cloudpipe server. Essentially, OpenVPN running on Linux.
Clustering service (senlin) The project that implements clustering services and libraries for the management of groups of homogeneous objects exposed by other OpenStack services.
command filter Lists allowed commands within the Compute rootwrap facility.
Common Internet File System (CIFS) A file sharing protocol. It is a public or open variation of the original Server Message Block (SMB) protocol developed and used by Microsoft. Like the SMB protocol, CIFS runs at a higher level and uses the TCP/IP protocol.
Common Libraries (oslo) The project that produces a set of python libraries containing code shared by Open- Stack projects. The APIs provided by these libraries should be high quality, stable, consistent, docu- mented and generally applicable.
community project A project that is not officially endorsed by the OpenStack Foundation. If the project is successful enough, it might be elevated to an incubated project and then to a core project, or it might be merged with the main code trunk.
compression Reducing the size of files by special encoding, the file can be decompressed again to its original content. OpenStack supports compression at the Linux file system level but does not support compression for things such as Object Storage objects or Image service VM images.
Compute API (Nova API) The nova-api daemon provides access to nova services. Can communicate with other APIs, such as the Amazon EC2 API.
compute controller The Compute component that chooses suitable hosts on which to start VM instances.
compute host Physical host dedicated to running compute nodes.
compute node A node that runs the nova-compute daemon that manages VM instances that provide a wide range of services, such as web applications and analytics.
 196
Appendix
 Compute service (nova) The OpenStack core project that implements services and associated libraries to provide massively-scalable, on-demand, self-service access to compute resources, including bare metal, virtual machines, and containers.
compute worker The Compute component that runs on each compute node and manages the VM instance lifecycle, including run, reboot, terminate, attach/detach volumes, and so on. Provided by the nova- compute daemon.
concatenated object A set of segment objects that Object Storage combines and sends to the client. conductor In Compute, conductor is the process that proxies database requests from the compute process.
Using conductor improves security because compute nodes do not need direct access to the database.
congress Code name for the Governance service.
consistency window The amount of time it takes for a new Object Storage object to become accessible to all clients.
console log Contains the output from a Linux VM console in Compute.
container Organizes and stores objects in Object Storage. Similar to the concept of a Linux directory but
cannot be nested. Alternative term for an Image service container format.
container auditor Checks for missing replicas or incorrect objects in specified Object Storage containers through queries to the SQLite back-end database.
container database A SQLite database that stores Object Storage containers and container metadata. The container server accesses this database.
container format A wrapper used by the Image service that contains a VM image and its associated metadata, such as machine state, OS disk size, and so on.
Container Infrastructure Management service (magnum) The project which provides a set of services for provisioning, scaling, and managing container orchestration engines.
container server An Object Storage server that manages containers.
container service The Object Storage component that provides container services, such as create, delete, list,
and so on.
content delivery network (CDN) A content delivery network is a specialized network that is used to distribute content to clients, typically located close to the client for increased performance.
controller node Alternative term for a cloud controller node.
core API Depending on context, the core API is either the OpenStack API or the main API of a specific core
project, such as Compute, Networking, Image service, and so on.
core service An official OpenStack service defined as core by DefCore Committee. Currently, consists of Block Storage service (cinder), Compute service (nova), Identity service (keystone), Image service (glance), Networking service (neutron), and Object Storage service (swift).
cost Under the Compute distributed scheduler, this is calculated by looking at the capabilities of each host relative to the flavor of the VM instance being requested.
credentials Data that is only known to or accessible by a user and used to verify that the user is who he says he is. Credentials are presented to the server during authentication. Examples include a password, secret key, digital certificate, and fingerprint.
CRL A Certificate Revocation List (CRL) in a PKI model is a list of certificates that have been revoked. End entities presenting these certificates should not be trusted.
User Guide (Release Version: 15.0.0)
 Appendix 197
 User Guide (Release Version: 15.0.0)
Cross-Origin Resource Sharing (CORS) A mechanism that allows many resources (for example, fonts, JavaScript) on a web page to be requested from another domain outside the domain from which the resource originated. In particular, JavaScript’s AJAX calls can use the XMLHttpRequest mechanism.
Crowbar An open source community project by SUSE that aims to provide all necessary services to quickly deploy and manage clouds.
current workload An element of the Compute capacity cache that is calculated based on the number of build, snapshot, migrate, and resize operations currently in progress on a given host.
customer Alternative term for project.
customization module A user-created Python module that is loaded by horizon to change the look and feel
of the dashboard.
D
daemon A process that runs in the background and waits for requests. May or may not listen on a TCP or UDP port. Do not confuse with a worker.
Dashboard (horizon) OpenStack project which provides an extensible, unified, web-based user interface for all OpenStack services.
data encryption Both Image service and Compute support encrypted virtual machine (VM) images (but not instances). In-transit data encryption is supported in OpenStack using technologies such as HTTPS, SSL, TLS, and SSH. Object Storage does not support object encryption at the application level but may support storage that uses disk encryption.
Data loss prevention (DLP) software Software programs used to protect sensitive information and prevent it from leaking outside a network boundary through the detection and denying of the data transportation.
Data Processing service (sahara) OpenStack project that provides a scalable data-processing stack and as- sociated management interfaces.
data store A database engine supported by the Database service.
database ID A unique ID given to each replica of an Object Storage database.
database replicator An Object Storage component that copies changes in the account, container, and object databases to other nodes.
Database service (trove) An integrated project that provides scalable and reliable Cloud Database-as-a- Service functionality for both relational and non-relational database engines.
deallocate The process of removing the association between a floating IP address and a fixed IP address. Once this association is removed, the floating IP returns to the address pool.
Debian A Linux distribution that is compatible with OpenStack.
deduplication The process of finding duplicate data at the disk block, file, and/or object level to minimize storage use—currently unsupported within OpenStack.
The default panel that is displayed when a user accesses the dashboard.
New users are assigned to this project if no project is specified when a user is created.
An Identity service token that is not associated with a specific project and is exchanged for a scoped token.
default panel
default project
default token
 198
Appendix
 delayed delete An option within Image service so that an image is deleted after a predefined number of seconds instead of immediately.
delivery mode Setting for the Compute RabbitMQ message delivery mode; can be set to either transient or persistent.
denial of service (DoS) Denial of service (DoS) is a short form for denial-of-service attack. This is a malicious attempt to prevent legitimate users from using a service.
deprecated auth An option within Compute that enables administrators to create and manage users through the nova-manage command as opposed to using the Identity service.
designate Code name for the DNS service.
Desktop-as-a-Service A platform that provides a suite of desktop environments that users access to receive a desktop experience from any location. This may provide general use, development, or even homoge- neous testing environments.
developer One of the default roles in the Compute RBAC system and the default role assigned to a new user.
device ID Maps Object Storage partitions to physical storage devices.
device weight Distributes partitions proportionately across Object Storage devices based on the storage ca- pacity of each device.
DevStack Community project that uses shell scripts to quickly build complete OpenStack development envi- ronments.
DHCP agent OpenStack Networking agent that provides DHCP services for virtual networks.
Diablo A grouped release of projects related to OpenStack that came out in the fall of 2011, the fourth release of OpenStack. It included Compute (nova 2011.3), Object Storage (swift 1.4.3), and the Image service (glance). Diablo is the code name for the fourth release of OpenStack. The design summit took place in the Bay Area near Santa Clara, California, US and Diablo is a nearby city.
direct consumer An element of the Compute RabbitMQ that comes to life when a RPC call is executed. It connects to a direct exchange through a unique exclusive queue, sends the message, and terminates.
direct exchange A routing table that is created within the Compute RabbitMQ during RPC calls; one is created for each RPC call that is invoked.
direct publisher Element of RabbitMQ that provides a response to an incoming MQ message.
disassociate The process of removing the association between a floating IP address and fixed IP and thus
returning the floating IP address to the address pool.
Discretionary Access Control (DAC) Governs the ability of subjects to access objects, while enabling users to make policy decisions and assign security attributes. The traditional UNIX system of users, groups, and read-write-execute permissions is an example of DAC.
disk encryption The ability to encrypt data at the file system, disk partition, or whole-disk level. Supported within Compute VMs.
disk format The underlying format that a disk image for a VM is stored as within the Image service back-end store. For example, AMI, ISO, QCOW2, VMDK, and so on.
dispersion In Object Storage, tools to test and ensure dispersion of objects and containers to ensure fault tolerance.
distributed virtual router (DVR) Mechanism for highly available multi-host routing when using OpenStack Networking (neutron).
User Guide (Release Version: 15.0.0)
 Appendix 199
 User Guide (Release Version: 15.0.0)
Django A web framework used extensively in horizon.
DNS record A record that specifies information about a particular domain and belongs to the domain.
DNS service (designate) OpenStack project that provides scalable, on demand, self service access to author- itative DNS services, in a technology-agnostic manner.
dnsmasq Daemon that provides DNS, DHCP, BOOTP, and TFTP services for virtual networks.
domain An Identity API v3 entity. Represents a collection of projects, groups and users that defines admin- istrative boundaries for managing OpenStack Identity entities. On the Internet, separates a website from other sites. Often, the domain name has two or more parts that are separated by dots. For example, yahoo.com, usa.gov, harvard.edu, or mail.yahoo.com. Also, a domain is an entity or container of all DNS-related information containing one or more records.
Domain Name System (DNS) A system by which Internet domain name-to-address and address-to-name res- olutions are determined. DNS helps navigate the Internet by translating the IP address into an address that is easier to remember. For example, translating 111.111.111.1 into www.yahoo.com. All domains and their components, such as mail servers, utilize DNS to resolve to the appropriate locations. DNS servers are usually set up in a master-slave relationship such that failure of the master invokes the slave. DNS servers might also be clustered or replicated such that changes made to one DNS server are au- tomatically propagated to other active servers. In Compute, the support that enables associating DNS entries with floating IP addresses, nodes, or cells so that hostnames are consistent across reboots.
download The transfer of data, usually in the form of files, from one computer to another.
durable exchange The Compute RabbitMQ message exchange that remains active when the server restarts. durable queue A Compute RabbitMQ message queue that remains active when the server restarts.
Dynamic Host Configuration Protocol (DHCP) A network protocol that configures devices that are con- nected to a network so that they can communicate on that network by using the Internet Protocol (IP). The protocol is implemented in a client-server model where DHCP clients request configuration data, such as an IP address, a default route, and one or more DNS server addresses from a DHCP server. A method to automatically configure networking for a host at boot time. Provided by both Networking and Compute.
Dynamic HyperText Markup Language (DHTML) Pages that use HTML, JavaScript, and Cascading Style Sheets to enable users to interact with a web page or show simple animation.
E
east-west traffic Network traffic between servers in the same cloud or data center. See also north-south traffic. EBS boot volume An Amazon EBS storage volume that contains a bootable VM image, currently unsupported
in OpenStack.
ebtables Filtering tool for a Linux bridging firewall, enabling filtering of network traffic passing through a Linux bridge. Used in Compute along with arptables, iptables, and ip6tables to ensure isolation of network communications.
EC2 The Amazon commercial compute product, similar to Compute.
EC2 access key Used along with an EC2 secret key to access the Compute EC2 API.
EC2 API OpenStack supports accessing the Amazon EC2 API through Compute.
EC2 Compatibility API A Compute component that enables OpenStack to communicate with Amazon EC2.
 200 Appendix
 EC2 secret key Used along with an EC2 access key when communicating with the Compute EC2 API; used to digitally sign each request.
Elastic Block Storage (EBS) The Amazon commercial block storage product.
encapsulation The practice of placing one packet type within another for the purposes of abstracting or se-
curing data. Examples include GRE, MPLS, or IPsec.
encryption OpenStack supports encryption technologies such as HTTPS, SSH, SSL, TLS, digital certificates, and data encryption.
endpoint See API endpoint.
endpoint registry Alternative term for an Identity service catalog.
endpoint template A list of URL and port number endpoints that indicate where a service, such as Object Storage, Compute, Identity, and so on, can be accessed.
entity Any piece of hardware or software that wants to connect to the network services provided by Net- working, the network connectivity service. An entity can make use of Networking by implementing a VIF.
ephemeral image A VM image that does not save changes made to its volumes and reverts them to their original state after the instance is terminated.
ephemeral volume Volume that does not save the changes made to it and reverts to its original state when the current user relinquishes control.
Essex
ESXi
ETag
A grouped release of projects related to OpenStack that came out in April 2012, the fifth release of OpenStack. It included Compute (nova 2012.1), Object Storage (swift 1.4.8), Image (glance), Identity (keystone), and Dashboard (horizon). Essex is the code name for the fifth release of OpenStack. The design summit took place in Boston, Massachusetts, US and Essex is a nearby city.
An OpenStack-supported hypervisor.
MD5 hash of an object within Object Storage, used to ensure data integrity.
euca2ools A collection of command-line tools for administering VMs; most are compatible with OpenStack.
Eucalyptus Kernel Image (EKI) Used along with an ERI to create an EMI.
Eucalyptus Machine Image (EMI) VM image container format supported by Image service.
Eucalyptus Ramdisk Image (ERI) Used along with an EKI to create an EMI.
evacuate The process of migrating one or all virtual machine (VM) instances from one host to another, com-
patible with both shared storage live migration and block migration.
exchange Alternative term for a RabbitMQ message exchange.
exchange type A routing algorithm in the Compute RabbitMQ.
exclusive queue Connected to by a direct consumer in RabbitMQ—Compute, the message can be consumed only by the current connection.
extended attributes (xattr) File system option that enables storage of additional information beyond owner, group, permissions, modification time, and so on. The underlying Object Storage file system must sup- port extended attributes.
extension Alternative term for an API extension or plug-in. In the context of Identity service, this is a call that is specific to the implementation, such as adding support for OpenID.
external network A network segment typically used for instance Internet access. Appendix
201
User Guide (Release Version: 15.0.0)

 User Guide (Release Version: 15.0.0)
extra specs Specifies additional requirements when Compute determines where to start a new instance. Ex- amples include a minimum amount of network bandwidth or a GPU.
F
FakeLDAP An easy method to create a local LDAP directory for testing Identity and Compute. Requires Redis.
fan-out exchange Within RabbitMQ and Compute, it is the messaging interface that is used by the scheduler service to receive capability messages from the compute, volume, and network nodes.
federated identity A method to establish trusts between identity providers and the OpenStack cloud. Fedora A Linux distribution compatible with OpenStack.
Fibre Channel Storage protocol similar in concept to TCP/IP; encapsulates SCSI commands and data. Fibre Channel over Ethernet (FCoE) The fibre channel protocol tunneled within Ethernet.
fill-first scheduler The Compute scheduling method that attempts to fill a host with VMs rather than starting new VMs on a variety of hosts.
filter The step in the Compute scheduling process when hosts that cannot run VMs are eliminated and not chosen.
firewall Used to restrict communications between hosts and/or nodes, implemented in Compute using iptables, arptables, ip6tables, and ebtables.
FireWall-as-a-Service (FWaaS) A Networking extension that provides perimeter firewall functionality. fixed IP address An IP address that is associated with the same instance each time that instance boots, is
generally not accessible to end users or the public Internet, and is used for management of the instance.
Flat Manager The Compute component that gives IP addresses to authorized nodes and assumes DHCP, DNS, and routing configuration and services are provided by something else.
flat mode injection A Compute networking method where the OS network configuration information is in- jected into the VM image before the instance starts.
flat network Virtual network type that uses neither VLANs nor tunnels to segregate project traffic. Each flat network typically requires a separate underlying physical interface defined by bridge mappings. However, a flat network can contain multiple subnets.
FlatDHCP Manager The Compute component that provides dnsmasq (DHCP, DNS, BOOTP, TFTP) and radvd (routing) services.
flavor Alternative term for a VM instance type.
flavor ID UUID for each Compute or Image service VM flavor or instance type.
floating IP address An IP address that a project can associate with a VM so that the instance has the same public IP address each time that it boots. You create a pool of floating IP addresses and assign them to instances as they are launched to maintain a consistent IP address for maintaining DNS assignment.
Folsom A grouped release of projects related to OpenStack that came out in the fall of 2012, the sixth release of OpenStack. It includes Compute (nova), Object Storage (swift), Identity (keystone), Networking (neutron), Image service (glance), and Volumes or Block Storage (cinder). Folsom is the code name for the sixth release of OpenStack. The design summit took place in San Francisco, California, US and Folsom is a nearby city.
 202
Appendix
 FormPost Object Storage middleware that uploads (posts) an image through a form on a web page.
freezer Code name for the Backup, Restore, and Disaster Recovery service.
front end The point where a user interacts with a service; can be an API endpoint, the dashboard, or a command-line tool.
G
gateway An IP address, typically assigned to a router, that passes network traffic between different networks. generic receive offload (GRO) Feature of certain network interface drivers that combines many smaller re-
ceived packets into a large packet before delivery to the kernel IP stack.
generic routing encapsulation (GRE) Protocol that encapsulates a wide variety of network layer protocols inside virtual point-to-point links.
glance Codename for the Image service.
glance API server Alternative name for the Image API.
glance registry Alternative term for the Image service image registry.
global endpoint template The Identity service endpoint template that contains services available to all projects.
GlusterFS A file system designed to aggregate NAS hosts, compatible with OpenStack.
gnocchi Part of the OpenStack Telemetry service; provides an indexer and time-series database.
golden image A method of operating system installation where a finalized disk image is created and then used by all nodes without modification.
Governance service (congress) The project that provides Governance-as-a-Service across any collection of cloud services in order to monitor, enforce, and audit policy over dynamic infrastructure.
Graphic Interchange Format (GIF) A type of image file that is commonly used for animated images on web pages.
Graphics Processing Unit (GPU) Choosing a host based on the existence of a GPU is currently unsupported in OpenStack.
Green Threads The cooperative threading model used by Python; reduces race conditions and only context switches when specific library calls are made. Each OpenStack service is its own thread.
Grizzly The code name for the seventh release of OpenStack. The design summit took place in San Diego, California, US and Grizzly is an element of the state flag of California.
Group An Identity v3 API entity. Represents a collection of users that is owned by a specific domain. guest OS An operating system instance running under the control of a hypervisor.
H
Hadoop Apache Hadoop is an open source software framework that supports data-intensive distributed appli- cations.
Hadoop Distributed File System (HDFS) A distributed, highly fault-tolerant file system designed to run on low-cost commodity hardware.
User Guide (Release Version: 15.0.0)
 Appendix 203
 User Guide (Release Version: 15.0.0)
handover An object state in Object Storage where a new replica of the object is automatically created due to a drive failure.
HAProxy Provides a load balancer for TCP and HTTP-based applications that spreads requests across multiple servers.
hard reboot A type of reboot where a physical or virtual power button is pressed as opposed to a graceful, proper shutdown of the operating system.
Havana The code name for the eighth release of OpenStack. The design summit took place in Portland, Oregon, US and Havana is an unincorporated community in Oregon.
health monitor Determines whether back-end members of a VIP pool can process a request. A pool can have several health monitors associated with it. When a pool has several monitors associated with it, all monitors check each member of the pool. All monitors must declare a member to be healthy for it to stay active.
heat Codename for the Orchestration service.
Heat Orchestration Template (HOT) Heat input in the format native to OpenStack.
high availability (HA) A high availability system design approach and associated service implementation ensures that a prearranged level of operational performance will be met during a contractual measurement period. High availability systems seek to minimize system downtime and data loss.
horizon Codename for the Dashboard.
horizon plug-in A plug-in for the OpenStack Dashboard (horizon).
host A physical computer, not a VM instance (node).
host aggregate A method to further subdivide availability zones into hypervisor pools, a collection of common hosts.
Host Bus Adapter (HBA) Device plugged into a PCI slot, such as a fibre channel or network card.
hybrid cloud A hybrid cloud is a composition of two or more clouds (private, community or public) that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect colocation, managed and/or dedicated services with cloud resources.
Hyper-V One of the hypervisors supported by OpenStack.
hyperlink Any kind of text that contains a link to some other site, commonly found in documents where
clicking on a word or words opens up a different website.
Hypertext Transfer Protocol (HTTP) An application protocol for distributed, collaborative, hypermedia in- formation systems. It is the foundation of data communication for the World Wide Web. Hypertext is structured text that uses logical links (hyperlinks) between nodes containing text. HTTP is the protocol to exchange or transfer hypertext.
Hypertext Transfer Protocol Secure (HTTPS) An encrypted communications protocol for secure communi- cation over a computer network, with especially wide deployment on the Internet. Technically, it is not a protocol in and of itself; rather, it is the result of simply layering the Hypertext Transfer Protocol (HTTP) on top of the TLS or SSL protocol, thus adding the security capabilities of TLS or SSL to standard HTTP communications. Most OpenStack API endpoints and many inter-component communications support HTTPS communication.
hypervisor Software that arbitrates and controls VM access to the actual underlying hardware. hypervisor pool A collection of hypervisors grouped together through host aggregates.
 204
Appendix
 I
Icehouse The code name for the ninth release of OpenStack. The design summit took place in Hong Kong and Ice House is a street in that city.
ID number Unique numeric ID associated with each user in Identity, conceptually similar to a Linux or LDAP UID.
Identity API Alternative term for the Identity service API.
Identity back end The source used by Identity service to retrieve user information; an OpenLDAP server, for
example.
identity provider A directory service, which allows users to login with a user name and password. It is a typical source of authentication tokens.
Identity service (keystone) The project that facilitates API client authentication, service discovery, dis- tributed multi-project authorization, and auditing. It provides a central directory of users mapped to the OpenStack services they can access. It also registers endpoints for OpenStack services and acts as a common authentication system.
Identity service API The API used to access the OpenStack Identity service provided through keystone.
IETF image
Internet Engineering Task Force (IETF) is an open standards organization that develops Internet stan- dards, particularly the standards pertaining to TCP/IP.
A collection of files for a specific operating system (OS) that you use to create or rebuild a server. OpenStack provides pre-built images. You can also create custom images, or snapshots, from servers that you have launched. Custom images can be used for data backups or as “gold” images for additional servers.
Image API The Image service API endpoint for management of VM images. Processes client requests for VMs, updates Image service metadata on the registry server, and communicates with the store adapter to upload VM images from the back-end store.
image cache Used by Image service to obtain images on the local host rather than re-downloading them from the image server each time one is requested.
image ID Combination of a URI and UUID used to access Image service VM images through the image API. image membership A list of projects that can access a given VM image within Image service.
image owner The project who owns an Image service virtual machine image.
image registry A list of VM images that are available through Image service.
Image service (glance) The OpenStack service that provide services and associated libraries to store, browse, share, distribute and manage bootable disk images, other data closely associated with initializing compute resources, and metadata definitions.
image status The current status of a VM image in Image service, not to be confused with the status of a running instance.
image store The back-end store used by Image service to store VM images, options include Object Storage, locally mounted file system, RADOS block devices, VMware datastore, or HTTP.
image UUID UUID used by Image service to uniquely identify each VM image.
incubated project A community project may be elevated to this status and is then promoted to a core project.
User Guide (Release Version: 15.0.0)
 Appendix 205
 User Guide (Release Version: 15.0.0)
Infrastructure Optimization service (watcher) OpenStack project that aims to provide a flexible and scal- able resource optimization service for multi-project OpenStack-based clouds.
Infrastructure-as-a-Service (IaaS) IaaS is a provisioning model in which an organization outsources physical components of a data center, such as storage, hardware, servers, and networking components. A service provider owns the equipment and is responsible for housing, operating and maintaining it. The client typically pays on a per-use basis. IaaS is a model for providing cloud services.
ingress filtering The process of filtering incoming network traffic. Supported by Compute.
INI format The OpenStack configuration files use an INI format to describe options and their values. It
consists of sections and key value pairs.
injection The process of putting a file into a virtual machine image before the instance is started.
Input/Output Operations Per Second (IOPS) IOPS are a common performance measurement used to benchmark computer storage devices like hard disk drives, solid state drives, and storage area networks.
instance A running VM, or a VM in a known state such as suspended, that can be used like a hardware server.
instance ID Alternative term for instance UUID.
instance state The current state of a guest VM image.
instance tunnels network A network segment used for instance traffic tunnels between compute nodes and the network node.
instance type Describes the parameters of the various virtual machine images that are available to users; includes parameters such as CPU, storage, and memory. Alternative term for flavor.
instance type ID Alternative term for a flavor ID.
instance UUID Unique ID assigned to each guest VM instance.
Intelligent Platform Management Interface (IPMI) IPMI is a standardized computer system interface used by system administrators for out-of-band management of computer systems and monitoring of their op- eration. In layman’s terms, it is a way to manage a computer using a direct network connection, whether it is turned on or not; connecting to the hardware rather than an operating system or login shell.
interface A physical or virtual device that provides connectivity to another device or medium.
interface ID Unique ID for a Networking VIF or vNIC in the form of a UUID.
Internet Control Message Protocol (ICMP) A network protocol used by network devices for control mes- sages. For example, ping uses ICMP to test connectivity.
Internet protocol (IP) Principal communications protocol in the internet protocol suite for relaying datagrams across network boundaries.
Internet Service Provider (ISP) Any business that provides Internet access to individuals or businesses. Internet Small Computer System Interface (iSCSI) Storage protocol that encapsulates SCSI frames for
transport over IP networks. Supported by Compute, Object Storage, and Image service.
IP address Number that is unique to every computer system on the Internet. Two versions of the Internet Protocol (IP) are in use for addresses: IPv4 and IPv6.
IP Address Management (IPAM) The process of automating IP address allocation, deallocation, and man- agement. Currently provided by Compute, melange, and Networking.
 206
Appendix
 ip6tables Tool used to set up, maintain, and inspect the tables of IPv6 packet filter rules in the Linux kernel. In OpenStack Compute, ip6tables is used along with arptables, ebtables, and iptables to create firewalls for both nodes and VMs.
ipset Extension to iptables that allows creation of firewall rules that match entire “sets” of IP addresses si- multaneously. These sets reside in indexed data structures to increase efficiency, particularly on systems with a large quantity of rules.
iptables Used along with arptables and ebtables, iptables create firewalls in Compute. iptables are the tables provided by the Linux kernel firewall (implemented as different Netfilter modules) and the chains and rules it stores. Different kernel modules and programs are currently used for different protocols: iptables applies to IPv4, ip6tables to IPv6, arptables to ARP, and ebtables to Ethernet frames. Requires root privilege to manipulate.
ironic Codename for the Bare Metal service.
iSCSI Qualified Name (IQN) IQN is the format most commonly used for iSCSI names, which uniquely iden- tify nodes in an iSCSI network. All IQNs follow the pattern iqn.yyyy-mm.domain:identifier, where ‘yyyy-mm’ is the year and month in which the domain was registered, ‘domain’ is the reversed domain name of the issuing organization, and ‘identifier’ is an optional string which makes each IQN under the same domain unique. For example, ‘iqn.2015-10.org.openstack.408ae959bce1’.
ISO9660 One of the VM image disk formats supported by Image service.
itsec A default role in the Compute RBAC system that can quarantine an instance in any project.
J
Java A programming language that is used to create systems that involve more than one computer by way of a network.
JavaScript A scripting language that is used to build web pages.
JavaScript Object Notation (JSON) One of the supported response formats in OpenStack.
jumbo frame Feature in modern Ethernet networks that supports frames up to approximately 9000 bytes.
Juno The code name for the tenth release of OpenStack. The design summit took place in Atlanta, Georgia, US and Juno is an unincorporated community in Georgia.
K
Kerberos A network authentication protocol which works on the basis of tickets. Kerberos allows nodes communication over a non-secure network, and allows nodes to prove their identity to one another in a secure manner.
kernel-based VM (KVM) An OpenStack-supported hypervisor. KVM is a full virtualization solution for Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V), ARM, IBM Power, and IBM zSeries. It consists of a loadable kernel module, that provides the core virtualization infrastruc- ture and a processor specific module.
Key Manager service (barbican) The project that produces a secret storage and generation system capable of providing key management for services wishing to enable encryption features.
keystone Codename of the Identity service.
User Guide (Release Version: 15.0.0)
 Appendix
207
 User Guide (Release Version: 15.0.0)
Kickstart A tool to automate system configuration and installation on Red Hat, Fedora, and CentOS-based Linux distributions.
Kilo The code name for the eleventh release of OpenStack. The design summit took place in Paris, France. Due to delays in the name selection, the release was known only as K. Because k is the unit symbol for kilo and the kilogram reference artifact is stored near Paris in the Pavillon de Breteuil in Sèvres, the community chose Kilo as the release name.
L
large object An object within Object Storage that is larger than 5 GB.
Launchpad The collaboration site for OpenStack.
Layer-2 (L2) agent OpenStack Networking agent that provides layer-2 connectivity for virtual networks.
Layer-2 network Term used in the OSI network architecture for the data link layer. The data link layer is responsible for media access control, flow control and detecting and possibly correcting errors that may occur in the physical layer.
Layer-3 (L3) agent OpenStack Networking agent that provides layer-3 (routing) services for virtual networks. Layer-3 network Term used in the OSI network architecture for the network layer. The network layer is
responsible for packet forwarding including routing from one node to another.
Liberty The code name for the twelfth release of OpenStack. The design summit took place in Vancouver, Canada and Liberty is the name of a village in the Canadian province of Saskatchewan.
libvirt Virtualization API library used by OpenStack to interact with many of its supported hypervisors. Lightweight Directory Access Protocol (LDAP) An application protocol for accessing and maintaining dis-
tributed directory information services over an IP network.
Linux Unix-like computer operating system assembled under the model of free and open-source software development and distribution.
Linux bridge Software that enables multiple VMs to share a single physical NIC within Compute.
Linux Bridge neutron plug-in Enables a Linux bridge to understand a Networking port, interface attachment,
and other abstractions.
Linux containers (LXC) An OpenStack-supported hypervisor.
live migration The ability within Compute to move running virtual machine instances from one host to another with only a small service interruption during switchover.
load balancer A load balancer is a logical device that belongs to a cloud account. It is used to distribute workloads between multiple back-end systems or services, based on the criteria defined as part of its configuration.
load balancing The process of spreading client requests between two or more nodes to improve performance and availability.
Load-Balancer-as-a-Service (LBaaS) Enables Networking to distribute incoming requests evenly between designated instances.
Load-balancing service (octavia) The project that aims to provide scalable, on demand, self service access to load-balancer services, in technology-agnostic manner.
 208
Appendix
 Logical Volume Manager (LVM) Provides a method of allocating space on mass-storage devices that is more flexible than conventional partitioning schemes.
M
magnum Code name for the Containers Infrastructure Management service.
management API Alternative term for an admin API.
management network A network segment used for administration, not accessible to the public Internet.
manager Logical groupings of related code, such as the Block Storage volume manager or network manager.
manifest Used to track segments of a large object within Object Storage.
manifest object A special Object Storage object that contains the manifest for a large object.
manila Codename for OpenStack Shared File Systems service.
manila-share Responsible for managing Shared File System Service devices, specifically the back-end de- vices.
maximum transmission unit (MTU) Maximum frame or packet size for a particular network medium. Typ- ically 1500 bytes for Ethernet networks.
mechanism driver A driver for the Modular Layer 2 (ML2) neutron plug-in that provides layer-2 connectivity for virtual instances. A single OpenStack installation can use multiple mechanism drivers.
melange Project name for OpenStack Network Information Service. To be merged with Networking. membership The association between an Image service VM image and a project. Enables images to be shared
with specified projects.
membership list A list of projects that can access a given VM image within Image service. memcached A distributed memory object caching system that is used by Object Storage for caching.
memory overcommit The ability to start new VM instances based on the actual memory usage of a host, as opposed to basing the decision on the amount of RAM each running instance thinks it has available. Also known as RAM overcommit.
message broker The software package used to provide AMQP messaging capabilities within Compute. De- fault package is RabbitMQ.
message bus The main virtual communication line used by all AMQP messages for inter-cloud communica- tions within Compute.
message queue Passes requests from clients to the appropriate workers and returns the output to the client after the job completes.
Message service (zaqar) The project that provides a messaging service that affords a variety of distributed application patterns in an efficient, scalable and highly available manner, and to create and maintain associated Python libraries and documentation.
Meta-Data Server (MDS) Stores CephFS metadata.
Metadata agent OpenStack Networking agent that provides metadata services for instances. migration The process of moving a VM instance from one host to another.
mistral Code name for Workflow service.
User Guide (Release Version: 15.0.0)
 Appendix 209
 User Guide (Release Version: 15.0.0)
Mitaka The code name for the thirteenth release of OpenStack. The design summit took place in Tokyo, Japan. Mitaka is a city in Tokyo.
Modular Layer 2 (ML2) neutron plug-in Can concurrently use multiple layer-2 networking technologies, such as 802.1Q and VXLAN, in Networking.
monasca Codename for OpenStack Monitoring.
Monitor (LBaaS) LBaaS feature that provides availability monitoring using the ping command, TCP, and
HTTP/HTTPS GET.
Monitor (Mon) A Ceph component that communicates with external clients, checks data state and consis- tency, and performs quorum functions.
Monitoring (monasca) The OpenStack service that provides a multi-project, highly scalable, performant, fault-tolerant monitoring-as-a-service solution for metrics, complex event processing and logging. To build an extensible platform for advanced monitoring services that can be used by both operators and projects to gain operational insight and visibility, ensuring availability and stability.
multi-factor authentication Authentication method that uses two or more credentials, such as a password and a private key. Currently not supported in Identity.
multi-host High-availability mode for legacy (nova) networking. Each compute node handles NAT and DHCP and acts as a gateway for all of the VMs on it. A networking failure on one compute node doesn’t affect VMs on other compute nodes.
multinic Facility in Compute that allows each virtual machine instance to have more than one VIF connected to it.
murano Codename for the Application Catalog service. N
Nebula Released as open source by NASA in 2010 and is the basis for Compute.
netadmin One of the default roles in the Compute RBAC system. Enables the user to allocate publicly acces-
sible IP addresses to instances and change firewall rules.
NetApp volume driver Enables Compute to communicate with NetApp storage devices through the NetApp OnCommand Provisioning Manager.
network A virtual network that provides connectivity between entities. For example, a collection of virtual ports that share network connectivity. In Networking terminology, a network is always a layer-2 network.
Network Address Translation (NAT) Process of modifying IP address information while in transit. Sup- ported by Compute and Networking.
network controller A Compute daemon that orchestrates the network configuration of nodes, including IP addresses, VLANs, and bridging. Also manages routing for both public and private networks.
Network File System (NFS) A method for making file systems available over the network. Supported by OpenStack.
network ID Unique ID assigned to each network segment within Networking. Same as network UUID.
network manager The Compute component that manages various network components, such as firewall rules, IP address allocation, and so on.
 210
Appendix
 network namespace Linux kernel feature that provides independent virtual networking instances on a single host with separate routing tables and interfaces. Similar to virtual routing and forwarding (VRF) services on physical network equipment.
network node Any compute node that runs the network worker daemon.
network segment Represents a virtual, isolated OSI layer-2 subnet in Networking.
Network Service Header (NSH) Provides a mechanism for metadata exchange along the instantiated service path.
Network Time Protocol (NTP) Method of keeping a clock for a host or node correct via communication with a trusted, accurate time source.
network UUID Unique ID for a Networking network segment.
network worker The nova-network worker daemon; provides services such as giving an IP address to a
booting nova instance.
Networking API (Neutron API) API used to access OpenStack Networking. Provides an extensible archi- tecture to enable custom plug-in creation.
Networking service (neutron) The OpenStack project which implements services and associated libraries to provide on-demand, scalable, and technology-agnostic network abstraction.
neutron Codename for OpenStack Networking service.
neutron API An alternative name for Networking API.
neutron manager Enables Compute and Networking integration, which enables Networking to perform net- work management for guest VMs.
neutron plug-in Interface within Networking that enables organizations to create custom plug-ins for ad- vanced features, such as QoS, ACLs, or IDS.
Newton The code name for the fourteenth release of OpenStack. The design summit took place in Austin, Texas, US. The release is named after “Newton House” which is located at 1013 E. Ninth St., Austin, TX. which is listed on the National Register of Historic Places.
Nexenta volume driver Provides support for NexentaStor devices in Compute.
NFV Orchestration Service (tacker) OpenStack service that aims to implement Network Function Virtu- alization (NFV) orchestration services and libraries for end-to-end life-cycle management of network services and Virtual Network Functions (VNFs).
Nginx An HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server.
No ACK Disables server-side message acknowledgment in the Compute RabbitMQ. Increases performance
but decreases reliability.
node A VM instance that runs on a host.
non-durable exchange Message exchange that is cleared when the service restarts. Its data is not written to persistent storage.
non-durable queue Message queue that is cleared when the service restarts. Its data is not written to persistent storage.
non-persistent volume Alternative term for an ephemeral volume.
north-south traffic Network traffic between a user or client (north) and a server (south), or traffic into the
cloud (south) and out of the cloud (north). See also east-west traffic.
User Guide (Release Version: 15.0.0)
 Appendix 211
 User Guide (Release Version: 15.0.0)
nova Codename for OpenStack Compute service.
Nova API Alternative term for the Compute API.
nova-network A Compute component that manages IP address allocation, firewalls, and other network-related tasks. This is the legacy networking option and an alternative to Networking.
O
object object
object
object object
object
object
A BLOB of data held by Object Storage; can be in any format.
auditor Opens all objects for an object server and verifies the MD5 hash, size, and metadata for each object.
expiration A configurable option within Object Storage to automatically delete objects after a specified amount of time has passed or a certain date is reached.
hash Unique ID for an Object Storage object.
path hash Used by Object Storage to determine the location of an object in the ring. Maps objects to
partitions.
replicator An Object Storage component that copies an object to remote partitions for fault tolerance. server An Object Storage component that is responsible for managing objects.
Object Storage API API used to access OpenStack Object Storage.
Object Storage Device (OSD) The Ceph storage daemon.
Object Storage service (swift) The OpenStack core project that provides eventually consistent and redundant storage and retrieval of fixed digital content.
object versioning Allows a user to set a flag on an Object Storage container so that all objects within the container are versioned.
Ocata The code name for the fifteenth release of OpenStack. The design summit will take place in Barcelona, Spain. Ocata is a beach north of Barcelona.
Octavia Code name for the Load-balancing service.
Oldie Term for an Object Storage process that runs for a long time. Can indicate a hung process.
Open Cloud Computing Interface (OCCI) A standardized interface for managing compute, data, and net- work resources, currently unsupported in OpenStack.
Open Virtualization Format (OVF) Standard for packaging VM images. Supported in OpenStack.
Open vSwitch Open vSwitch is a production quality, multilayer virtual switch licensed under the open source Apache 2.0 license. It is designed to enable massive network automation through programmatic exten- sion, while still supporting standard management interfaces and protocols (for example NetFlow, sFlow, SPAN, RSPAN, CLI, LACP, 802.1ag).
Open vSwitch (OVS) agent Provides an interface to the underlying Open vSwitch service for the Networking plug-in.
Open vSwitch neutron plug-in Provides support for Open vSwitch in Networking. OpenLDAP An open source LDAP server. Supported by both Compute and Identity.
 212
Appendix
 OpenStack OpenStack is a cloud operating system that controls large pools of compute, storage, and net- working resources throughout a data center, all managed through a dashboard that gives administrators control while empowering their users to provision resources through a web interface. OpenStack is an open source project licensed under the Apache License 2.0.
OpenStack code name Each OpenStack release has a code name. Code names ascend in alphabetical order: Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, Icehouse, Juno, Kilo, Liberty, Mitaka, Newton, Ocata, Pike, Queens, and Rocky. Code names are cities or counties near where the correspond- ing OpenStack design summit took place. An exception, called the Waldon exception, is granted to elements of the state flag that sound especially cool. Code names are chosen by popular vote.
openSUSE A Linux distribution that is compatible with OpenStack.
operator The person responsible for planning and maintaining an OpenStack installation.
optional service An official OpenStack service defined as optional by DefCore Committee. Currently, con- sists of Dashboard (horizon), Telemetry service (Telemetry), Orchestration service (heat), Database ser- vice (trove), Bare Metal service (ironic), and so on.
Orchestration service (heat) The OpenStack service which orchestrates composite cloud applications using a declarative template format through an OpenStack-native REST API.
orphan In the context of Object Storage, this is a process that is not terminated after an upgrade, restart, or reload of the service.
Oslo Codename for the Common Libraries project. P
panko Part of the OpenStack Telemetry service; provides event storage.
parent cell If a requested resource, such as CPU time, disk storage, or memory, is not available in the parent
cell, the request is forwarded to associated child cells.
partition A unit of storage within Object Storage used to store objects. It exists on top of devices and is replicated for fault tolerance.
partition index Contains the locations of all Object Storage partitions within the ring.
partition shift value Used by Object Storage to determine which partition data should reside on.
path MTU discovery (PMTUD) Mechanism in IP networks to detect end-to-end MTU and adjust packet size accordingly.
pause A VM state where no changes occur (no changes in memory, network communications stop, etc); the VM is frozen but not shut down.
PCI passthrough Gives guest VMs exclusive access to a PCI device. Currently supported in OpenStack Havana and later releases.
persistent message A message that is stored both in memory and on disk. The message is not lost after a failure or restart.
persistent volume Changes to these types of disk volumes are saved.
personality file A file used to customize a Compute instance. It can be used to inject SSH keys or a specific
network configuration.
User Guide (Release Version: 15.0.0)
 Appendix 213
 User Guide (Release Version: 15.0.0)
Pike The code name for the sixteenth release of OpenStack. The design summit will take place in Boston, Massachusetts, US. The release is named after the Massachusetts Turnpike, abbreviated commonly as the Mass Pike, which is the easternmost stretch of Interstate 90.
Platform-as-a-Service (PaaS) Provides to the consumer the ability to deploy applications through a program- ming language or tools supported by the cloud platform provider. An example of Platform-as-a-Service is an Eclipse/Java programming platform provided with no downloads required.
plug-in Software component providing the actual implementation for Networking APIs, or for Compute APIs, depending on the context.
policy service Component of Identity that provides a rule-management interface and a rule-based authoriza- tion engine.
policy-based routing (PBR) Provides a mechanism to implement packet forwarding and routing according to the policies defined by the network administrator.
pool A logical set of devices, such as web servers, that you group together to receive and process traffic. The load balancing function chooses which member of the pool handles the new requests or connections received on the VIP address. Each VIP has one pool.
pool member An application that runs on the back-end server in a load-balancing system.
port A virtual network port within Networking; VIFs / vNICs are connected to a port.
port UUID Unique ID for a Networking port.
preseed A tool to automate system configuration and installation on Debian-based Linux distributions. private image An Image service VM image that is only available to specified projects.
private IP address An IP address used for management and administration, not available to the public Inter- net.
private network The Network Controller provides virtual networks to enable compute servers to interact with each other and with the public network. All machines must have a public and private network interface. A private network interface can be a flat or VLAN network interface. A flat network interface is controlled by the flat_interface with flat managers. A VLAN network interface is controlled by the vlan_interface option with VLAN managers.
project Projects represent the base unit of “ownership” in OpenStack, in that all resources in OpenStack should be owned by a specific project. In OpenStack Identity, a project must be owned by a specific domain.
project ID Unique ID assigned to each project by the Identity service.
project VPN Alternative term for a cloudpipe.
promiscuous mode Causes the network interface to pass all traffic it receives to the host rather than passing only the frames addressed to it.
protected property Generally, extra properties on an Image service image to which only cloud administra- tors have access. Limits which user roles can perform CRUD operations on that property. The cloud administrator can configure any image property as protected.
provider An administrator who has access to all hosts and instances.
proxy node A node that provides the Object Storage proxy service.
proxy server Users of Object Storage interact with the service through the proxy server, which in turn looks up the location of the requested data within the ring and returns the results to the user.
public API An API endpoint used for both service-to-service communication and end-user interactions.
214 Appendix

 public image An Image service VM image that is available to all projects.
public IP address An IP address that is accessible to end-users.
public key authentication Authentication method that uses keys rather than passwords.
public network The Network Controller provides virtual networks to enable compute servers to interact with each other and with the public network. All machines must have a public and private network interface. The public network interface is controlled by the public_interface option.
Puppet An operating system configuration-management tool supported by OpenStack. Python Programming language used extensively in OpenStack.
Q
QEMU Copy On Write 2 (QCOW2) One of the VM image disk formats supported by Image service.
Qpid Message queue software supported by OpenStack; an alternative to RabbitMQ.
Quality of Service (QoS) The ability to guarantee certain network or storage requirements to satisfy a Service Level Agreement (SLA) between an application provider and end users. Typically includes performance requirements like networking bandwidth, latency, jitter correction, and reliability as well as storage per- formance in Input/Output Operations Per Second (IOPS), throttling agreements, and performance expec- tations at peak load.
quarantine If Object Storage finds objects, containers, or accounts that are corrupt, they are placed in this state, are not replicated, cannot be read by clients, and a correct copy is re-replicated.
Queens The code name for the seventeenth release of OpenStack. The design summit will take place in Sydney, Australia. The release is named after the Queens Pound river in the South Coast region of New South Wales.
Quick EMUlator (QEMU) QEMU is a generic and open source machine emulator and virtualizer. One of the hypervisors supported by OpenStack, generally used for development purposes.
quota In Compute and Block Storage, the ability to set resource limits on a per-project basis. R
RabbitMQ The default message queue software used by OpenStack.
Rackspace Cloud Files Released as open source by Rackspace in 2010; the basis for Object Storage.
RADOS Block Device (RBD) Ceph component that enables a Linux block device to be striped over multiple distributed data stores.
radvd The router advertisement daemon, used by the Compute VLAN manager and FlatDHCP manager to provide routing services for VM instances.
rally Codename for the Benchmark service.
RAM filter The Compute setting that enables or disables RAM overcommitment.
RAM overcommit The ability to start new VM instances based on the actual memory usage of a host, as opposed to basing the decision on the amount of RAM each running instance thinks it has available. Also known as memory overcommit.
User Guide (Release Version: 15.0.0)
 Appendix 215
 User Guide (Release Version: 15.0.0)
rate limit Configurable option within Object Storage to limit database writes on a per-account and/or per- container basis.
raw One of the VM image disk formats supported by Image service; an unstructured disk image.
rebalance The process of distributing Object Storage partitions across all drives in the ring; used during initial
ring creation and after ring reconfiguration.
reboot Either a soft or hard reboot of a server. With a soft reboot, the operating system is signaled to restart, which enables a graceful shutdown of all processes. A hard reboot is the equivalent of power cycling the server. The virtualization platform should ensure that the reboot action has completed successfully, even in cases in which the underlying domain/VM is paused or halted/stopped.
rebuild Removes all data on the server and replaces it with the specified image. Server ID and IP addresses remain the same.
Recon An Object Storage component that collects meters.
record Belongs to a particular domain and is used to specify information about the domain. There are several types of DNS records. Each record type contains particular information used to describe the purpose of that record. Examples include mail exchange (MX) records, which specify the mail server for a particular domain; and name server (NS) records, which specify the authoritative name servers for a domain.
record ID A number within a database that is incremented each time a change is made. Used by Object Storage when replicating.
Red Hat Enterprise Linux (RHEL) A Linux distribution that is compatible with OpenStack.
reference architecture A recommended architecture for an OpenStack cloud.
region A discrete OpenStack environment with dedicated API endpoints that typically shares only the Identity (keystone) with other regions.
registry Alternative term for the Image service registry.
registry server An Image service that provides VM image metadata information to clients. Reliable, Autonomic Distributed Object Store (RADOS)
A collection of components that provides object storage within Ceph. Similar to OpenStack Object Storage.
Remote Procedure Call (RPC) The method used by the Compute RabbitMQ for intra-service communica- tions.
replica Provides data redundancy and fault tolerance by creating copies of Object Storage objects, accounts, and containers so that they are not lost when the underlying storage fails.
replica count The number of replicas of the data in an Object Storage ring.
replication
replicator
The process of copying data to a separate physical device for fault tolerance and performance. The Object Storage back-end process that creates and manages object replicas.
Unique ID assigned to each request sent to Compute.
an administrator to mount the file systems for an instance to correct the problem.
resize Converts an existing server to a different flavor, which scales the server up or down. The original server is saved to enable rollback if a problem occurs. All resizes must be tested and explicitly confirmed, at which time the original server is removed.
request ID
rescue image A special type of VM image that is booted when an instance is placed into rescue mode. Allows
 216
Appendix
 RESTful A kind of web service API that uses REST, or Representational State Transfer. REST is the style of architecture for hypermedia systems that is used for the World Wide Web.
ring An entity that maps Object Storage data to partitions. A separate ring exists for each service, such as account, object, and container.
ring builder Builds and manages rings within Object Storage, assigns partitions to devices, and pushes the configuration to other storage nodes.
Rocky The code name for the eightteenth release of OpenStack. The design summit will take place in Van- couver, Kanada. The release is named after the Rocky Mountains.
role A personality that a user assumes to perform a specific set of operations. A role includes a set of rights and privileges. A user assuming that role inherits those rights and privileges.
Role Based Access Control (RBAC) Provides a predefined list of actions that the user can perform, such as start or stop VMs, reset passwords, and so on. Supported in both Identity and Compute and can be configured using the dashboard.
role ID Alphanumeric ID assigned to each Identity service role.
Root Cause Analysis (RCA) service (Vitrage) OpenStack project that aims to organize, analyze and visual- ize OpenStack alarms and events, yield insights regarding the root cause of problems and deduce their existence before they are directly detected.
rootwrap A feature of Compute that allows the unprivileged “nova” user to run a specified list of commands as the Linux root user.
round-robin scheduler Type of Compute scheduler that evenly distributes instances among available hosts.
router A physical or virtual network device that passes network traffic between different networks.
routing key The Compute direct exchanges, fanout exchanges, and topic exchanges use this key to determine how to process a message; processing varies depending on exchange type.
RPC driver Modular system that allows the underlying message queue software of Compute to be changed. For example, from RabbitMQ to ZeroMQ or Qpid.
rsync Used by Object Storage to push object replicas.
RXTX cap Absolute limit on the amount of network traffic a Compute VM instance can send and receive. RXTX quota Soft limit on the amount of network traffic a Compute VM instance can send and receive.
S
sahara Codename for the Data Processing service.
SAML assertion Contains information about a user as provided by the identity provider. It is an indication
that a user has been authenticated.
scheduler manager A Compute component that determines where VM instances should start. Uses modular design to support a variety of scheduler types.
scoped token An Identity service API access token that is associated with a specific project.
scrubber Checks for and deletes unused VMs; the component of Image service that implements delayed
delete.
secret key String of text known only by the user; used along with an access key to make requests to the Compute API.
User Guide (Release Version: 15.0.0)
 Appendix 217
 User Guide (Release Version: 15.0.0)
secure boot Process whereby the system firmware validates the authenticity of the code involved in the boot process.
secure shell (SSH) Open source tool used to access remote hosts through an encrypted communications chan- nel, SSH key injection is supported by Compute.
security group A set of network traffic filtering rules that are applied to a Compute instance.
segmented object An Object Storage large object that has been broken up into pieces. The re-assembled
object is called a concatenated object.
self-service For IaaS, ability for a regular (non-privileged) account to manage a virtual infrastructure compo- nent such as networks without involving an administrator.
SELinux Linux kernel security module that provides the mechanism for supporting access control policies.
senlin Code name for the Clustering service.
server Computer that provides explicit services to the client software running on that system, often managing a variety of computer operations. A server is a VM instance in the Compute system. Flavor and image are requisite elements when creating a server.
server image Alternative term for a VM image.
server UUID Unique ID assigned to each guest VM instance.
service An OpenStack service, such as Compute, Object Storage, or Image service. Provides one or more endpoints through which users can access resources and perform operations.
service catalog Alternative term for the Identity service catalog.
Service Function Chain (SFC) For a given service, SFC is the abstracted view of the required service func-
tions and the order in which they are to be applied.
service ID Unique ID assigned to each service that is available in the Identity service catalog.
Service Level Agreement (SLA) Contractual obligations that ensure the availability of a service.
service project Special project that contains all services that are listed in the catalog.
service provider A system that provides services to other system entities. In case of federated identity, Open- Stack Identity is the service provider.
service registration An Identity service feature that enables services, such as Compute, to automatically reg- ister with the catalog.
service token An administrator-defined token used by Compute to communicate securely with the Identity service.
session back end The method of storage used by horizon to track client sessions, such as local memory, cook- ies, a database, or memcached.
session persistence A feature of the load-balancing service. It attempts to force subsequent connections to a service to be redirected to the same node as long as it is online.
session storage A horizon component that stores and tracks client session information. Implemented through the Django sessions framework.
share A remote, mountable file system in the context of the Shared File Systems service. You can mount a share to, and access a share from, several hosts by several users at a time.
 218
Appendix
 share network An entity in the context of the Shared File Systems service that encapsulates interaction with the Networking service. If the driver you selected runs in the mode requiring such kind of interaction, you need to specify the share network to create a share.
Shared File Systems API A Shared File Systems service that provides a stable RESTful API. The service au- thenticates and routes requests throughout the Shared File Systems service. There is python-manilaclient to interact with the API.
Shared File Systems service (manila) The service that provides a set of services for management of shared file systems in a multi-project cloud environment, similar to how OpenStack provides block-based stor- age management through the OpenStack Block Storage service project. With the Shared File Systems service, you can create a remote file system and mount the file system on your instances. You can also read and write data from your instances to and from your file system.
shared IP address An IP address that can be assigned to a VM instance within the shared IP group. Public IP addresses can be shared across multiple servers for use in various high-availability scenarios. When an IP address is shared to another server, the cloud network restrictions are modified to enable each server to listen to and respond on that IP address. You can optionally specify that the target server network configuration be modified. Shared IP addresses can be used with many standard heartbeat facilities, such as keepalive, that monitor for failure and manage IP failover.
shared IP group A collection of servers that can share IPs with other members of the group. Any server in a group can share one or more public IPs with any other server in the group. With the exception of the first server in a shared IP group, servers must be launched into shared IP groups. A server may be a member of only one shared IP group.
shared storage Block storage that is simultaneously accessible by multiple clients, for example, NFS.
Sheepdog Distributed block storage system for QEMU, supported by OpenStack.
Simple Cloud Identity Management (SCIM) Specification for managing identity in the cloud, currently un- supported by OpenStack.
Simple Protocol for Independent Computing Environments (SPICE) SPICE provides remote desktop ac- cess to guest virtual machines. It is an alternative to VNC. SPICE is supported by OpenStack.
Single-root I/O Virtualization (SR-IOV) A specification that, when implemented by a physical PCIe device, enables it to appear as multiple separate PCIe devices. This enables multiple virtualized guests to share direct access to the physical device, offering improved performance over an equivalent virtual device. Currently supported in OpenStack Havana and later releases.
SmokeStack Runs automated tests against the core OpenStack API; written in Rails.
snapshot A point-in-time copy of an OpenStack storage volume or image. Use storage volume snapshots to
back up volumes. Use image snapshots to back up data, or as “gold” images for additional servers.
soft reboot A controlled reboot where a VM instance is properly restarted through operating system com- mands.
Software Development Lifecycle Automation service (solum) OpenStack project that aims to make cloud services easier to consume and integrate with application development process by automating the source- to-image process, and simplifying app-centric deployment.
Software-defined networking (SDN) Provides an approach for network administrators to manage computer network services through abstraction of lower-level functionality.
SolidFire Volume Driver The Block Storage driver for the SolidFire iSCSI storage appliance. solum Code name for the Software Development Lifecycle Automation service.
User Guide (Release Version: 15.0.0)
 Appendix 219
 User Guide (Release Version: 15.0.0)
spread-first scheduler The Compute VM scheduling algorithm that attempts to start a new VM on the host with the least amount of load.
SQLAlchemy An open source SQL toolkit for Python, used in OpenStack.
SQLite A lightweight SQL database, used as the default persistent storage method in many OpenStack ser-
vices.
stack A set of OpenStack resources created and managed by the Orchestration service according to a given template (either an AWS CloudFormation template or a Heat Orchestration Template (HOT)).
StackTach Community project that captures Compute AMQP communications; useful for debugging.
static IP address Alternative term for a fixed IP address.
StaticWeb WSGI middleware component of Object Storage that serves container data as a static web page.
storage back end The method that a service uses for persistent storage, such as iSCSI, NFS, or local disk.
storage manager A XenAPI component that provides a pluggable interface to support a wide variety of per- sistent storage back ends.
storage manager back end A persistent storage method supported by XenAPI, such as iSCSI or NFS. storage node An Object Storage node that provides container services, account services, and object services;
controls the account databases, container databases, and object storage.
storage services Collective name for the Object Storage object services, container services, and account ser- vices.
strategy Specifies the authentication source used by Image service or Identity. In the Database service, it refers to the extensions implemented for a data store.
subdomain A domain within a parent domain. Subdomains cannot be registered. Subdomains enable you to delegate domains. Subdomains can themselves have subdomains, so third-level, fourth-level, fifth-level, and deeper levels of nesting are possible.
subnet Logical subdivision of an IP network.
SUSE Linux Enterprise Server (SLES) A Linux distribution that is compatible with OpenStack.
suspend The VM instance is paused and its state is saved to disk of the host.
swap Disk-based virtual memory used by operating systems to provide more memory than is actually available on the system.
swauth An authentication and authorization service for Object Storage, implemented through WSGI middle- ware; uses Object Storage itself as the persistent backing store.
swift Codename for OpenStack Object Storage service.
swift All in One (SAIO) Creates a full Object Storage development environment within a single VM.
swift middleware
swift proxy server
swift storage node
Collective term for Object Storage components that provide additional functionality. Acts as the gatekeeper to Object Storage and is responsible for authenticating the user. A node that runs Object Storage account, container, and object services.
sync point Point in time since the last container and accounts database sync among nodes within Object Stor-
age.
sysadmin One of the default roles in the Compute RBAC system. Enables a user to add other users to a project, interact with VM images that are associated with the project, and start and stop VM instances.
 220 Appendix
 system usage A Compute component that, along with the notification system, collects meters and usage in- formation. This information can be used for billing.
T
tacker Code name for the NFV Orchestration service
Telemetry service (telemetry) The OpenStack project which collects measurements of the utilization of the physical and virtual resources comprising deployed clouds, persists this data for subsequent retrieval and analysis, and triggers actions when defined criteria are met.
TempAuth An authentication facility within Object Storage that enables Object Storage itself to perform authentication and authorization. Frequently used in testing and development.
Tempest Automated software test suite designed to run against the trunk of the OpenStack core project. TempURL An Object Storage middleware component that enables creation of URLs for temporary object
access.
tenant A group of users; used to isolate access to Compute resources. An alternative term for a project.
Tenant API An API that is accessible to projects.
tenant endpoint An Identity service API endpoint that is associated with one or more projects.
tenant ID An alternative term for project ID.
token An alpha-numeric string of text used to access OpenStack APIs and resources.
token services An Identity service component that manages and validates tokens after a user or project has been authenticated.
tombstone Used to mark Object Storage objects that have been deleted; ensures that the object is not updated on another node after it has been deleted.
topic publisher A process that is created when a RPC call is executed; used to push the message to the topic exchange.
Torpedo Community project used to run automated tests against the OpenStack API.
transaction ID Unique ID assigned to each Object Storage request; used for debugging and tracing.
transient Alternative term for non-durable.
transient exchange Alternative term for a non-durable exchange.
transient message A message that is stored in memory and is lost after the server is restarted.
transient queue Alternative term for a non-durable queue.
TripleO OpenStack-on-OpenStack program. The code name for the OpenStack Deployment program.
trove Codename for OpenStack Database service.
trusted platform module (TPM) Specialized microprocessor for incorporating cryptographic keys into de- vices for authenticating and securing a hardware platform.
U
Ubuntu A Debian-based Linux distribution.
User Guide (Release Version: 15.0.0)
 Appendix
221
 User Guide (Release Version: 15.0.0)
unscoped token Alternative term for an Identity service default token.
updater Collective term for a group of Object Storage components that processes queued and failed updates
for containers and objects.
user In OpenStack Identity, entities represent individual API consumers and are owned by a specific domain. In OpenStack Compute, a user can be associated with roles, projects, or both.
user data A blob of data that the user can specify when they launch an instance. The instance can access this data through the metadata service or config drive. Commonly used to pass a shell script that the instance runs on boot.
User Mode Linux (UML) An OpenStack-supported hypervisor. V
VIF UUID Unique ID assigned to each Networking VIF.
Virtual Central Processing Unit (vCPU) Subdivides physical CPUs. Instances can then use those divisions. Virtual Disk Image (VDI) One of the VM image disk formats supported by Image service.
Virtual Extensible LAN (VXLAN) A network virtualization technology that attempts to reduce the scalabil- ity problems associated with large cloud computing deployments. It uses a VLAN-like encapsulation technique to encapsulate Ethernet frames within UDP packets.
Virtual Hard Disk (VHD) One of the VM image disk formats supported by Image service.
virtual IP address (VIP) An Internet Protocol (IP) address configured on the load balancer for use by clients connecting to a service that is load balanced. Incoming connections are distributed to back-end nodes based on the configuration of the load balancer.
virtual machine (VM) An operating system instance that runs on top of a hypervisor. Multiple VMs can run at the same time on the same physical host.
virtual network An L2 network segment within Networking.
Virtual Network Computing (VNC) Open source GUI and CLI tools used for remote console access to VMs.
Supported by Compute.
Virtual Network InterFace (VIF) An interface that is plugged into a port in a Networking network. Typically a virtual network interface belonging to a VM.
virtual networking A generic term for virtualization of network functions such as switching, routing, load balancing, and security using a combination of VMs and overlays on physical network infrastructure.
virtual port Attachment point where a virtual interface connects to a virtual network.
virtual private network (VPN) Provided by Compute in the form of cloudpipes, specialized instances that
are used to create VPNs on a per-project basis.
virtual server Alternative term for a VM or guest.
virtual switch (vSwitch) Software that runs on a host or node and provides the features and functions of a hardware-based network switch.
virtual VLAN Alternative term for a virtual network. VirtualBox An OpenStack-supported hypervisor. Vitrage Code name for the Root Cause Analysis service.
 222
Appendix
 VLAN manager A Compute component that provides dnsmasq and radvd and sets up forwarding to and from cloudpipe instances.
VLAN network The Network Controller provides virtual networks to enable compute servers to interact with each other and with the public network. All machines must have a public and private network interface. A VLAN network is a private network interface, which is controlled by the vlan_interface option with VLAN managers.
VM disk (VMDK) One of the VM image disk formats supported by Image service.
VM image Alternative term for an image.
VM Remote Control (VMRC) Method to access VM instance consoles using a web browser. Supported by Compute.
VMware API Supports interaction with VMware products in Compute.
VMware NSX Neutron plug-in Provides support for VMware NSX in Neutron.
VNC proxy A Compute component that provides users access to the consoles of their VM instances through VNC or VMRC.
volume Disk-based data storage generally represented as an iSCSI target with a file system that supports extended attributes; can be persistent or ephemeral.
Volume API Alternative name for the Block Storage API.
volume controller A Block Storage component that oversees and coordinates storage volume actions.
volume driver Alternative term for a volume plug-in.
volume ID Unique ID applied to each storage volume under the Block Storage control.
volume manager A Block Storage component that creates, attaches, and detaches persistent storage volumes.
volume node A Block Storage node that runs the cinder-volume daemon.
volume plug-in Provides support for new and specialized types of back-end storage for the Block Storage volume manager.
volume worker A cinder component that interacts with back-end storage to manage the creation and deletion of volumes and the creation of compute volumes, provided by the cinder-volume daemon.
vSphere An OpenStack-supported hypervisor. W
Watcher Code name for the Infrastructure Optimization service.
weight Used by Object Storage devices to determine which storage devices are suitable for the job. Devices
are weighted by size.
weighted cost The sum of each cost used when deciding where to start a new VM instance in Compute.
weighting A Compute process that determines the suitability of the VM instances for a job for a particular host. For example, not enough RAM on the host, too many CPUs on the host, and so on.
worker A daemon that listens to a queue and carries out tasks in response to messages. For example, the cinder-volume worker manages volume creation and deletion on storage arrays.
User Guide (Release Version: 15.0.0)
 Appendix 223
 User Guide (Release Version: 15.0.0)
Workflow service (mistral) The OpenStack service that provides a simple YAML-based language to write workflows (tasks and transition rules) and a service that allows to upload them, modify, run them at scale and in a highly available manner, manage and monitor workflow execution state and state of individual tasks.
X
X.509
X.509 is the most widely used standard for defining digital certificates. It is a data structure that contains the subject (entity) identifiable information such as its name along with its public key. The certificate can contain a few other attributes as well depending upon the version. The most recent and standard version of X.509 is v3.
Xen Xen is a hypervisor using a microkernel design, providing services that allow multiple computer operating systems to execute on the same computer hardware concurrently.
Xen API The Xen administrative API, which is supported by Compute.
Xen Cloud Platform (XCP) An OpenStack-supported hypervisor.
Xen Storage Manager Volume Driver A Block Storage volume plug-in that enables communication with the Xen Storage Manager API.
XenServer An OpenStack-supported hypervisor.
XFS High-performance 64-bit file system created by Silicon Graphics. Excels in parallel I/O operations and
data consistency.
Z
zaqar Codename for the Message service.
ZeroMQ Message queue software supported by OpenStack. An alternative to RabbitMQ. Also spelled 0MQ. Zuul Tool used in OpenStack development to ensure correctly ordered testing of changes in parallel.
 224 Appendix
Symbols applet, 192
6to4, 190
Application Catalog service (murano), 192
Application Programming Interface (API), 192 A application server, 192
absolute limit, 190
access control list (ACL), 190
access key, 190
account, 190
account auditor, 190
account database, 190
account reaper, 190
account server, 190
account service, 190
accounting, 190
Active Directory, 190
active/active configuration, 191
active/passive configuration, 191
address pool, 191
Address Resolution Protocol (ARP), 191
admin API, 191
admin server, 191
administrator, 191
Advanced Message Queuing Protocol (AMQP), 191 Advanced RISC Machine (ARM), 191
alert, 191
allocate, 191
Amazon Kernel Image (AKI), 191
Amazon Machine Image (AMI), 191
Amazon Ramdisk Image (ARI), 191
Anvil, 191
aodh, 191
Apache, 191
Apache License 2.0, 191
Apache Web Server, 191
API endpoint, 191
API extension, 191
API extension plug-in, 191
API key, 191
API server, 191
API token, 192
API version, 192
Application Service Provider (ASP), 192 arptables, 192
associate, 192
Asynchronous JavaScript and XML (AJAX), 192 ATA over Ethernet (AoE), 192
attach, 192
attachment (network), 192
auditing, 192
auditor, 192
Austin, 192
auth node, 192
authentication, 192
authentication token, 192
AuthN, 192
authorization, 192
authorization node, 192
AuthZ, 192
Auto ACK, 192
auto declare, 193
availability zone, 193
AWS CloudFormation template, 193
B
back end, 193
back-end catalog, 193
back-end store, 193
Backup, Restore, and Disaster Recovery service
(freezer), 193 bandwidth, 193
barbican, 193
bare, 193
Bare Metal service (ironic), 193 base image, 193
Bell-LaPadula model, 193 Benchmark service (rally), 193 Bexar, 193
binary, 193
INDEX
 225
 User Guide (Release Version: 15.0.0)
bit, 193
bits per second (BPS), 193
block device, 194
block migration, 194
Block Storage API, 194
Block Storage service (cinder), 194
BMC (Baseboard Management Controller), 194 bootable disk image, 194
Bootstrap Protocol (BOOTP), 194
Border Gateway Protocol (BGP), 194
browser, 194
builder file, 194
bursting, 194
button class, 194
byte, 194
cloud-init, 196
cloudadmin, 196 Cloudbase-Init, 196
cloudpipe, 196
cloudpipe image, 196 Clustering service (senlin), 196 command filter, 196
Common Internet File System (CIFS), 196 Common Libraries (oslo), 196
community project, 196
compression, 196
Compute API (Nova API), 196 compute controller, 196 compute host, 196
compute node, 196
Compute service (nova), 197 compute worker, 197 concatenated object, 197 conductor, 197
congress, 197
consistency window, 197
console log, 197
container, 197
container auditor, 197
container database, 197
container format, 197
Container Infrastructure Management service (mag-
num), 197 container server, 197
container service, 197
content delivery network (CDN), 197 controller node, 197
core API, 197
core service, 197
cost, 197
credentials, 197
CRL, 197
Cross-Origin Resource Sharing (CORS), 198 Crowbar, 198
current workload, 198
customer, 198
customization module, 198
D
daemon, 198
Dashboard (horizon), 198
data encryption, 198
Data loss prevention (DLP) software, 198 Data Processing service (sahara), 198 data store, 198
database ID, 198
C
cache pruner, 194 Cactus, 194
CALL, 194 capability, 194 capacity cache, 194 capacity updater, 195 CAST, 195
catalog, 195
catalog service, 195 ceilometer, 195
cell, 195
cell forwarding, 195
cell manager, 195
CentOS, 195
Ceph, 195
CephFS, 195
certificate authority (CA), 195
Challenge-Handshake (CHAP), 195
chance scheduler, 195 changes since, 195 Chef, 195
child cell, 195
Authentication
Protocol
cinder, 195
CirrOS, 195
Cisco neutron plug-in, 195
cloud architect, 195
Cloud Auditing Data Federation (CADF), 195
cloud computing, 196
cloud controller, 196
cloud controller node, 196
Cloud Data Management Interface (CDMI), 196 Cloud Infrastructure Management Interface (CIMI),
196
 226
Index
 database replicator, 198
Database service (trove), 198
deallocate, 198
Debian, 198
deduplication, 198
default panel, 198
default project, 198
default token, 198
delayed delete, 199
delivery mode, 199
denial of service (DoS), 199
deprecated auth, 199
designate, 199
Desktop-as-a-Service, 199
developer, 199
device ID, 199
device weight, 199
DevStack, 199
DHCP agent, 199
Diablo, 199
direct consumer, 199
direct exchange, 199
direct publisher, 199
disassociate, 199
Discretionary Access Control (DAC), 199
disk encryption, 199
disk format, 199
dispersion, 199
distributed virtual router (DVR), 199
Django, 200
DNS record, 200
DNS service (designate), 200
dnsmasq, 200
domain, 200
Domain Name System (DNS), 200
download, 200
durable exchange, 200
durable queue, 200
Dynamic Host Configuration Protocol (DHCP), 200 Dynamic HyperText Markup Language (DHTML),
200
E
east-west traffic, 200 EBS boot volume, 200 ebtables, 200
EC2, 200
EC2 access key, 200
EC2 API, 200
EC2 Compatibility API, 200 EC2 secret key, 201
Elastic Block Storage (EBS), 201 encapsulation, 201
encryption, 201
endpoint, 201
endpoint registry, 201 endpoint template, 201 entity, 201
ephemeral image, 201 ephemeral volume, 201 Essex, 201
ESXi, 201
ETag, 201
euca2ools, 201
Eucalyptus Kernel Image (EKI), 201 Eucalyptus Machine Image (EMI), 201 Eucalyptus Ramdisk Image (ERI), 201 evacuate, 201
exchange, 201
exchange type, 201
exclusive queue, 201
extended attributes (xattr), 201 extension, 201
external network, 201
extra specs, 202
F
FakeLDAP, 202
fan-out exchange, 202
federated identity, 202
Fedora, 202
Fibre Channel, 202
Fibre Channel over Ethernet (FCoE), 202 fill-first scheduler, 202
filter, 202
firewall, 202
FireWall-as-a-Service (FWaaS), 202 fixed IP address, 202
Flat Manager, 202
flat mode injection, 202
flat network, 202
FlatDHCP Manager, 202
flavor, 202
flavor ID, 202
floating IP address, 202
Folsom, 202
FormPost, 203
freezer, 203
front end, 203
G
gateway, 203
generic receive offload (GRO), 203
User Guide (Release Version: 15.0.0)
 Index
227
 User Guide (Release Version: 15.0.0)
generic routing encapsulation (GRE), 203 glance, 203
glance API server, 203
glance registry, 203
global endpoint template, 203 GlusterFS, 203
gnocchi, 203
golden image, 203
Governance service (congress), 203 Graphic Interchange Format (GIF), 203 Graphics Processing Unit (GPU), 203 Green Threads, 203
Grizzly, 203 Group, 203 guest OS, 203
H
Hadoop, 203
Hadoop Distributed File System (HDFS), 203 handover, 204
HAProxy, 204
hard reboot, 204
Havana, 204
health monitor, 204
heat, 204
Heat Orchestration Template (HOT), 204
high availability (HA), 204
horizon, 204
horizon plug-in, 204
host, 204
host aggregate, 204
Host Bus Adapter (HBA), 204
hybrid cloud, 204
Hyper-V, 204
hyperlink, 204
Hypertext Transfer Protocol (HTTP), 204 Hypertext Transfer Protocol Secure (HTTPS), 204 hypervisor, 204
hypervisor pool, 204
image ID, 205
image membership, 205
image owner, 205
image registry, 205
Image service (glance), 205
image status, 205
image store, 205
image UUID, 205
incubated project, 205
Infrastructure Optimization service (watcher), 206 Infrastructure-as-a-Service (IaaS), 206
ingress filtering, 206
INI format, 206
injection, 206
Input/Output Operations Per Second (IOPS), 206 instance, 206
instance ID, 206
instance state, 206
instance tunnels network, 206
instance type, 206
instance type ID, 206
instance UUID, 206
Intelligent Platform Management Interface (IPMI),
206 interface, 206
interface ID, 206
Internet Control Message Protocol (ICMP), 206 Internet protocol (IP), 206
Internet Service Provider (ISP), 206
Internet Small Computer System Interface (iSCSI),
206 IP address, 206
IP Address Management (IPAM), 206 ip6tables, 207
ipset, 207
iptables, 207
ironic, 207
iSCSI Qualified Name (IQN), 207 ISO9660, 207
itsec, 207
J
Java, 207
JavaScript, 207
JavaScript Object Notation (JSON), 207 jumbo frame, 207
Juno, 207
K
Kerberos, 207
kernel-based VM (KVM), 207
Key Manager service (barbican), 207
I
Icehouse, 205
ID number, 205
Identity API, 205
Identity back end, 205
identity provider, 205
Identity service (keystone), 205 Identity service API, 205
IETF, 205
image, 205
Image API, 205
image cache, 205
 228
Index
 keystone, 207 Kickstart, 208 Kilo, 208
L
large object, 208 Launchpad, 208 Layer-2 (L2) agent, 208 Layer-2 network, 208 Layer-3 (L3) agent, 208 Layer-3 network, 208 Liberty, 208
libvirt, 208
Lightweight Directory Access Protocol (LDAP), 208 Linux, 208
Linux bridge, 208
Linux Bridge neutron plug-in, 208
Linux containers (LXC), 208
live migration, 208
load balancer, 208
load balancing, 208
Load-Balancer-as-a-Service (LBaaS), 208 Load-balancing service (octavia), 208
Logical Volume Manager (LVM), 209
M
magnum, 209
management API, 209
management network, 209
manager, 209
manifest, 209
manifest object, 209
manila, 209
manila-share, 209
maximum transmission unit (MTU), 209 mechanism driver, 209
melange, 209
membership, 209
membership list, 209
memcached, 209
memory overcommit, 209
message broker, 209
message bus, 209
message queue, 209
Message service (zaqar), 209
Meta-Data Server (MDS), 209
Metadata agent, 209
migration, 209
mistral, 209
Mitaka, 210
Modular Layer 2 (ML2) neutron plug-in, 210 monasca, 210
Monitor (LBaaS), 210
Monitor (Mon), 210 Monitoring (monasca), 210 multi-factor authentication, 210 multi-host, 210
multinic, 210 murano, 210
N
Nebula, 210
netadmin, 210
NetApp volume driver, 210
network, 210
Network Address Translation (NAT), 210 network controller, 210
Network File System (NFS), 210 network ID, 210
network manager, 210
network namespace, 211
network node, 211
network segment, 211
Network Service Header (NSH), 211 Network Time Protocol (NTP), 211 network UUID, 211
network worker, 211
Networking API (Neutron API), 211 Networking service (neutron), 211 neutron, 211
neutron API, 211
neutron manager, 211
neutron plug-in, 211
Newton, 211
Nexenta volume driver, 211
NFV Orchestration Service (tacker), 211 Nginx, 211
No ACK, 211
node, 211
non-durable exchange, 211
non-durable queue, 211
non-persistent volume, 211
north-south traffic, 211
nova, 212
Nova API, 212
nova-network, 212
O
object, 212
object auditor, 212 object expiration, 212 object hash, 212 object path hash, 212 object replicator, 212
User Guide (Release Version: 15.0.0)
 Index
229
 User Guide (Release Version: 15.0.0)
object server, 212
Object Storage API, 212
Object Storage Device (OSD), 212
Object Storage service (swift), 212
object versioning, 212
Ocata, 212
Octavia, 212
Oldie, 212
Open Cloud Computing Interface (OCCI), 212 Open Virtualization Format (OVF), 212
Open vSwitch, 212
Open vSwitch (OVS) agent, 212
Open vSwitch neutron plug-in, 212 OpenLDAP, 212
OpenStack, 213
OpenStack code name, 213
openSUSE, 213
operator, 213
optional service, 213
Orchestration service (heat), 213
orphan, 213
Oslo, 213
P
panko, 213
parent cell, 213
partition, 213
partition index, 213
partition shift value, 213
path MTU discovery (PMTUD), 213 pause, 213
PCI passthrough, 213
persistent message, 213
persistent volume, 213
personality file, 213
Pike, 214
Platform-as-a-Service (PaaS), 214 plug-in, 214
policy service, 214
policy-based routing (PBR), 214 pool, 214
pool member, 214
port, 214
port UUID, 214
preseed, 214
private image, 214
private IP address, 214
private network, 214
project, 214
project ID, 214
project VPN, 214
promiscuous mode, 214 protected property, 214 provider, 214
proxy node, 214
proxy server, 214
public API, 214
public image, 215
public IP address, 215
public key authentication, 215 public network, 215
Puppet, 215 Python, 215
Q
QEMU Copy On Write 2 (QCOW2), 215 Qpid, 215
Quality of Service (QoS), 215 quarantine, 215
Queens, 215
Quick EMUlator (QEMU), 215 quota, 215
R
RabbitMQ, 215
Rackspace Cloud Files, 215
RADOS Block Device (RBD), 215
radvd, 215
rally, 215
RAM filter, 215
RAM overcommit, 215
rate limit, 216
raw, 216
rebalance, 216
reboot, 216
rebuild, 216
Recon, 216
record, 216
record ID, 216
Red Hat Enterprise Linux (RHEL), 216
reference architecture, 216
region, 216
registry, 216
registry server, 216
Reliable, Autonomic Distributed Object Store, 216 Remote Procedure Call (RPC), 216
replica, 216
replica count, 216
replication, 216
replicator, 216
request ID, 216
rescue image, 216
resize, 216
 230
Index
 RESTful, 217 ring, 217
ring builder, 217 Rocky, 217
role, 217
Role Based Access Control (RBAC), 217
role ID, 217
Root Cause Analysis (RCA) service (Vitrage), 217 rootwrap, 217
round-robin scheduler, 217
router, 217
routing key, 217
RPC driver, 217
rsync, 217
RXTX cap, 217
RXTX quota, 217
S
sahara, 217
SAML assertion, 217
scheduler manager, 217
scoped token, 217
scrubber, 217
secret key, 217
secure boot, 218
secure shell (SSH), 218
security group, 218
segmented object, 218
self-service, 218
SELinux, 218
senlin, 218
server, 218
server image, 218
server UUID, 218
service, 218
service catalog, 218
Service Function Chain (SFC), 218 service ID, 218
Service Level Agreement (SLA), 218 service project, 218
service provider, 218
service registration, 218
service token, 218
session back end, 218
session persistence, 218
session storage, 218
share, 218
share network, 219
Shared File Systems API, 219
Shared File Systems service (manila), 219 shared IP address, 219
shared IP group, 219
shared storage, 219
Sheepdog, 219
Simple Cloud Identity Management (SCIM), 219 Simple Protocol for Independent Computing Environ-
ments (SPICE), 219
Single-root I/O Virtualization (SR-IOV), 219 SmokeStack, 219
snapshot, 219
soft reboot, 219
Software Development Lifecycle Automation service
(solum), 219
Software-defined networking (SDN), 219 SolidFire Volume Driver, 219
solum, 219
spread-first scheduler, 220
SQLAlchemy, 220
SQLite, 220
stack, 220
StackTach, 220
static IP address, 220
StaticWeb, 220
storage back end, 220
storage manager, 220
storage manager back end, 220
storage node, 220
storage services, 220
strategy, 220
subdomain, 220
subnet, 220
SUSE Linux Enterprise Server (SLES), 220 suspend, 220
swap, 220
swauth, 220
swift, 220
swift All in One (SAIO), 220
swift middleware, 220
swift proxy server, 220
swift storage node, 220
sync point, 220
sysadmin, 220
system usage, 221
T
tacker, 221
Telemetry service (telemetry), 221 TempAuth, 221
Tempest, 221
TempURL, 221
tenant, 221
Tenant API, 221
User Guide (Release Version: 15.0.0)
 Index
231
 User Guide (Release Version: 15.0.0)
tenant endpoint, 221 tenant ID, 221
token, 221
token services, 221 tombstone, 221
topic publisher, 221 Torpedo, 221 transaction ID, 221 transient, 221
transient exchange, 221 transient message, 221 transient queue, 221 TripleO, 221
trove, 221
trusted platform module (TPM), 221
U
Ubuntu, 221
unscoped token, 222
updater, 222
user, 222
user data, 222
User Mode Linux (UML), 222
V
VIF UUID, 222
Virtual Central Processing Unit (vCPU), 222 Virtual Disk Image (VDI), 222
Virtual Extensible LAN (VXLAN), 222 Virtual Hard Disk (VHD), 222
virtual IP address (VIP), 222
virtual machine (VM), 222
virtual network, 222
Virtual Network Computing (VNC), 222 Virtual Network InterFace (VIF), 222
virtual networking, 222
virtual port, 222
virtual private network (VPN), 222
virtual server, 222
virtual switch (vSwitch), 222
virtual VLAN, 222
VirtualBox, 222
Vitrage, 222
VLAN manager, 223
VLAN network, 223
VM disk (VMDK), 223
VM image, 223
VM Remote Control (VMRC), 223 VMware API, 223
VMware NSX Neutron plug-in, 223
VNC proxy, 223
volume, 223
Volume API, 223 volume controller, 223 volume driver, 223 volume ID, 223 volume manager, 223 volume node, 223 volume plug-in, 223 volume worker, 223 vSphere, 223
W
Watcher, 223
weight, 223
weighted cost, 223
weighting, 223
worker, 223
Workflow service (mistral), 224
X
X.509, 224
Xen, 224
Xen API, 224
Xen Cloud Platform (XCP), 224
Xen Storage Manager Volume Driver, 224 XenServer, 224
XFS, 224 Z
zaqar, 224 ZeroMQ, 224 Zuul, 224
 232
Index
High Availability Guide
Release Version: 15.0.0
OpenStack contributors
May 12, 2017
CONTENTS
Abstract 1
Contents 2
Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Introduction to OpenStack high availability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Configuring the basic environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
Configuring the shared services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
Configuring the controller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Configuring the networking services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
Configuring storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
Configuring the compute node . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Index 86
i
ABSTRACT
This guide describes how to install and configure OpenStack for high availability. It supplements the Installation
Tutorials and Guides and assumes that you are familiar with the material in those guides.
This guide documents OpenStack Ocata, Newton, and Mitaka releases.
Warning: This guide is a work-in-progress and changing rapidly while we continue to test and enhance
the guidance. There are open TODO items throughout and available on the OpenStack manuals bug list.
Please help where you are able.
1
CONTENTS
Conventions
The OpenStack documentation uses several typesetting conventions.
Notices
Notices take these forms:
Note: A comment with additional information that explains a part of the text.
Important: Something you must be aware of before proceeding.
Tip: An extra but helpful piece of practical advice.
Caution: Helpful information that prevents the user from making mistakes.
Warning: Critical information about the risk of data loss or security issues.
Command prompts
$ command
Any user, including the root user, can run commands that are prefixed with the $ prompt.
# command
The root user must run commands that are prefixed with the # prompt. You can also prefix these commands
with the sudo command, if available, to run them.
Introduction to OpenStack high availability
High availability systems seek to minimize the following issues:
1. System downtime: Occurs when a user-facing service is unavailable beyond a specified maximum
amount of time.
2. Data loss: Accidental deletion or destruction of data.
2
High Availability Guide (Release Version: 15.0.0)
Most high availability systems guarantee protection against system downtime and data loss only in the event
of a single failure. However, they are also expected to protect against cascading failures, where a single failure
deteriorates into a series of consequential failures. Many service providers guarantee a Service Level Agreement
(SLA) including uptime percentage of computing service, which is calculated based on the available time and
system downtime excluding planned outage time.
Redundancy and failover
High availability is implemented with redundant hardware running redundant instances of each service. If one
piece of hardware running one instance of a service fails, the system can then failover to use another instance
of a service that is running on hardware that did not fail.
A crucial aspect of high availability is the elimination of single points of failure (SPOFs). A SPOF is an
individual piece of equipment or software that causes system downtime or data loss if it fails. In order to
eliminate SPOFs, check that mechanisms exist for redundancy of:
• Network components, such as switches and routers
• Applications and automatic service migration
• Storage components
• Facility services such as power, air conditioning, and fire protection
In the event that a component fails and a back-up system must take on its load, most high availability systems
will replace the failed component as quickly as possible to maintain necessary redundancy. This way time spent
in a degraded protection state is minimized.
Most high availability systems fail in the event of multiple independent (non-consequential) failures. In this
case, most implementations favor protecting data over maintaining availability.
High availability systems typically achieve an uptime percentage of 99.99% or more, which roughly equates to
less than an hour of cumulative downtime per year. In order to achieve this, high availability systems should
keep recovery times after a failure to about one to two minutes, sometimes significantly less.
OpenStack currently meets such availability requirements for its own infrastructure services, meaning that an
uptime of 99.99% is feasible for the OpenStack infrastructure proper. However, OpenStack does not guarantee
99.99% availability for individual guest instances.
This document discusses some common methods of implementing highly available systems, with an emphasis
on the core OpenStack services and other open source services that are closely aligned with OpenStack.
You will need to address high availability concerns for any applications software that you run on your OpenStack
environment. The important thing is to make sure that your services are redundant and available. How you
achieve that is up to you.
Stateless versus stateful services
The following are the definitions of stateless and stateful services:
Stateless service A service that provides a response after your request and then requires no further attention. To
make a stateless service highly available, you need to provide redundant instances and load balance them.
OpenStack services that are stateless include nova-api, nova-conductor, glance-api, keystoneapi,
neutron-api, and nova-scheduler.
Introduction to OpenStack high availability 3
High Availability Guide (Release Version: 15.0.0)
Stateful service A service where subsequent requests to the service depend on the results of the first request.
Stateful services are more difficult to manage because a single action typically involves more than one
request. Providing additional instances and load balancing does not solve the problem. For example,
if the horizon user interface reset itself every time you went to a new page, it would not be very useful.
OpenStack services that are stateful include the OpenStack database and message queue. Making
stateful services highly available can depend on whether you choose an active/passive or active/active
configuration.
Active/passive versus active/active
Stateful services can be configured as active/passive or active/active, which are defined as follows:
active/passive configuration Maintains a redundant instance that can be brought online when the active service
fails. For example, OpenStack writes to the main database while maintaining a disaster recovery database
that can be brought online if the main database fails.
A typical active/passive installation for a stateful service maintains a replacement resource that can be
brought online when required. Requests are handled using a virtual IP address (VIP) that facilitates returning
to service with minimal reconfiguration. A separate application (such as Pacemaker or Corosync)
monitors these services, bringing the backup online as necessary.
active/active configuration Each service also has a backup but manages both the main and redundant systems
concurrently. This way, if there is a failure, the user is unlikely to notice. The backup system is already
online and takes on increased load while the main system is fixed and brought back online.
Typically, an active/active installation for a stateless service maintains a redundant instance, and requests
are load balanced using a virtual IP address and a load balancer such as HAProxy.
A typical active/active installation for a stateful service includes redundant services, with all instances
having an identical state. In other words, updates to one instance of a database update all other instances.
This way a request to one instance is the same as a request to any other. A load balancer manages the
traffic to these systems, ensuring that operational systems always handle the request.
Clusters and quorums
The quorum specifies the minimal number of nodes that must be functional in a cluster of redundant nodes in
order for the cluster to remain functional. When one node fails and failover transfers control to other nodes,
the system must ensure that data and processes remain sane. To determine this, the contents of the remaining
nodes are compared and, if there are discrepancies, a majority rules algorithm is implemented.
For this reason, each cluster in a high availability environment should have an odd number of nodes and the
quorum is defined as more than a half of the nodes. If multiple nodes fail so that the cluster size falls below the
quorum value, the cluster itself fails.
For example, in a seven-node cluster, the quorum should be set to floor(7/2) + 1 == 4. If quorum is
four and four nodes fail simultaneously, the cluster itself would fail, whereas it would continue to function, if
no more than three nodes fail. If split to partitions of three and four nodes respectively, the quorum of four
nodes would continue to operate the majority partition and stop or fence the minority one (depending on the
no-quorum-policy cluster configuration).
And the quorum could also have been set to three, just as a configuration example.
4 Introduction to OpenStack high availability
High Availability Guide (Release Version: 15.0.0)
Note: We do not recommend setting the quorum to a value less than floor(n/2) + 1 as it would likely cause
a split-brain in a face of network partitions.
When four nodes fail simultaneously, the cluster would continue to function as well. But if split to partitions
of three and four nodes respectively, the quorum of three would have made both sides to attempt to fence the
other and host resources. Without fencing enabled, it would go straight to running two copies of each resource.
This is why setting the quorum to a value less than floor(n/2) + 1 is dangerous. However it may be required
for some specific cases, such as a temporary measure at a point it is known with 100% certainty that the other
nodes are down.
When configuring an OpenStack environment for study or demonstration purposes, it is possible to turn off the
quorum checking. Production systems should always run with quorum enabled.
Single-controller high availability mode
OpenStack supports a single-controller high availability mode that is managed by the services that manage
highly available environments but is not actually highly available because no redundant controllers are configured
to use for failover. This environment can be used for study and demonstration but is not appropriate for a
production environment.
It is possible to add controllers to such an environment to convert it into a truly highly available environment.
High availability is not for every user. It presents some challenges. High availability may be too complex for
databases or systems with large amounts of data. Replication can slow large systems down. Different setups
have different prerequisites. Read the guidelines for each setup.
Important: High availability is turned off as the default in OpenStack setups.
Configuring the basic environment
This chapter describes the basic environment for high availability, such as hardware, operating system, common
services.
Hardware considerations for high availability
When you use high availability, consider the hardware requirements needed for your application.
Hardware setup
The following are the standard hardware requirements:
• Provider networks: See the Overview -> Networking Option 1: Provider networks section of the Install
Tutorials and Guides depending on your distribution.
• Self-service networks: See the Overview -> Networking Option 2: Self-service networks section of the
Install Tutorials and Guides depending on your distribution.
Configuring the basic environment 5
High Availability Guide (Release Version: 15.0.0)
OpenStack does not require a significant amount of resources and the following minimum requirements should
support a proof-of-concept high availability environment with core services and several instances:
Node type Processor Cores Memory Storage NIC
controller node 4 12 GB 120 GB 2
compute node 8+ 12+ GB 120+ GB 2
We recommended that the maximum latency between any two controller nodes is 2 milliseconds. Although the
cluster software can be tuned to operate at higher latencies, some vendors insist on this value before agreeing
to support the installation.
You can use the ping command to find the latency between two servers.
Virtualized hardware
For demonstrations and studying, you can set up a test environment on virtual machines (VMs). This has the
following benefits:
• One physical server can support multiple nodes, each of which supports almost any number of network
interfaces.
• You can take periodic snap shots throughout the installation process and roll back to a working configuration
in the event of a problem.
However, running an OpenStack environment on VMs degrades the performance of your instances, particularly
if your hypervisor or processor lacks support for hardware acceleration of nested VMs.
Note: When installing highly available OpenStack on VMs, be sure that your hypervisor permits promiscuous
mode and disables MAC address filtering on the external network.
Installing the operating system
The first step in setting up your highly available OpenStack cluster is to install the operating system on each
node. Follow the instructions in the Environment section of the Installation Tutorials and Guides depending on
your distribution.
The OpenStack Installation Tutorials and Guides also include a list of the services that use passwords with
important notes about using them.
Note: Before following this guide to configure the highly available OpenStack cluster, ensure the IP 10.0.0.
11 and hostname controller are not in use.
This guide uses the following example IP addresses:
# controller
10.0.0.11 controller # virtual IP
10.0.0.12 controller1
10.0.0.13 controller2
10.0.0.14 controller3
6 Configuring the basic environment
High Availability Guide (Release Version: 15.0.0)
Configure NTP
You must configure NTP to properly synchronize services among nodes. We recommend that you configure the
controller node to reference more accurate (lower stratum) servers and other nodes to reference the controller
node. For more information, see the Installation Tutorials and Guides.
Installing Memcached
Most OpenStack services can use Memcached to store ephemeral data such as tokens. Although Memcached
does not support typical forms of redundancy such as clustering, OpenStack services can use almost any number
of instances by configuring multiple hostnames or IP addresses.
The Memcached client implements hashing to balance objects among the instances. Failure of an instance only
impacts a percentage of the objects, and the client automatically removes it from the list of instances.
To install and configure Memcached, read the official documentation.
Memory caching is managed by oslo.cache. This ensures consistency across all projects when using multiple
Memcached servers. The following is an example configuration with three hosts:
Memcached_servers = controller1:11211,controller2:11211,controller3:11211
By default, controller1 handles the caching service. If the host goes down, controller2 or controller3
will complete the service.
For more information about Memcached installation, see the Environment -> Memcached section in the Installation
Tutorials and Guides depending on your distribution.
Configuring the shared services
This chapter describes the shared services for high availability, such as database, messaging service.
Database (Galera Cluster) for high availability
Configuration
Before you launch Galera Cluster, you need to configure the server and the database to operate as part of the
cluster.
Configuring the server
Certain services running on the underlying operating system of your OpenStack database may block Galera
Cluster from normal operation or prevent mysqld from achieving network connectivity with the cluster.
Firewall
Galera Cluster requires that you open the following ports to network traffic:
• On 3306, Galera Cluster uses TCP for database client connections and State Snapshot Transfers methods
that require the client, (that is, mysqldump).
Configuring the shared services 7
High Availability Guide (Release Version: 15.0.0)
• On 4567, Galera Cluster uses TCP for replication traffic. Multicast replication uses both TCP and UDP
on this port.
• On 4568, Galera Cluster uses TCP for Incremental State Transfers.
• On 4444, Galera Cluster uses TCP for all other State Snapshot Transfer methods.
See also:
For more information on firewalls, see firewalls and default ports in OpenStack Administrator Guide.
This can be achieved using the iptables command:
# iptables --append INPUT --in-interface eth0 \
--protocol tcp --match tcp --dport ${PORT} \
--source ${NODE-IP-ADDRESS} --jump ACCEPT
Make sure to save the changes once you are done. This will vary depending on your distribution:
• For Ubuntu
• For Fedora
Alternatively, make modifications using the firewall-cmd utility for FirewallD that is available on many
Linux distributions:
# firewall-cmd --add-service=mysql --permanent
# firewall-cmd --add-port=3306/tcp --permanent
SELinux
Security-Enhanced Linux is a kernel module for improving security on Linux operating systems. It is commonly
enabled and configured by default on Red Hat-based distributions. In the context of Galera Cluster, systems
with SELinux may block the database service, keep it from starting, or prevent it from establishing network
connections with the cluster.
To configure SELinux to permit Galera Cluster to operate, you may need to use the semanage utility to open
the ports it uses. For example:
# semanage port -a -t mysqld_port_t -p tcp 3306
Older versions of some distributions, which do not have an up-to-date policy for securing Galera, may also
require SELinux to be more relaxed about database access and actions:
# semanage permissive -a mysqld_t
Note: Bear in mind, leaving SELinux in permissive mode is not a good security practice. Over the longer
term, you need to develop a security policy for Galera Cluster and then switch SELinux back into enforcing
mode.
For more information on configuring SELinux to work with Galera Cluster, see the SELinux Documentation
8 Configuring the shared services
High Availability Guide (Release Version: 15.0.0)
AppArmor
Application Armor is a kernel module for improving security on Linux operating systems. It is developed by
Canonical and commonly used on Ubuntu-based distributions. In the context of Galera Cluster, systems with
AppArmor may block the database service from operating normally.
To configure AppArmor to work with Galera Cluster, complete the following steps on each cluster node:
1. Create a symbolic link for the database server in the disable directory:
# ln -s /etc/apparmor.d/usr /etc/apparmor.d/disable/.sbin.mysqld
2. Restart AppArmor. For servers that use init, run the following command:
# service apparmor restart
For servers that use systemd, run the following command:
# systemctl restart apparmor
AppArmor now permits Galera Cluster to operate.
Database configuration
MySQL databases, including MariaDB and Percona XtraDB, manage their configurations using a my.cnf file,
which is typically located in the /etc directory. Configuration options available in these databases are also
available in Galera Cluster, with some restrictions and several additions.
[mysqld]
datadir=/var/lib/mysql
socket=/var/lib/mysql/mysql.sock
user=mysql
binlog_format=ROW
bind-address=10.0.0.12
# InnoDB Configuration
default_storage_engine=innodb
innodb_autoinc_lock_mode=2
innodb_flush_log_at_trx_commit=0
innodb_buffer_pool_size=122M
# Galera Cluster Configuration
wsrep_provider=/usr/lib/libgalera_smm.so
wsrep_provider_options="pc.recovery=TRUE;gcache.size=300M"
wsrep_cluster_name="my_example_cluster"
wsrep_cluster_address="gcomm://GALERA1-IP,GALERA2-IP,GALERA3-IP"
wsrep_sst_method=rsync
Configuring mysqld
While all of the configuration parameters available to the standard MySQL, MariaDB, or Percona XtraDB
database servers are available in Galera Cluster, there are some that you must define an outset to avoid conflict
or unexpected behavior.
Configuring the shared services 9
High Availability Guide (Release Version: 15.0.0)
• Ensure that the database server is not bound only to the localhost: 127.0.0.1. Also, do not bind it to
0.0.0.0. Binding to the localhost or 0.0.0.0 makes mySQL bind to all IP addresses on the machine,
including the virtual IP address causing HAProxy not to start. Instead, bind to the management IP address
of the controller node to enable access by other nodes through the management network:
bind-address=10.0.0.12
• Ensure that the binary log format is set to use row-level replication, as opposed to statement-level replication:
binlog_format=ROW
Configuring InnoDB
Galera Cluster does not support non-transactional storage engines and requires that you use InnoDB by default.
There are some additional parameters that you must define to avoid conflicts.
• Ensure that the default storage engine is set to InnoDB:
default_storage_engine=InnoDB
• Ensure that the InnoDB locking mode for generating auto-increment values is set to 2, which is the
interleaved locking mode:
innodb_autoinc_lock_mode=2
Do not change this value. Other modes may cause INSERT statements on tables with auto-increment
columns to fail as well as unresolved deadlocks that leave the system unresponsive.
• Ensure that the InnoDB log buffer is written to file once per second, rather than on each commit, to
improve performance:
innodb_flush_log_at_trx_commit=0
Setting this parameter to 1 or 2 can improve performance, but it introduces certain dangers. Operating
system failures can erase the last second of transactions. While you can recover this data from another
node, if the cluster goes down at the same time (in the event of a data center power outage), you lose this
data permanently.
• Define the InnoDB memory buffer pool size. The default value is 128 MB, but to compensate for Galera
Cluster’s additional memory usage, scale your usual value back by 5%:
innodb_buffer_pool_size=122M
Configuring wsrep replication
Galera Cluster configuration parameters all have the wsrep_ prefix. You must define the following parameters
for each cluster node in your OpenStack database.
• wsrep Provider: The Galera Replication Plugin serves as the wsrep provider for Galera Cluster. It is
installed on your system as the libgalera_smm.so file. Define the path to this file in your my.cnf:
10 Configuring the shared services
High Availability Guide (Release Version: 15.0.0)
wsrep_provider="/usr/lib/libgalera_smm.so"
• Cluster Name: Define an arbitrary name for your cluster.
wsrep_cluster_name="my_example_cluster"
You must use the same name on every cluster node. The connection fails when this value does not match.
• Cluster Address: List the IP addresses for each cluster node.
wsrep_cluster_address="gcomm://192.168.1.1,192.168.1.2,192.168.1.3"
Replace the IP addresses given here with comma-separated list of each OpenStack database in your
cluster.
• Node Name: Define the logical name of the cluster node.
wsrep_node_name="Galera1"
• Node Address: Define the IP address of the cluster node.
wsrep_node_address="192.168.1.1"
Additional parameters
For a complete list of the available parameters, run the SHOW VARIABLES command from within the database
client:
SHOW VARIABLES LIKE 'wsrep_%';
+------------------------------+-------+
| Variable_name | Value |
+------------------------------+-------+
| wsrep_auto_increment_control | ON |
+------------------------------+-------+
| wsrep_causal_reads | OFF |
+------------------------------+-------+
| wsrep_certify_nonPK | ON |
+------------------------------+-------+
| ... | ... |
+------------------------------+-------+
| wsrep_sync_wait | 0 |
+------------------------------+-------+
For documentation about these parameters, wsrep provider option, and status variables available in Galera
Cluster, see the Galera cluster Reference.
Management
When you finish installing and configuring the OpenStack database, you can initialize the Galera Cluster.
Configuring the shared services 11
High Availability Guide (Release Version: 15.0.0)
Prerequisites
• Database hosts with Galera Cluster installed
• A minimum of three hosts
• No firewalls between the hosts
• SELinux and AppArmor set to permit access to mysqld
• The correct path to libgalera_smm.so given to the wsrep_provider parameter
Initializing the cluster
In the Galera Cluster, the Primary Component is the cluster of database servers that replicate into each other. In
the event that a cluster node loses connectivity with the Primary Component, it defaults into a non-operational
state, to avoid creating or serving inconsistent data.
By default, cluster nodes do not start as part of a Primary Component. In the Primary Component, replication
and state transfers bring all databases to the same state.
To start the cluster, complete the following steps:
1. Initialize the Primary Component on one cluster node. For servers that use init, run the following
command:
# service mysql start --wsrep-new-cluster
For servers that use systemd, run the following command:
# systemctl start mariadb --wsrep-new-cluster
2. Once the database server starts, check the cluster status using the wsrep_cluster_size status variable.
From the database client, run the following command:
SHOW STATUS LIKE 'wsrep_cluster_size';
+--------------------+-------+
| Variable_name | Value |
+--------------------+-------+
| wsrep_cluster_size | 1 |
+--------------------+-------+
3. Start the database server on all other cluster nodes. For servers that use init, run the following command:
# service mysql start
For servers that use systemd, run the following command:
# systemctl start mariadb
4. When you have all cluster nodes started, log into the database client of any cluster node and check the
wsrep_cluster_size status variable again:
SHOW STATUS LIKE 'wsrep_cluster_size';
12 Configuring the shared services
High Availability Guide (Release Version: 15.0.0)
+--------------------+-------+
| Variable_name | Value |
+--------------------+-------+
| wsrep_cluster_size | 3 |
+--------------------+-------+
When each cluster node starts, it checks the IP addresses given to the wsrep_cluster_address parameter.
It then attempts to establish network connectivity with a database server running there. Once it establishes a
connection, it attempts to join the Primary Component, requesting a state transfer as needed to bring itself into
sync with the cluster.
Note: In the event that you need to restart any cluster node, you can do so. When the database server comes
back it, it establishes connectivity with the Primary Component and updates itself to any changes it may have
missed while down.
Restarting the cluster
Individual cluster nodes can stop and be restarted without issue. When a database loses its connection or restarts,
the Galera Cluster brings it back into sync once it reestablishes connection with the Primary Component. In
the event that you need to restart the entire cluster, identify the most advanced cluster node and initialize the
Primary Component on that node.
To find the most advanced cluster node, you need to check the sequence numbers, or the seqnos, on the last
committed transaction for each. You can find this by viewing grastate.dat file in database directory:
$ cat /path/to/datadir/grastate.dat
# Galera saved state
version: 3.8
uuid: 5ee99582-bb8d-11e2-b8e3-23de375c1d30
seqno: 8204503945773
Alternatively, if the database server is running, use the wsrep_last_committed status variable:
SHOW STATUS LIKE 'wsrep_last_committed';
+----------------------+--------+
| Variable_name | Value |
+----------------------+--------+
| wsrep_last_committed | 409745 |
+----------------------+--------+
This value increments with each transaction, so the most advanced node has the highest sequence number and
therefore is the most up to date.
Configuration tips
Deployment strategies
Galera can be configured using one of the following strategies:
Configuring the shared services 13
High Availability Guide (Release Version: 15.0.0)
• Each instance has its own IP address:
OpenStack services are configured with the list of these IP addresses so they can select one of the addresses
from those available.
• Galera runs behind HAProxy:
HAProxy load balances incoming requests and exposes just one IP address for all the clients.
Galera synchronous replication guarantees a zero slave lag. The failover procedure completes once
HAProxy detects that the active back end has gone down and switches to the backup one, which is then
marked as UP. If no back ends are UP, the failover procedure finishes only when the Galera Cluster has
been successfully reassembled. The SLA is normally no more than 5 minutes.
• Use MySQL/Galera in active/passive mode to avoid deadlocks on SELECT ... FOR UPDATE type
queries (used, for example, by nova and neutron). This issue is discussed in the following:
– IMPORTANT: MySQL Galera does *not* support SELECT ... FOR UPDATE
– Understanding reservations, concurrency, and locking in Nova
Configuring HAProxy
If you use HAProxy as a load-balancing client to provide access to the Galera Cluster, as described in the
HAProxy, you can use the clustercheck utility to improve health checks.
1. Create a configuration file for clustercheck at /etc/sysconfig/clustercheck:
MYSQL_USERNAME="clustercheck_user"
MYSQL_PASSWORD="my_clustercheck_password"
MYSQL_HOST="localhost"
MYSQL_PORT="3306"
Note: For Ubuntu 16.04.1: Create a configuration file for clustercheck at /etc/default/clustercheck.
1. Log in to the database client and grant the clustercheck user PROCESS privileges:
GRANT PROCESS ON *.* TO 'clustercheck_user'@'localhost'
IDENTIFIED BY 'my_clustercheck_password';
FLUSH PRIVILEGES;
You only need to do this on one cluster node. Galera Cluster replicates the user to all the others.
2. Create a configuration file for the HAProxy monitor service, at /etc/xinetd.d/galera-monitor:
service galera-monitor
{
port = 9200
disable = no
socket_type = stream
protocol = tcp
wait = no
user = root
group = root
groups = yes
14 Configuring the shared services
High Availability Guide (Release Version: 15.0.0)
server = /usr/bin/clustercheck
type = UNLISTED
per_source = UNLIMITED
log_on_success =
log_on_failure = HOST
flags = REUSE
}
3. Start the xinetd daemon for clustercheck. For servers that use init, run the following commands:
# service xinetd enable
# service xinetd start
For servers that use systemd, run the following commands:
# systemctl daemon-reload
# systemctl enable xinetd
# systemctl start xinetd
The first step is to install the database that sits at the heart of the cluster. To implement high availability, run an
instance of the database on each controller node and use Galera Cluster to provide replication between them.
Galera Cluster is a synchronous multi-master database cluster, based on MySQL and the InnoDB storage engine.
It is a high-availability service that provides high system uptime, no data loss, and scalability for growth.
You can achieve high availability for the OpenStack database in many different ways, depending on the type of
database that you want to use. There are three implementations of Galera Cluster available to you:
• Galera Cluster for MySQL: The MySQL reference implementation from Codership, Oy.
• MariaDB Galera Cluster: The MariaDB implementation of Galera Cluster, which is commonly supported
in environments based on Red Hat distributions.
• Percona XtraDB Cluster: The XtraDB implementation of Galera Cluster from Percona.
In addition to Galera Cluster, you can also achieve high availability through other database options, such as
PostgreSQL, which has its own replication system.
Messaging service for high availability
An AMQP (Advanced Message Queuing Protocol) compliant message bus is required for most OpenStack
components in order to coordinate the execution of jobs entered into the system.
The most popular AMQP implementation used in OpenStack installations is RabbitMQ.
RabbitMQ nodes fail over on the application and the infrastructure layers.
The application layer is controlled by the oslo.messaging configuration options for multiple AMQP hosts.
If the AMQP node fails, the application reconnects to the next one configured within the specified reconnect
interval. The specified reconnect interval constitutes its SLA.
On the infrastructure layer, the SLA is the time for which RabbitMQ cluster reassembles. Several cases are
possible. The Mnesia keeper node is the master of the corresponding Pacemaker resource for RabbitMQ. When
it fails, the result is a full AMQP cluster downtime interval. Normally, its SLA is no more than several minutes.
Failure of another node that is a slave of the corresponding Pacemaker resource for RabbitMQ results in no
AMQP cluster downtime at all.
Making the RabbitMQ service highly available involves the following steps:
Configuring the shared services 15
High Availability Guide (Release Version: 15.0.0)
• Install RabbitMQ
• Configure RabbitMQ for HA queues
• Configure OpenStack services to use RabbitMQ HA queues
Note: Access to RabbitMQ is not normally handled by HAProxy. Instead, consumers must be supplied with
the full list of hosts running RabbitMQ with rabbit_hosts and turn on the rabbit_ha_queues option. For
more information, read the core issue. For more detail, read the history and solution.
Install RabbitMQ
The commands for installing RabbitMQ are specific to the Linux distribution you are using.
For Ubuntu or Debian:
For RHEL, Fedora, or CentOS:
For openSUSE:
For SLES 12:
Note: For SLES 12, the packages are signed by GPG key 893A90DAD85F9316. You should verify the
fingerprint of the imported GPG key before using it.
Key ID: 893A90DAD85F9316
Key Name: Cloud:OpenStack OBS Project <Cloud:OpenStack@build.opensuse.org>
Key Fingerprint: 35B34E18ABC1076D66D5A86B893A90DAD85F9316
Key Created: Tue Oct 8 13:34:21 2013
Key Expires: Thu Dec 17 13:34:21 2015
For more information, see the official installation manual for the distribution:
• Debian and Ubuntu
• RPM based (RHEL, Fedora, CentOS, openSUSE)
Configure RabbitMQ for HA queues
The following components/services can work with HA queues:
• OpenStack Compute
• OpenStack Block Storage
• OpenStack Networking
• Telemetry
Consider that, while exchanges and bindings survive the loss of individual nodes, queues and their messages
do not because a queue and its contents are located on one node. If we lose this node, we also lose the queue.
Mirrored queues in RabbitMQ improve the availability of service since it is resilient to failures.
16 Configuring the shared services
High Availability Guide (Release Version: 15.0.0)
Production servers should run (at least) three RabbitMQ servers for testing and demonstration purposes, however
it is possible to run only two servers. In this section, we configure two nodes, called rabbit1 and rabbit2.
To build a broker, ensure that all nodes have the same Erlang cookie file.
1. Stop RabbitMQ and copy the cookie from the first node to each of the other node(s):
# scp /var/lib/rabbitmq/.erlang.cookie root@NODE:/var/lib/rabbitmq/.erlang.cookie
2. On each target node, verify the correct owner, group, and permissions of the file erlang.cookie:
# chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie
# chmod 400 /var/lib/rabbitmq/.erlang.cookie
3. Start the message queue service on all nodes and configure it to start when the system boots. On Ubuntu,
it is configured by default.
On CentOS, RHEL, openSUSE, and SLES:
# systemctl enable rabbitmq-server.service
# systemctl start rabbitmq-server.service
4. Verify that the nodes are running:
# rabbitmqctl cluster_status
Cluster status of node rabbit@NODE...
[{nodes,[{disc,[rabbit@NODE]}]},
{running_nodes,[rabbit@NODE]},
{partitions,[]}]
...done.
5. Run the following commands on each node except the first one:
# rabbitmqctl stop_app
Stopping node rabbit@NODE...
...done.
# rabbitmqctl join_cluster --ram rabbit@rabbit1
# rabbitmqctl start_app
Starting node rabbit@NODE ...
...done.
Note: The default node type is a disc node. In this guide, nodes join the cluster as RAM nodes.
1. Verify the cluster status:
# rabbitmqctl cluster_status
Cluster status of node rabbit@NODE...
[{nodes,[{disc,[rabbit@rabbit1]},{ram,[rabbit@NODE]}]}, \
{running_nodes,[rabbit@NODE,rabbit@rabbit1]}]
If the cluster is working, you can create usernames and passwords for the queues.
2. To ensure that all queues except those with auto-generated names are mirrored across all running nodes,
set the ha-mode policy key to all by running the following command on one of the nodes:
Configuring the shared services 17
High Availability Guide (Release Version: 15.0.0)
# rabbitmqctl set_policy ha-all '^(?!amq\.).*' '{"ha-mode": "all"}'
More information is available in the RabbitMQ documentation:
• Highly Available Queues
• Clustering Guide
Note: As another option to make RabbitMQ highly available, RabbitMQ contains the OCF scripts for the
Pacemaker cluster resource agents since version 3.5.7. It provides the active/active RabbitMQ cluster with
mirrored queues. For more information, see Auto-configuration of a cluster with a Pacemaker.
Configure OpenStack services to use Rabbit HA queues
Configure the OpenStack components to use at least two RabbitMQ nodes.
Use these steps to configurate all services using RabbitMQ:
1. RabbitMQ HA cluster host:port pairs:
rabbit_hosts=rabbit1:5672,rabbit2:5672,rabbit3:5672
2. Retry connecting with RabbitMQ:
rabbit_retry_interval=1
3. How long to back-off for between retries when connecting to RabbitMQ:
rabbit_retry_backoff=2
4. Maximum retries with trying to connect to RabbitMQ (infinite by default):
rabbit_max_retries=0
5. Use durable queues in RabbitMQ:
rabbit_durable_queues=true
6. Use HA queues in RabbitMQ (x-ha-policy: all):
rabbit_ha_queues=true
Note: If you change the configuration from an old set-up that did not use HA queues, restart the service:
# rabbitmqctl stop_app
# rabbitmqctl reset
# rabbitmqctl start_app
18 Configuring the shared services
High Availability Guide (Release Version: 15.0.0)
Configuring the controller
The cloud controller runs on the management network and must talk to all other services.
The Pacemaker architecture
What is a cluster manager?
At its core, a cluster is a distributed finite state machine capable of co-ordinating the startup and recovery of
inter-related services across a set of machines.
Even a distributed or replicated application that is able to survive failures on one or more machines can benefit
from a cluster manager because a cluster manager has the following capabilities:
1. Awareness of other applications in the stack
While SYS-V init replacements like systemd can provide deterministic recovery of a complex stack of
services, the recovery is limited to one machine and lacks the context of what is happening on other
machines. This context is crucial to determine the difference between a local failure, and clean startup
and recovery after a total site failure.
2. Awareness of instances on other machines
Services like RabbitMQ and Galera have complicated boot-up sequences that require co-ordination, and
often serialization, of startup operations across all machines in the cluster. This is especially true after a
site-wide failure or shutdown where you must first determine the last machine to be active.
3. A shared implementation and calculation of quorum
It is very important that all members of the system share the same view of who their peers are and whether
or not they are in the majority. Failure to do this leads very quickly to an internal split-brain state. This
is where different parts of the system are pulling in different and incompatible directions.
4. Data integrity through fencing (a non-responsive process does not imply it is not doing anything)
A single application does not have sufficient context to know the difference between failure of a machine
and failure of the application on a machine. The usual practice is to assume the machine is dead and
continue working, however this is highly risky. A rogue process or machine could still be responding to
requests and generally causing havoc. The safer approach is to make use of remotely accessible power
switches and/or network switches and SAN controllers to fence (isolate) the machine before continuing.
5. Automated recovery of failed instances
While the application can still run after the failure of several instances, it may not have sufficient capacity
to serve the required volume of requests. A cluster can automatically recover failed instances to prevent
additional load induced failures.
For these reasons, we highly recommend the use of a cluster manager like Pacemaker.
Deployment flavors
It is possible to deploy three different flavors of the Pacemaker architecture. The two extremes are Collapsed
(where every component runs on every node) and Segregated (where every component runs in its own 3+
node cluster).
Configuring the controller 19
High Availability Guide (Release Version: 15.0.0)
Regardless of which flavor you choose, we recommend that clusters contain at least three nodes so that you can
take advantage of quorum.
Quorum becomes important when a failure causes the cluster to split in two or more partitions. In this situation,
you want the majority members of the system to ensure the minority are truly dead (through fencing) and
continue to host resources. For a two-node cluster, no side has the majority and you can end up in a situation
where both sides fence each other, or both sides are running the same services. This can lead to data corruption.
Clusters with an even number of hosts suffer from similar issues. A single network failure could easily cause
a N:N split where neither side retains a majority. For this reason, we recommend an odd number of cluster
members when scaling up.
You can have up to 16 cluster members (this is currently limited by the ability of corosync to scale higher). In
extreme cases, 32 and even up to 64 nodes could be possible. However, this is not well tested.
Collapsed
In a collapsed configuration, there is a single cluster of 3 or more nodes on which every component is running.
This scenario has the advantage of requiring far fewer, if more powerful, machines. Additionally, being part of
a single cluster allows you to accurately model the ordering dependencies between components.
This scenario can be visualized as below.
You would choose this option if you prefer to have fewer but more powerful boxes.
This is the most common option and the one we document here.
Segregated
In this configuration, each service runs in a dedicated cluster of 3 or more nodes.
The benefits of this approach are the physical isolation between components and the ability to add capacity to
specific components.
20 Configuring the controller
High Availability Guide (Release Version: 15.0.0)
You would choose this option if you prefer to have more but less powerful boxes.
This scenario can be visualized as below, where each box below represents a cluster of three or more guests.
Mixed
It is also possible to follow a segregated approach for one or more components that are expected to be a bottleneck
and use a collapsed approach for the remainder.
Proxy server
Almost all services in this stack benefit from being proxied. Using a proxy server provides the following
capabilities:
1. Load distribution
Many services can act in an active/active capacity, however, they usually require an external mechanism
for distributing requests to one of the available instances. The proxy server can serve this role.
2. API isolation
By sending all API access through the proxy, you can clearly identify service interdependencies. You
can also move them to locations other than localhost to increase capacity if the need arises.
3. Simplified process for adding/removing of nodes
Since all API access is directed to the proxy, adding or removing nodes has no impact on the configuration
of other services. This can be very useful in upgrade scenarios where an entirely new set of machines
can be configured and tested in isolation before telling the proxy to direct traffic there instead.
4. Enhanced failure detection
The proxy can be configured as a secondary mechanism for detecting service failures. It can even be
configured to look for nodes in a degraded state (such as being too far behind in the replication) and take
them out of circulation.
Configuring the controller 21
High Availability Guide (Release Version: 15.0.0)
The following components are currently unable to benefit from the use of a proxy server:
• RabbitMQ
• Memcached
• MongoDB
We recommend HAProxy as the load balancer, however, there are many alternative load balancing solutions in
the marketplace.
Generally, we use round-robin to distribute load amongst instances of active/active services. Alternatively,
Galera uses stack-table options to ensure that incoming connection to virtual IP (VIP) are directed to only
one of the available back ends. This helps avoid lock contention and prevent deadlocks, although Galera can
run active/active. Used in combination with the httpchk option, this ensure only nodes that are in sync with
their peers are allowed to handle requests.
Pacemaker cluster stack
Pacemaker cluster stack is a state-of-the-art high availability and load balancing stack for the Linux platform.
Pacemaker is used to make OpenStack infrastructure highly available.
Note: It is storage and application-agnostic, and in no way specific to OpenStack.
Pacemaker relies on the Corosync messaging layer for reliable cluster communications. Corosync implements
the Totem single-ring ordering and membership protocol. It also provides UDP and InfiniBand based messaging,
quorum, and cluster membership to Pacemaker.
Pacemaker does not inherently understand the applications it manages. Instead, it relies on resource agents
(RAs) that are scripts that encapsulate the knowledge of how to start, stop, and check the health of each application
managed by the cluster.
These agents must conform to one of the OCF, SysV Init, Upstart, or Systemd standards.
Pacemaker ships with a large set of OCF agents (such as those managing MySQL databases, virtual IP addresses,
and RabbitMQ), but can also use any agents already installed on your system and can be extended with your
own (see the developer guide).
The steps to implement the Pacemaker cluster stack are:
• Install packages
• Set up the cluster with pcs
• Start Corosync
• Start Pacemaker
• Set basic cluster properties
Install packages
On any host that is meant to be part of a Pacemaker cluster, establish cluster communications through the
Corosync messaging layer. This involves installing the following packages (and their dependencies, which
your package manager usually installs automatically):
22 Configuring the controller
High Availability Guide (Release Version: 15.0.0)
• pacemaker
• pcs (CentOS or RHEL) or crmsh
• corosync
• fence-agents (CentOS or RHEL) or cluster-glue
• resource-agents
• libqb0
Set up the cluster with pcs
1. Make sure pcs is running and configured to start at boot time:
$ systemctl enable pcsd
$ systemctl start pcsd
2. Set a password for hacluster user on each host:
$ echo my-secret-password-no-dont-use-this-one \
| passwd --stdin hacluster
Note: Since the cluster is a single administrative domain, it is acceptable to use the same password on
all nodes.
3. Use that password to authenticate to the nodes that will make up the cluster:
$ pcs cluster auth controller1 controller2 controller3 \
-u hacluster -p my-secret-password-no-dont-use-this-one --force
Note: The -p option is used to give the password on command line and makes it easier to script.
4. Create and name the cluster. Then, start it and enable all components to auto-start at boot time:
$ pcs cluster setup --force --name my-first-openstack-cluster \
controller1 controller2 controller3
$ pcs cluster start --all
$ pcs cluster enable --all
Note: In Red Hat Enterprise Linux or CentOS environments, this is a recommended path to perform configuration.
For more information, see the RHEL docs.
Set up the cluster with crmsh
After installing the Corosync package, you must create the /etc/corosync/corosync.conf configuration
file.
Configuring the controller 23
High Availability Guide (Release Version: 15.0.0)
Note: For Ubuntu, you should also enable the Corosync service in the /etc/default/corosync configuration
file.
Corosync can be configured to work with either multicast or unicast IP addresses or to use the votequorum
library.
• Set up Corosync with multicast
• Set up Corosync with unicast
• Set up Corosync with votequorum library
Set up Corosync with multicast
Most distributions ship an example configuration file (corosync.conf.example) as part of the documentation
bundled with the Corosync package. An example Corosync configuration file is shown below:
Example Corosync configuration file for multicast (‘‘corosync.conf‘‘)
totem {
version: 2
# Time (in ms) to wait for a token (1)
token: 10000
# How many token retransmits before forming a new
# configuration
token_retransmits_before_loss_const: 10
# Turn off the virtual synchrony filter
vsftype: none
# Enable encryption (2)
secauth: on
# How many threads to use for encryption/decryption
threads: 0
# This specifies the redundant ring protocol, which may be
# none, active, or passive. (3)
rrp_mode: active
# The following is a two-ring multicast configuration. (4)
interface {
ringnumber: 0
bindnetaddr: 10.0.0.0
mcastaddr: 239.255.42.1
mcastport: 5405
}
interface {
ringnumber: 1
bindnetaddr: 10.0.42.0
mcastaddr: 239.255.42.2
mcastport: 5405
}
24 Configuring the controller
High Availability Guide (Release Version: 15.0.0)
}
amf {
mode: disabled
}
service {
# Load the Pacemaker Cluster Resource Manager (5)
ver: 1
name: pacemaker
}
aisexec {
user: root
group: root
}
logging {
fileline: off
to_stderr: yes
to_logfile: no
to_syslog: yes
syslog_facility: daemon
debug: off
timestamp: on
logger_subsys {
subsys: AMF
debug: off
tags: enter|leave|trace1|trace2|trace3|trace4|trace6
}}
Note the following:
• The token value specifies the time, in milliseconds, during which the Corosync token is expected to
be transmitted around the ring. When this timeout expires, the token is declared lost, and after token_retransmits_before_loss_const
lost tokens, the non-responding processor (cluster node)
is declared dead. token × token_retransmits_before_loss_const is the maximum time a node
is allowed to not respond to cluster messages before being considered dead. The default for token is 1000
milliseconds (1 second), with 4 allowed retransmits. These defaults are intended to minimize failover
times, but can cause frequent false alarms and unintended failovers in case of short network interruptions.
The values used here are safer, albeit with slightly extended failover times.
• With secauth enabled, Corosync nodes mutually authenticates using a 128-byte shared secret stored in
the /etc/corosync/authkey file. This can be generated with the corosync-keygen utility. Cluster
communications are encrypted when using secauth.
• In Corosync, configurations use redundant networking (with more than one interface). This means you
must select a Redundant Ring Protocol (RRP) mode other than none. We recommend active as the
RRP mode.
Note the following about the recommended interface configuration:
– Each configured interface must have a unique ringnumber, starting with 0.
– The bindnetaddr is the network address of the interfaces to bind to. The example uses two
network addresses of /24 IPv4 subnets.
– Multicast groups (mcastaddr) must not be reused across cluster boundaries. No two distinct clusConfiguring
the controller 25
High Availability Guide (Release Version: 15.0.0)
ters should ever use the same multicast group. Be sure to select multicast addresses compliant with
RFC 2365, “Administratively Scoped IP Multicast”.
– For firewall configurations, Corosync communicates over UDP only, and uses mcastport (for
receives) and mcastport - 1 (for sends).
• The service declaration for the Pacemaker service may be placed in the corosync.conf file directly or
in its own separate file, /etc/corosync/service.d/pacemaker.
Note: If you are using Corosync version 2 on Ubuntu 14.04, remove or comment out lines under the
service stanza. These stanzas enable Pacemaker to start up. Another potential problem is the boot and
shutdown order of Corosync and Pacemaker. To force Pacemaker to start after Corosync and stop before
Corosync, fix the start and kill symlinks manually:
# update-rc.d pacemaker start 20 2 3 4 5 . stop 00 0 1 6 .
The Pacemaker service also requires an additional configuration file /etc/corosync/uidgid.d/
pacemaker to be created with the following content:
uidgid {
uid: hacluster
gid: haclient
}
• Once created, synchronize the corosync.conf file (and the authkey file if the secauth option is enabled)
across all cluster nodes.
Set up Corosync with unicast
For environments that do not support multicast, Corosync should be configured for unicast. An example fragment
of the corosync.conf file for unicastis is shown below:
Corosync configuration file fragment for unicast (‘‘corosync.conf‘‘)
totem {
#...
interface {
ringnumber: 0
bindnetaddr: 10.0.0.0
broadcast: yes (1)
mcastport: 5405
}
interface {
ringnumber: 1
bindnetaddr: 10.0.42.0
broadcast: yes
mcastport: 5405
}
transport: udpu (2)
}
nodelist { (3)
node {
ring0_addr: 10.0.0.12
26 Configuring the controller
High Availability Guide (Release Version: 15.0.0)
ring1_addr: 10.0.42.12
nodeid: 1
}
node {
ring0_addr: 10.0.0.13
ring1_addr: 10.0.42.13
nodeid: 2
}
node {
ring0_addr: 10.0.0.14
ring1_addr: 10.0.42.14
nodeid: 3
}
}
#...
Note the following:
• If the broadcast parameter is set to yes, the broadcast address is used for communication. If this option
is set, the mcastaddr parameter should not be set.
• The transport directive controls the transport mechanism. To avoid the use of multicast entirely, specify
the udpu unicast transport parameter. This requires specifying the list of members in the nodelist
directive. This potentially makes up the membership before deployment. The default is udp. The transport
type can also be set to udpu or iba.
• Within the nodelist directive, it is possible to specify specific information about the nodes in the cluster.
The directive can contain only the node sub-directive, which specifies every node that should be a
member of the membership, and where non-default options are needed. Every node must have at least
the ring0_addr field filled.
Note: For UDPU, every node that should be a member of the membership must be specified.
Possible options are:
– ring{X}_addr specifies the IP address of one of the nodes. {X} is the ring number.
– nodeid is optional when using IPv4 and required when using IPv6. This is a 32-bit value specifying
the node identifier delivered to the cluster membership service. If this is not specified with IPv4,
the node ID is determined from the 32-bit IP address of the system to which the system is bound
with ring identifier of 0. The node identifier value of zero is reserved and should not be used.
Set up Corosync with votequorum library
The votequorum library is part of the Corosync project. It provides an interface to the vote-based quorum service
and it must be explicitly enabled in the Corosync configuration file. The main role of votequorum library is to
avoid split-brain situations, but it also provides a mechanism to:
• Query the quorum status
• List the nodes known to the quorum service
• Receive notifications of quorum state changes
• Change the number of votes assigned to a node
Configuring the controller 27
High Availability Guide (Release Version: 15.0.0)
• Change the number of expected votes for a cluster to be quorate
• Connect an additional quorum device to allow small clusters remain quorate during node outages
The votequorum library has been created to replace and eliminate qdisk, the disk-based quorum daemon for
CMAN, from advanced cluster configurations.
A sample votequorum service configuration in the corosync.conf file is:
quorum {
provider: corosync_votequorum (1)
expected_votes: 7 (2)
wait_for_all: 1 (3)
last_man_standing: 1 (4)
last_man_standing_window: 10000 (5)
}
Note the following:
• Specifying corosync_votequorum enables the votequorum library. This is the only required option.
• The cluster is fully operational with expected_votes set to 7 nodes (each node has 1 vote), quorum: 4.
If a list of nodes is specified as nodelist, the expected_votes value is ignored.
• When you start up a cluster (all nodes down) and set wait_for_all to 1, the cluster quorum is held until
all nodes are online and have joined the cluster for the first time. This parameter is new in Corosync 2.0.
• Setting last_man_standing to 1 enables the Last Man Standing (LMS) feature. By default, it is disabled
(set to 0). If a cluster is on the quorum edge (expected_votes: set to 7; online nodes: set
to 4) for longer than the time specified for the last_man_standing_window parameter, the cluster can
recalculate quorum and continue operating even if the next node will be lost. This logic is repeated until
the number of online nodes in the cluster reaches 2. In order to allow the cluster to step down from 2
members to only 1, the auto_tie_breaker parameter needs to be set. We do not recommended this for
production environments.
• last_man_standing_window specifies the time, in milliseconds, required to recalculate quorum after
one or more hosts have been lost from the cluster. To perform a new quorum recalculation, the cluster
must have quorum for at least the interval specified for last_man_standing_window. The default is
10000ms.
Start Corosync
Corosync is started as a regular system service. Depending on your distribution, it may ship with an LSB init
script, an upstart job, or a Systemd unit file.
• Start corosync with the LSB init script:
# /etc/init.d/corosync start
Alternatively:
# service corosync start
• Start corosync with upstart:
# start corosync
28 Configuring the controller
High Availability Guide (Release Version: 15.0.0)
• Start corosync with systemd unit file:
# systemctl start corosync
You can now check the corosync connectivity with one of these tools.
Use the corosync-cfgtool utility with the -s option to get a summary of the health of the communication
rings:
# corosync-cfgtool -s
Printing ring status.
Local node ID 435324542
RING ID 0
id = 10.0.0.82
status = ring 0 active with no faults
RING ID 1
id = 10.0.42.100
status = ring 1 active with no faults
Use the corosync-objctl utility to dump the Corosync cluster member list:
# corosync-objctl runtime.totem.pg.mrp.srp.members
runtime.totem.pg.mrp.srp.435324542.ip=r(0) ip(10.0.0.82) r(1) ip(10.0.42.100)
runtime.totem.pg.mrp.srp.435324542.join_count=1
runtime.totem.pg.mrp.srp.435324542.status=joined
runtime.totem.pg.mrp.srp.983895584.ip=r(0) ip(10.0.0.87) r(1) ip(10.0.42.254)
runtime.totem.pg.mrp.srp.983895584.join_count=1
runtime.totem.pg.mrp.srp.983895584.status=joined
You should see a status=joined entry for each of your constituent cluster nodes.
Note: If you are using Corosync version 2, use the corosync-cmapctl utility instead of corosync-objctl;
it is a direct replacement.
Start Pacemaker
After the corosync service have been started and you have verified that the cluster is communicating properly,
you can start pacemakerd, the Pacemaker master control process. Choose one from the following four ways
to start it:
1. Start pacemaker with the LSB init script:
# /etc/init.d/pacemaker start
Alternatively:
# service pacemaker start
2. Start pacemaker with upstart:
# start pacemaker
3. Start pacemaker with the systemd unit file:
Configuring the controller 29
High Availability Guide (Release Version: 15.0.0)
# systemctl start pacemaker
After the pacemaker service has started, Pacemaker creates a default empty cluster configuration with no
resources. Use the crm_mon utility to observe the status of pacemaker:
# crm_mon -1
Last updated: Sun Oct 7 21:07:52 2012
Last change: Sun Oct 7 20:46:00 2012 via cibadmin on controller2
Stack: openais
Current DC: controller2 - partition with quorum
Version: 1.1.6-9971ebba4494012a93c03b40a2c58ec0eb60f50c
3 Nodes configured, 3 expected votes
0 Resources configured.
Online: [ controller3 controller2 controller1 ]
...
Set basic cluster properties
After you set up your Pacemaker cluster, set a few basic cluster properties:
• crmsh
$ crm configure property pe-warn-series-max="1000" \
pe-input-series-max="1000" \
pe-error-series-max="1000" \
cluster-recheck-interval="5min"
• pcs
$ pcs property set pe-warn-series-max=1000 \
pe-input-series-max=1000 \
pe-error-series-max=1000 \
cluster-recheck-interval=5min
Note the following:
• Setting the pe-warn-series-max, pe-input-series-max, and pe-error-series-max parameters
to 1000 instructs Pacemaker to keep a longer history of the inputs processed and errors and warnings
generated by its Policy Engine. This history is useful if you need to troubleshoot the cluster.
• Pacemaker uses an event-driven approach to cluster state processing. The cluster-recheckinterval
parameter (which defaults to 15 minutes) defines the interval at which certain Pacemaker
actions occur. It is usually prudent to reduce this to a shorter interval, such as 5 or 3 minutes.
By default, STONITH is enabled in Pacemaker, but STONITH mechanisms (to shutdown a node via IPMI or
ssh) are not configured. In this case Pacemaker will refuse to start any resources. For production cluster it is
recommended to configure appropriate STONITH mechanisms. But for demo or testing purposes STONITH
can be disabled completely as follows:
• crmsh
$ crm configure property stonith-enabled=false
30 Configuring the controller
High Availability Guide (Release Version: 15.0.0)
• pcs
$ pcs property set stonith-enabled=false
After you make these changes, commit the updated configuration.
Configure the VIP
You must select and assign a virtual IP address (VIP) that can freely float between cluster nodes.
This configuration creates vip, a virtual IP address for use by the API node (10.0.0.11).
For crmsh:
# crm configure primitive vip ocf:heartbeat:IPaddr2 \
params ip="10.0.0.11" cidr_netmask="24" op monitor interval="30s"
For pcs:
# pcs resource create vip ocf:heartbeat:IPaddr2 \
params ip="10.0.0.11" cidr_netmask="24" op monitor interval="30s"
HAProxy
HAProxy provides a fast and reliable HTTP reverse proxy and load balancer for TCP or HTTP applications. It
is particularly suited for web crawling under very high loads while needing persistence or Layer 7 processing.
It realistically supports tens of thousands of connections with recent hardware.
Each instance of HAProxy configures its front end to accept connections only to the virtual IP (VIP) address.
The HAProxy back end (termination point) is a list of all the IP addresses of instances for load balancing.
Note: Ensure your HAProxy installation is not a single point of failure, it is advisable to have multiple HAProxy
instances running.
You can also ensure the availability by other means, using Keepalived or Pacemaker.
Alternatively, you can use a commercial load balancer, which is hardware or software. We recommend a hardware
load balancer as it generally has good performance.
For detailed instructions about installing HAProxy on your nodes, see the HAProxy official documentation.
Configuring HAProxy
1. Restart the HAProxy service.
2. Locate your HAProxy instance on each OpenStack controller in your environment. The following is an
example /etc/haproxy/haproxy.cfg configuration file. Configure your instance using the following
configuration file, you will need a copy of it on each controller node.
global
chroot /var/lib/haproxy
daemon
group haproxy
Configuring the controller 31
High Availability Guide (Release Version: 15.0.0)
maxconn 4000
pidfile /var/run/haproxy.pid
user haproxy
defaults
log global
maxconn 4000
option redispatch
retries 3
timeout http-request 10s
timeout queue 1m
timeout connect 10s
timeout client 1m
timeout server 1m
timeout check 10s
listen dashboard_cluster
bind <Virtual IP>:443
balance source
option tcpka
option httpchk
option tcplog
server controller1 10.0.0.12:443 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:443 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:443 check inter 2000 rise 2 fall 5
listen galera_cluster
bind <Virtual IP>:3306
balance source
option mysql-check
server controller1 10.0.0.12:3306 check port 9200 inter 2000 rise 2 fall 5
server controller2 10.0.0.13:3306 backup check port 9200 inter 2000 rise 2 fall 5
server controller3 10.0.0.14:3306 backup check port 9200 inter 2000 rise 2 fall 5
listen glance_api_cluster
bind <Virtual IP>:9292
balance source
option tcpka
option httpchk
option tcplog
server controller1 10.0.0.12:9292 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:9292 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:9292 check inter 2000 rise 2 fall 5
listen glance_registry_cluster
bind <Virtual IP>:9191
balance source
option tcpka
option tcplog
server controller1 10.0.0.12:9191 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:9191 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:9191 check inter 2000 rise 2 fall 5
listen keystone_admin_cluster
bind <Virtual IP>:35357
balance source
option tcpka
32 Configuring the controller
High Availability Guide (Release Version: 15.0.0)
option httpchk
option tcplog
server controller1 10.0.0.12:35357 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:35357 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:35357 check inter 2000 rise 2 fall 5
listen keystone_public_internal_cluster
bind <Virtual IP>:5000
balance source
option tcpka
option httpchk
option tcplog
server controller1 10.0.0.12:5000 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:5000 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:5000 check inter 2000 rise 2 fall 5
listen nova_ec2_api_cluster
bind <Virtual IP>:8773
balance source
option tcpka
option tcplog
server controller1 10.0.0.12:8773 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:8773 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:8773 check inter 2000 rise 2 fall 5
listen nova_compute_api_cluster
bind <Virtual IP>:8774
balance source
option tcpka
option httpchk
option tcplog
server controller1 10.0.0.12:8774 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:8774 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:8774 check inter 2000 rise 2 fall 5
listen nova_metadata_api_cluster
bind <Virtual IP>:8775
balance source
option tcpka
option tcplog
server controller1 10.0.0.12:8775 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:8775 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:8775 check inter 2000 rise 2 fall 5
listen cinder_api_cluster
bind <Virtual IP>:8776
balance source
option tcpka
option httpchk
option tcplog
server controller1 10.0.0.12:8776 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:8776 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:8776 check inter 2000 rise 2 fall 5
listen ceilometer_api_cluster
bind <Virtual IP>:8777
balance source
Configuring the controller 33
High Availability Guide (Release Version: 15.0.0)
option tcpka
option tcplog
server controller1 10.0.0.12:8777 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:8777 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:8777 check inter 2000 rise 2 fall 5
listen nova_vncproxy_cluster
bind <Virtual IP>:6080
balance source
option tcpka
option tcplog
server controller1 10.0.0.12:6080 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:6080 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:6080 check inter 2000 rise 2 fall 5
listen neutron_api_cluster
bind <Virtual IP>:9696
balance source
option tcpka
option httpchk
option tcplog
server controller1 10.0.0.12:9696 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:9696 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:9696 check inter 2000 rise 2 fall 5
listen swift_proxy_cluster
bind <Virtual IP>:8080
balance source
option tcplog
option tcpka
server controller1 10.0.0.12:8080 check inter 2000 rise 2 fall 5
server controller2 10.0.0.13:8080 check inter 2000 rise 2 fall 5
server controller3 10.0.0.14:8080 check inter 2000 rise 2 fall 5
Note: The Galera cluster configuration directive backup indicates that two of the three controllers are
standby nodes. This ensures that only one node services write requests because OpenStack support for
multi-node writes is not yet production-ready.
Note: The Telemetry API service configuration does not have the option httpchk directive as it
cannot process this check properly.
1. Configure the kernel parameter to allow non-local IP binding. This allows running HAProxy instances
to bind to a VIP for failover. Add following line to /etc/sysctl.conf:
net.ipv4.ip_nonlocal_bind = 1
2. Restart the host or, to make changes work immediately, invoke:
$ sysctl -p
3. Add HAProxy to the cluster and ensure the VIPs can only run on machines where HAProxy is active:
pcs
34 Configuring the controller
High Availability Guide (Release Version: 15.0.0)
$ pcs resource create lb-haproxy systemd:haproxy --clone
$ pcs constraint order start vip then lb-haproxy-clone kind=Optional
$ pcs constraint colocation add lb-haproxy-clone with vip
crmsh
$ crm cib new conf-haproxy
$ crm configure primitive haproxy lsb:haproxy op monitor interval="1s"
$ crm configure clone haproxy-clone haproxy
$ crm configure colocation vip-with-haproxy inf: vip haproxy-clone
$ crm configure order haproxy-after-vip mandatory: vip haproxy-clone
Memcached
Memcached is a general-purpose distributed memory caching system. It is used to speed up dynamic databasedriven
websites by caching data and objects in RAM to reduce the number of times an external data source must
be read.
Memcached is a memory cache demon that can be used by most OpenStack services to store ephemeral data,
such as tokens.
Access to Memcached is not handled by HAProxy because replicated access is currently in an experimental
state. Instead, OpenStack services must be supplied with the full list of hosts running Memcached.
The Memcached client implements hashing to balance objects among the instances. Failure of an instance
impacts only a percentage of the objects and the client automatically removes it from the list of instances. The
SLA is several minutes.
Highly available Identity API
Making the OpenStack Identity service highly available in active and passive mode involves:
• Prerequisites
• Configure OpenStack Identity service
• Configure OpenStack services to use the highly available OpenStack Identity
Prerequisites
Before beginning, ensure you have read the OpenStack Identity service getting started documentation.
Add OpenStack Identity resource to Pacemaker
The following section(s) detail how to add the OpenStack Identity resource to Pacemaker on SUSE and Red
Hat.
SUSE
SUSE Enterprise Linux and SUSE-based distributions, such as openSUSE, use a set of OCF agents for controlling
OpenStack services.
Configuring the controller 35
High Availability Guide (Release Version: 15.0.0)
1. Run the following commands to download the OpenStack Identity resource to Pacemaker:
# cd /usr/lib/ocf/resource.d
# mkdir openstack
# cd openstack
# wget https://git.openstack.org/cgit/openstack/openstack-resource-agents/plain/ocf/
,→keystone
# chmod a+rx *
2. Add the Pacemaker configuration for the OpenStack Identity resource by running the following command
to connect to the Pacemaker cluster:
# crm configure
3. Add the following cluster resources:
clone p_keystone ocf:openstack:keystone \
params config="/etc/keystone/keystone.conf" os_password="secretsecret" os_username=
,→"admin" os_tenant_name="admin" os_auth_url="http://10.0.0.11:5000/v2.0/" \
op monitor interval="30s" timeout="30s"
Note: This configuration creates p_keystone, a resource for managing the OpenStack Identity service.
4. Commit your configuration changes from the crm configure menu with the following command:
# commit
The crm configure supports batch input. You may have to copy and paste the above lines into your live
Pacemaker configuration, and then make changes as required.
For example, you may enter edit p_ip_keystone from the crm configure menu and edit the resource to
match your preferred virtual IP address.
Pacemaker now starts the OpenStack Identity service and its dependent resources on all of your nodes.
Red Hat
For Red Hat Enterprise Linux and Red Hat-based Linux distributions, the following process uses Systemd unit
files.
# pcs resource create openstack-keystone systemd:openstack-keystone --clone interleave=true
Configure OpenStack Identity service
1. Edit the keystone.conf file to change the values of the bind(2) parameters:
bind_host = 10.0.0.12
public_bind_host = 10.0.0.12
admin_bind_host = 10.0.0.12
The admin_bind_host parameter lets you use a private network for admin access.
36 Configuring the controller
High Availability Guide (Release Version: 15.0.0)
2. To be sure that all data is highly available, ensure that everything is stored in the MySQL database (which
is also highly available):
[catalog]
driver = keystone.catalog.backends.sql.Catalog
# ...
[identity]
driver = keystone.identity.backends.sql.Identity
# ...
3. If the Identity service will be sending ceilometer notifications and your message bus is configured for
high availability, you will need to ensure that the Identity service is correctly configured to use it. For
details on how to configure the Identity service for this kind of deployment, see Messaging service for
high availability.
Configure OpenStack services to use the highly available OpenStack Identity
Your OpenStack services now point their OpenStack Identity configuration to the highly available virtual cluster
IP address.
1. For OpenStack Compute, (if your OpenStack Identity service IP address is 10.0.0.11) use the following
configuration in the api-paste.ini file:
auth_host = 10.0.0.11
2. Create the OpenStack Identity Endpoint with this IP address.
Note: If you are using both private and public IP addresses, create two virtual IP addresses and define
the endpoint. For example:
$ openstack endpoint create --region $KEYSTONE_REGION \
$service-type public http://PUBLIC_VIP:5000/v2.0
$ openstack endpoint create --region $KEYSTONE_REGION \
$service-type admin http://10.0.0.11:35357/v2.0
$ openstack endpoint create --region $KEYSTONE_REGION \
$service-type internal http://10.0.0.11:5000/v2.0
3. If you are using the horizon Dashboard, edit the local_settings.py file to include the following:
OPENSTACK_HOST = 10.0.0.11
Highly available Telemetry
The Telemetry service provides a data collection service and an alarming service.
Telemetry polling agent
The Telemetry polling agent can be configured to partition its polling workload between multiple agents. This
enables high availability (HA).
Configuring the controller 37
High Availability Guide (Release Version: 15.0.0)
Both the central and the compute agent can run in an HA deployment. This means that multiple instances of
these services can run in parallel with workload partitioning among these running instances.
The Tooz library provides the coordination within the groups of service instances. It provides an API above
several back ends that can be used for building distributed applications.
Tooz supports various drivers including the following back end solutions:
• Zookeeper: Recommended solution by the Tooz project.
• Redis: Recommended solution by the Tooz project.
• Memcached: Recommended for testing.
You must configure a supported Tooz driver for the HA deployment of the Telemetry services.
For information about the required configuration options to set in the ceilometer.conf, see the coordination
section in the OpenStack Configuration Reference.
Note: Only one instance for the central and compute agent service(s) is able to run and function correctly if
the backend_url option is not set.
The availability check of the instances is provided by heartbeat messages. When the connection with an instance
is lost, the workload will be reassigned within the remaining instances in the next polling cycle.
Note: Memcached uses a timeout value, which should always be set to a value that is higher than the heartbeat
value set for Telemetry.
For backward compatibility and supporting existing deployments, the central agent configuration supports using
different configuration files. This is for groups of service instances that are running in parallel. For enabling this
configuration, set a value for the partitioning_group_prefix option in the polling section in the OpenStack
Configuration Reference.
Warning: For each sub-group of the central agent pool with the same partitioning_group_prefix, a
disjoint subset of meters must be polled to avoid samples being missing or duplicated. The list of meters to
poll can be set in the /etc/ceilometer/pipeline.yaml configuration file. For more information about
pipelines see the Data processing and pipelines section.
To enable the compute agent to run multiple instances simultaneously with workload partitioning, the workload_partitioning
option must be set to True under the compute section in the ceilometer.conf configuration
file.
Overview of highly available controllers
OpenStack is a set of services exposed to the end users as HTTP(s) APIs. Additionally, for your own internal
usage, OpenStack requires an SQL database server and AMQP broker. The physical servers, where all the
components are running, are called controllers. This modular OpenStack architecture allows you to duplicate
all the components and run them on different controllers. By making all the components redundant, it is possible
to make OpenStack highly available.
In general, we can divide all the OpenStack components into three categories:
38 Configuring the controller
High Availability Guide (Release Version: 15.0.0)
• OpenStack APIs: APIs that are HTTP(s) stateless services written in python, easy to duplicate and mostly
easy to load balance.
• The SQL relational database server provides stateful type consumed by other components. Supported
databases are MySQL, MariaDB, and PostgreSQL. Making the SQL database redundant is complex.
• Advanced Message Queuing Protocol (AMQP) provides OpenStack internal stateful communication service.
Common deployment architectures
We recommend two primary architectures for making OpenStack highly available.
The architectures differ in the sets of services managed by the cluster.
Both use a cluster manager, such as Pacemaker or Veritas, to orchestrate the actions of the various services
across a set of machines. Because we are focused on FOSS, we refer to these as Pacemaker architectures.
Traditionally, Pacemaker has been positioned as an all-encompassing solution. However, as OpenStack services
have matured, they are increasingly able to run in an active/active configuration and gracefully tolerate the
disappearance of the APIs on which they depend.
With this in mind, some vendors are restricting Pacemaker’s use to services that must operate in an active/passive
mode (such as cinder-volume), those with multiple states (for example, Galera), and those with complex
bootstrapping procedures (such as RabbitMQ).
The majority of services, needing no real orchestration, are handled by systemd on each node. This approach
avoids the need to coordinate service upgrades or location changes with the cluster and has the added advantage
of more easily scaling beyond Corosync’s 16 node limit. However, it will generally require the addition of an
enterprise monitoring solution such as Nagios or Sensu for those wanting centralized failure reporting.
Configuring the networking services
Run Networking DHCP agent
The OpenStack Networking (neutron) service has a scheduler that lets you run multiple agents across nodes.
The DHCP agent can be natively highly available.
To configure the number of DHCP agents per network, modify the dhcp_agents_per_network parameter in
the /etc/neutron/neutron.conf file. By default this is set to 1. To achieve high availability, assign more
than one DHCP agent per network. For more information, see High-availability for DHCP.
Run Networking L3 agent
The Networking (neutron) service L3 agent is scalable, due to the scheduler that supports Virtual Router Redundancy
Protocol (VRRP) to distribute virtual routers across multiple nodes. For more information about the
VRRP and keepalived, see Linux bridge: High availability using VRRP and Open vSwitch: High availability
using VRRP.
To enable high availability for configured routers, edit the /etc/neutron/neutron.conf file to set the following
values:
Configuring the networking services 39
High Availability Guide (Release Version: 15.0.0)
Table 1: /etc/neutron/neutron.conf parameters for high availability
Parameter Value Description
l3_ha True All routers are highly available by default.
allow_automatic_l3agent_failover True Set automatic L3 agent failover for routers
max_l3_agents_per_router 2 or more Maximum number of network nodes to use for the HA
router.
min_l3_agents_per_router 2 or more Minimum number of network nodes to use for the HA
router. A new router can be created only if this number
of network nodes are available.
Configure networking on each node. See the basic information about configuring networking in the Networking
service section of the Install Tutorials and Guides, depending on your distribution.
OpenStack network nodes contain:
• Networking DHCP agent
• Neutron L3 agent
• Networking L2 agent
Note: The L2 agent cannot be distributed and highly available. Instead, it must be installed on each
data forwarding node to control the virtual network driver such as Open vSwitch or Linux Bridge. One
L2 agent runs per node and controls its virtual interfaces.
Note: For Liberty, you can not have the standalone network nodes. The Networking services are run on the
controller nodes. In this guide, the term network nodes is used for convenience.
Configuring storage
Highly available Image API
The OpenStack Image service offers a service for discovering, registering, and retrieving virtual machine images.
To make the OpenStack Image API service highly available in active/passive mode, you must:
• Add OpenStack Image API resource to Pacemaker
• Configure OpenStack Image service API
• Configure OpenStack services to use the highly available OpenStack Image API
Prerequisites
Before beginning, ensure that you are familiar with the documentation for installing the OpenStack Image API
service. See the Image service section in the Installation Tutorials and Guides, depending on your distribution.
40 Configuring storage
High Availability Guide (Release Version: 15.0.0)
Add OpenStack Image API resource to Pacemaker
1. Download the resource agent to your system:
# cd /usr/lib/ocf/resource.d/openstack
# wget https://git.openstack.org/cgit/openstack/openstack-resource-agents/plain/ocf/
,→glance-api
# chmod a+rx *
2. Add the Pacemaker configuration for the OpenStack Image API resource. Use the following command
to connect to the Pacemaker cluster:
crm configure
Note: The crm configure command supports batch input. Copy and paste the lines in the next step
into your live Pacemaker configuration and then make changes as required.
For example, you may enter edit p_ip_glance-api from the crm configure menu and edit the
resource to match your preferred virtual IP address.
3. Add the following cluster resources:
primitive p_glance-api ocf:openstack:glance-api \
params config="/etc/glance/glance-api.conf" \
os_password="secretsecret" \
os_username="admin" os_tenant_name="admin" \
os_auth_url="http://10.0.0.11:5000/v2.0/" \
op monitor interval="30s" timeout="30s"
This configuration creates p_glance-api, a resource for managing the OpenStack Image API service.
4. Commit your configuration changes by entering the following command from the crm configure
menu:
commit
Pacemaker then starts the OpenStack Image API service and its dependent resources on one of your nodes.
Configure OpenStack Image service API
Edit the /etc/glance/glance-api.conf file to configure the OpenStack Image service:
# We have to use MySQL connection to store data:
sql_connection=mysql://glance:password@10.0.0.11/glance
# Alternatively, you can switch to pymysql,
# a new Python 3 compatible library and use
# sql_connection=mysql+pymysql://glance:password@10.0.0.11/glance
# and be ready when everything moves to Python 3.
# Ref: https://wiki.openstack.org/wiki/PyMySQL_evaluation
# We bind OpenStack Image API to the VIP:
bind_host = 10.0.0.11
# Connect to OpenStack Image registry service:
Configuring storage 41
High Availability Guide (Release Version: 15.0.0)
registry_host = 10.0.0.11
# We send notifications to High Available RabbitMQ:
notifier_strategy = rabbit
rabbit_host = 10.0.0.11
[TODO: need more discussion of these parameters]
Configure OpenStack services to use the highly available OpenStack Image API
Your OpenStack services must now point their OpenStack Image API configuration to the highly available,
virtual cluster IP address instead of pointing to the physical IP address of an OpenStack Image API server as
you would in a non-HA cluster.
For example, if your OpenStack Image API service IP address is 10.0.0.11 (as in the configuration explained
here), you would use the following configuration in your nova.conf file:
[glance]
# ...
api_servers = 10.0.0.11
# ...
You must also create the OpenStack Image API endpoint with this IP address. If you are using both private and
public IP addresses, create two virtual IP addresses and define your endpoint. For example:
$ openstack endpoint create --region $KEYSTONE_REGION \
image public http://PUBLIC_VIP:9292
$ openstack endpoint create --region $KEYSTONE_REGION \
image admin http://10.0.0.11:9292
$ openstack endpoint create --region $KEYSTONE_REGION \
image internal http://10.0.0.11:9292
Highly available Block Storage API
Cinder provides Block-Storage-as-a-Service suitable for performance sensitive scenarios such as databases,
expandable file systems, or providing a server with access to raw block level storage.
Persistent block storage can survive instance termination and can also be moved across instances like any
external storage device. Cinder also has volume snapshots capability for backing up the volumes.
Making the Block Storage API service highly available in active/passive mode involves:
• Add Block Storage API resource to Pacemaker
• Configure Block Storage API service
• Configure OpenStack services to use the highly available Block Storage API
In theory, you can run the Block Storage service as active/active. However, because of sufficient concerns, we
recommend running the volume component as active/passive only.
You can read more about these concerns on the Red Hat Bugzilla and there is a psuedo roadmap for addressing
them upstream.
42 Configuring storage
High Availability Guide (Release Version: 15.0.0)
Add Block Storage API resource to Pacemaker
On RHEL-based systems, create resources for cinder’s systemd agents and create constraints to enforce
startup/shutdown ordering:
pcs resource create openstack-cinder-api systemd:openstack-cinder-api --clone
,→interleave=true
pcs resource create openstack-cinder-scheduler systemd:openstack-cinder-scheduler --clone
,→interleave=true
pcs resource create openstack-cinder-volume systemd:openstack-cinder-volume
pcs constraint order start openstack-cinder-api-clone then openstack-cinder-scheduler-clone
pcs constraint colocation add openstack-cinder-scheduler-clone with openstack-cinder-api-
,→clone
pcs constraint order start openstack-cinder-scheduler-clone then openstack-cinder-volume
pcs constraint colocation add openstack-cinder-volume with openstack-cinder-scheduler-clone
If the Block Storage service runs on the same nodes as the other services, then it is advisable to also include:
pcs constraint order start openstack-keystone-clone then openstack-cinder-api-clone
Alternatively, instead of using systemd agents, download and install the OCF resource agent:
# cd /usr/lib/ocf/resource.d/openstack
# wget https://git.openstack.org/cgit/openstack/openstack-resource-agents/plain/ocf/cinder-
,→api
# chmod a+rx *
You can now add the Pacemaker configuration for Block Storage API resource. Connect to the Pacemaker
cluster with the crm configure command and add the following cluster resources:
primitive p_cinder-api ocf:openstack:cinder-api \
params config="/etc/cinder/cinder.conf" \
os_password="secretsecret" \
os_username="admin" \
os_tenant_name="admin" \
keystone_get_token_url="http://10.0.0.11:5000/v2.0/tokens" \
op monitor interval="30s" timeout="30s"
This configuration creates p_cinder-api, a resource for managing the Block Storage API service.
The command crm configure supports batch input, copy and paste the lines above into your live Pacemaker
configuration and then make changes as required. For example, you may enter edit p_ip_cinder-api from
the crm configure menu and edit the resource to match your preferred virtual IP address.
Once completed, commit your configuration changes by entering commit from the crm configure menu.
Pacemaker then starts the Block Storage API service and its dependent resources on one of your nodes.
Configure Block Storage API service
Edit the /etc/cinder/cinder.conf file. For example, on a RHEL-based system:
1 [DEFAULT]
2 # This is the name which we should advertise ourselves as and for
3 # A/P installations it should be the same everywhere
4 host = cinder-cluster-1
Configuring storage 43
High Availability Guide (Release Version: 15.0.0)
5
6 # Listen on the Block Storage VIP
7 osapi_volume_listen = 10.0.0.11
8
9 auth_strategy = keystone
10 control_exchange = cinder
11
12 volume_driver = cinder.volume.drivers.nfs.NfsDriver
13 nfs_shares_config = /etc/cinder/nfs_exports
14 nfs_sparsed_volumes = true
15 nfs_mount_options = v3
16
17 [database]
18 sql_connection = mysql://cinder:CINDER_DBPASS@10.0.0.11/cinder
19 max_retries = -1
20
21 [keystone_authtoken]
22 # 10.0.0.11 is the Keystone VIP
23 identity_uri = http://10.0.0.11:35357/
24 auth_uri = http://10.0.0.11:5000/
25 admin_tenant_name = service
26 admin_user = cinder
27 admin_password = CINDER_PASS
28
29 [oslo_messaging_rabbit]
30 # Explicitly list the rabbit hosts as it doesn't play well with HAProxy
31 rabbit_hosts = 10.0.0.12,10.0.0.13,10.0.0.14
32 # As a consequence, we also need HA queues
33 rabbit_ha_queues = True
34 heartbeat_timeout_threshold = 60
35 heartbeat_rate = 2
Replace CINDER_DBPASS with the password you chose for the Block Storage database. Replace CINDER_PASS
with the password you chose for the cinder user in the Identity service.
This example assumes that you are using NFS for the physical storage, which will almost never be true in a
production installation.
If you are using the Block Storage service OCF agent, some settings will be filled in for you, resulting in a
shorter configuration file:
1 # We have to use MySQL connection to store data:
2 sql_connection = mysql://cinder:CINDER_DBPASS@10.0.0.11/cinder
3 # Alternatively, you can switch to pymysql,
4 # a new Python 3 compatible library and use
5 # sql_connection = mysql+pymysql://cinder:CINDER_DBPASS@10.0.0.11/cinder
6 # and be ready when everything moves to Python 3.
7 # Ref: https://wiki.openstack.org/wiki/PyMySQL_evaluation
8
9 # We bind Block Storage API to the VIP:
10 osapi_volume_listen = 10.0.0.11
11
12 # We send notifications to High Available RabbitMQ:
13 notifier_strategy = rabbit
14 rabbit_host = 10.0.0.11
Replace CINDER_DBPASS with the password you chose for the Block Storage database.
44 Configuring storage
High Availability Guide (Release Version: 15.0.0)
Configure OpenStack services to use the highly available Block Storage API
Your OpenStack services must now point their Block Storage API configuration to the highly available, virtual
cluster IP address rather than a Block Storage API server’s physical IP address as you would for a non-HA
environment.
Create the Block Storage API endpoint with this IP.
If you are using both private and public IP addresses, create two virtual IPs and define your endpoint. For
example:
$ openstack endpoint create volume --region $KEYSTONE_REGION \
--publicurl 'http://PUBLIC_VIP:8776/v1/%(tenant_id)s' \
--adminurl 'http://10.0.0.11:8776/v1/%(tenant_id)s' \
--internalurl 'http://10.0.0.11:8776/v1/%(tenant_id)s'
Highly available Shared File Systems API
Making the Shared File Systems (manila) API service highly available in active/passive mode involves:
• Add Shared File Systems API resource to Pacemaker
• Configure Shared File Systems API service
• Configure OpenStack services to use HA Shared File Systems API
Add Shared File Systems API resource to Pacemaker
1. Download the resource agent to your system:
# cd /usr/lib/ocf/resource.d/openstack
# wget https://git.openstack.org/cgit/openstack/openstack-resource-agents/plain/ocf/
,→manila-api
# chmod a+rx *
2. Add the Pacemaker configuration for the Shared File Systems API resource. Connect to the Pacemaker
cluster with the following command:
# crm configure
Note: The crm configure supports batch input. Copy and paste the lines in the next step into your
live Pacemaker configuration and then make changes as required.
For example, you may enter edit p_ip_manila-api from the crm configure menu and edit the
resource to match your preferred virtual IP address.
3. Add the following cluster resources:
primitive p_manila-api ocf:openstack:manila-api \
params config="/etc/manila/manila.conf" \
os_password="secretsecret" \
os_username="admin" \
os_tenant_name="admin" \
Configuring storage 45
High Availability Guide (Release Version: 15.0.0)
keystone_get_token_url="http://10.0.0.11:5000/v2.0/tokens" \
op monitor interval="30s" timeout="30s"
This configuration creates p_manila-api, a resource for managing the Shared File Systems API service.
4. Commit your configuration changes by entering the following command from the crm configure
menu:
# commit
Pacemaker now starts the Shared File Systems API service and its dependent resources on one of your nodes.
Configure Shared File Systems API service
Edit the /etc/manila/manila.conf file:
1 # We have to use MySQL connection to store data:
2 sql_connection = mysql+pymysql://manila:password@10.0.0.11/manila?charset=utf8
3
4 # We bind Shared File Systems API to the VIP:
5 osapi_volume_listen = 10.0.0.11
6
7 # We send notifications to High Available RabbitMQ:
8 notifier_strategy = rabbit
9 rabbit_host = 10.0.0.11
Configure OpenStack services to use HA Shared File Systems API
Your OpenStack services must now point their Shared File Systems API configuration to the highly available,
virtual cluster IP address rather than a Shared File Systems API server’s physical IP address as you would for
a non-HA environment.
You must create the Shared File Systems API endpoint with this IP.
If you are using both private and public IP addresses, you should create two virtual IPs and define your endpoints
like this:
$ openstack endpoint create --region RegionOne \
sharev2 public 'http://PUBLIC_VIP:8786/v2/%(tenant_id)s'
$ openstack endpoint create --region RegionOne \
sharev2 internal 'http://10.0.0.11:8786/v2/%(tenant_id)s'
$ openstack endpoint create --region RegionOne \
sharev2 admin 'http://10.0.0.11:8786/v2/%(tenant_id)s'
Storage back end
An OpenStack environment includes multiple data pools for the VMs:
• Ephemeral storage is allocated for an instance and is deleted when the instance is deleted. The Compute
service manages ephemeral storage and by default, Compute stores ephemeral drives as files on
46 Configuring storage
High Availability Guide (Release Version: 15.0.0)
local disks on the compute node. As an alternative, you can use Ceph RBD as the storage back end for
ephemeral storage.
• Persistent storage exists outside all instances. Two types of persistent storage are provided:
– The Block Storage service (cinder) that can use LVM or Ceph RBD as the storage back end.
– The Image service (glance) that can use the Object Storage service (swift) or Ceph RBD as the
storage back end.
For more information about configuring storage back ends for the different storage options, see Manage volumes
in the OpenStack Administrator Guide.
This section discusses ways to protect against data loss in your OpenStack environment.
RAID drives
Configuring RAID on the hard drives that implement storage protects your data against a hard drive failure. If
the node itself fails, data may be lost. In particular, all volumes stored on an LVM node can be lost.
Ceph
Ceph RBD is an innately high availability storage back end. It creates a storage cluster with multiple nodes
that communicate with each other to replicate and redistribute data dynamically. A Ceph RBD storage cluster
provides a single shared set of storage nodes that can handle all classes of persistent and ephemeral data (glance,
cinder, and nova) that are required for OpenStack instances.
Ceph RBD provides object replication capabilities by storing Block Storage volumes as Ceph RBD objects.
Ceph RBD ensures that each replica of an object is stored on a different node. This means that your volumes
are protected against hard drive and node failures, or even the failure of the data center itself.
When Ceph RBD is used for ephemeral volumes as well as block and image storage, it supports live migration
of VMs with ephemeral drives. LVM only supports live migration of volume-backed VMs.
Making the Block Storage (cinder) API service highly available in active/active mode involves:
• Configuring Block Storage to listen on the VIP address
• Managing the Block Storage API daemon with the Pacemaker cluster manager
• Configuring OpenStack services to use this IP address
Configuring the compute node
The Installation Tutorials and Guides provide instructions for installing multiple compute nodes. To make the
compute nodes highly available, you must configure the environment to include multiple instances of the API
and other services.
Configuring high availability for instances
As of September 2016, the OpenStack High Availability community is designing and developing an official
and unified way to provide high availability for instances. We are developing automatic recovery from failures
of hardware or hypervisor-related software on the compute node, or other failures that could prevent instances
from functioning correctly, such as, issues with a cinder volume I/O path.
Configuring the compute node 47
High Availability Guide (Release Version: 15.0.0)
More details are available in the user story co-authored by OpenStack’s HA community and Product Working
Group (PWG), where this feature is identified as missing functionality in OpenStack, which should be addressed
with high priority.
Existing solutions
The architectural challenges of instance HA and several currently existing solutions were presented in a talk at
the Austin summit, for which slides are also available.
The code for three of these solutions can be found online at the following links:
• a mistral-based auto-recovery workflow, by Intel
• masakari, by NTT
• OCF RAs, as used by Red Hat and SUSE
Current upstream work
Work is in progress on a unified approach, which combines the best aspects of existing upstream solutions.
More details are available on the HA VMs user story wiki.
To get involved with this work, see the section on the HA community.
Appendix
HA community
The OpenStack HA community holds weekly IRC meetings to discuss a range of topics relating to HA in
OpenStack. Everyone interested is encouraged to attend. The logs of all previous meetings are available to
read.
You can contact the HA community directly in the #openstack-ha channel on Freenode IRC, or by sending mail
to the openstack-dev or openstack-docs mailing list with the [HA] prefix in the Subject header.
Community support
The following resources are available to help you run and use OpenStack. The OpenStack community constantly
improves and adds to the main features of OpenStack, but if you have any questions, do not hesitate to ask. Use
the following resources to get OpenStack support and troubleshoot your installations.
Documentation
For the available OpenStack documentation, see docs.openstack.org.
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at
OpenStack Documentation Mailing List, join our IRC channel #openstack-doc on the freenode IRC network,
or report a bug.
The following books explain how to install an OpenStack cloud and its associated components:
• Installation Tutorial for openSUSE Leap 42.2 and SUSE Linux Enterprise Server 12 SP2
48 Appendix
High Availability Guide (Release Version: 15.0.0)
• Installation Tutorial for Red Hat Enterprise Linux 7 and CentOS 7
• Installation Tutorial for Ubuntu 16.04 (LTS)
The following books explain how to configure and run an OpenStack cloud:
• Architecture Design Guide
• Administrator Guide
• Configuration Reference
• Operations Guide
• Networking Guide
• High Availability Guide
• Security Guide
• Virtual Machine Image Guide
The following books explain how to use the OpenStack Dashboard and command-line clients:
• End User Guide
• Command-Line Interface Reference
The following documentation provides reference and guidance information for the OpenStack APIs:
• API Guide
The following guide provides how to contribute to OpenStack documentation:
• Documentation Contributor Guide
ask.openstack.org
During the set up or testing of OpenStack, you might have questions about how a specific task is completed or
be in a situation where a feature does not work correctly. Use the ask.openstack.org site to ask questions and
get answers. When you visit the Ask OpenStack site, scan the recently asked questions to see whether your
question has already been answered. If not, ask a new question. Be sure to give a clear, concise summary in the
title and provide as much detail as possible in the description. Paste in your command output or stack traces,
links to screen shots, and any other information which might be useful.
OpenStack mailing lists
A great way to get answers and insights is to post your question or problematic scenario to the OpenStack
mailing list. You can learn from and help others who might have similar issues. To subscribe or view the
archives, go to the general OpenStack mailing list. If you are interested in the other mailing lists for specific
projects or development, refer to Mailing Lists.
The OpenStack wiki
The OpenStack wiki contains a broad range of topics but some of the information can be difficult to find or is a
few pages deep. Fortunately, the wiki search feature enables you to search by title or content. If you search for
specific information, such as about networking or OpenStack Compute, you can find a large amount of relevant
Appendix 49
High Availability Guide (Release Version: 15.0.0)
material. More is being added all the time, so be sure to check back often. You can find the search box in the
upper-right corner of any OpenStack wiki page.
The Launchpad Bugs area
The OpenStack community values your set up and testing efforts and wants your feedback. To log a bug, you
must sign up for a Launchpad account. You can view existing bugs and report bugs in the Launchpad Bugs
area. Use the search feature to determine whether the bug has already been reported or already been fixed. If it
still seems like your bug is unreported, fill out a bug report.
Some tips:
• Give a clear, concise summary.
• Provide as much detail as possible in the description. Paste in your command output or stack traces, links
to screen shots, and any other information which might be useful.
• Be sure to include the software and package versions that you are using, especially
if you are using a development branch, such as, "Kilo release" vs git commit
bc79c3ecc55929bac585d04a03475b72e06a3208.
• Any deployment-specific information is helpful, such as whether you are using Ubuntu 14.04 or are
performing a multi-node installation.
The following Launchpad Bugs areas are available:
• Bugs: OpenStack Block Storage (cinder)
• Bugs: OpenStack Compute (nova)
• Bugs: OpenStack Dashboard (horizon)
• Bugs: OpenStack Identity (keystone)
• Bugs: OpenStack Image service (glance)
• Bugs: OpenStack Networking (neutron)
• Bugs: OpenStack Object Storage (swift)
• Bugs: Application catalog (murano)
• Bugs: Bare metal service (ironic)
• Bugs: Clustering service (senlin)
• Bugs: Container Infrastructure Management service (magnum)
• Bugs: Data processing service (sahara)
• Bugs: Database service (trove)
• Bugs: Deployment service (fuel)
• Bugs: DNS service (designate)
• Bugs: Key Manager Service (barbican)
• Bugs: Monitoring (monasca)
• Bugs: Orchestration (heat)
• Bugs: Rating (cloudkitty)
50 Appendix
High Availability Guide (Release Version: 15.0.0)
• Bugs: Shared file systems (manila)
• Bugs: Telemetry (ceilometer)
• Bugs: Telemetry v3 (gnocchi)
• Bugs: Workflow service (mistral)
• Bugs: Messaging service (zaqar)
• Bugs: OpenStack API Documentation (developer.openstack.org)
• Bugs: OpenStack Documentation (docs.openstack.org)
The OpenStack IRC channel
The OpenStack community lives in the #openstack IRC channel on the Freenode network. You can hang out, ask
questions, or get immediate feedback for urgent and pressing issues. To install an IRC client or use a browserbased
client, go to https://webchat.freenode.net/. You can also use Colloquy (Mac OS X), mIRC (Windows),
or XChat (Linux). When you are in the IRC channel and want to share code or command output, the generally
accepted method is to use a Paste Bin. The OpenStack project has one at Paste. Just paste your longer amounts
of text or logs in the web form and you get a URL that you can paste into the channel. The OpenStack IRC
channel is #openstack on irc.freenode.net. You can find a list of all OpenStack IRC channels on the IRC
page on the wiki.
Documentation feedback
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at
OpenStack Documentation Mailing List, or report a bug.
OpenStack distribution packages
The following Linux distributions provide community-supported packages for OpenStack:
• Debian: https://wiki.debian.org/OpenStack
• CentOS, Fedora, and Red Hat Enterprise Linux: https://www.rdoproject.org/
• openSUSE and SUSE Linux Enterprise Server: https://en.opensuse.org/Portal:OpenStack
• Ubuntu: https://wiki.ubuntu.com/ServerTeam/CloudArchive
Glossary
This glossary offers a list of terms and definitions to define a vocabulary for OpenStack-related concepts.
To add to OpenStack glossary, clone the openstack/openstack-manuals repository and update the source file
doc/common/glossary.rst through the OpenStack contribution process.
0-9
6to4 A mechanism that allows IPv6 packets to be transmitted over an IPv4 network, providing a strategy for
migrating to IPv6.
Appendix 51
High Availability Guide (Release Version: 15.0.0)
A
absolute limit Impassable limits for guest VMs. Settings include total RAM size, maximum number of vCPUs,
and maximum disk size.
access control list (ACL) A list of permissions attached to an object. An ACL specifies which users or system
processes have access to objects. It also defines which operations can be performed on specified objects.
Each entry in a typical ACL specifies a subject and an operation. For instance, the ACL entry (Alice,
delete) for a file gives Alice permission to delete the file.
access key Alternative term for an Amazon EC2 access key. See EC2 access key.
account The Object Storage context of an account. Do not confuse with a user account from an authentication
service, such as Active Directory, /etc/passwd, OpenLDAP, OpenStack Identity, and so on.
account auditor Checks for missing replicas and incorrect or corrupted objects in a specified Object Storage
account by running queries against the back-end SQLite database.
account database A SQLite database that contains Object Storage accounts and related metadata and that the
accounts server accesses.
account reaper An Object Storage worker that scans for and deletes account databases and that the account
server has marked for deletion.
account server Lists containers in Object Storage and stores container information in the account database.
account service An Object Storage component that provides account services such as list, create, modify, and
audit. Do not confuse with OpenStack Identity service, OpenLDAP, or similar user-account services.
accounting The Compute service provides accounting information through the event notification and system
usage data facilities.
Active Directory Authentication and identity service by Microsoft, based on LDAP. Supported in OpenStack.
active/active configuration In a high-availability setup with an active/active configuration, several systems
share the load together and if one fails, the load is distributed to the remaining systems.
active/passive configuration In a high-availability setup with an active/passive configuration, systems are set
up to bring additional resources online to replace those that have failed.
address pool A group of fixed and/or floating IP addresses that are assigned to a project and can be used by
or assigned to the VM instances in a project.
Address Resolution Protocol (ARP) The protocol by which layer-3 IP addresses are resolved into layer-2
link local addresses.
admin API A subset of API calls that are accessible to authorized administrators and are generally not accessible
to end users or the public Internet. They can exist as a separate service (keystone) or can be a
subset of another API (nova).
admin server In the context of the Identity service, the worker process that provides access to the admin API.
administrator The person responsible for installing, configuring, and managing an OpenStack cloud.
Advanced Message Queuing Protocol (AMQP) The open standard messaging protocol used by OpenStack
components for intra-service communications, provided by RabbitMQ, Qpid, or ZeroMQ.
Advanced RISC Machine (ARM) Lower power consumption CPU often found in mobile and embedded
devices. Supported by OpenStack.
52 Appendix
High Availability Guide (Release Version: 15.0.0)
alert The Compute service can send alerts through its notification system, which includes a facility to create
custom notification drivers. Alerts can be sent to and displayed on the dashboard.
allocate The process of taking a floating IP address from the address pool so it can be associated with a fixed
IP on a guest VM instance.
Amazon Kernel Image (AKI) Both a VM container format and disk format. Supported by Image service.
Amazon Machine Image (AMI) Both a VM container format and disk format. Supported by Image service.
Amazon Ramdisk Image (ARI) Both a VM container format and disk format. Supported by Image service.
Anvil A project that ports the shell script-based project named DevStack to Python.
aodh Part of the OpenStack Telemetry service; provides alarming functionality.
Apache The Apache Software Foundation supports the Apache community of open-source software projects.
These projects provide software products for the public good.
Apache License 2.0 All OpenStack core projects are provided under the terms of the Apache License 2.0
license.
Apache Web Server The most common web server software currently used on the Internet.
API endpoint The daemon, worker, or service that a client communicates with to access an API. API endpoints
can provide any number of services, such as authentication, sales data, performance meters, Compute
VM commands, census data, and so on.
API extension Custom modules that extend some OpenStack core APIs.
API extension plug-in Alternative term for a Networking plug-in or Networking API extension.
API key Alternative term for an API token.
API server Any node running a daemon or worker that provides an API endpoint.
API token Passed to API requests and used by OpenStack to verify that the client is authorized to run the
requested operation.
API version In OpenStack, the API version for a project is part of the URL. For example, example.com/
nova/v1/foobar.
applet A Java program that can be embedded into a web page.
Application Catalog service (murano) The project that provides an application catalog service so that users
can compose and deploy composite environments on an application abstraction level while managing
the application lifecycle.
Application Programming Interface (API) A collection of specifications used to access a service, application,
or program. Includes service calls, required parameters for each call, and the expected return
values.
application server A piece of software that makes available another piece of software over a network.
Application Service Provider (ASP) Companies that rent specialized applications that help businesses and
organizations provide additional services with lower cost.
arptables Tool used for maintaining Address Resolution Protocol packet filter rules in the Linux kernel firewall
modules. Used along with iptables, ebtables, and ip6tables in Compute to provide firewall services
for VMs.
associate The process associating a Compute floating IP address with a fixed IP address.
Appendix 53
High Availability Guide (Release Version: 15.0.0)
Asynchronous JavaScript and XML (AJAX) A group of interrelated web development techniques used on
the client-side to create asynchronous web applications. Used extensively in horizon.
ATA over Ethernet (AoE) A disk storage protocol tunneled within Ethernet.
attach The process of connecting a VIF or vNIC to a L2 network in Networking. In the context of Compute,
this process connects a storage volume to an instance.
attachment (network) Association of an interface ID to a logical port. Plugs an interface into a port.
auditing Provided in Compute through the system usage data facility.
auditor A worker process that verifies the integrity of Object Storage objects, containers, and accounts. Auditors
is the collective term for the Object Storage account auditor, container auditor, and object auditor.
Austin The code name for the initial release of OpenStack. The first design summit took place in Austin,
Texas, US.
auth node Alternative term for an Object Storage authorization node.
authentication The process that confirms that the user, process, or client is really who they say they are
through private key, secret token, password, fingerprint, or similar method.
authentication token A string of text provided to the client after authentication. Must be provided by the user
or process in subsequent requests to the API endpoint.
AuthN The Identity service component that provides authentication services.
authorization The act of verifying that a user, process, or client is authorized to perform an action.
authorization node An Object Storage node that provides authorization services.
AuthZ The Identity component that provides high-level authorization services.
Auto ACK Configuration setting within RabbitMQ that enables or disables message acknowledgment. Enabled
by default.
auto declare A Compute RabbitMQ setting that determines whether a message exchange is automatically
created when the program starts.
availability zone An Amazon EC2 concept of an isolated area that is used for fault tolerance. Do not confuse
with an OpenStack Compute zone or cell.
AWS CloudFormation template AWS CloudFormation allows Amazon Web Services (AWS) users to create
and manage a collection of related resources. The Orchestration service supports a CloudFormationcompatible
format (CFN).
B
back end Interactions and processes that are obfuscated from the user, such as Compute volume mount, data
transmission to an iSCSI target by a daemon, or Object Storage object integrity checks.
back-end catalog The storage method used by the Identity service catalog service to store and retrieve information
about API endpoints that are available to the client. Examples include an SQL database, LDAP
database, or KVS back end.
back-end store The persistent data store used to save and retrieve information for a service, such as lists of
Object Storage objects, current state of guest VMs, lists of user names, and so on. Also, the method that
the Image service uses to get and store VM images. Options include Object Storage, locally mounted
file system, RADOS block devices, VMware datastore, and HTTP.
54 Appendix
High Availability Guide (Release Version: 15.0.0)
Backup, Restore, and Disaster Recovery service (freezer) The project that provides integrated tooling for
backing up, restoring, and recovering file systems, instances, or database backups.
bandwidth The amount of available data used by communication resources, such as the Internet. Represents
the amount of data that is used to download things or the amount of data available to download.
barbican Code name of the Key Manager service.
bare An Image service container format that indicates that no container exists for the VM image.
Bare Metal service (ironic) The OpenStack service that provides a service and associated libraries capable
of managing and provisioning physical machines in a security-aware and fault-tolerant manner.
base image An OpenStack-provided image.
Bell-LaPadula model A security model that focuses on data confidentiality and controlled access to classified
information. This model divides the entities into subjects and objects. The clearance of a subject is
compared to the classification of the object to determine if the subject is authorized for the specific
access mode. The clearance or classification scheme is expressed in terms of a lattice.
Benchmark service (rally) OpenStack project that provides a framework for performance analysis and benchmarking
of individual OpenStack components as well as full production OpenStack cloud deployments.
Bexar A grouped release of projects related to OpenStack that came out in February of 2011. It included only
Compute (nova) and Object Storage (swift). Bexar is the code name for the second release of OpenStack.
The design summit took place in San Antonio, Texas, US, which is the county seat for Bexar county.
binary Information that consists solely of ones and zeroes, which is the language of computers.
bit A bit is a single digit number that is in base of 2 (either a zero or one). Bandwidth usage is measured in
bits per second.
bits per second (BPS) The universal measurement of how quickly data is transferred from place to place.
block device A device that moves data in the form of blocks. These device nodes interface the devices, such
as hard disks, CD-ROM drives, flash drives, and other addressable regions of memory.
block migration A method of VM live migration used by KVM to evacuate instances from one host to another
with very little downtime during a user-initiated switchover. Does not require shared storage. Supported
by Compute.
Block Storage API An API on a separate endpoint for attaching, detaching, and creating block storage for
compute VMs.
Block Storage service (cinder) The OpenStack service that implement services and libraries to provide ondemand,
self-service access to Block Storage resources via abstraction and automation on top of other
block storage devices.
BMC (Baseboard Management Controller) The intelligence in the IPMI architecture, which is a specialized
micro-controller that is embedded on the motherboard of a computer and acts as a server. Manages the
interface between system management software and platform hardware.
bootable disk image A type of VM image that exists as a single, bootable file.
Bootstrap Protocol (BOOTP) A network protocol used by a network client to obtain an IP address from a
configuration server. Provided in Compute through the dnsmasq daemon when using either the FlatDHCP
manager or VLAN manager network manager.
Border Gateway Protocol (BGP) The Border Gateway Protocol is a dynamic routing protocol that connects
autonomous systems. Considered the backbone of the Internet, this protocol connects disparate networks
to form a larger network.
Appendix 55
High Availability Guide (Release Version: 15.0.0)
browser Any client software that enables a computer or device to access the Internet.
builder file Contains configuration information that Object Storage uses to reconfigure a ring or to re-create
it from scratch after a serious failure.
bursting The practice of utilizing a secondary environment to elastically build instances on-demand when the
primary environment is resource constrained.
button class A group of related button types within horizon. Buttons to start, stop, and suspend VMs are in
one class. Buttons to associate and disassociate floating IP addresses are in another class, and so on.
byte Set of bits that make up a single character; there are usually 8 bits to a byte.
C
cache pruner A program that keeps the Image service VM image cache at or below its configured maximum
size.
Cactus An OpenStack grouped release of projects that came out in the spring of 2011. It included Compute
(nova), Object Storage (swift), and the Image service (glance). Cactus is a city in Texas, US and is the
code name for the third release of OpenStack. When OpenStack releases went from three to six months
long, the code name of the release changed to match a geography nearest the previous summit.
CALL One of the RPC primitives used by the OpenStack message queue software. Sends a message and
waits for a response.
capability Defines resources for a cell, including CPU, storage, and networking. Can apply to the specific
services within a cell or a whole cell.
capacity cache A Compute back-end database table that contains the current workload, amount of free RAM,
and number of VMs running on each host. Used to determine on which host a VM starts.
capacity updater A notification driver that monitors VM instances and updates the capacity cache as needed.
CAST One of the RPC primitives used by the OpenStack message queue software. Sends a message and does
not wait for a response.
catalog A list of API endpoints that are available to a user after authentication with the Identity service.
catalog service An Identity service that lists API endpoints that are available to a user after authentication
with the Identity service.
ceilometer Part of the OpenStack Telemetry service; gathers and stores metrics from other OpenStack services.
cell Provides logical partitioning of Compute resources in a child and parent relationship. Requests are passed
from parent cells to child cells if the parent cannot provide the requested resource.
cell forwarding A Compute option that enables parent cells to pass resource requests to child cells if the parent
cannot provide the requested resource.
cell manager The Compute component that contains a list of the current capabilities of each host within the
cell and routes requests as appropriate.
CentOS A Linux distribution that is compatible with OpenStack.
Ceph Massively scalable distributed storage system that consists of an object store, block store, and POSIXcompatible
distributed file system. Compatible with OpenStack.
CephFS The POSIX-compliant file system provided by Ceph.
56 Appendix
High Availability Guide (Release Version: 15.0.0)
certificate authority (CA) In cryptography, an entity that issues digital certificates. The digital certificate
certifies the ownership of a public key by the named subject of the certificate. This enables others
(relying parties) to rely upon signatures or assertions made by the private key that corresponds to the
certified public key. In this model of trust relationships, a CA is a trusted third party for both the subject
(owner) of the certificate and the party relying upon the certificate. CAs are characteristic of many public
key infrastructure (PKI) schemes. In OpenStack, a simple certificate authority is provided by Compute
for cloudpipe VPNs and VM image decryption.
Challenge-Handshake Authentication Protocol (CHAP) An iSCSI authentication method supported by
Compute.
chance scheduler A scheduling method used by Compute that randomly chooses an available host from the
pool.
changes since A Compute API parameter that downloads changes to the requested item since your last request,
instead of downloading a new, fresh set of data and comparing it against the old data.
Chef An operating system configuration management tool supporting OpenStack deployments.
child cell If a requested resource such as CPU time, disk storage, or memory is not available in the parent
cell, the request is forwarded to its associated child cells. If the child cell can fulfill the request, it does.
Otherwise, it attempts to pass the request to any of its children.
cinder Codename for Block Storage service.
CirrOS A minimal Linux distribution designed for use as a test image on clouds such as OpenStack.
Cisco neutron plug-in A Networking plug-in for Cisco devices and technologies, including UCS and Nexus.
cloud architect A person who plans, designs, and oversees the creation of clouds.
Cloud Auditing Data Federation (CADF) Cloud Auditing Data Federation (CADF) is a specification for
audit event data. CADF is supported by OpenStack Identity.
cloud computing A model that enables access to a shared pool of configurable computing resources, such as
networks, servers, storage, applications, and services, that can be rapidly provisioned and released with
minimal management effort or service provider interaction.
cloud controller Collection of Compute components that represent the global state of the cloud; talks to services,
such as Identity authentication, Object Storage, and node/storage workers through a queue.
cloud controller node A node that runs network, volume, API, scheduler, and image services. Each service
may be broken out into separate nodes for scalability or availability.
Cloud Data Management Interface (CDMI) SINA standard that defines a RESTful API for managing objects
in the cloud, currently unsupported in OpenStack.
Cloud Infrastructure Management Interface (CIMI) An in-progress specification for cloud management.
Currently unsupported in OpenStack.
cloud-init A package commonly installed in VM images that performs initialization of an instance after boot
using information that it retrieves from the metadata service, such as the SSH public key and user data.
cloudadmin One of the default roles in the Compute RBAC system. Grants complete system access.
Cloudbase-Init A Windows project providing guest initialization features, similar to cloud-init.
cloudpipe A compute service that creates VPNs on a per-project basis.
cloudpipe image A pre-made VM image that serves as a cloudpipe server. Essentially, OpenVPN running on
Linux.
Appendix 57
High Availability Guide (Release Version: 15.0.0)
Clustering service (senlin) The project that implements clustering services and libraries for the management
of groups of homogeneous objects exposed by other OpenStack services.
command filter Lists allowed commands within the Compute rootwrap facility.
Common Internet File System (CIFS) A file sharing protocol. It is a public or open variation of the original
Server Message Block (SMB) protocol developed and used by Microsoft. Like the SMB protocol, CIFS
runs at a higher level and uses the TCP/IP protocol.
Common Libraries (oslo) The project that produces a set of python libraries containing code shared by OpenStack
projects. The APIs provided by these libraries should be high quality, stable, consistent, documented
and generally applicable.
community project A project that is not officially endorsed by the OpenStack Foundation. If the project is
successful enough, it might be elevated to an incubated project and then to a core project, or it might be
merged with the main code trunk.
compression Reducing the size of files by special encoding, the file can be decompressed again to its original
content. OpenStack supports compression at the Linux file system level but does not support compression
for things such as Object Storage objects or Image service VM images.
Compute API (Nova API) The nova-api daemon provides access to nova services. Can communicate with
other APIs, such as the Amazon EC2 API.
compute controller The Compute component that chooses suitable hosts on which to start VM instances.
compute host Physical host dedicated to running compute nodes.
compute node A node that runs the nova-compute daemon that manages VM instances that provide a wide
range of services, such as web applications and analytics.
Compute service (nova) The OpenStack core project that implements services and associated libraries to
provide massively-scalable, on-demand, self-service access to compute resources, including bare metal,
virtual machines, and containers.
compute worker The Compute component that runs on each compute node and manages the VM instance
lifecycle, including run, reboot, terminate, attach/detach volumes, and so on. Provided by the novacompute
daemon.
concatenated object A set of segment objects that Object Storage combines and sends to the client.
conductor In Compute, conductor is the process that proxies database requests from the compute process.
Using conductor improves security because compute nodes do not need direct access to the database.
congress Code name for the Governance service.
consistency window The amount of time it takes for a new Object Storage object to become accessible to all
clients.
console log Contains the output from a Linux VM console in Compute.
container Organizes and stores objects in Object Storage. Similar to the concept of a Linux directory but
cannot be nested. Alternative term for an Image service container format.
container auditor Checks for missing replicas or incorrect objects in specified Object Storage containers
through queries to the SQLite back-end database.
container database A SQLite database that stores Object Storage containers and container metadata. The
container server accesses this database.
58 Appendix
High Availability Guide (Release Version: 15.0.0)
container format A wrapper used by the Image service that contains a VM image and its associated metadata,
such as machine state, OS disk size, and so on.
Container Infrastructure Management service (magnum) The project which provides a set of services for
provisioning, scaling, and managing container orchestration engines.
container server An Object Storage server that manages containers.
container service The Object Storage component that provides container services, such as create, delete, list,
and so on.
content delivery network (CDN) A content delivery network is a specialized network that is used to distribute
content to clients, typically located close to the client for increased performance.
controller node Alternative term for a cloud controller node.
core API Depending on context, the core API is either the OpenStack API or the main API of a specific core
project, such as Compute, Networking, Image service, and so on.
core service An official OpenStack service defined as core by DefCore Committee. Currently, consists of
Block Storage service (cinder), Compute service (nova), Identity service (keystone), Image service
(glance), Networking service (neutron), and Object Storage service (swift).
cost Under the Compute distributed scheduler, this is calculated by looking at the capabilities of each host
relative to the flavor of the VM instance being requested.
credentials Data that is only known to or accessible by a user and used to verify that the user is who he says
he is. Credentials are presented to the server during authentication. Examples include a password, secret
key, digital certificate, and fingerprint.
CRL A Certificate Revocation List (CRL) in a PKI model is a list of certificates that have been revoked. End
entities presenting these certificates should not be trusted.
Cross-Origin Resource Sharing (CORS) A mechanism that allows many resources (for example, fonts,
JavaScript) on a web page to be requested from another domain outside the domain from which the
resource originated. In particular, JavaScript’s AJAX calls can use the XMLHttpRequest mechanism.
Crowbar An open source community project by SUSE that aims to provide all necessary services to quickly
deploy and manage clouds.
current workload An element of the Compute capacity cache that is calculated based on the number of build,
snapshot, migrate, and resize operations currently in progress on a given host.
customer Alternative term for project.
customization module A user-created Python module that is loaded by horizon to change the look and feel
of the dashboard.
D
daemon A process that runs in the background and waits for requests. May or may not listen on a TCP or
UDP port. Do not confuse with a worker.
Dashboard (horizon) OpenStack project which provides an extensible, unified, web-based user interface for
all OpenStack services.
data encryption Both Image service and Compute support encrypted virtual machine (VM) images (but not
instances). In-transit data encryption is supported in OpenStack using technologies such as HTTPS,
Appendix 59
High Availability Guide (Release Version: 15.0.0)
SSL, TLS, and SSH. Object Storage does not support object encryption at the application level but may
support storage that uses disk encryption.
Data loss prevention (DLP) software Software programs used to protect sensitive information and prevent
it from leaking outside a network boundary through the detection and denying of the data transportation.
Data Processing service (sahara) OpenStack project that provides a scalable data-processing stack and associated
management interfaces.
data store A database engine supported by the Database service.
database ID A unique ID given to each replica of an Object Storage database.
database replicator An Object Storage component that copies changes in the account, container, and object
databases to other nodes.
Database service (trove) An integrated project that provides scalable and reliable Cloud Database-as-aService
functionality for both relational and non-relational database engines.
deallocate The process of removing the association between a floating IP address and a fixed IP address. Once
this association is removed, the floating IP returns to the address pool.
Debian A Linux distribution that is compatible with OpenStack.
deduplication The process of finding duplicate data at the disk block, file, and/or object level to minimize
storage use—currently unsupported within OpenStack.
default panel The default panel that is displayed when a user accesses the dashboard.
default project New users are assigned to this project if no project is specified when a user is created.
default token An Identity service token that is not associated with a specific project and is exchanged for a
scoped token.
delayed delete An option within Image service so that an image is deleted after a predefined number of seconds
instead of immediately.
delivery mode Setting for the Compute RabbitMQ message delivery mode; can be set to either transient or
persistent.
denial of service (DoS) Denial of service (DoS) is a short form for denial-of-service attack. This is a malicious
attempt to prevent legitimate users from using a service.
deprecated auth An option within Compute that enables administrators to create and manage users through
the nova-manage command as opposed to using the Identity service.
designate Code name for the DNS service.
Desktop-as-a-Service A platform that provides a suite of desktop environments that users access to receive
a desktop experience from any location. This may provide general use, development, or even homogeneous
testing environments.
developer One of the default roles in the Compute RBAC system and the default role assigned to a new user.
device ID Maps Object Storage partitions to physical storage devices.
device weight Distributes partitions proportionately across Object Storage devices based on the storage capacity
of each device.
DevStack Community project that uses shell scripts to quickly build complete OpenStack development environments.
DHCP agent OpenStack Networking agent that provides DHCP services for virtual networks.
60 Appendix
High Availability Guide (Release Version: 15.0.0)
Diablo A grouped release of projects related to OpenStack that came out in the fall of 2011, the fourth release
of OpenStack. It included Compute (nova 2011.3), Object Storage (swift 1.4.3), and the Image service
(glance). Diablo is the code name for the fourth release of OpenStack. The design summit took place in
the Bay Area near Santa Clara, California, US and Diablo is a nearby city.
direct consumer An element of the Compute RabbitMQ that comes to life when a RPC call is executed. It
connects to a direct exchange through a unique exclusive queue, sends the message, and terminates.
direct exchange A routing table that is created within the Compute RabbitMQ during RPC calls; one is created
for each RPC call that is invoked.
direct publisher Element of RabbitMQ that provides a response to an incoming MQ message.
disassociate The process of removing the association between a floating IP address and fixed IP and thus
returning the floating IP address to the address pool.
Discretionary Access Control (DAC) Governs the ability of subjects to access objects, while enabling users
to make policy decisions and assign security attributes. The traditional UNIX system of users, groups,
and read-write-execute permissions is an example of DAC.
disk encryption The ability to encrypt data at the file system, disk partition, or whole-disk level. Supported
within Compute VMs.
disk format The underlying format that a disk image for a VM is stored as within the Image service back-end
store. For example, AMI, ISO, QCOW2, VMDK, and so on.
dispersion In Object Storage, tools to test and ensure dispersion of objects and containers to ensure fault
tolerance.
distributed virtual router (DVR) Mechanism for highly available multi-host routing when using OpenStack
Networking (neutron).
Django A web framework used extensively in horizon.
DNS record A record that specifies information about a particular domain and belongs to the domain.
DNS service (designate) OpenStack project that provides scalable, on demand, self service access to authoritative
DNS services, in a technology-agnostic manner.
dnsmasq Daemon that provides DNS, DHCP, BOOTP, and TFTP services for virtual networks.
domain An Identity API v3 entity. Represents a collection of projects, groups and users that defines administrative
boundaries for managing OpenStack Identity entities. On the Internet, separates a website from
other sites. Often, the domain name has two or more parts that are separated by dots. For example,
yahoo.com, usa.gov, harvard.edu, or mail.yahoo.com. Also, a domain is an entity or container of all
DNS-related information containing one or more records.
Domain Name System (DNS) A system by which Internet domain name-to-address and address-to-name resolutions
are determined. DNS helps navigate the Internet by translating the IP address into an address
that is easier to remember. For example, translating 111.111.111.1 into www.yahoo.com. All domains
and their components, such as mail servers, utilize DNS to resolve to the appropriate locations. DNS
servers are usually set up in a master-slave relationship such that failure of the master invokes the slave.
DNS servers might also be clustered or replicated such that changes made to one DNS server are automatically
propagated to other active servers. In Compute, the support that enables associating DNS
entries with floating IP addresses, nodes, or cells so that hostnames are consistent across reboots.
download The transfer of data, usually in the form of files, from one computer to another.
durable exchange The Compute RabbitMQ message exchange that remains active when the server restarts.
Appendix 61
High Availability Guide (Release Version: 15.0.0)
durable queue A Compute RabbitMQ message queue that remains active when the server restarts.
Dynamic Host Configuration Protocol (DHCP) A network protocol that configures devices that are connected
to a network so that they can communicate on that network by using the Internet Protocol (IP).
The protocol is implemented in a client-server model where DHCP clients request configuration data,
such as an IP address, a default route, and one or more DNS server addresses from a DHCP server. A
method to automatically configure networking for a host at boot time. Provided by both Networking and
Compute.
Dynamic HyperText Markup Language (DHTML) Pages that use HTML, JavaScript, and Cascading Style
Sheets to enable users to interact with a web page or show simple animation.
E
east-west traffic Network traffic between servers in the same cloud or data center. See also north-south traffic.
EBS boot volume An Amazon EBS storage volume that contains a bootable VM image, currently unsupported
in OpenStack.
ebtables Filtering tool for a Linux bridging firewall, enabling filtering of network traffic passing through
a Linux bridge. Used in Compute along with arptables, iptables, and ip6tables to ensure isolation of
network communications.
EC2 The Amazon commercial compute product, similar to Compute.
EC2 access key Used along with an EC2 secret key to access the Compute EC2 API.
EC2 API OpenStack supports accessing the Amazon EC2 API through Compute.
EC2 Compatibility API A Compute component that enables OpenStack to communicate with Amazon EC2.
EC2 secret key Used along with an EC2 access key when communicating with the Compute EC2 API; used
to digitally sign each request.
Elastic Block Storage (EBS) The Amazon commercial block storage product.
encapsulation The practice of placing one packet type within another for the purposes of abstracting or securing
data. Examples include GRE, MPLS, or IPsec.
encryption OpenStack supports encryption technologies such as HTTPS, SSH, SSL, TLS, digital certificates,
and data encryption.
endpoint See API endpoint.
endpoint registry Alternative term for an Identity service catalog.
endpoint template A list of URL and port number endpoints that indicate where a service, such as Object
Storage, Compute, Identity, and so on, can be accessed.
entity Any piece of hardware or software that wants to connect to the network services provided by Networking,
the network connectivity service. An entity can make use of Networking by implementing a
VIF.
ephemeral image A VM image that does not save changes made to its volumes and reverts them to their
original state after the instance is terminated.
ephemeral volume Volume that does not save the changes made to it and reverts to its original state when the
current user relinquishes control.
62 Appendix
High Availability Guide (Release Version: 15.0.0)
Essex A grouped release of projects related to OpenStack that came out in April 2012, the fifth release of
OpenStack. It included Compute (nova 2012.1), Object Storage (swift 1.4.8), Image (glance), Identity
(keystone), and Dashboard (horizon). Essex is the code name for the fifth release of OpenStack. The
design summit took place in Boston, Massachusetts, US and Essex is a nearby city.
ESXi An OpenStack-supported hypervisor.
ETag MD5 hash of an object within Object Storage, used to ensure data integrity.
euca2ools A collection of command-line tools for administering VMs; most are compatible with OpenStack.
Eucalyptus Kernel Image (EKI) Used along with an ERI to create an EMI.
Eucalyptus Machine Image (EMI) VM image container format supported by Image service.
Eucalyptus Ramdisk Image (ERI) Used along with an EKI to create an EMI.
evacuate The process of migrating one or all virtual machine (VM) instances from one host to another, compatible
with both shared storage live migration and block migration.
exchange Alternative term for a RabbitMQ message exchange.
exchange type A routing algorithm in the Compute RabbitMQ.
exclusive queue Connected to by a direct consumer in RabbitMQ—Compute, the message can be consumed
only by the current connection.
extended attributes (xattr) File system option that enables storage of additional information beyond owner,
group, permissions, modification time, and so on. The underlying Object Storage file system must support
extended attributes.
extension Alternative term for an API extension or plug-in. In the context of Identity service, this is a call that
is specific to the implementation, such as adding support for OpenID.
external network A network segment typically used for instance Internet access.
extra specs Specifies additional requirements when Compute determines where to start a new instance. Examples
include a minimum amount of network bandwidth or a GPU.
F
FakeLDAP An easy method to create a local LDAP directory for testing Identity and Compute. Requires
Redis.
fan-out exchange Within RabbitMQ and Compute, it is the messaging interface that is used by the scheduler
service to receive capability messages from the compute, volume, and network nodes.
federated identity A method to establish trusts between identity providers and the OpenStack cloud.
Fedora A Linux distribution compatible with OpenStack.
Fibre Channel Storage protocol similar in concept to TCP/IP; encapsulates SCSI commands and data.
Fibre Channel over Ethernet (FCoE) The fibre channel protocol tunneled within Ethernet.
fill-first scheduler The Compute scheduling method that attempts to fill a host with VMs rather than starting
new VMs on a variety of hosts.
filter The step in the Compute scheduling process when hosts that cannot run VMs are eliminated and not
chosen.
Appendix 63
High Availability Guide (Release Version: 15.0.0)
firewall Used to restrict communications between hosts and/or nodes, implemented in Compute using iptables,
arptables, ip6tables, and ebtables.
FireWall-as-a-Service (FWaaS) A Networking extension that provides perimeter firewall functionality.
fixed IP address An IP address that is associated with the same instance each time that instance boots, is
generally not accessible to end users or the public Internet, and is used for management of the instance.
Flat Manager The Compute component that gives IP addresses to authorized nodes and assumes DHCP, DNS,
and routing configuration and services are provided by something else.
flat mode injection A Compute networking method where the OS network configuration information is injected
into the VM image before the instance starts.
flat network Virtual network type that uses neither VLANs nor tunnels to segregate project traffic. Each
flat network typically requires a separate underlying physical interface defined by bridge mappings.
However, a flat network can contain multiple subnets.
FlatDHCP Manager The Compute component that provides dnsmasq (DHCP, DNS, BOOTP, TFTP) and
radvd (routing) services.
flavor Alternative term for a VM instance type.
flavor ID UUID for each Compute or Image service VM flavor or instance type.
floating IP address An IP address that a project can associate with a VM so that the instance has the same
public IP address each time that it boots. You create a pool of floating IP addresses and assign them to
instances as they are launched to maintain a consistent IP address for maintaining DNS assignment.
Folsom A grouped release of projects related to OpenStack that came out in the fall of 2012, the sixth release
of OpenStack. It includes Compute (nova), Object Storage (swift), Identity (keystone), Networking
(neutron), Image service (glance), and Volumes or Block Storage (cinder). Folsom is the code name
for the sixth release of OpenStack. The design summit took place in San Francisco, California, US and
Folsom is a nearby city.
FormPost Object Storage middleware that uploads (posts) an image through a form on a web page.
freezer Code name for the Backup, Restore, and Disaster Recovery service.
front end The point where a user interacts with a service; can be an API endpoint, the dashboard, or a
command-line tool.
G
gateway An IP address, typically assigned to a router, that passes network traffic between different networks.
generic receive offload (GRO) Feature of certain network interface drivers that combines many smaller received
packets into a large packet before delivery to the kernel IP stack.
generic routing encapsulation (GRE) Protocol that encapsulates a wide variety of network layer protocols
inside virtual point-to-point links.
glance Codename for the Image service.
glance API server Alternative name for the Image API.
glance registry Alternative term for the Image service image registry.
global endpoint template The Identity service endpoint template that contains services available to all
projects.
64 Appendix
High Availability Guide (Release Version: 15.0.0)
GlusterFS A file system designed to aggregate NAS hosts, compatible with OpenStack.
gnocchi Part of the OpenStack Telemetry service; provides an indexer and time-series database.
golden image A method of operating system installation where a finalized disk image is created and then used
by all nodes without modification.
Governance service (congress) The project that provides Governance-as-a-Service across any collection of
cloud services in order to monitor, enforce, and audit policy over dynamic infrastructure.
Graphic Interchange Format (GIF) A type of image file that is commonly used for animated images on web
pages.
Graphics Processing Unit (GPU) Choosing a host based on the existence of a GPU is currently unsupported
in OpenStack.
Green Threads The cooperative threading model used by Python; reduces race conditions and only context
switches when specific library calls are made. Each OpenStack service is its own thread.
Grizzly The code name for the seventh release of OpenStack. The design summit took place in San Diego,
California, US and Grizzly is an element of the state flag of California.
Group An Identity v3 API entity. Represents a collection of users that is owned by a specific domain.
guest OS An operating system instance running under the control of a hypervisor.
H
Hadoop Apache Hadoop is an open source software framework that supports data-intensive distributed applications.
Hadoop Distributed File System (HDFS) A distributed, highly fault-tolerant file system designed to run on
low-cost commodity hardware.
handover An object state in Object Storage where a new replica of the object is automatically created due to
a drive failure.
HAProxy Provides a load balancer for TCP and HTTP-based applications that spreads requests across multiple
servers.
hard reboot A type of reboot where a physical or virtual power button is pressed as opposed to a graceful,
proper shutdown of the operating system.
Havana The code name for the eighth release of OpenStack. The design summit took place in Portland,
Oregon, US and Havana is an unincorporated community in Oregon.
health monitor Determines whether back-end members of a VIP pool can process a request. A pool can
have several health monitors associated with it. When a pool has several monitors associated with it, all
monitors check each member of the pool. All monitors must declare a member to be healthy for it to stay
active.
heat Codename for the Orchestration service.
Heat Orchestration Template (HOT) Heat input in the format native to OpenStack.
high availability (HA) A high availability system design approach and associated service implementation
ensures that a prearranged level of operational performance will be met during a contractual measurement
period. High availability systems seek to minimize system downtime and data loss.
horizon Codename for the Dashboard.
Appendix 65
High Availability Guide (Release Version: 15.0.0)
horizon plug-in A plug-in for the OpenStack Dashboard (horizon).
host A physical computer, not a VM instance (node).
host aggregate A method to further subdivide availability zones into hypervisor pools, a collection of common
hosts.
Host Bus Adapter (HBA) Device plugged into a PCI slot, such as a fibre channel or network card.
hybrid cloud A hybrid cloud is a composition of two or more clouds (private, community or public) that
remain distinct entities but are bound together, offering the benefits of multiple deployment models.
Hybrid cloud can also mean the ability to connect colocation, managed and/or dedicated services with
cloud resources.
Hyper-V One of the hypervisors supported by OpenStack.
hyperlink Any kind of text that contains a link to some other site, commonly found in documents where
clicking on a word or words opens up a different website.
Hypertext Transfer Protocol (HTTP) An application protocol for distributed, collaborative, hypermedia information
systems. It is the foundation of data communication for the World Wide Web. Hypertext is
structured text that uses logical links (hyperlinks) between nodes containing text. HTTP is the protocol
to exchange or transfer hypertext.
Hypertext Transfer Protocol Secure (HTTPS) An encrypted communications protocol for secure communication
over a computer network, with especially wide deployment on the Internet. Technically, it is not a
protocol in and of itself; rather, it is the result of simply layering the Hypertext Transfer Protocol (HTTP)
on top of the TLS or SSL protocol, thus adding the security capabilities of TLS or SSL to standard HTTP
communications. Most OpenStack API endpoints and many inter-component communications support
HTTPS communication.
hypervisor Software that arbitrates and controls VM access to the actual underlying hardware.
hypervisor pool A collection of hypervisors grouped together through host aggregates.
I
Icehouse The code name for the ninth release of OpenStack. The design summit took place in Hong Kong
and Ice House is a street in that city.
ID number Unique numeric ID associated with each user in Identity, conceptually similar to a Linux or LDAP
UID.
Identity API Alternative term for the Identity service API.
Identity back end The source used by Identity service to retrieve user information; an OpenLDAP server, for
example.
identity provider A directory service, which allows users to login with a user name and password. It is a
typical source of authentication tokens.
Identity service (keystone) The project that facilitates API client authentication, service discovery, distributed
multi-project authorization, and auditing. It provides a central directory of users mapped to
the OpenStack services they can access. It also registers endpoints for OpenStack services and acts as a
common authentication system.
Identity service API The API used to access the OpenStack Identity service provided through keystone.
66 Appendix
High Availability Guide (Release Version: 15.0.0)
IETF Internet Engineering Task Force (IETF) is an open standards organization that develops Internet standards,
particularly the standards pertaining to TCP/IP.
image A collection of files for a specific operating system (OS) that you use to create or rebuild a server.
OpenStack provides pre-built images. You can also create custom images, or snapshots, from servers
that you have launched. Custom images can be used for data backups or as “gold” images for additional
servers.
Image API The Image service API endpoint for management of VM images. Processes client requests for
VMs, updates Image service metadata on the registry server, and communicates with the store adapter to
upload VM images from the back-end store.
image cache Used by Image service to obtain images on the local host rather than re-downloading them from
the image server each time one is requested.
image ID Combination of a URI and UUID used to access Image service VM images through the image API.
image membership A list of projects that can access a given VM image within Image service.
image owner The project who owns an Image service virtual machine image.
image registry A list of VM images that are available through Image service.
Image service (glance) The OpenStack service that provide services and associated libraries to store, browse,
share, distribute and manage bootable disk images, other data closely associated with initializing compute
resources, and metadata definitions.
image status The current status of a VM image in Image service, not to be confused with the status of a running
instance.
image store The back-end store used by Image service to store VM images, options include Object Storage,
locally mounted file system, RADOS block devices, VMware datastore, or HTTP.
image UUID UUID used by Image service to uniquely identify each VM image.
incubated project A community project may be elevated to this status and is then promoted to a core project.
Infrastructure Optimization service (watcher) OpenStack project that aims to provide a flexible and scalable
resource optimization service for multi-project OpenStack-based clouds.
Infrastructure-as-a-Service (IaaS) IaaS is a provisioning model in which an organization outsources physical
components of a data center, such as storage, hardware, servers, and networking components. A service
provider owns the equipment and is responsible for housing, operating and maintaining it. The client
typically pays on a per-use basis. IaaS is a model for providing cloud services.
ingress filtering The process of filtering incoming network traffic. Supported by Compute.
INI format The OpenStack configuration files use an INI format to describe options and their values. It
consists of sections and key value pairs.
injection The process of putting a file into a virtual machine image before the instance is started.
Input/Output Operations Per Second (IOPS) IOPS are a common performance measurement used to
benchmark computer storage devices like hard disk drives, solid state drives, and storage area networks.
instance A running VM, or a VM in a known state such as suspended, that can be used like a hardware server.
instance ID Alternative term for instance UUID.
instance state The current state of a guest VM image.
Appendix 67
High Availability Guide (Release Version: 15.0.0)
instance tunnels network A network segment used for instance traffic tunnels between compute nodes and
the network node.
instance type Describes the parameters of the various virtual machine images that are available to users;
includes parameters such as CPU, storage, and memory. Alternative term for flavor.
instance type ID Alternative term for a flavor ID.
instance UUID Unique ID assigned to each guest VM instance.
Intelligent Platform Management Interface (IPMI) IPMI is a standardized computer system interface used
by system administrators for out-of-band management of computer systems and monitoring of their operation.
In layman’s terms, it is a way to manage a computer using a direct network connection, whether
it is turned on or not; connecting to the hardware rather than an operating system or login shell.
interface A physical or virtual device that provides connectivity to another device or medium.
interface ID Unique ID for a Networking VIF or vNIC in the form of a UUID.
Internet Control Message Protocol (ICMP) A network protocol used by network devices for control messages.
For example, ping uses ICMP to test connectivity.
Internet protocol (IP) Principal communications protocol in the internet protocol suite for relaying datagrams
across network boundaries.
Internet Service Provider (ISP) Any business that provides Internet access to individuals or businesses.
Internet Small Computer System Interface (iSCSI) Storage protocol that encapsulates SCSI frames for
transport over IP networks. Supported by Compute, Object Storage, and Image service.
IP address Number that is unique to every computer system on the Internet. Two versions of the Internet
Protocol (IP) are in use for addresses: IPv4 and IPv6.
IP Address Management (IPAM) The process of automating IP address allocation, deallocation, and management.
Currently provided by Compute, melange, and Networking.
ip6tables Tool used to set up, maintain, and inspect the tables of IPv6 packet filter rules in the Linux kernel.
In OpenStack Compute, ip6tables is used along with arptables, ebtables, and iptables to create firewalls
for both nodes and VMs.
ipset Extension to iptables that allows creation of firewall rules that match entire “sets” of IP addresses simultaneously.
These sets reside in indexed data structures to increase efficiency, particularly on systems
with a large quantity of rules.
iptables Used along with arptables and ebtables, iptables create firewalls in Compute. iptables are the tables
provided by the Linux kernel firewall (implemented as different Netfilter modules) and the chains and
rules it stores. Different kernel modules and programs are currently used for different protocols: iptables
applies to IPv4, ip6tables to IPv6, arptables to ARP, and ebtables to Ethernet frames. Requires root
privilege to manipulate.
ironic Codename for the Bare Metal service.
iSCSI Qualified Name (IQN) IQN is the format most commonly used for iSCSI names, which uniquely identify
nodes in an iSCSI network. All IQNs follow the pattern iqn.yyyy-mm.domain:identifier, where
‘yyyy-mm’ is the year and month in which the domain was registered, ‘domain’ is the reversed domain
name of the issuing organization, and ‘identifier’ is an optional string which makes each IQN under the
same domain unique. For example, ‘iqn.2015-10.org.openstack.408ae959bce1’.
ISO9660 One of the VM image disk formats supported by Image service.
itsec A default role in the Compute RBAC system that can quarantine an instance in any project.
68 Appendix
High Availability Guide (Release Version: 15.0.0)
J
Java A programming language that is used to create systems that involve more than one computer by way of
a network.
JavaScript A scripting language that is used to build web pages.
JavaScript Object Notation (JSON) One of the supported response formats in OpenStack.
jumbo frame Feature in modern Ethernet networks that supports frames up to approximately 9000 bytes.
Juno The code name for the tenth release of OpenStack. The design summit took place in Atlanta, Georgia,
US and Juno is an unincorporated community in Georgia.
K
Kerberos A network authentication protocol which works on the basis of tickets. Kerberos allows nodes
communication over a non-secure network, and allows nodes to prove their identity to one another in a
secure manner.
kernel-based VM (KVM) An OpenStack-supported hypervisor. KVM is a full virtualization solution for
Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V), ARM, IBM Power,
and IBM zSeries. It consists of a loadable kernel module, that provides the core virtualization infrastructure
and a processor specific module.
Key Manager service (barbican) The project that produces a secret storage and generation system capable
of providing key management for services wishing to enable encryption features.
keystone Codename of the Identity service.
Kickstart A tool to automate system configuration and installation on Red Hat, Fedora, and CentOS-based
Linux distributions.
Kilo The code name for the eleventh release of OpenStack. The design summit took place in Paris, France.
Due to delays in the name selection, the release was known only as K. Because k is the unit symbol
for kilo and the kilogram reference artifact is stored near Paris in the Pavillon de Breteuil in Sèvres, the
community chose Kilo as the release name.
L
large object An object within Object Storage that is larger than 5 GB.
Launchpad The collaboration site for OpenStack.
Layer-2 (L2) agent OpenStack Networking agent that provides layer-2 connectivity for virtual networks.
Layer-2 network Term used in the OSI network architecture for the data link layer. The data link layer is
responsible for media access control, flow control and detecting and possibly correcting errors that may
occur in the physical layer.
Layer-3 (L3) agent OpenStack Networking agent that provides layer-3 (routing) services for virtual networks.
Layer-3 network Term used in the OSI network architecture for the network layer. The network layer is
responsible for packet forwarding including routing from one node to another.
Liberty The code name for the twelfth release of OpenStack. The design summit took place in Vancouver,
Canada and Liberty is the name of a village in the Canadian province of Saskatchewan.
Appendix 69
High Availability Guide (Release Version: 15.0.0)
libvirt Virtualization API library used by OpenStack to interact with many of its supported hypervisors.
Lightweight Directory Access Protocol (LDAP) An application protocol for accessing and maintaining distributed
directory information services over an IP network.
Linux Unix-like computer operating system assembled under the model of free and open-source software
development and distribution.
Linux bridge Software that enables multiple VMs to share a single physical NIC within Compute.
Linux Bridge neutron plug-in Enables a Linux bridge to understand a Networking port, interface attachment,
and other abstractions.
Linux containers (LXC) An OpenStack-supported hypervisor.
live migration The ability within Compute to move running virtual machine instances from one host to another
with only a small service interruption during switchover.
load balancer A load balancer is a logical device that belongs to a cloud account. It is used to distribute
workloads between multiple back-end systems or services, based on the criteria defined as part of its
configuration.
load balancing The process of spreading client requests between two or more nodes to improve performance
and availability.
Load-Balancer-as-a-Service (LBaaS) Enables Networking to distribute incoming requests evenly between
designated instances.
Load-balancing service (octavia) The project that aims to provide scalable, on demand, self service access
to load-balancer services, in technology-agnostic manner.
Logical Volume Manager (LVM) Provides a method of allocating space on mass-storage devices that is more
flexible than conventional partitioning schemes.
M
magnum Code name for the Containers Infrastructure Management service.
management API Alternative term for an admin API.
management network A network segment used for administration, not accessible to the public Internet.
manager Logical groupings of related code, such as the Block Storage volume manager or network manager.
manifest Used to track segments of a large object within Object Storage.
manifest object A special Object Storage object that contains the manifest for a large object.
manila Codename for OpenStack Shared File Systems service.
manila-share Responsible for managing Shared File System Service devices, specifically the back-end devices.
maximum transmission unit (MTU) Maximum frame or packet size for a particular network medium. Typically
1500 bytes for Ethernet networks.
mechanism driver A driver for the Modular Layer 2 (ML2) neutron plug-in that provides layer-2 connectivity
for virtual instances. A single OpenStack installation can use multiple mechanism drivers.
melange Project name for OpenStack Network Information Service. To be merged with Networking.
70 Appendix
High Availability Guide (Release Version: 15.0.0)
membership The association between an Image service VM image and a project. Enables images to be shared
with specified projects.
membership list A list of projects that can access a given VM image within Image service.
memcached A distributed memory object caching system that is used by Object Storage for caching.
memory overcommit The ability to start new VM instances based on the actual memory usage of a host, as
opposed to basing the decision on the amount of RAM each running instance thinks it has available. Also
known as RAM overcommit.
message broker The software package used to provide AMQP messaging capabilities within Compute. Default
package is RabbitMQ.
message bus The main virtual communication line used by all AMQP messages for inter-cloud communications
within Compute.
message queue Passes requests from clients to the appropriate workers and returns the output to the client
after the job completes.
Message service (zaqar) The project that provides a messaging service that affords a variety of distributed
application patterns in an efficient, scalable and highly available manner, and to create and maintain
associated Python libraries and documentation.
Meta-Data Server (MDS) Stores CephFS metadata.
Metadata agent OpenStack Networking agent that provides metadata services for instances.
migration The process of moving a VM instance from one host to another.
mistral Code name for Workflow service.
Mitaka The code name for the thirteenth release of OpenStack. The design summit took place in Tokyo,
Japan. Mitaka is a city in Tokyo.
Modular Layer 2 (ML2) neutron plug-in Can concurrently use multiple layer-2 networking technologies,
such as 802.1Q and VXLAN, in Networking.
monasca Codename for OpenStack Monitoring.
Monitor (LBaaS) LBaaS feature that provides availability monitoring using the ping command, TCP, and
HTTP/HTTPS GET.
Monitor (Mon) A Ceph component that communicates with external clients, checks data state and consistency,
and performs quorum functions.
Monitoring (monasca) The OpenStack service that provides a multi-project, highly scalable, performant,
fault-tolerant monitoring-as-a-service solution for metrics, complex event processing and logging. To
build an extensible platform for advanced monitoring services that can be used by both operators and
projects to gain operational insight and visibility, ensuring availability and stability.
multi-factor authentication Authentication method that uses two or more credentials, such as a password
and a private key. Currently not supported in Identity.
multi-host High-availability mode for legacy (nova) networking. Each compute node handles NAT and DHCP
and acts as a gateway for all of the VMs on it. A networking failure on one compute node doesn’t affect
VMs on other compute nodes.
multinic Facility in Compute that allows each virtual machine instance to have more than one VIF connected
to it.
murano Codename for the Application Catalog service.
Appendix 71
High Availability Guide (Release Version: 15.0.0)
N
Nebula Released as open source by NASA in 2010 and is the basis for Compute.
netadmin One of the default roles in the Compute RBAC system. Enables the user to allocate publicly accessible
IP addresses to instances and change firewall rules.
NetApp volume driver Enables Compute to communicate with NetApp storage devices through the NetApp
OnCommand Provisioning Manager.
network A virtual network that provides connectivity between entities. For example, a collection of virtual
ports that share network connectivity. In Networking terminology, a network is always a layer-2 network.
Network Address Translation (NAT) Process of modifying IP address information while in transit. Supported
by Compute and Networking.
network controller A Compute daemon that orchestrates the network configuration of nodes, including IP
addresses, VLANs, and bridging. Also manages routing for both public and private networks.
Network File System (NFS) A method for making file systems available over the network. Supported by
OpenStack.
network ID Unique ID assigned to each network segment within Networking. Same as network UUID.
network manager The Compute component that manages various network components, such as firewall rules,
IP address allocation, and so on.
network namespace Linux kernel feature that provides independent virtual networking instances on a single
host with separate routing tables and interfaces. Similar to virtual routing and forwarding (VRF) services
on physical network equipment.
network node Any compute node that runs the network worker daemon.
network segment Represents a virtual, isolated OSI layer-2 subnet in Networking.
Network Service Header (NSH) Provides a mechanism for metadata exchange along the instantiated service
path.
Network Time Protocol (NTP) Method of keeping a clock for a host or node correct via communication with
a trusted, accurate time source.
network UUID Unique ID for a Networking network segment.
network worker The nova-network worker daemon; provides services such as giving an IP address to a
booting nova instance.
Networking API (Neutron API) API used to access OpenStack Networking. Provides an extensible architecture
to enable custom plug-in creation.
Networking service (neutron) The OpenStack project which implements services and associated libraries to
provide on-demand, scalable, and technology-agnostic network abstraction.
neutron Codename for OpenStack Networking service.
neutron API An alternative name for Networking API.
neutron manager Enables Compute and Networking integration, which enables Networking to perform network
management for guest VMs.
neutron plug-in Interface within Networking that enables organizations to create custom plug-ins for advanced
features, such as QoS, ACLs, or IDS.
72 Appendix
High Availability Guide (Release Version: 15.0.0)
Newton The code name for the fourteenth release of OpenStack. The design summit took place in Austin,
Texas, US. The release is named after “Newton House” which is located at 1013 E. Ninth St., Austin,
TX. which is listed on the National Register of Historic Places.
Nexenta volume driver Provides support for NexentaStor devices in Compute.
NFV Orchestration Service (tacker) OpenStack service that aims to implement Network Function Virtualization
(NFV) orchestration services and libraries for end-to-end life-cycle management of network
services and Virtual Network Functions (VNFs).
Nginx An HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server.
No ACK Disables server-side message acknowledgment in the Compute RabbitMQ. Increases performance
but decreases reliability.
node A VM instance that runs on a host.
non-durable exchange Message exchange that is cleared when the service restarts. Its data is not written to
persistent storage.
non-durable queue Message queue that is cleared when the service restarts. Its data is not written to persistent
storage.
non-persistent volume Alternative term for an ephemeral volume.
north-south traffic Network traffic between a user or client (north) and a server (south), or traffic into the
cloud (south) and out of the cloud (north). See also east-west traffic.
nova Codename for OpenStack Compute service.
Nova API Alternative term for the Compute API.
nova-network A Compute component that manages IP address allocation, firewalls, and other network-related
tasks. This is the legacy networking option and an alternative to Networking.
O
object A BLOB of data held by Object Storage; can be in any format.
object auditor Opens all objects for an object server and verifies the MD5 hash, size, and metadata for each
object.
object expiration A configurable option within Object Storage to automatically delete objects after a specified
amount of time has passed or a certain date is reached.
object hash Unique ID for an Object Storage object.
object path hash Used by Object Storage to determine the location of an object in the ring. Maps objects to
partitions.
object replicator An Object Storage component that copies an object to remote partitions for fault tolerance.
object server An Object Storage component that is responsible for managing objects.
Object Storage API API used to access OpenStack Object Storage.
Object Storage Device (OSD) The Ceph storage daemon.
Object Storage service (swift) The OpenStack core project that provides eventually consistent and redundant
storage and retrieval of fixed digital content.
Appendix 73
High Availability Guide (Release Version: 15.0.0)
object versioning Allows a user to set a flag on an Object Storage container so that all objects within the
container are versioned.
Ocata The code name for the fifteenth release of OpenStack. The design summit will take place in Barcelona,
Spain. Ocata is a beach north of Barcelona.
Octavia Code name for the Load-balancing service.
Oldie Term for an Object Storage process that runs for a long time. Can indicate a hung process.
Open Cloud Computing Interface (OCCI) A standardized interface for managing compute, data, and network
resources, currently unsupported in OpenStack.
Open Virtualization Format (OVF) Standard for packaging VM images. Supported in OpenStack.
Open vSwitch Open vSwitch is a production quality, multilayer virtual switch licensed under the open source
Apache 2.0 license. It is designed to enable massive network automation through programmatic extension,
while still supporting standard management interfaces and protocols (for example NetFlow, sFlow,
SPAN, RSPAN, CLI, LACP, 802.1ag).
Open vSwitch (OVS) agent Provides an interface to the underlying Open vSwitch service for the Networking
plug-in.
Open vSwitch neutron plug-in Provides support for Open vSwitch in Networking.
OpenLDAP An open source LDAP server. Supported by both Compute and Identity.
OpenStack OpenStack is a cloud operating system that controls large pools of compute, storage, and networking
resources throughout a data center, all managed through a dashboard that gives administrators
control while empowering their users to provision resources through a web interface. OpenStack is an
open source project licensed under the Apache License 2.0.
OpenStack code name Each OpenStack release has a code name. Code names ascend in alphabetical order:
Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, Icehouse, Juno, Kilo, Liberty, Mitaka,
Newton, Ocata, Pike, Queens, and Rocky. Code names are cities or counties near where the corresponding
OpenStack design summit took place. An exception, called the Waldon exception, is granted to
elements of the state flag that sound especially cool. Code names are chosen by popular vote.
openSUSE A Linux distribution that is compatible with OpenStack.
operator The person responsible for planning and maintaining an OpenStack installation.
optional service An official OpenStack service defined as optional by DefCore Committee. Currently, consists
of Dashboard (horizon), Telemetry service (Telemetry), Orchestration service (heat), Database service
(trove), Bare Metal service (ironic), and so on.
Orchestration service (heat) The OpenStack service which orchestrates composite cloud applications using
a declarative template format through an OpenStack-native REST API.
orphan In the context of Object Storage, this is a process that is not terminated after an upgrade, restart, or
reload of the service.
Oslo Codename for the Common Libraries project.
P
panko Part of the OpenStack Telemetry service; provides event storage.
parent cell If a requested resource, such as CPU time, disk storage, or memory, is not available in the parent
cell, the request is forwarded to associated child cells.
74 Appendix
High Availability Guide (Release Version: 15.0.0)
partition A unit of storage within Object Storage used to store objects. It exists on top of devices and is
replicated for fault tolerance.
partition index Contains the locations of all Object Storage partitions within the ring.
partition shift value Used by Object Storage to determine which partition data should reside on.
path MTU discovery (PMTUD) Mechanism in IP networks to detect end-to-end MTU and adjust packet size
accordingly.
pause A VM state where no changes occur (no changes in memory, network communications stop, etc); the
VM is frozen but not shut down.
PCI passthrough Gives guest VMs exclusive access to a PCI device. Currently supported in OpenStack
Havana and later releases.
persistent message A message that is stored both in memory and on disk. The message is not lost after a
failure or restart.
persistent volume Changes to these types of disk volumes are saved.
personality file A file used to customize a Compute instance. It can be used to inject SSH keys or a specific
network configuration.
Pike The code name for the sixteenth release of OpenStack. The design summit will take place in Boston,
Massachusetts, US. The release is named after the Massachusetts Turnpike, abbreviated commonly as
the Mass Pike, which is the easternmost stretch of Interstate 90.
Platform-as-a-Service (PaaS) Provides to the consumer the ability to deploy applications through a programming
language or tools supported by the cloud platform provider. An example of Platform-as-a-Service
is an Eclipse/Java programming platform provided with no downloads required.
plug-in Software component providing the actual implementation for Networking APIs, or for Compute APIs,
depending on the context.
policy service Component of Identity that provides a rule-management interface and a rule-based authorization
engine.
policy-based routing (PBR) Provides a mechanism to implement packet forwarding and routing according
to the policies defined by the network administrator.
pool A logical set of devices, such as web servers, that you group together to receive and process traffic.
The load balancing function chooses which member of the pool handles the new requests or connections
received on the VIP address. Each VIP has one pool.
pool member An application that runs on the back-end server in a load-balancing system.
port A virtual network port within Networking; VIFs / vNICs are connected to a port.
port UUID Unique ID for a Networking port.
preseed A tool to automate system configuration and installation on Debian-based Linux distributions.
private image An Image service VM image that is only available to specified projects.
private IP address An IP address used for management and administration, not available to the public Internet.
private network The Network Controller provides virtual networks to enable compute servers to interact
with each other and with the public network. All machines must have a public and private network
interface. A private network interface can be a flat or VLAN network interface. A flat network interface
Appendix 75
High Availability Guide (Release Version: 15.0.0)
is controlled by the flat_interface with flat managers. A VLAN network interface is controlled by the
vlan_interface option with VLAN managers.
project Projects represent the base unit of “ownership” in OpenStack, in that all resources in OpenStack should
be owned by a specific project. In OpenStack Identity, a project must be owned by a specific domain.
project ID Unique ID assigned to each project by the Identity service.
project VPN Alternative term for a cloudpipe.
promiscuous mode Causes the network interface to pass all traffic it receives to the host rather than passing
only the frames addressed to it.
protected property Generally, extra properties on an Image service image to which only cloud administrators
have access. Limits which user roles can perform CRUD operations on that property. The cloud
administrator can configure any image property as protected.
provider An administrator who has access to all hosts and instances.
proxy node A node that provides the Object Storage proxy service.
proxy server Users of Object Storage interact with the service through the proxy server, which in turn looks
up the location of the requested data within the ring and returns the results to the user.
public API An API endpoint used for both service-to-service communication and end-user interactions.
public image An Image service VM image that is available to all projects.
public IP address An IP address that is accessible to end-users.
public key authentication Authentication method that uses keys rather than passwords.
public network The Network Controller provides virtual networks to enable compute servers to interact with
each other and with the public network. All machines must have a public and private network interface.
The public network interface is controlled by the public_interface option.
Puppet An operating system configuration-management tool supported by OpenStack.
Python Programming language used extensively in OpenStack.
Q
QEMU Copy On Write 2 (QCOW2) One of the VM image disk formats supported by Image service.
Qpid Message queue software supported by OpenStack; an alternative to RabbitMQ.
Quality of Service (QoS) The ability to guarantee certain network or storage requirements to satisfy a Service
Level Agreement (SLA) between an application provider and end users. Typically includes performance
requirements like networking bandwidth, latency, jitter correction, and reliability as well as storage performance
in Input/Output Operations Per Second (IOPS), throttling agreements, and performance expectations
at peak load.
quarantine If Object Storage finds objects, containers, or accounts that are corrupt, they are placed in this
state, are not replicated, cannot be read by clients, and a correct copy is re-replicated.
Queens The code name for the seventeenth release of OpenStack. The design summit will take place in
Sydney, Australia. The release is named after the Queens Pound river in the South Coast region of New
South Wales.
Quick EMUlator (QEMU) QEMU is a generic and open source machine emulator and virtualizer. One of
the hypervisors supported by OpenStack, generally used for development purposes.
76 Appendix
High Availability Guide (Release Version: 15.0.0)
quota In Compute and Block Storage, the ability to set resource limits on a per-project basis.
R
RabbitMQ The default message queue software used by OpenStack.
Rackspace Cloud Files Released as open source by Rackspace in 2010; the basis for Object Storage.
RADOS Block Device (RBD) Ceph component that enables a Linux block device to be striped over multiple
distributed data stores.
radvd The router advertisement daemon, used by the Compute VLAN manager and FlatDHCP manager to
provide routing services for VM instances.
rally Codename for the Benchmark service.
RAM filter The Compute setting that enables or disables RAM overcommitment.
RAM overcommit The ability to start new VM instances based on the actual memory usage of a host, as
opposed to basing the decision on the amount of RAM each running instance thinks it has available.
Also known as memory overcommit.
rate limit Configurable option within Object Storage to limit database writes on a per-account and/or percontainer
basis.
raw One of the VM image disk formats supported by Image service; an unstructured disk image.
rebalance The process of distributing Object Storage partitions across all drives in the ring; used during initial
ring creation and after ring reconfiguration.
reboot Either a soft or hard reboot of a server. With a soft reboot, the operating system is signaled to restart,
which enables a graceful shutdown of all processes. A hard reboot is the equivalent of power cycling
the server. The virtualization platform should ensure that the reboot action has completed successfully,
even in cases in which the underlying domain/VM is paused or halted/stopped.
rebuild Removes all data on the server and replaces it with the specified image. Server ID and IP addresses
remain the same.
Recon An Object Storage component that collects meters.
record Belongs to a particular domain and is used to specify information about the domain. There are several
types of DNS records. Each record type contains particular information used to describe the purpose of
that record. Examples include mail exchange (MX) records, which specify the mail server for a particular
domain; and name server (NS) records, which specify the authoritative name servers for a domain.
record ID A number within a database that is incremented each time a change is made. Used by Object
Storage when replicating.
Red Hat Enterprise Linux (RHEL) A Linux distribution that is compatible with OpenStack.
reference architecture A recommended architecture for an OpenStack cloud.
region A discrete OpenStack environment with dedicated API endpoints that typically shares only the Identity
(keystone) with other regions.
registry Alternative term for the Image service registry.
registry server An Image service that provides VM image metadata information to clients.
Appendix 77
High Availability Guide (Release Version: 15.0.0)
Reliable, Autonomic Distributed Object Store (RADOS)
A collection of components that provides object storage within Ceph. Similar to OpenStack Object
Storage.
Remote Procedure Call (RPC) The method used by the Compute RabbitMQ for intra-service communications.
replica Provides data redundancy and fault tolerance by creating copies of Object Storage objects, accounts,
and containers so that they are not lost when the underlying storage fails.
replica count The number of replicas of the data in an Object Storage ring.
replication The process of copying data to a separate physical device for fault tolerance and performance.
replicator The Object Storage back-end process that creates and manages object replicas.
request ID Unique ID assigned to each request sent to Compute.
rescue image A special type of VM image that is booted when an instance is placed into rescue mode. Allows
an administrator to mount the file systems for an instance to correct the problem.
resize Converts an existing server to a different flavor, which scales the server up or down. The original server
is saved to enable rollback if a problem occurs. All resizes must be tested and explicitly confirmed, at
which time the original server is removed.
RESTful A kind of web service API that uses REST, or Representational State Transfer. REST is the style of
architecture for hypermedia systems that is used for the World Wide Web.
ring An entity that maps Object Storage data to partitions. A separate ring exists for each service, such as
account, object, and container.
ring builder Builds and manages rings within Object Storage, assigns partitions to devices, and pushes the
configuration to other storage nodes.
Rocky The code name for the eightteenth release of OpenStack. The design summit will take place in Vancouver,
Kanada. The release is named after the Rocky Mountains.
role A personality that a user assumes to perform a specific set of operations. A role includes a set of rights
and privileges. A user assuming that role inherits those rights and privileges.
Role Based Access Control (RBAC) Provides a predefined list of actions that the user can perform, such
as start or stop VMs, reset passwords, and so on. Supported in both Identity and Compute and can be
configured using the dashboard.
role ID Alphanumeric ID assigned to each Identity service role.
Root Cause Analysis (RCA) service (Vitrage) OpenStack project that aims to organize, analyze and visualize
OpenStack alarms and events, yield insights regarding the root cause of problems and deduce their
existence before they are directly detected.
rootwrap A feature of Compute that allows the unprivileged “nova” user to run a specified list of commands
as the Linux root user.
round-robin scheduler Type of Compute scheduler that evenly distributes instances among available hosts.
router A physical or virtual network device that passes network traffic between different networks.
routing key The Compute direct exchanges, fanout exchanges, and topic exchanges use this key to determine
how to process a message; processing varies depending on exchange type.
78 Appendix
High Availability Guide (Release Version: 15.0.0)
RPC driver Modular system that allows the underlying message queue software of Compute to be changed.
For example, from RabbitMQ to ZeroMQ or Qpid.
rsync Used by Object Storage to push object replicas.
RXTX cap Absolute limit on the amount of network traffic a Compute VM instance can send and receive.
RXTX quota Soft limit on the amount of network traffic a Compute VM instance can send and receive.
S
sahara Codename for the Data Processing service.
SAML assertion Contains information about a user as provided by the identity provider. It is an indication
that a user has been authenticated.
scheduler manager A Compute component that determines where VM instances should start. Uses modular
design to support a variety of scheduler types.
scoped token An Identity service API access token that is associated with a specific project.
scrubber Checks for and deletes unused VMs; the component of Image service that implements delayed
delete.
secret key String of text known only by the user; used along with an access key to make requests to the
Compute API.
secure boot Process whereby the system firmware validates the authenticity of the code involved in the boot
process.
secure shell (SSH) Open source tool used to access remote hosts through an encrypted communications channel,
SSH key injection is supported by Compute.
security group A set of network traffic filtering rules that are applied to a Compute instance.
segmented object An Object Storage large object that has been broken up into pieces. The re-assembled
object is called a concatenated object.
self-service For IaaS, ability for a regular (non-privileged) account to manage a virtual infrastructure component
such as networks without involving an administrator.
SELinux Linux kernel security module that provides the mechanism for supporting access control policies.
senlin Code name for the Clustering service.
server Computer that provides explicit services to the client software running on that system, often managing
a variety of computer operations. A server is a VM instance in the Compute system. Flavor and image
are requisite elements when creating a server.
server image Alternative term for a VM image.
server UUID Unique ID assigned to each guest VM instance.
service An OpenStack service, such as Compute, Object Storage, or Image service. Provides one or more
endpoints through which users can access resources and perform operations.
service catalog Alternative term for the Identity service catalog.
Service Function Chain (SFC) For a given service, SFC is the abstracted view of the required service functions
and the order in which they are to be applied.
service ID Unique ID assigned to each service that is available in the Identity service catalog.
Appendix 79
High Availability Guide (Release Version: 15.0.0)
Service Level Agreement (SLA) Contractual obligations that ensure the availability of a service.
service project Special project that contains all services that are listed in the catalog.
service provider A system that provides services to other system entities. In case of federated identity, OpenStack
Identity is the service provider.
service registration An Identity service feature that enables services, such as Compute, to automatically register
with the catalog.
service token An administrator-defined token used by Compute to communicate securely with the Identity
service.
session back end The method of storage used by horizon to track client sessions, such as local memory, cookies,
a database, or memcached.
session persistence A feature of the load-balancing service. It attempts to force subsequent connections to a
service to be redirected to the same node as long as it is online.
session storage A horizon component that stores and tracks client session information. Implemented through
the Django sessions framework.
share A remote, mountable file system in the context of the Shared File Systems service. You can mount a
share to, and access a share from, several hosts by several users at a time.
share network An entity in the context of the Shared File Systems service that encapsulates interaction with
the Networking service. If the driver you selected runs in the mode requiring such kind of interaction,
you need to specify the share network to create a share.
Shared File Systems API A Shared File Systems service that provides a stable RESTful API. The service authenticates
and routes requests throughout the Shared File Systems service. There is python-manilaclient
to interact with the API.
Shared File Systems service (manila) The service that provides a set of services for management of shared
file systems in a multi-project cloud environment, similar to how OpenStack provides block-based storage
management through the OpenStack Block Storage service project. With the Shared File Systems
service, you can create a remote file system and mount the file system on your instances. You can also
read and write data from your instances to and from your file system.
shared IP address An IP address that can be assigned to a VM instance within the shared IP group. Public IP
addresses can be shared across multiple servers for use in various high-availability scenarios. When an
IP address is shared to another server, the cloud network restrictions are modified to enable each server
to listen to and respond on that IP address. You can optionally specify that the target server network
configuration be modified. Shared IP addresses can be used with many standard heartbeat facilities,
such as keepalive, that monitor for failure and manage IP failover.
shared IP group A collection of servers that can share IPs with other members of the group. Any server in a
group can share one or more public IPs with any other server in the group. With the exception of the first
server in a shared IP group, servers must be launched into shared IP groups. A server may be a member
of only one shared IP group.
shared storage Block storage that is simultaneously accessible by multiple clients, for example, NFS.
Sheepdog Distributed block storage system for QEMU, supported by OpenStack.
Simple Cloud Identity Management (SCIM) Specification for managing identity in the cloud, currently unsupported
by OpenStack.
Simple Protocol for Independent Computing Environments (SPICE) SPICE provides remote desktop access
to guest virtual machines. It is an alternative to VNC. SPICE is supported by OpenStack.
80 Appendix
High Availability Guide (Release Version: 15.0.0)
Single-root I/O Virtualization (SR-IOV) A specification that, when implemented by a physical PCIe device,
enables it to appear as multiple separate PCIe devices. This enables multiple virtualized guests to share
direct access to the physical device, offering improved performance over an equivalent virtual device.
Currently supported in OpenStack Havana and later releases.
SmokeStack Runs automated tests against the core OpenStack API; written in Rails.
snapshot A point-in-time copy of an OpenStack storage volume or image. Use storage volume snapshots to
back up volumes. Use image snapshots to back up data, or as “gold” images for additional servers.
soft reboot A controlled reboot where a VM instance is properly restarted through operating system commands.
Software Development Lifecycle Automation service (solum) OpenStack project that aims to make cloud
services easier to consume and integrate with application development process by automating the sourceto-image
process, and simplifying app-centric deployment.
Software-defined networking (SDN) Provides an approach for network administrators to manage computer
network services through abstraction of lower-level functionality.
SolidFire Volume Driver The Block Storage driver for the SolidFire iSCSI storage appliance.
solum Code name for the Software Development Lifecycle Automation service.
spread-first scheduler The Compute VM scheduling algorithm that attempts to start a new VM on the host
with the least amount of load.
SQLAlchemy An open source SQL toolkit for Python, used in OpenStack.
SQLite A lightweight SQL database, used as the default persistent storage method in many OpenStack services.
stack A set of OpenStack resources created and managed by the Orchestration service according to a given
template (either an AWS CloudFormation template or a Heat Orchestration Template (HOT)).
StackTach Community project that captures Compute AMQP communications; useful for debugging.
static IP address Alternative term for a fixed IP address.
StaticWeb WSGI middleware component of Object Storage that serves container data as a static web page.
storage back end The method that a service uses for persistent storage, such as iSCSI, NFS, or local disk.
storage manager A XenAPI component that provides a pluggable interface to support a wide variety of persistent
storage back ends.
storage manager back end A persistent storage method supported by XenAPI, such as iSCSI or NFS.
storage node An Object Storage node that provides container services, account services, and object services;
controls the account databases, container databases, and object storage.
storage services Collective name for the Object Storage object services, container services, and account services.
strategy Specifies the authentication source used by Image service or Identity. In the Database service, it
refers to the extensions implemented for a data store.
subdomain A domain within a parent domain. Subdomains cannot be registered. Subdomains enable you to
delegate domains. Subdomains can themselves have subdomains, so third-level, fourth-level, fifth-level,
and deeper levels of nesting are possible.
subnet Logical subdivision of an IP network.
Appendix 81
High Availability Guide (Release Version: 15.0.0)
SUSE Linux Enterprise Server (SLES) A Linux distribution that is compatible with OpenStack.
suspend The VM instance is paused and its state is saved to disk of the host.
swap Disk-based virtual memory used by operating systems to provide more memory than is actually available
on the system.
swauth An authentication and authorization service for Object Storage, implemented through WSGI middleware;
uses Object Storage itself as the persistent backing store.
swift Codename for OpenStack Object Storage service.
swift All in One (SAIO) Creates a full Object Storage development environment within a single VM.
swift middleware Collective term for Object Storage components that provide additional functionality.
swift proxy server Acts as the gatekeeper to Object Storage and is responsible for authenticating the user.
swift storage node A node that runs Object Storage account, container, and object services.
sync point Point in time since the last container and accounts database sync among nodes within Object Storage.
sysadmin One of the default roles in the Compute RBAC system. Enables a user to add other users to a project,
interact with VM images that are associated with the project, and start and stop VM instances.
system usage A Compute component that, along with the notification system, collects meters and usage information.
This information can be used for billing.
T
tacker Code name for the NFV Orchestration service
Telemetry service (telemetry) The OpenStack project which collects measurements of the utilization of the
physical and virtual resources comprising deployed clouds, persists this data for subsequent retrieval and
analysis, and triggers actions when defined criteria are met.
TempAuth An authentication facility within Object Storage that enables Object Storage itself to perform
authentication and authorization. Frequently used in testing and development.
Tempest Automated software test suite designed to run against the trunk of the OpenStack core project.
TempURL An Object Storage middleware component that enables creation of URLs for temporary object
access.
tenant A group of users; used to isolate access to Compute resources. An alternative term for a project.
Tenant API An API that is accessible to projects.
tenant endpoint An Identity service API endpoint that is associated with one or more projects.
tenant ID An alternative term for project ID.
token An alpha-numeric string of text used to access OpenStack APIs and resources.
token services An Identity service component that manages and validates tokens after a user or project has
been authenticated.
tombstone Used to mark Object Storage objects that have been deleted; ensures that the object is not updated
on another node after it has been deleted.
82 Appendix
High Availability Guide (Release Version: 15.0.0)
topic publisher A process that is created when a RPC call is executed; used to push the message to the topic
exchange.
Torpedo Community project used to run automated tests against the OpenStack API.
transaction ID Unique ID assigned to each Object Storage request; used for debugging and tracing.
transient Alternative term for non-durable.
transient exchange Alternative term for a non-durable exchange.
transient message A message that is stored in memory and is lost after the server is restarted.
transient queue Alternative term for a non-durable queue.
TripleO OpenStack-on-OpenStack program. The code name for the OpenStack Deployment program.
trove Codename for OpenStack Database service.
trusted platform module (TPM) Specialized microprocessor for incorporating cryptographic keys into devices
for authenticating and securing a hardware platform.
U
Ubuntu A Debian-based Linux distribution.
unscoped token Alternative term for an Identity service default token.
updater Collective term for a group of Object Storage components that processes queued and failed updates
for containers and objects.
user In OpenStack Identity, entities represent individual API consumers and are owned by a specific domain.
In OpenStack Compute, a user can be associated with roles, projects, or both.
user data A blob of data that the user can specify when they launch an instance. The instance can access this
data through the metadata service or config drive. Commonly used to pass a shell script that the instance
runs on boot.
User Mode Linux (UML) An OpenStack-supported hypervisor.
V
VIF UUID Unique ID assigned to each Networking VIF.
Virtual Central Processing Unit (vCPU) Subdivides physical CPUs. Instances can then use those divisions.
Virtual Disk Image (VDI) One of the VM image disk formats supported by Image service.
Virtual Extensible LAN (VXLAN) A network virtualization technology that attempts to reduce the scalability
problems associated with large cloud computing deployments. It uses a VLAN-like encapsulation
technique to encapsulate Ethernet frames within UDP packets.
Virtual Hard Disk (VHD) One of the VM image disk formats supported by Image service.
virtual IP address (VIP) An Internet Protocol (IP) address configured on the load balancer for use by clients
connecting to a service that is load balanced. Incoming connections are distributed to back-end nodes
based on the configuration of the load balancer.
virtual machine (VM) An operating system instance that runs on top of a hypervisor. Multiple VMs can run
at the same time on the same physical host.
Appendix 83
High Availability Guide (Release Version: 15.0.0)
virtual network An L2 network segment within Networking.
Virtual Network Computing (VNC) Open source GUI and CLI tools used for remote console access to VMs.
Supported by Compute.
Virtual Network InterFace (VIF) An interface that is plugged into a port in a Networking network. Typically
a virtual network interface belonging to a VM.
virtual networking A generic term for virtualization of network functions such as switching, routing, load
balancing, and security using a combination of VMs and overlays on physical network infrastructure.
virtual port Attachment point where a virtual interface connects to a virtual network.
virtual private network (VPN) Provided by Compute in the form of cloudpipes, specialized instances that
are used to create VPNs on a per-project basis.
virtual server Alternative term for a VM or guest.
virtual switch (vSwitch) Software that runs on a host or node and provides the features and functions of a
hardware-based network switch.
virtual VLAN Alternative term for a virtual network.
VirtualBox An OpenStack-supported hypervisor.
Vitrage Code name for the Root Cause Analysis service.
VLAN manager A Compute component that provides dnsmasq and radvd and sets up forwarding to and from
cloudpipe instances.
VLAN network The Network Controller provides virtual networks to enable compute servers to interact with
each other and with the public network. All machines must have a public and private network interface.
A VLAN network is a private network interface, which is controlled by the vlan_interface option
with VLAN managers.
VM disk (VMDK) One of the VM image disk formats supported by Image service.
VM image Alternative term for an image.
VM Remote Control (VMRC) Method to access VM instance consoles using a web browser. Supported by
Compute.
VMware API Supports interaction with VMware products in Compute.
VMware NSX Neutron plug-in Provides support for VMware NSX in Neutron.
VNC proxy A Compute component that provides users access to the consoles of their VM instances through
VNC or VMRC.
volume Disk-based data storage generally represented as an iSCSI target with a file system that supports
extended attributes; can be persistent or ephemeral.
Volume API Alternative name for the Block Storage API.
volume controller A Block Storage component that oversees and coordinates storage volume actions.
volume driver Alternative term for a volume plug-in.
volume ID Unique ID applied to each storage volume under the Block Storage control.
volume manager A Block Storage component that creates, attaches, and detaches persistent storage volumes.
volume node A Block Storage node that runs the cinder-volume daemon.
84 Appendix
High Availability Guide (Release Version: 15.0.0)
volume plug-in Provides support for new and specialized types of back-end storage for the Block Storage
volume manager.
volume worker A cinder component that interacts with back-end storage to manage the creation and deletion
of volumes and the creation of compute volumes, provided by the cinder-volume daemon.
vSphere An OpenStack-supported hypervisor.
W
Watcher Code name for the Infrastructure Optimization service.
weight Used by Object Storage devices to determine which storage devices are suitable for the job. Devices
are weighted by size.
weighted cost The sum of each cost used when deciding where to start a new VM instance in Compute.
weighting A Compute process that determines the suitability of the VM instances for a job for a particular
host. For example, not enough RAM on the host, too many CPUs on the host, and so on.
worker A daemon that listens to a queue and carries out tasks in response to messages. For example, the
cinder-volume worker manages volume creation and deletion on storage arrays.
Workflow service (mistral) The OpenStack service that provides a simple YAML-based language to write
workflows (tasks and transition rules) and a service that allows to upload them, modify, run them at scale
and in a highly available manner, manage and monitor workflow execution state and state of individual
tasks.
X
X.509 X.509 is the most widely used standard for defining digital certificates. It is a data structure that contains
the subject (entity) identifiable information such as its name along with its public key. The certificate can
contain a few other attributes as well depending upon the version. The most recent and standard version
of X.509 is v3.
Xen Xen is a hypervisor using a microkernel design, providing services that allow multiple computer operating
systems to execute on the same computer hardware concurrently.
Xen API The Xen administrative API, which is supported by Compute.
Xen Cloud Platform (XCP) An OpenStack-supported hypervisor.
Xen Storage Manager Volume Driver A Block Storage volume plug-in that enables communication with
the Xen Storage Manager API.
XenServer An OpenStack-supported hypervisor.
XFS High-performance 64-bit file system created by Silicon Graphics. Excels in parallel I/O operations and
data consistency.
Z
zaqar Codename for the Message service.
ZeroMQ Message queue software supported by OpenStack. An alternative to RabbitMQ. Also spelled 0MQ.
Zuul Tool used in OpenStack development to ensure correctly ordered testing of changes in parallel.
Appendix 85
INDEX
Symbols
6to4, 51
A
absolute limit, 52
access control list (ACL), 52
access key, 52
account, 52
account auditor, 52
account database, 52
account reaper, 52
account server, 52
account service, 52
accounting, 52
Active Directory, 52
active/active configuration, 52
active/passive configuration, 52
address pool, 52
Address Resolution Protocol (ARP), 52
admin API, 52
admin server, 52
administrator, 52
Advanced Message Queuing Protocol (AMQP), 52
Advanced RISC Machine (ARM), 52
alert, 53
allocate, 53
Amazon Kernel Image (AKI), 53
Amazon Machine Image (AMI), 53
Amazon Ramdisk Image (ARI), 53
Anvil, 53
aodh, 53
Apache, 53
Apache License 2.0, 53
Apache Web Server, 53
API endpoint, 53
API extension, 53
API extension plug-in, 53
API key, 53
API server, 53
API token, 53
API version, 53
applet, 53
Application Catalog service (murano), 53
Application Programming Interface (API), 53
application server, 53
Application Service Provider (ASP), 53
arptables, 53
associate, 53
Asynchronous JavaScript and XML (AJAX), 54
ATA over Ethernet (AoE), 54
attach, 54
attachment (network), 54
auditing, 54
auditor, 54
Austin, 54
auth node, 54
authentication, 54
authentication token, 54
AuthN, 54
authorization, 54
authorization node, 54
AuthZ, 54
Auto ACK, 54
auto declare, 54
availability zone, 54
AWS CloudFormation template, 54
B
back end, 54
back-end catalog, 54
back-end store, 54
Backup, Restore, and Disaster Recovery service
(freezer), 55
bandwidth, 55
barbican, 55
bare, 55
Bare Metal service (ironic), 55
base image, 55
Bell-LaPadula model, 55
Benchmark service (rally), 55
Bexar, 55
binary, 55
86
High Availability Guide (Release Version: 15.0.0)
bit, 55
bits per second (BPS), 55
block device, 55
block migration, 55
Block Storage API, 55
Block Storage service (cinder), 55
BMC (Baseboard Management Controller), 55
bootable disk image, 55
Bootstrap Protocol (BOOTP), 55
Border Gateway Protocol (BGP), 55
browser, 56
builder file, 56
bursting, 56
button class, 56
byte, 56
C
cache pruner, 56
Cactus, 56
CALL, 56
capability, 56
capacity cache, 56
capacity updater, 56
CAST, 56
catalog, 56
catalog service, 56
ceilometer, 56
cell, 56
cell forwarding, 56
cell manager, 56
CentOS, 56
Ceph, 56
CephFS, 56
certificate authority (CA), 57
Challenge-Handshake Authentication Protocol
(CHAP), 57
chance scheduler, 57
changes since, 57
Chef, 57
child cell, 57
cinder, 57
CirrOS, 57
Cisco neutron plug-in, 57
cloud architect, 57
Cloud Auditing Data Federation (CADF), 57
cloud computing, 57
cloud controller, 57
cloud controller node, 57
Cloud Data Management Interface (CDMI), 57
Cloud Infrastructure Management Interface (CIMI),
57
cloud-init, 57
cloudadmin, 57
Cloudbase-Init, 57
cloudpipe, 57
cloudpipe image, 57
Clustering service (senlin), 58
command filter, 58
Common Internet File System (CIFS), 58
Common Libraries (oslo), 58
community project, 58
compression, 58
Compute API (Nova API), 58
compute controller, 58
compute host, 58
compute node, 58
Compute service (nova), 58
compute worker, 58
concatenated object, 58
conductor, 58
congress, 58
consistency window, 58
console log, 58
container, 58
container auditor, 58
container database, 58
container format, 59
Container Infrastructure Management service (magnum),
59
container server, 59
container service, 59
content delivery network (CDN), 59
controller node, 59
core API, 59
core service, 59
cost, 59
credentials, 59
CRL, 59
Cross-Origin Resource Sharing (CORS), 59
Crowbar, 59
current workload, 59
customer, 59
customization module, 59
D
daemon, 59
Dashboard (horizon), 59
data encryption, 59
Data loss prevention (DLP) software, 60
Data Processing service (sahara), 60
data store, 60
database ID, 60
Index 87
High Availability Guide (Release Version: 15.0.0)
database replicator, 60
Database service (trove), 60
deallocate, 60
Debian, 60
deduplication, 60
default panel, 60
default project, 60
default token, 60
delayed delete, 60
delivery mode, 60
denial of service (DoS), 60
deprecated auth, 60
designate, 60
Desktop-as-a-Service, 60
developer, 60
device ID, 60
device weight, 60
DevStack, 60
DHCP agent, 60
Diablo, 61
direct consumer, 61
direct exchange, 61
direct publisher, 61
disassociate, 61
Discretionary Access Control (DAC), 61
disk encryption, 61
disk format, 61
dispersion, 61
distributed virtual router (DVR), 61
Django, 61
DNS record, 61
DNS service (designate), 61
dnsmasq, 61
domain, 61
Domain Name System (DNS), 61
download, 61
durable exchange, 61
durable queue, 62
Dynamic Host Configuration Protocol (DHCP), 62
Dynamic HyperText Markup Language (DHTML), 62
E
east-west traffic, 62
EBS boot volume, 62
ebtables, 62
EC2, 62
EC2 access key, 62
EC2 API, 62
EC2 Compatibility API, 62
EC2 secret key, 62
Elastic Block Storage (EBS), 62
encapsulation, 62
encryption, 62
endpoint, 62
endpoint registry, 62
endpoint template, 62
entity, 62
ephemeral image, 62
ephemeral volume, 62
Essex, 63
ESXi, 63
ETag, 63
euca2ools, 63
Eucalyptus Kernel Image (EKI), 63
Eucalyptus Machine Image (EMI), 63
Eucalyptus Ramdisk Image (ERI), 63
evacuate, 63
exchange, 63
exchange type, 63
exclusive queue, 63
extended attributes (xattr), 63
extension, 63
external network, 63
extra specs, 63
F
FakeLDAP, 63
fan-out exchange, 63
federated identity, 63
Fedora, 63
Fibre Channel, 63
Fibre Channel over Ethernet (FCoE), 63
fill-first scheduler, 63
filter, 63
firewall, 64
FireWall-as-a-Service (FWaaS), 64
fixed IP address, 64
Flat Manager, 64
flat mode injection, 64
flat network, 64
FlatDHCP Manager, 64
flavor, 64
flavor ID, 64
floating IP address, 64
Folsom, 64
FormPost, 64
freezer, 64
front end, 64
G
gateway, 64
generic receive offload (GRO), 64
generic routing encapsulation (GRE), 64
88 Index
High Availability Guide (Release Version: 15.0.0)
glance, 64
glance API server, 64
glance registry, 64
global endpoint template, 64
GlusterFS, 65
gnocchi, 65
golden image, 65
Governance service (congress), 65
Graphic Interchange Format (GIF), 65
Graphics Processing Unit (GPU), 65
Green Threads, 65
Grizzly, 65
Group, 65
guest OS, 65
H
Hadoop, 65
Hadoop Distributed File System (HDFS), 65
handover, 65
HAProxy, 65
hard reboot, 65
Havana, 65
health monitor, 65
heat, 65
Heat Orchestration Template (HOT), 65
high availability (HA), 65
horizon, 65
horizon plug-in, 66
host, 66
host aggregate, 66
Host Bus Adapter (HBA), 66
hybrid cloud, 66
Hyper-V, 66
hyperlink, 66
Hypertext Transfer Protocol (HTTP), 66
Hypertext Transfer Protocol Secure (HTTPS), 66
hypervisor, 66
hypervisor pool, 66
I
Icehouse, 66
ID number, 66
Identity API, 66
Identity back end, 66
identity provider, 66
Identity service (keystone), 66
Identity service API, 66
IETF, 67
image, 67
Image API, 67
image cache, 67
image ID, 67
image membership, 67
image owner, 67
image registry, 67
Image service (glance), 67
image status, 67
image store, 67
image UUID, 67
incubated project, 67
Infrastructure Optimization service (watcher), 67
Infrastructure-as-a-Service (IaaS), 67
ingress filtering, 67
INI format, 67
injection, 67
Input/Output Operations Per Second (IOPS), 67
instance, 67
instance ID, 67
instance state, 67
instance tunnels network, 68
instance type, 68
instance type ID, 68
instance UUID, 68
Intelligent Platform Management Interface (IPMI), 68
interface, 68
interface ID, 68
Internet Control Message Protocol (ICMP), 68
Internet protocol (IP), 68
Internet Service Provider (ISP), 68
Internet Small Computer System Interface (iSCSI), 68
IP address, 68
IP Address Management (IPAM), 68
ip6tables, 68
ipset, 68
iptables, 68
ironic, 68
iSCSI Qualified Name (IQN), 68
ISO9660, 68
itsec, 68
J
Java, 69
JavaScript, 69
JavaScript Object Notation (JSON), 69
jumbo frame, 69
Juno, 69
K
Kerberos, 69
kernel-based VM (KVM), 69
Key Manager service (barbican), 69
keystone, 69
Kickstart, 69
Kilo, 69
Index 89
High Availability Guide (Release Version: 15.0.0)
L
large object, 69
Launchpad, 69
Layer-2 (L2) agent, 69
Layer-2 network, 69
Layer-3 (L3) agent, 69
Layer-3 network, 69
Liberty, 69
libvirt, 70
Lightweight Directory Access Protocol (LDAP), 70
Linux, 70
Linux bridge, 70
Linux Bridge neutron plug-in, 70
Linux containers (LXC), 70
live migration, 70
load balancer, 70
load balancing, 70
Load-Balancer-as-a-Service (LBaaS), 70
Load-balancing service (octavia), 70
Logical Volume Manager (LVM), 70
M
magnum, 70
management API, 70
management network, 70
manager, 70
manifest, 70
manifest object, 70
manila, 70
manila-share, 70
maximum transmission unit (MTU), 70
mechanism driver, 70
melange, 70
membership, 71
membership list, 71
memcached, 71
memory overcommit, 71
message broker, 71
message bus, 71
message queue, 71
Message service (zaqar), 71
Meta-Data Server (MDS), 71
Metadata agent, 71
migration, 71
mistral, 71
Mitaka, 71
Modular Layer 2 (ML2) neutron plug-in, 71
monasca, 71
Monitor (LBaaS), 71
Monitor (Mon), 71
Monitoring (monasca), 71
multi-factor authentication, 71
multi-host, 71
multinic, 71
murano, 71
N
Nebula, 72
netadmin, 72
NetApp volume driver, 72
network, 72
Network Address Translation (NAT), 72
network controller, 72
Network File System (NFS), 72
network ID, 72
network manager, 72
network namespace, 72
network node, 72
network segment, 72
Network Service Header (NSH), 72
Network Time Protocol (NTP), 72
network UUID, 72
network worker, 72
Networking API (Neutron API), 72
Networking service (neutron), 72
neutron, 72
neutron API, 72
neutron manager, 72
neutron plug-in, 72
Newton, 73
Nexenta volume driver, 73
NFV Orchestration Service (tacker), 73
Nginx, 73
No ACK, 73
node, 73
non-durable exchange, 73
non-durable queue, 73
non-persistent volume, 73
north-south traffic, 73
nova, 73
Nova API, 73
nova-network, 73
O
object, 73
object auditor, 73
object expiration, 73
object hash, 73
object path hash, 73
object replicator, 73
object server, 73
Object Storage API, 73
Object Storage Device (OSD), 73
90 Index
High Availability Guide (Release Version: 15.0.0)
Object Storage service (swift), 73
object versioning, 74
Ocata, 74
Octavia, 74
Oldie, 74
Open Cloud Computing Interface (OCCI), 74
Open Virtualization Format (OVF), 74
Open vSwitch, 74
Open vSwitch (OVS) agent, 74
Open vSwitch neutron plug-in, 74
OpenLDAP, 74
OpenStack, 74
OpenStack code name, 74
openSUSE, 74
operator, 74
optional service, 74
Orchestration service (heat), 74
orphan, 74
Oslo, 74
P
panko, 74
parent cell, 74
partition, 75
partition index, 75
partition shift value, 75
path MTU discovery (PMTUD), 75
pause, 75
PCI passthrough, 75
persistent message, 75
persistent volume, 75
personality file, 75
Pike, 75
Platform-as-a-Service (PaaS), 75
plug-in, 75
policy service, 75
policy-based routing (PBR), 75
pool, 75
pool member, 75
port, 75
port UUID, 75
preseed, 75
private image, 75
private IP address, 75
private network, 75
project, 76
project ID, 76
project VPN, 76
promiscuous mode, 76
protected property, 76
provider, 76
proxy node, 76
proxy server, 76
public API, 76
public image, 76
public IP address, 76
public key authentication, 76
public network, 76
Puppet, 76
Python, 76
Q
QEMU Copy On Write 2 (QCOW2), 76
Qpid, 76
Quality of Service (QoS), 76
quarantine, 76
Queens, 76
Quick EMUlator (QEMU), 76
quota, 77
R
RabbitMQ, 77
Rackspace Cloud Files, 77
RADOS Block Device (RBD), 77
radvd, 77
rally, 77
RAM filter, 77
RAM overcommit, 77
rate limit, 77
raw, 77
rebalance, 77
reboot, 77
rebuild, 77
Recon, 77
record, 77
record ID, 77
Red Hat Enterprise Linux (RHEL), 77
reference architecture, 77
region, 77
registry, 77
registry server, 77
Reliable, Autonomic Distributed Object Store, 78
Remote Procedure Call (RPC), 78
replica, 78
replica count, 78
replication, 78
replicator, 78
request ID, 78
rescue image, 78
resize, 78
RESTful, 78
ring, 78
ring builder, 78
Index 91
High Availability Guide (Release Version: 15.0.0)
Rocky, 78
role, 78
Role Based Access Control (RBAC), 78
role ID, 78
Root Cause Analysis (RCA) service (Vitrage), 78
rootwrap, 78
round-robin scheduler, 78
router, 78
routing key, 78
RPC driver, 79
rsync, 79
RXTX cap, 79
RXTX quota, 79
S
sahara, 79
SAML assertion, 79
scheduler manager, 79
scoped token, 79
scrubber, 79
secret key, 79
secure boot, 79
secure shell (SSH), 79
security group, 79
segmented object, 79
self-service, 79
SELinux, 79
senlin, 79
server, 79
server image, 79
server UUID, 79
service, 79
service catalog, 79
Service Function Chain (SFC), 79
service ID, 79
Service Level Agreement (SLA), 80
service project, 80
service provider, 80
service registration, 80
service token, 80
session back end, 80
session persistence, 80
session storage, 80
share, 80
share network, 80
Shared File Systems API, 80
Shared File Systems service (manila), 80
shared IP address, 80
shared IP group, 80
shared storage, 80
Sheepdog, 80
Simple Cloud Identity Management (SCIM), 80
Simple Protocol for Independent Computing Environments
(SPICE), 80
Single-root I/O Virtualization (SR-IOV), 81
SmokeStack, 81
snapshot, 81
soft reboot, 81
Software Development Lifecycle Automation service
(solum), 81
Software-defined networking (SDN), 81
SolidFire Volume Driver, 81
solum, 81
spread-first scheduler, 81
SQLAlchemy, 81
SQLite, 81
stack, 81
StackTach, 81
static IP address, 81
StaticWeb, 81
storage back end, 81
storage manager, 81
storage manager back end, 81
storage node, 81
storage services, 81
strategy, 81
subdomain, 81
subnet, 81
SUSE Linux Enterprise Server (SLES), 82
suspend, 82
swap, 82
swauth, 82
swift, 82
swift All in One (SAIO), 82
swift middleware, 82
swift proxy server, 82
swift storage node, 82
sync point, 82
sysadmin, 82
system usage, 82
T
tacker, 82
Telemetry service (telemetry), 82
TempAuth, 82
Tempest, 82
TempURL, 82
tenant, 82
Tenant API, 82
tenant endpoint, 82
tenant ID, 82
token, 82
92 Index
High Availability Guide (Release Version: 15.0.0)
token services, 82
tombstone, 82
topic publisher, 83
Torpedo, 83
transaction ID, 83
transient, 83
transient exchange, 83
transient message, 83
transient queue, 83
TripleO, 83
trove, 83
trusted platform module (TPM), 83
U
Ubuntu, 83
unscoped token, 83
updater, 83
user, 83
user data, 83
User Mode Linux (UML), 83
V
VIF UUID, 83
Virtual Central Processing Unit (vCPU), 83
Virtual Disk Image (VDI), 83
Virtual Extensible LAN (VXLAN), 83
Virtual Hard Disk (VHD), 83
virtual IP address (VIP), 83
virtual machine (VM), 83
virtual network, 84
Virtual Network Computing (VNC), 84
Virtual Network InterFace (VIF), 84
virtual networking, 84
virtual port, 84
virtual private network (VPN), 84
virtual server, 84
virtual switch (vSwitch), 84
virtual VLAN, 84
VirtualBox, 84
Vitrage, 84
VLAN manager, 84
VLAN network, 84
VM disk (VMDK), 84
VM image, 84
VM Remote Control (VMRC), 84
VMware API, 84
VMware NSX Neutron plug-in, 84
VNC proxy, 84
volume, 84
Volume API, 84
volume controller, 84
volume driver, 84
volume ID, 84
volume manager, 84
volume node, 84
volume plug-in, 85
volume worker, 85
vSphere, 85
W
Watcher, 85
weight, 85
weighted cost, 85
weighting, 85
worker, 85
Workflow service (mistral), 85
X
X.509, 85
Xen, 85
Xen API, 85
Xen Cloud Platform (XCP), 85
Xen Storage Manager Volume Driver, 85
XenServer, 85
XFS, 85
Z
zaqar, 85
ZeroMQ, 85
Zuul, 85
Index 93
Operations Guide
Release Version: 15.0.0
OpenStack contributors
May 12, 2017
CONTENTS
Abstract 1
Contents 2
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Factors affecting OpenStack deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Planning for deploying and provisioning OpenStack . . . . . . . . . . . . . . . . . . . . . . . . . 13
Capacity planning and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Lay of the Land . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
Managing Projects and Users . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
User-Facing Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
Maintenance, Failures, and Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Network Troubleshooting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
Logging and Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
Backup and Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
Customization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
Advanced Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
Upgrades . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
Index 209
i
ABSTRACT
This guide provides information about operating OpenStack clouds.
We recommend that you turn to the Installation Tutorials and Guides, which contains a step-by-step guide on
how to manually install the OpenStack packages and dependencies on your cloud.
While it is important for an operator to be familiar with the steps involved in deploying OpenStack, we also
strongly encourage you to evaluate OpenStack deployment tools and configuration-management tools, such as
Puppet or Chef, which can help automate this deployment process.
In this guide, we assume that you have successfully deployed an OpenStack cloud and are able to perform basic
operations such as adding images, booting instances, and attaching volumes.
As your focus turns to stable operations, we recommend that you do skim this guide to get a sense of the content.
Some of this content is useful to read in advance so that you can put best practices into effect to simplify your
life in the long run. Other content is more useful as a reference that you might turn to when an unexpected event
occurs (such as a power failure), or to troubleshoot a particular problem.
1
CONTENTS
Acknowledgements
The OpenStack Foundation supported the creation of this book with plane tickets to Austin, lodging (including
one adventurous evening without power after a windstorm), and delicious food. For about USD $10,000, we
could collaborate intensively for a week in the same room at the Rackspace Austin office. The authors are all
members of the OpenStack Foundation, which you can join. Go to the Foundation web site.
We want to acknowledge our excellent host Rackers at Rackspace in Austin:
• Emma Richards of Rackspace Guest Relations took excellent care of our lunch orders and even set aside
a pile of sticky notes that had fallen off the walls.
• Betsy Hagemeier, a Fanatical Executive Assistant, took care of a room reshuffle and helped us settle in
for the week.
• The Real Estate team at Rackspace in Austin, also known as “The Victors,” were super responsive.
• Adam Powell in Racker IT supplied us with bandwidth each day and second monitors for those of us
needing more screens.
• On Wednesday night we had a fun happy hour with the Austin OpenStack Meetup group and Racker
Katie Schmidt took great care of our group.
We also had some excellent input from outside of the room:
• Tim Bell from CERN gave us feedback on the outline before we started and reviewed it mid-week.
• Sébastien Han has written excellent blogs and generously gave his permission for re-use.
• Oisin Feeley read it, made some edits, and provided emailed feedback right when we asked.
Inside the book sprint room with us each day was our book sprint facilitator Adam Hyde. Without his tireless
support and encouragement, we would have thought a book of this scope was impossible in five days. Adam
has proven the book sprint method effectively again and again. He creates both tools and faith in collaborative
authoring at www.booksprints.net.
We couldn’t have pulled it off without so much supportive help and encouragement.
Preface
OpenStack is an open source platform that lets you build an Infrastructure-as-a-Service (IaaS) cloud that runs
on commodity hardware.
Introduction to OpenStack
OpenStack believes in open source, open design, and open development, all in an open community that encourages
participation by anyone. The long-term vision for OpenStack is to produce a ubiquitous open source cloud
computing platform that meets the needs of public and private cloud providers regardless of size. OpenStack
services control large pools of compute, storage, and networking resources throughout a data center.
2
Operations Guide (Release Version: 15.0.0)
The technology behind OpenStack consists of a series of interrelated projects delivering various components for
a cloud infrastructure solution. Each service provides an open API so that all of these resources can be managed
through a dashboard that gives administrators control while empowering users to provision resources through
a web interface, a command-line client, or software development kits that support the API. Many OpenStack
APIs are extensible, meaning you can keep compatibility with a core set of calls while providing access to more
resources and innovating through API extensions. The OpenStack project is a global collaboration of developers
and cloud computing technologists. The project produces an open standard cloud computing platform for both
public and private clouds. By focusing on ease of implementation, massive scalability, a variety of rich features,
and tremendous extensibility, the project aims to deliver a practical and reliable cloud solution for all types of
organizations.
Getting Started with OpenStack
As an open source project, one of the unique aspects of OpenStack is that it has many different levels at which
you can begin to engage with it—you don’t have to do everything yourself.
Using OpenStack
You could ask, “Do I even need to build a cloud?” If you want to start using a compute or storage service by
just swiping your credit card, you can go to eNovance, HP, Rackspace, or other organizations to start using their
public OpenStack clouds. Using their OpenStack cloud resources is similar to accessing the publicly available
Amazon Web Services Elastic Compute Cloud (EC2) or Simple Storage Solution (S3).
Plug and Play OpenStack
However, the enticing part of OpenStack might be to build your own private cloud, and there are several ways
to accomplish this goal. Perhaps the simplest of all is an appliance-style solution. You purchase an appliance,
unpack it, plug in the power and the network, and watch it transform into an OpenStack cloud with minimal
additional configuration.
However, hardware choice is important for many applications, so if that applies to you, consider that there
are several software distributions available that you can run on servers, storage, and network products of your
choosing. Canonical (where OpenStack replaced Eucalyptus as the default cloud option in 2011), Red Hat,
and SUSE offer enterprise OpenStack solutions and support. You may also want to take a look at some of the
specialized distributions, such as those from Rackspace, Piston, SwiftStack, or Cloudscaling.
Alternatively, if you want someone to help guide you through the decisions about the underlying hardware or
your applications, perhaps adding in a few features or integrating components along the way, consider contacting
one of the system integrators with OpenStack experience, such as Mirantis or Metacloud.
If your preference is to build your own OpenStack expertise internally, a good way to kick-start that might be
to attend or arrange a training session. The OpenStack Foundation has a Training Marketplace where you can
look for nearby events. Also, the OpenStack community is working to produce open source training materials.
Roll Your Own OpenStack
However, this guide has a different audience—those seeking flexibility from the OpenStack framework by
deploying do-it-yourself solutions.
OpenStack is designed for horizontal scalability, so you can easily add new compute, network, and storage
resources to grow your cloud over time. In addition to the pervasiveness of massive OpenStack public clouds,
Preface 3
Operations Guide (Release Version: 15.0.0)
many organizations, such as PayPal, Intel, and Comcast, build large-scale private clouds. OpenStack offers
much more than a typical software package because it lets you integrate a number of different technologies to
construct a cloud. This approach provides great flexibility, but the number of options might be daunting at first.
Who This Book Is For
This book is for those of you starting to run OpenStack clouds as well as those of you who were handed an
operational one and want to keep it running well. Perhaps you’re on a DevOps team, perhaps you are a system
administrator starting to dabble in the cloud, or maybe you want to get on the OpenStack cloud team at your
company. This book is for all of you.
This guide assumes that you are familiar with a Linux distribution that supports OpenStack, SQL databases,
and virtualization. You must be comfortable administering and configuring multiple Linux machines for networking.
You must install and maintain an SQL database and occasionally run queries against it.
One of the most complex aspects of an OpenStack cloud is the networking configuration. You should be familiar
with concepts such as DHCP, Linux bridges, VLANs, and iptables. You must also have access to a network
hardware expert who can configure the switches and routers required in your OpenStack cloud.
Note: Cloud computing is quite an advanced topic, and this book requires a lot of background knowledge.
However, if you are fairly new to cloud computing, we recommend that you make use of the Glossary at the
back of the book, as well as the online documentation for OpenStack and additional resources mentioned in this
book in Resources.
Further Reading
There are other books on the OpenStack documentation website that can help you get the job done.
Installation Tutorials and Guides Describes a manual installation process, as in, by hand, without automation,
for multiple distributions based on a packaging system:
• OpenStack Installation Tutorial for openSUSE and SUSE Linux Enterprise
• OpenStack Installation Tutorial for Red Hat Enterprise Linux and CentOS
• OpenStack Installation Tutorial for Ubuntu
OpenStack Configuration Reference Contains a reference listing of all configuration options for core and
integrated OpenStack services by release version
OpenStack Architecture Design Guide Contains guidelines for designing an OpenStack cloud
OpenStack Administrator Guide Contains how-to information for managing an OpenStack cloud as needed
for your use cases, such as storage, computing, or software-defined-networking
OpenStack High Availability Guide Describes potential strategies for making your OpenStack services and
related controllers and data stores highly available
OpenStack Security Guide Provides best practices and conceptual information about securing an OpenStack
cloud
Virtual Machine Image Guide Shows you how to obtain, create, and modify virtual machine images that are
compatible with OpenStack
4 Preface
Operations Guide (Release Version: 15.0.0)
OpenStack End User Guide Shows OpenStack end users how to create and manage resources in an OpenStack
cloud with the OpenStack dashboard and OpenStack client commands
OpenStack Networking Guide This guide targets OpenStack administrators seeking to deploy and manage
OpenStack Networking (neutron).
OpenStack API Guide A brief overview of how to send REST API requests to endpoints for OpenStack services
How This Book Is Organized
This book contains several parts to show best practices and tips for the repeated operations for running OpenStack
clouds.
Lay of the Land This chapter is written to let you get your hands wrapped around your OpenStack cloud
through command-line tools and understanding what is already set up in your cloud.
Managing Projects and Users This chapter walks through user-enabling processes that all admins must face
to manage users, give them quotas to parcel out resources, and so on.
User-Facing Operations This chapter shows you how to use OpenStack cloud resources and how to train your
users.
Maintenance, Failures, and Debugging This chapter goes into the common failures that the authors have seen
while running clouds in production, including troubleshooting.
Network Troubleshooting Because network troubleshooting is especially difficult with virtual resources, this
chapter is chock-full of helpful tips and tricks for tracing network traffic, finding the root cause of networking
failures, and debugging related services, such as DHCP and DNS.
Logging and Monitoring This chapter shows you where OpenStack places logs and how to best read and manage
logs for monitoring purposes.
Backup and Recovery This chapter describes what you need to back up within OpenStack as well as best
practices for recovering backups.
Customization For readers who need to get a specialized feature into OpenStack, this chapter describes how
to use DevStack to write custom middleware or a custom scheduler to rebalance your resources.
Advanced Configuration Much of OpenStack is driver-oriented, so you can plug in different solutions to the
base set of services. This chapter describes some advanced configuration topics.
Upgrades This chapter provides upgrade information based on the architectures used in this book.
Back matter:
Use Cases You can read a small selection of use cases from the OpenStack community with some technical
details and further resources.
Tales From the Cryp^H^H^H^H Cloud These are shared legendary tales of image disappearances, VM massacres,
and crazy troubleshooting techniques that result in hard-learned lessons and wisdom.
Working with Roadmaps Read about how to track the OpenStack roadmap through the open and transparent
development processes.
Resources So many OpenStack resources are available online because of the fast-moving nature of the project,
but there are also resources listed here that the authors found helpful while learning themselves.
Glossary A list of terms used in this book is included, which is a subset of the larger OpenStack glossary
available online.
Preface 5
Operations Guide (Release Version: 15.0.0)
Why and How We Wrote This Book
We wrote this book because we have deployed and maintained OpenStack clouds for at least a year and we
wanted to share this knowledge with others. After months of being the point people for an OpenStack cloud,
we also wanted to have a document to hand to our system administrators so that they’d know how to operate
the cloud on a daily basis—both reactively and pro-actively. We wanted to provide more detailed technical
information about the decisions that deployers make along the way.
We wrote this book to help you:
• Design and create an architecture for your first nontrivial OpenStack cloud. After you read this guide,
you’ll know which questions to ask and how to organize your compute, networking, and storage resources
and the associated software packages.
• Perform the day-to-day tasks required to administer a cloud.
We wrote this book in a book sprint, which is a facilitated, rapid development production method for books.
For more information, see the BookSprints site. Your authors cobbled this book together in five days during
February 2013, fueled by caffeine and the best takeout food that Austin, Texas, could offer.
On the first day, we filled white boards with colorful sticky notes to start to shape this nebulous book about how
to architect and operate clouds:
We wrote furiously from our own experiences and bounced ideas between each other. At regular intervals we
reviewed the shape and organization of the book and further molded it, leading to what you see today.
The team includes:
Tom Fifield After learning about scalability in computing from particle physics experiments, such as ATLAS
at the Large Hadron Collider (LHC) at CERN, Tom worked on OpenStack clouds in production to support
the Australian public research sector. Tom currently serves as an OpenStack community manager and
works on OpenStack documentation in his spare time.
Diane Fleming Diane works on the OpenStack API documentation tirelessly. She helped out wherever she
could on this project.
6 Preface
Operations Guide (Release Version: 15.0.0)
Anne Gentle Anne is the documentation coordinator for OpenStack and also served as an individual contributor
to the Google Documentation Summit in 2011, working with the Open Street Maps team. She has
worked on book sprints in the past, with FLOSS Manuals’ Adam Hyde facilitating. Anne lives in Austin,
Texas.
Lorin Hochstein An academic turned software-developer-slash-operator, Lorin worked as the lead architect
for Cloud Services at Nimbis Services, where he deploys OpenStack for technical computing applications.
He has been working with OpenStack since the Cactus release. Previously, he worked on highperformance
computing extensions for OpenStack at University of Southern California’s Information
Sciences Institute (USC-ISI).
Adam Hyde Adam facilitated this book sprint. He also founded the book sprint methodology and is the
most experienced book-sprint facilitator around. See BookSprints for more information. Adam founded
FLOSS Manuals—a community of some 3,000 individuals developing Free Manuals about Free Software.
He is also the founder and project manager for Booktype, an open source project for writing,
editing, and publishing books online and in print.
Jonathan Proulx Jon has been piloting an OpenStack cloud as a senior technical architect at the MIT Computer
Science and Artificial Intelligence Lab for his researchers to have as much computing power as they need.
He started contributing to OpenStack documentation and reviewing the documentation so that he could
accelerate his learning.
Everett Toews Everett is a developer advocate at Rackspace making OpenStack and the Rackspace Cloud easy
to use. Sometimes developer, sometimes advocate, and sometimes operator, he’s built web applications,
taught workshops, given presentations around the world, and deployed OpenStack for production use by
academia and business.
Joe Topjian Joe has designed and deployed several clouds at Cybera, a nonprofit where they are building einfrastructure
to support entrepreneurs and local researchers in Alberta, Canada. He also actively maintains
and operates these clouds as a systems architect, and his experiences have generated a wealth of
troubleshooting skills for cloud environments.
OpenStack community members Many individual efforts keep a community book alive. Our community
members updated content for this book year-round. Also, a year after the first sprint, Jon Proulx hosted
a second two-day mini-sprint at MIT with the goal of updating the book for the latest release. Since the
book’s inception, more than 30 contributors have supported this book. We have a tool chain for reviews,
continuous builds, and translations. Writers and developers continuously review patches, enter doc bugs,
edit content, and fix doc bugs. We want to recognize their efforts!
The following people have contributed to this book: Akihiro Motoki, Alejandro Avella, Alexandra Settle,
Andreas Jaeger, Andy McCallum, Benjamin Stassart, Chandan Kumar, Chris Ricker, David Cramer,
David Wittman, Denny Zhang, Emilien Macchi, Gauvain Pocentek, Ignacio Barrio, James E. Blair, Jay
Clark, Jeff White, Jeremy Stanley, K Jonathan Harker, KATO Tomoyuki, Lana Brindley, Laura Alves,
Lee Li, Lukasz Jernas, Mario B. Codeniera, Matthew Kassawara, Michael Still, Monty Taylor, Nermina
Miller, Nigel Williams, Phil Hopkins, Russell Bryant, Sahid Orentino Ferdjaoui, Sandy Walsh, Sascha
Peilicke, Sean M. Collins, Sergey Lukjanov, Shilla Saebi, Stephen Gordon, Summer Long, Uwe Stuehler,
Vaibhav Bhatkar, Veronica Musso, Ying Chun “Daisy” Guo, Zhengguang Ou, and ZhiQiang Fan.
How to Contribute to This Book
The genesis of this book was an in-person event, but now that the book is in your hands, we want you to
contribute to it. OpenStack documentation follows the coding principles of iterative work, with bug logging,
investigating, and fixing. We also store the source content on GitHub and invite collaborators through the
OpenStack Gerrit installation, which offers reviews. For the O’Reilly edition of this book, we are using the
Preface 7
Operations Guide (Release Version: 15.0.0)
company’s Atlas system, which also stores source content on GitHub and enables collaboration among contributors.
Learn more about how to contribute to the OpenStack docs at OpenStack Documentation Contributor Guide.
If you find a bug and can’t fix it or aren’t sure it’s really a doc bug, log a bug at OpenStack Manuals. Tag the
bug under Extra options with the ops-guide tag to indicate that the bug is in this guide. You can assign the
bug to yourself if you know how to fix it. Also, a member of the OpenStack doc-core team can triage the doc
bug.
Conventions
The OpenStack documentation uses several typesetting conventions.
Notices
Notices take these forms:
Note: A comment with additional information that explains a part of the text.
Important: Something you must be aware of before proceeding.
Tip: An extra but helpful piece of practical advice.
Caution: Helpful information that prevents the user from making mistakes.
Warning: Critical information about the risk of data loss or security issues.
Command prompts
$ command
Any user, including the root user, can run commands that are prefixed with the $ prompt.
# command
The root user must run commands that are prefixed with the # prompt. You can also prefix these commands
with the sudo command, if available, to run them.
8 Conventions
Operations Guide (Release Version: 15.0.0)
Factors affecting OpenStack deployment
Security requirements
When deploying OpenStack in an enterprise as a private cloud, it is usually behind the firewall and within
the trusted network alongside existing systems. Users are employees that are bound by the company security
requirements. This tends to drive most of the security domains towards a more trusted model. However, when
deploying OpenStack in a public facing role, no assumptions can be made and the attack vectors significantly
increase.
Consider the following security implications and requirements:
• Managing the users for both public and private clouds. The Identity service allows for LDAP to be part
of the authentication process. This may ease user management if integrating into existing systems.
• User authentication requests include sensitive information including usernames, passwords, and authentication
tokens. It is strongly recommended to place API services behind hardware that performs SSL
termination.
• Negative or hostile users who would attack or compromise the security of your deployment regardless
of firewalls or security agreements.
• Attack vectors increase further in a public facing OpenStack deployment. For example, the API endpoints
and the software behind it become vulnerable to hostile entities attempting to gain unauthorized access
or prevent access to services. You should provide appropriate filtering and periodic security auditing.
Warning: Be mindful of consistency when utilizing third party clouds to explore authentication options.
For more information OpenStack Security, see the OpenStack Security Guide.
Security domains
A security domain comprises of users, applications, servers or networks that share common trust requirements
and expectations within a system. Typically they have the same authentication and authorization requirements
and users.
Security domains include:
Public security domains The public security domain can refer to the internet as a whole or networks over
which you have no authority. This domain is considered untrusted. For example, in a hybrid cloud
deployment, any information traversing between and beyond the clouds is in the public domain and
untrustworthy.
Guest security domains The guest security domain handles compute data generated by instances on the cloud,
but not services that support the operation of the cloud, such as API calls. Public cloud providers and
private cloud providers who do not have stringent controls on instance use or who allow unrestricted
internet access to instances should consider this domain to be untrusted. Private cloud providers may
want to consider this network as internal and therefore trusted only if they have controls in place to
assert that they trust instances and all their tenants.
Management security domains The management security domain is where services interact. Sometimes referred
to as the control plane, the networks in this domain transport confidential data such as configuration
Factors affecting OpenStack deployment 9
Operations Guide (Release Version: 15.0.0)
parameters, user names, and passwords. In most deployments this domain is considered trusted when it
is behind an organization’s firewall.
Data security domains The data security domain is primarily concerned with information pertaining to the
storage services within OpenStack. The data that crosses this network has high integrity and confidentiality
requirements and, depending on the type of deployment, may also have strong availability
requirements. The trust level of this network is heavily dependent on other deployment decisions.
These security domains can be individually or collectively mapped to an OpenStack deployment. The cloud
operator should be aware of the appropriate security concerns. Security domains should be mapped out against
your specific OpenStack deployment topology. The domains and their trust requirements depend upon whether
the cloud instance is public, private, or hybrid.
Hypervisor security
The hypervisor also requires a security assessment. In a public cloud, organizations typically do not have control
over the choice of hypervisor. Properly securing your hypervisor is important. Attacks made upon the unsecured
hypervisor are called a hypervisor breakout. Hypervisor breakout describes the event of a compromised or
malicious instance breaking out of the resource controls of the hypervisor and gaining access to the bare metal
operating system and hardware resources.
Hypervisor security is not an issue if the security of instances is not important. However, enterprises can
minimize vulnerability by avoiding hardware sharing with others in a public cloud.
Baremetal security
There are other services worth considering that provide a bare metal instance instead of a cloud. In other cases,
it is possible to replicate a second private cloud by integrating with a private Cloud-as-a-Service deployment.
The organization does not buy the hardware, but also does not share with other tenants. It is also possible to
use a provider that hosts a bare-metal public cloud instance for which the hardware is dedicated only to one
customer, or a provider that offers private Cloud-as-a-Service.
Important: Each cloud implements services differently. Understand the security requirements of every cloud
that handles the organization’s data or workloads.
Networking security
Consider security implications and requirements before designing the physical and logical network topologies.
Make sure that the networks are properly segregated and traffic flows are going to the correct destinations
without crossing through locations that are undesirable. Consider the following factors:
• Firewalls
• Overlay interconnects for joining separated tenant networks
• Routing through or avoiding specific networks
How networks attach to hypervisors can expose security vulnerabilities. To mitigate hypervisor breakouts,
separate networks from other systems and schedule instances for the network onto dedicated Compute nodes.
This prevents attackers from having access to the networks from a compromised instance.
10 Factors affecting OpenStack deployment
Operations Guide (Release Version: 15.0.0)
Multi-site security
Securing a multi-site OpenStack installation brings several challenges. Tenants may expect a tenant-created
network to be secure. In a multi-site installation the use of a non-private connection between sites may be
required. This may mean that traffic would be visible to third parties and, in cases where an application requires
security, this issue requires mitigation. In these instances, install a VPN or encrypted connection between sites
to conceal sensitive traffic.
Identity is another security consideration. Authentication centralization provides a single authentication point
for users across the deployment, and a single administration point for traditional create, read, update, and delete
operations. Centralized authentication is also useful for auditing purposes because all authentication tokens
originate from the same source.
Tenants in multi-site installations need isolation from each other. The main challenge is ensuring tenant networks
function across regions which is not currently supported in OpenStack Networking (neutron). Therefore
an external system may be required to manage mapping. Tenant networks may contain sensitive information
requiring accurate and consistent mapping to ensure that a tenant in one site does not connect to a different
tenant in another site.
Legal requirements
Using remote resources for collection, processing, storage, and retrieval provides potential benefits to businesses.
With the rapid growth of data within organizations, businesses need to be proactive about their data
storage strategies from a compliance point of view.
Most countries have legislative and regulatory requirements governing the storage and management of data in
cloud environments. This is particularly relevant for public, community and hybrid cloud models, to ensure
data privacy and protection for organizations using a third party cloud provider.
Common areas of regulation include:
• Data retention policies ensuring storage of persistent data and records management to meet data archival
requirements.
• Data ownership policies governing the possession and responsibility for data.
• Data sovereignty policies governing the storage of data in foreign countries or otherwise separate jurisdictions.
• Data compliance policies governing certain types of information needing to reside in certain locations
due to regulatory issues - and more importantly, cannot reside in other locations for the same reason.
• Data location policies ensuring that the services deployed to the cloud are used according to laws and
regulations in place for the employees, foreign subsidiaries, or third parties.
• Disaster recovery policies ensuring regular data backups and relocation of cloud applications to another
supplier in scenarios where a provider may go out of business, or their data center could become inoperable.
• Security breach policies governing the ways to notify individuals through cloud provider’s systems or
other means if their personal data gets compromised in any way.
• Industry standards policy governing additional requirements on what type of cardholder data may or may
not be stored and how it is to be protected.
This is an example of such legal frameworks:
Factors affecting OpenStack deployment 11
Operations Guide (Release Version: 15.0.0)
Data storage regulations in Europe are currently driven by provisions of the Data protection framework. Financial
Industry Regulatory Authority works on this in the United States.
Privacy and security are spread over different industry-specific laws and regulations:
• Health Insurance Portability and Accountability Act (HIPAA)
• Gramm-Leach-Bliley Act (GLBA)
• Payment Card Industry Data Security Standard (PCI DSS)
• Family Educational Rights and Privacy Act (FERPA)
Cloud security architecture
Cloud security architecture should recognize the issues that arise with security management, which addresses
these issues with security controls. Cloud security controls are put in place to safeguard any weaknesses in the
system, and reduce the effect of an attack.
The following security controls are described below.
Deterrent controls: Typically reduce the threat level by informing potential attackers that there will be adverse
consequences for them if they proceed.
Preventive controls: Strengthen the system against incidents, generally by reducing if not actually eliminating
vulnerabilities.
Detective controls: Intended to detect and react appropriately to any incidents that occur. System and network
security monitoring, including intrusion detection and prevention arrangements, are typically employed
to detect attacks on cloud systems and the supporting communications infrastructure.
Corrective controls: Reduce the consequences of an incident, normally by limiting the damage. They come
into effect during or after an incident. Restoring system backups in order to rebuild a compromised
system is an example of a corrective control.
For more information, see See also NIST Special Publication 800-53.
Software licensing
The many different forms of license agreements for software are often written with the use of dedicated hardware
in mind. This model is relevant for the cloud platform itself, including the hypervisor operating system,
supporting software for items such as database, RPC, backup, and so on. Consideration must be made when
offering Compute service instances and applications to end users of the cloud, since the license terms for that
software may need some adjustment to be able to operate economically in the cloud.
Multi-site OpenStack deployments present additional licensing considerations over and above regular OpenStack
clouds, particularly where site licenses are in use to provide cost efficient access to software licenses. The
licensing for host operating systems, guest operating systems, OpenStack distributions (if applicable), softwaredefined
infrastructure including network controllers and storage systems, and even individual applications need
to be evaluated.
Topics to consider include:
• The definition of what constitutes a site in the relevant licenses, as the term does not necessarily denote
a geographic or otherwise physically isolated location.
12 Factors affecting OpenStack deployment
Operations Guide (Release Version: 15.0.0)
• Differentiations between “hot” (active) and “cold” (inactive) sites, where significant savings may be
made in situations where one site is a cold standby for disaster recovery purposes only.
• Certain locations might require local vendors to provide support and services for each site which may
vary with the licensing agreement in place.
Planning for deploying and provisioning OpenStack
The decisions you make with respect to provisioning and deployment will affect your maintenance of the cloud.
Your configuration management will be able to evolve over time. However, more thought and design need to
be done for upfront choices about deployment, disk partitioning, and network configuration.
A critical part of a cloud’s scalability is the amount of effort that it takes to run your cloud. To minimize the
operational cost of running your cloud, set up and use an automated deployment and configuration infrastructure
with a configuration management system, such as Puppet or Chef. Combined, these systems greatly reduce
manual effort and the chance for operator error.
This infrastructure includes systems to automatically install the operating system’s initial configuration and
later coordinate the configuration of all services automatically and centrally, which reduces both manual effort
and the chance for error. Examples include Ansible, CFEngine, Chef, Puppet, and Salt. You can even use
OpenStack to deploy OpenStack, named TripleO (OpenStack On OpenStack).
Automated deployment
An automated deployment system installs and configures operating systems on new servers, without intervention,
after the absolute minimum amount of manual work, including physical racking, MAC-to-IP assignment,
and power configuration. Typically, solutions rely on wrappers around PXE boot and TFTP servers for the
basic operating system install and then hand off to an automated configuration management system.
Both Ubuntu and Red Hat Enterprise Linux include mechanisms for configuring the operating system, including
preseed and kickstart, that you can use after a network boot. Typically, these are used to bootstrap an automated
configuration system. Alternatively, you can use an image-based approach for deploying the operating system,
such as systemimager. You can use both approaches with a virtualized infrastructure, such as when you run
VMs to separate your control services and physical infrastructure.
When you create a deployment plan, focus on a few vital areas because they are very hard to modify post
deployment. The next two sections talk about configurations for:
• Disk partitioning and disk array setup for scalability
• Networking configuration just for PXE booting
Disk partitioning and RAID
At the very base of any operating system are the hard drives on which the operating system (OS) is installed.
You must complete the following configurations on the server’s hard drives:
• Partitioning, which provides greater flexibility for layout of operating system and swap space, as described
below.
• Adding to a RAID array (RAID stands for redundant array of independent disks), based on the number of
disks you have available, so that you can add capacity as your cloud grows. Some options are described
in more detail below.
Planning for deploying and provisioning OpenStack 13
Operations Guide (Release Version: 15.0.0)
The simplest option to get started is to use one hard drive with two partitions:
• File system to store files and directories, where all the data lives, including the root partition that starts
and runs the system.
• Swap space to free up memory for processes, as an independent area of the physical disk used only for
swapping and nothing else.
RAID is not used in this simplistic one-drive setup because generally for production clouds, you want to ensure
that if one disk fails, another can take its place. Instead, for production, use more than one disk. The number
of disks determine what types of RAID arrays to build.
We recommend that you choose one of the following multiple disk options:
Option 1 Partition all drives in the same way in a horizontal fashion, as shown in Partition setup of drives.
With this option, you can assign different partitions to different RAID arrays. You can allocate partition 1
of disk one and two to the /boot partition mirror. You can make partition 2 of all disks the root partition
mirror. You can use partition 3 of all disks for a cinder-volumes LVM partition running on a RAID
10 array.
Fig. 1: Partition setup of drives
While you might end up with unused partitions, such as partition 1 in disk three and four of this example,
this option allows for maximum utilization of disk space. I/O performance might be an issue as a result
of all disks being used for all tasks.
Option 2 Add all raw disks to one large RAID array, either hardware or software based. You can partition this
large array with the boot, root, swap, and LVM areas. This option is simple to implement and uses all
partitions. However, disk I/O might suffer.
Option 3 Dedicate entire disks to certain partitions. For example, you could allocate disk one and two entirely
to the boot, root, and swap partitions under a RAID 1 mirror. Then, allocate disk three and four entirely
to the LVM partition, also under a RAID 1 mirror. Disk I/O should be better because I/O is focused on
dedicated tasks. However, the LVM partition is much smaller.
Tip: You may find that you can automate the partitioning itself. For example, MIT uses Fully Automatic
Installation (FAI) to do the initial PXE-based partition and then install using a combination of min/max and
percentage-based partitioning.
As with most architecture choices, the right answer depends on your environment. If you are using existing
hardware, you know the disk density of your servers and can determine some decisions based on the options
above. If you are going through a procurement process, your user’s requirements also help you determine hardware
purchases. Here are some examples from a private cloud providing web developers custom environments
at AT&T. This example is from a specific deployment, so your existing hardware or procurement opportunity
may vary from this. AT&T uses three types of hardware in its deployment:
14 Planning for deploying and provisioning OpenStack
Operations Guide (Release Version: 15.0.0)
• Hardware for controller nodes, used for all stateless OpenStack API services. About 32–64 GB memory,
small attached disk, one processor, varied number of cores, such as 6–12.
• Hardware for compute nodes. Typically 256 or 144 GB memory, two processors, 24 cores. 4–6 TB direct
attached storage, typically in a RAID 5 configuration.
• Hardware for storage nodes. Typically for these, the disk space is optimized for the lowest cost per GB
of storage while maintaining rack-space efficiency.
Again, the right answer depends on your environment. You have to make your decision based on the trade-offs
between space utilization, simplicity, and I/O performance.
Network configuration
Network configuration is a very large topic that spans multiple areas of this book. For now, make sure that your
servers can PXE boot and successfully communicate with the deployment server.
For example, you usually cannot configure NICs for VLANs when PXE booting. Additionally, you usually
cannot PXE boot with bonded NICs. If you run into this scenario, consider using a simple 1 GB switch in a
private network on which only your cloud communicates.
Automated configuration
The purpose of automatic configuration management is to establish and maintain the consistency of a system
without using human intervention. You want to maintain consistency in your deployments so that you can have
the same cloud every time, repeatably. Proper use of automatic configuration-management tools ensures that
components of the cloud systems are in particular states, in addition to simplifying deployment, and configuration
change propagation.
These tools also make it possible to test and roll back changes, as they are fully repeatable. Conveniently, a
large body of work has been done by the OpenStack community in this space. Puppet, a configuration management
tool, even provides official modules for OpenStack projects in an OpenStack infrastructure system
known as Puppet OpenStack. Chef configuration management is provided within https://git.openstack.org/
cgit/openstack/openstack-chef-repo. Additional configuration management systems include Juju, Ansible, and
Salt. Also, PackStack is a command-line utility for Red Hat Enterprise Linux and derivatives that uses Puppet
modules to support rapid deployment of OpenStack on existing servers over an SSH connection.
An integral part of a configuration-management system is the item that it controls. You should carefully consider
all of the items that you want, or do not want, to be automatically managed. For example, you may not want to
automatically format hard drives with user data.
Remote management
In our experience, most operators don’t sit right next to the servers running the cloud, and many don’t necessarily
enjoy visiting the data center. OpenStack should be entirely remotely configurable, but sometimes not
everything goes according to plan.
In this instance, having an out-of-band access into nodes running OpenStack components is a boon. The IPMI
protocol is the de facto standard here, and acquiring hardware that supports it is highly recommended to achieve
that lights-out data center aim.
Planning for deploying and provisioning OpenStack 15
Operations Guide (Release Version: 15.0.0)
In addition, consider remote power control as well. While IPMI usually controls the server’s power state, having
remote access to the PDU that the server is plugged into can really be useful for situations when everything
seems wedged.
Other considerations
You can save time by understanding the use cases for the cloud you want to create. Use cases for OpenStack
are varied. Some include object storage only; others require preconfigured compute resources to speed
development-environment set up; and others need fast provisioning of compute resources that are already secured
per tenant with private networks. Your users may have need for highly redundant servers to make sure
their legacy applications continue to run. Perhaps a goal would be to architect these legacy applications so that
they run on multiple instances in a cloudy, fault-tolerant way, but not make it a goal to add to those clusters
over time. Your users may indicate that they need scaling considerations because of heavy Windows server use.
You can save resources by looking at the best fit for the hardware you have in place already. You might have
some high-density storage hardware available. You could format and repurpose those servers for OpenStack
Object Storage. All of these considerations and input from users help you build your use case and your deployment
plan.
Tip: For further research about OpenStack deployment, investigate the supported and documented preconfigured,
prepackaged installers for OpenStack from companies such as Canonical, Cisco, Cloudscaling, IBM,
Metacloud, Mirantis, Rackspace, Red Hat, SUSE, and SwiftStack.
Capacity planning and scaling
Cloud-based applications typically request more discrete hardware (horizontal scaling) as opposed to traditional
applications, which require larger hardware to scale (vertical scaling).
OpenStack is designed to be horizontally scalable. Rather than switching to larger servers, you procure more
servers and simply install identically configured services. Ideally, you scale out and load balance among groups
of functionally identical services (for example, compute nodes or nova-api nodes), that communicate on a
message bus.
Determining cloud scalability
Determining the scalability of your cloud and how to improve it requires balancing many variables. No one
solution meets everyone’s scalability goals. However, it is helpful to track a number of metrics. You can define
virtual hardware templates called “flavors” in OpenStack, which will impact your cloud scaling decisions.
These templates define sizes for memory in RAM, root disk size, amount of ephemeral data disk space available,
and the number of CPU cores.
The default OpenStack flavors are shown in Table. OpenStack default flavors.
16 Capacity planning and scaling
Operations Guide (Release Version: 15.0.0)
Table 1: Table. OpenStack default flavors
Name Virtual cores Memory Disk Ephemeral
m1.tiny 1 512 MB 1 GB 0 GB
m1.small 1 2 GB 10 GB 20 GB
m1.medium 2 4 GB 10 GB 40 GB
m1.large 4 8 GB 10 GB 80 GB
m1.xlarge 8 16 GB 10 GB 160 GB
The starting point is the core count of your cloud. By applying some ratios, you can gather information about:
• The number of virtual machines (VMs) you expect to run, ((overcommit fraction × cores) /
virtual cores per instance)
• How much storage is required (flavor disk size × number of instances)
You can use these ratios to determine how much additional infrastructure you need to support your cloud.
Here is an example using the ratios for gathering scalability information for the number of VMs expected as
well as the storage needed. The following numbers support (200 / 2) × 16 = 1600 VM instances and require 80
TB of storage for /var/lib/nova/instances:
• 200 physical cores.
• Most instances are size m1.medium (two virtual cores, 50 GB of storage).
• Default CPU overcommit ratio (cpu_allocation_ratio in the nova.conf file) of 16:1.
Note: Regardless of the overcommit ratio, an instance can not be placed on any physical node with fewer raw
(pre-overcommit) resources than instance flavor requires.
However, you need more than the core count alone to estimate the load that the API services, database servers,
and queue servers are likely to encounter. You must also consider the usage patterns of your cloud.
As a specific example, compare a cloud that supports a managed web-hosting platform with one running integration
tests for a development project that creates one VM per code commit. In the former, the heavy work
of creating a VM happens only every few months, whereas the latter puts constant heavy load on the cloud
controller. You must consider your average VM lifetime, as a larger number generally means less load on the
cloud controller.
Aside from the creation and termination of VMs, you must consider the impact of users accessing the service
particularly on nova-api and its associated database. Listing instances garners a great deal of information
and, given the frequency with which users run this operation, a cloud with a large number of users can increase
the load significantly. This can occur even without their knowledge. For example, leaving the OpenStack
dashboard instances tab open in the browser refreshes the list of VMs every 30 seconds.
After you consider these factors, you can determine how many cloud controller cores you require. A typical
eight core, 8 GB of RAM server is sufficient for up to a rack of compute nodes — given the above caveats.
You must also consider key hardware specifications for the performance of user VMs, as well as budget and
performance needs, including storage performance (spindles/core), memory availability (RAM/core), network
bandwidth hardware specifications and (Gbps/core), and overall CPU performance (CPU/core).
Tip: For a discussion of metric tracking, including how to extract metrics from your cloud, see the OpenStack
Capacity planning and scaling 17
Operations Guide (Release Version: 15.0.0)
Operations Guide.
Adding cloud controller nodes
You can facilitate the horizontal expansion of your cloud by adding nodes. Adding compute nodes is straightforward
since they are easily picked up by the existing installation. However, you must consider some important
points when you design your cluster to be highly available.
A cloud controller node runs several different services. You can install services that communicate only using the
message queue internally— nova-scheduler and nova-console on a new server for expansion. However,
other integral parts require more care.
You should load balance user-facing services such as dashboard, nova-api, or the Object Storage proxy. Use
any standard HTTP load-balancing method (DNS round robin, hardware load balancer, or software such as
Pound or HAProxy). One caveat with dashboard is the VNC proxy, which uses the WebSocket protocol—
something that an L7 load balancer might struggle with. See also Horizon session storage.
You can configure some services, such as nova-api and glance-api, to use multiple processes by changing
a flag in their configuration file allowing them to share work between multiple cores on the one machine.
Tip: Several options are available for MySQL load balancing, and the supported AMQP brokers have built-in
clustering support. Information on how to configure these and many of the other services can be found in the
Operations Guide.
Segregating your cloud
Segregating your cloud is needed when users require different regions for legal considerations for data storage,
redundancy across earthquake fault lines, or for low-latency API calls. It can be segregated by cells, regions,
availability zones, or host aggregates.
Each method provides different functionality and can be best divided into two groups:
• Cells and regions, which segregate an entire cloud and result in running separate Compute deployments.
• Availability zones and host aggregates, which merely divide a single Compute deployment.
Table. OpenStack segregation methods provides a comparison view of each segregation method currently provided
by OpenStack Compute.
18 Capacity planning and scaling
Operations Guide (Release Version: 15.0.0)
Table 2: Table. OpenStack segregation methods
Cells Regions Availability zones Host
aggregates
Use A single API endpoint for
compute, or you require a
second level of scheduling.
Discrete regions with
separate API endpoints and
no coordination between
regions.
Logical separation
within your nova
deployment for
physical isolation or
redundancy.
To schedule
a group of
hosts with
common
features.
Example
A cloud with multiple sites
where you can schedule
VMs “anywhere” or on a
particular site.
A cloud with multiple sites,
where you schedule VMs to
a particular site and you
want a shared
infrastructure.
A single-site cloud
with equipment fed
by separate power
supplies.
Scheduling
to hosts with
trusted
hardware
support.
Overhead
Considered experimental.
A new service, nova-cells.
Each cell has a full nova
installation except
nova-api.
A different API endpoint
for every region. Each
region has a full nova
installation.
Configuration
changes to
nova.conf.
Configuration
changes
to
nova.conf.
Shared
services
Keystone, nova-api Keystone Keystone, All nova
services
Keystone,
All nova
services
Cells and regions
OpenStack Compute cells are designed to allow running the cloud in a distributed fashion without having to use
more complicated technologies, or be invasive to existing nova installations. Hosts in a cloud are partitioned
into groups called cells. Cells are configured in a tree. The top-level cell (“API cell”) has a host that runs
the nova-api service, but no nova-compute services. Each child cell runs all of the other typical nova-*
services found in a regular installation, except for the nova-api service. Each cell has its own message queue
and database service and also runs nova-cells, which manages the communication between the API cell and
child cells.
This allows for a single API server being used to control access to multiple cloud installations. Introducing a
second level of scheduling (the cell selection), in addition to the regular nova-scheduler selection of hosts,
provides greater flexibility to control where virtual machines are run.
Unlike having a single API endpoint, regions have a separate API endpoint per installation, allowing for a more
discrete separation. Users wanting to run instances across sites have to explicitly select a region. However, the
additional complexity of a running a new service is not required.
The OpenStack dashboard (horizon) can be configured to use multiple regions. This can be configured through
the AVAILABLE_REGIONS parameter.
Availability zones and host aggregates
You can use availability zones, host aggregates, or both to partition a nova deployment. Both methods are
configured and implemented in a similar way.
Capacity planning and scaling 19
Operations Guide (Release Version: 15.0.0)
Availability zone
This enables you to arrange OpenStack compute hosts into logical groups and provides a form of physical
isolation and redundancy from other availability zones, such as by using a separate power supply or network
equipment.
You define the availability zone in which a specified compute host resides locally on each server. An availability
zone is commonly used to identify a set of servers that have a common attribute. For instance, if some of
the racks in your data center are on a separate power source, you can put servers in those racks in their own
availability zone. Availability zones can also help separate different classes of hardware.
When users provision resources, they can specify from which availability zone they want their instance to
be built. This allows cloud consumers to ensure that their application resources are spread across disparate
machines to achieve high availability in the event of hardware failure.
Host aggregates zone
This enables you to partition OpenStack Compute deployments into logical groups for load balancing and instance
distribution. You can use host aggregates to further partition an availability zone. For example, you might
use host aggregates to partition an availability zone into groups of hosts that either share common resources,
such as storage and network, or have a special property, such as trusted computing hardware.
A common use of host aggregates is to provide information for use with the nova-scheduler. For example,
you might use a host aggregate to group a set of hosts that share specific flavors or images.
The general case for this is setting key-value pairs in the aggregate metadata and matching key-value pairs in
flavor’s extra_specs metadata. The AggregateInstanceExtraSpecsFilter in the filter scheduler will
enforce that instances be scheduled only on hosts in aggregates that define the same key to the same value.
An advanced use of this general concept allows different flavor types to run with different CPU and RAM
allocation ratios so that high-intensity computing loads and low-intensity development and testing systems
can share the same cloud without either starving the high-use systems or wasting resources on low-utilization
systems. This works by setting metadata in your host aggregates and matching extra_specs in your flavor
types.
The first step is setting the aggregate metadata keys cpu_allocation_ratio and ram_allocation_ratio to
a floating-point value. The filter schedulers AggregateCoreFilter and AggregateRamFilter will use those
values rather than the global defaults in nova.conf when scheduling to hosts in the aggregate. Be cautious
when using this feature, since each host can be in multiple aggregates, but should have only one allocation ratio
for each resources. It is up to you to avoid putting a host in multiple aggregates that define different values for
the same resource.
This is the first half of the equation. To get flavor types that are guaranteed a particular ratio, you must set
the extra_specs in the flavor type to the key-value pair you want to match in the aggregate. For example,
if you define extra_specs cpu_allocation_ratio to “1.0”, then instances of that type will run in
aggregates only where the metadata key cpu_allocation_ratio is also defined as “1.0.” In practice, it is
better to define an additional key-value pair in the aggregate metadata to match on rather than match directly
on cpu_allocation_ratio or core_allocation_ratio. This allows better abstraction. For example, by
defining a key overcommit and setting a value of “high,” “medium,” or “low,” you could then tune the numeric
allocation ratios in the aggregates without also needing to change all flavor types relating to them.
Note: Previously, all services had an availability zone. Currently, only the nova-compute service has its own
availability zone. Services such as nova-scheduler, nova-network, and nova-conductor have always
20 Capacity planning and scaling
Operations Guide (Release Version: 15.0.0)
spanned all availability zones.
When you run any of the following operations, the services appear in their own internal availability zone
(CONF.internal_service_availability_zone):
• openstack host list (os-hosts)
• euca-describe-availability-zones verbose
• openstack compute service list
The internal availability zone is hidden in euca-describe-availability_zones (nonverbose).
CONF.node_availability_zone has been renamed to CONF.default_availability_zone and is used only by the
nova-api and nova-scheduler services.
CONF.node_availability_zone still works but is deprecated.
Scalable Hardware
While several resources already exist to help with deploying and installing OpenStack, it’s very important to
make sure that you have your deployment planned out ahead of time. This guide presumes that you have set
aside a rack for the OpenStack cloud but also offers suggestions for when and what to scale.
Hardware Procurement
“The Cloud” has been described as a volatile environment where servers can be created and terminated at will.
While this may be true, it does not mean that your servers must be volatile. Ensuring that your cloud’s hardware
is stable and configured correctly means that your cloud environment remains up and running.
OpenStack can be deployed on any hardware supported by an OpenStack compatible Linux distribution.
Hardware does not have to be consistent, but it should at least have the same type of CPU to support instance
migration.
The typical hardware recommended for use with OpenStack is the standard value-for-money offerings that most
hardware vendors stock. It should be straightforward to divide your procurement into building blocks such as
“compute,” “object storage,” and “cloud controller,” and request as many of these as you need. Alternatively,
any existing servers you have that meet performance requirements and virtualization technology are likely to
support OpenStack.
Capacity Planning
OpenStack is designed to increase in size in a straightforward manner. Taking into account the considerations
previous mentioned, particularly on the sizing of the cloud controller, it should be possible to procure additional
compute or object storage nodes as needed. New nodes do not need to be the same specification or vendor as
existing nodes.
For compute nodes, nova-scheduler will manage differences in sizing with core count and RAM. However,
you should consider that the user experience changes with differing CPU speeds. When adding object storage
nodes, a weight should be specified that reflects the capability of the node.
Monitoring the resource usage and user growth will enable you to know when to procure. The Logging and
Monitoring chapte in the Operations Guide details some useful metrics.
Capacity planning and scaling 21
Operations Guide (Release Version: 15.0.0)
Burn-in Testing
The chances of failure for the server’s hardware are high at the start and the end of its life. As a result, dealing
with hardware failures while in production can be avoided by appropriate burn-in testing to attempt to trigger
the early-stage failures. The general principle is to stress the hardware to its limits. Examples of burn-in tests
include running a CPU or disk benchmark for several days.
Lay of the Land
This chapter helps you set up your working environment and use it to take a look around your cloud.
Using the OpenStack Dashboard for Administration
As a cloud administrative user, you can use the OpenStack dashboard to create and manage projects, users,
images, and flavors. Users are allowed to create and manage images within specified projects and to share
images, depending on the Image service configuration. Typically, the policy configuration allows admin users
only to set quotas and create and manage services. The dashboard provides an Admin tab with a System Panel
and an Identity tab. These interfaces give you access to system information and usage as well as to settings for
configuring what end users can do. Refer to the OpenStack Administrator Guide for detailed how-to information
about using the dashboard as an admin user.
Command-Line Tools
We recommend using a combination of the OpenStack command-line interface (CLI) tools and the OpenStack
dashboard for administration. Some users with a background in other cloud technologies may be using the EC2
Compatibility API, which uses naming conventions somewhat different from the native API.
The pip utility is used to manage package installation from the PyPI archive and is available in the python-pip
package in most Linux distributions. While each OpenStack project has its own client, they are being deprecated
in favour of a common OpenStack client. It is generally recommended to install the OpenStack client.
Tip: To perform testing and orchestration, it is usually easier to install the OpenStack CLI tools in a dedicated
VM in the cloud. We recommend that you keep the VM installation simple. All the tools should be installed
from a single OpenStack release version. If you need to run tools from multiple OpenStack releases, then we
recommend that you run with multiple VMs that are each running a dedicated version.
Install OpenStack command-line clients
For instructions on installing, upgrading, or removing command-line clients, see the Install the OpenStack
command-line clients section in OpenStack End User Guide.
Note: If you support the EC2 API on your cloud, you should also install the euca2ools package or some other
EC2 API tool so that you can get the same view your users have. Using EC2 API-based tools is mostly out of
the scope of this guide, though we discuss getting credentials for use with it.
22 Lay of the Land
Operations Guide (Release Version: 15.0.0)
Administrative Command-Line Tools
There are also several *-manage command-line tools. These are installed with the project’s services on the
cloud controller and do not need to be installed separately:
• nova-manage
• glance-manage
• keystone-manage
• cinder-manage
Unlike the CLI tools mentioned above, the *-manage tools must be run from the cloud controller, as root,
because they need read access to the config files such as /etc/nova/nova.conf and to make queries directly
against the database rather than against the OpenStack API endpoints.
Warning: The existence of the *-manage tools is a legacy issue. It is a goal of the OpenStack project
to eventually migrate all of the remaining functionality in the *-manage tools into the API-based tools.
Until that day, you need to SSH into the cloud controller node to perform some maintenance operations that
require one of the *-manage tools.
Getting Credentials
You must have the appropriate credentials if you want to use the command-line tools to make queries against
your OpenStack cloud. By far, the easiest way to obtain authentication credentials to use with command-line
clients is to use the OpenStack dashboard. Select Project, click the Project tab, and click Access & Security
on the Compute category. On the Access & Security page, click the API Access tab to display two buttons,
Download OpenStack RC File and Download EC2 Credentials, which let you generate files that you can source
in your shell to populate the environment variables the command-line tools require to know where your service
endpoints and your authentication information are. The user you logged in to the dashboard dictates the filename
for the openrc file, such as demo-openrc.sh. When logged in as admin, the file is named admin-openrc.sh.
The generated file looks something like this:
#!/usr/bin/env bash
# To use an OpenStack cloud you need to authenticate against the Identity
# service named keystone, which returns a **Token** and **Service Catalog**.
# The catalog contains the endpoints for all services the user/tenant has
# access to - such as Compute, Image Service, Identity, Object Storage, Block
# Storage, and Networking (code-named nova, glance, keystone, swift,
# cinder, and neutron).
#
# *NOTE*: Using the 3 *Identity API* does not necessarily mean any other
# OpenStack API is version 3. For example, your cloud provider may implement
# Image API v1.1, Block Storage API v2, and Compute API v2.0. OS_AUTH_URL is
# only for the Identity API served through keystone.
export OS_AUTH_URL=http://203.0.113.10:5000/v3
# With the addition of Keystone we have standardized on the term **project**
# as the entity that owns the resources.
export OS_PROJECT_ID=98333aba48e756fa8f629c83a818ad57
export OS_PROJECT_NAME="test-project"
export OS_USER_DOMAIN_NAME="default"
Lay of the Land 23
Operations Guide (Release Version: 15.0.0)
if [ -z "$OS_USER_DOMAIN_NAME" ]; then unset OS_USER_DOMAIN_NAME; fi
# In addition to the owning entity (tenant), OpenStack stores the entity
# performing the action as the **user**.
export OS_USERNAME="demo"
# With Keystone you pass the keystone password.
echo "Please enter your OpenStack Password for project $OS_PROJECT_NAME as user $OS_
,→USERNAME: "
read -sr OS_PASSWORD_INPUT
export OS_PASSWORD=$OS_PASSWORD_INPUT
# If your configuration has multiple regions, we set that information here.
# OS_REGION_NAME is optional and only valid in certain environments.
export OS_REGION_NAME="RegionOne"
# Don't leave a blank variable, unset it if it was empty
if [ -z "$OS_REGION_NAME" ]; then unset OS_REGION_NAME; fi
export OS_INTERFACE=public
export OS_IDENTITY_API_VERSION=3
Warning: This does not save your password in plain text, which is a good thing. But when you source or
run the script, it prompts you for your password and then stores your response in the environment variable
OS_PASSWORD. It is important to note that this does require interactivity. It is possible to store a value
directly in the script if you require a noninteractive operation, but you then need to be extremely cautious
with the security and permissions of this file.
EC2 compatibility credentials can be downloaded by selecting Project, then Compute, then Access & Security,
then API Access to display the Download EC2 Credentials button. Click the button to generate a ZIP file with
server x509 certificates and a shell script fragment. Create a new directory in a secure location because these
are live credentials containing all the authentication information required to access your cloud identity, unlike
the default user-openrc. Extract the ZIP file here. You should have cacert.pem, cert.pem, ec2rc.sh, and
pk.pem. The ec2rc.sh is similar to this:
#!/bin/bash
NOVARC=$(readlink -f "${BASH_SOURCE:-${0}}" 2>/dev/null) ||\
NOVARC=$(python -c 'import os,sys; \
print os.path.abspath(os.path.realpath(sys.argv[1]))' "${BASH_SOURCE:-${0}}")
NOVA_KEY_DIR=${NOVARC%/*}
export EC2_ACCESS_KEY=df7f93ec47e84ef8a347bbb3d598449a
export EC2_SECRET_KEY=ead2fff9f8a344e489956deacd47e818
export EC2_URL=http://203.0.113.10:8773/services/Cloud
export EC2_USER_ID=42 # nova does not use user id, but bundling requires it
export EC2_PRIVATE_KEY=${NOVA_KEY_DIR}/pk.pem
export EC2_CERT=${NOVA_KEY_DIR}/cert.pem
export NOVA_CERT=${NOVA_KEY_DIR}/cacert.pem
export EUCALYPTUS_CERT=${NOVA_CERT} # euca-bundle-image seems to require this
alias ec2-bundle-image="ec2-bundle-image --cert $EC2_CERT --privatekey \
$EC2_PRIVATE_KEY --user 42 --ec2cert $NOVA_CERT"
alias ec2-upload-bundle="ec2-upload-bundle -a $EC2_ACCESS_KEY -s \
$EC2_SECRET_KEY --url $S3_URL --ec2cert $NOVA_CERT"
24 Lay of the Land
Operations Guide (Release Version: 15.0.0)
To put the EC2 credentials into your environment, source the ec2rc.sh file.
Inspecting API Calls
The command-line tools can be made to show the OpenStack API calls they make by passing the --debug flag
to them. For example:
# openstack --debug server list
This example shows the HTTP requests from the client and the responses from the endpoints, which can be
helpful in creating custom tools written to the OpenStack API.
Using cURL for further inspection
Underlying the use of the command-line tools is the OpenStack API, which is a RESTful API that runs over
HTTP. There may be cases where you want to interact with the API directly or need to use it because of a
suspected bug in one of the CLI tools. The best way to do this is to use a combination of cURL and another
tool, such as jq, to parse the JSON from the responses.
The first thing you must do is authenticate with the cloud using your credentials to get an authentication token.
Your credentials are a combination of username, password, and tenant (project). You can extract these values
from the openrc.sh discussed above. The token allows you to interact with your other service endpoints
without needing to reauthenticate for every request. Tokens are typically good for 24 hours, and when the
token expires, you are alerted with a 401 (Unauthorized) response and you can request another token.
1. Look at your OpenStack service catalog:
$ curl -s -X POST http://203.0.113.10:35357/v2.0/tokens \
-d '{"auth": {"passwordCredentials": {"username":"test-user", "password":"test-
,→password"}, "tenantName":"test-project"}}' \
-H "Content-type: application/json" | jq .
2. Read through the JSON response to get a feel for how the catalog is laid out.
To make working with subsequent requests easier, store the token in an environment variable:
$ TOKEN=`curl -s -X POST http://203.0.113.10:35357/v2.0/tokens \
-d '{"auth": {"passwordCredentials": {"username":"test-user", "password":"test-
,→password"}, "tenantName":"test-project"}}' \
-H "Content-type: application/json" | jq -r .access.token.id`
Now you can refer to your token on the command line as $TOKEN.
3. Pick a service endpoint from your service catalog, such as compute. Try a request, for example, listing
instances (servers):
$ curl -s \
-H "X-Auth-Token: $TOKEN" \
http://203.0.113.10:8774/v2.0/98333aba48e756fa8f629c83a818ad57/servers | jq .
To discover how API requests should be structured, read the OpenStack API Reference. To chew through the
responses using jq, see the jq Manual.
Lay of the Land 25
Operations Guide (Release Version: 15.0.0)
The -s flag used in the cURL commands above are used to prevent the progress meter from being shown. If
you are having trouble running cURL commands, you’ll want to remove it. Likewise, to help you troubleshoot
cURL commands, you can include the -v flag to show you the verbose output. There are many more extremely
useful features in cURL; refer to the man page for all the options.
Servers and Services
As an administrator, you have a few ways to discover what your OpenStack cloud looks like simply by using
the OpenStack tools available. This section gives you an idea of how to get an overview of your cloud, its
shape, size, and current state.
First, you can discover what servers belong to your OpenStack cloud by running:
# openstack compute service list --long
The output looks like the following:
+----+------------------+-------------------+------+---------+-------+----------------------
,→------+-----------------+
| Id | Binary | Host | Zone | Status | State | Updated_at
,→ | Disabled Reason |
+----+------------------+-------------------+------+---------+-------+----------------------
,→------+-----------------+
| 1 | nova-cert | cloud.example.com | nova | enabled | up | 2016-01-05T17:20:38.
,→000000 | - |
| 2 | nova-compute | c01.example.com | nova | enabled | up | 2016-01-05T17:20:38.
,→000000 | - |
| 3 | nova-compute | c01.example.com. | nova | enabled | up | 2016-01-05T17:20:38.
,→000000 | - |
| 4 | nova-compute | c01.example.com | nova | enabled | up | 2016-01-05T17:20:38.
,→000000 | - |
| 5 | nova-compute | c01.example.com | nova | enabled | up | 2016-01-05T17:20:38.
,→000000 | - |
| 6 | nova-compute | c01.example.com | nova | enabled | up | 2016-01-05T17:20:38.
,→000000 | - |
| 7 | nova-conductor | cloud.example.com | nova | enabled | up | 2016-01-05T17:20:38.
,→000000 | - |
| 8 | nova-cert | cloud.example.com | nova | enabled | up | 2016-01-05T17:20:42.
,→000000 | - |
| 9 | nova-scheduler | cloud.example.com | nova | enabled | up | 2016-01-05T17:20:38.
,→000000 | - |
| 10 | nova-consoleauth | cloud.example.com | nova | enabled | up | 2016-01-05T17:20:35.
,→000000 | - |
+----+------------------+-------------------+------+---------+-------+----------------------
,→------+-----------------+
The output shows that there are five compute nodes and one cloud controller. You see all the services in the
up state, which indicates that the services are up and running. If a service is in a down state, it is no longer
available. This is an indication that you should troubleshoot why the service is down.
If you are using cinder, run the following command to see a similar listing:
# cinder-manage host list | sort
host zone
c01.example.com nova
c02.example.com nova
26 Lay of the Land
Operations Guide (Release Version: 15.0.0)
c03.example.com nova
c04.example.com nova
c05.example.com nova
cloud.example.com nova
With these two tables, you now have a good overview of what servers and services make up your cloud.
You can also use the Identity service (keystone) to see what services are available in your cloud as well as what
endpoints have been configured for the services.
The following command requires you to have your shell environment configured with the proper administrative
variables:
$ openstack catalog list
+----------+------------+-------------------------------------------------------------------
,→--------------+
| Name | Type | Endpoints
,→ |
+----------+------------+-------------------------------------------------------------------
,→--------------+
| nova | compute | RegionOne
,→ |
| | | public: http://192.168.122.10:8774/v2/
,→9faa845768224258808fc17a1bb27e5e |
| | | RegionOne
,→ |
| | | internal: http://192.168.122.10:8774/v2/
,→9faa845768224258808fc17a1bb27e5e |
| | | RegionOne
,→ |
| | | admin: http://192.168.122.10:8774/v2/
,→9faa845768224258808fc17a1bb27e5e |
| | |
,→ |
| cinderv2 | volumev2 | RegionOne
,→ |
| | | public: http://192.168.122.10:8776/v2/
,→9faa845768224258808fc17a1bb27e5e |
| | | RegionOne
,→ |
| | | internal: http://192.168.122.10:8776/v2/
,→9faa845768224258808fc17a1bb27e5e |
| | | RegionOne
,→ |
| | | admin: http://192.168.122.10:8776/v2/
,→9faa845768224258808fc17a1bb27e5e |
| | |
,→ |
The preceding output has been truncated to show only two services. You will see one service entry for each
service that your cloud provides. Note how the endpoint domain can be different depending on the endpoint
type. Different endpoint domains per type are not required, but this can be done for different reasons, such as
endpoint privacy or network traffic segregation.
You can find the version of the Compute installation by using the OpenStack command-line client:
Lay of the Land 27
Operations Guide (Release Version: 15.0.0)
# openstack --version
Diagnose Your Compute Nodes
You can obtain extra information about virtual machines that are running—their CPU usage, the memory, the
disk I/O or network I/O—per instance, by running the nova diagnostics command with a server ID:
$ nova diagnostics <serverID>
The output of this command varies depending on the hypervisor because hypervisors support different attributes.
The following demonstrates the difference between the two most popular hypervisors. Here is example output
when the hypervisor is Xen:
+----------------+-----------------+
| Property | Value |
+----------------+-----------------+
| cpu0 | 4.3627 |
| memory | 1171088064.0000 |
| memory_target | 1171088064.0000 |
| vbd_xvda_read | 0.0 |
| vbd_xvda_write | 0.0 |
| vif_0_rx | 3223.6870 |
| vif_0_tx | 0.0 |
| vif_1_rx | 104.4955 |
| vif_1_tx | 0.0 |
+----------------+-----------------+
While the command should work with any hypervisor that is controlled through libvirt (KVM, QEMU, or LXC),
it has been tested only with KVM. Here is the example output when the hypervisor is KVM:
+------------------+------------+
| Property | Value |
+------------------+------------+
| cpu0_time | 2870000000 |
| memory | 524288 |
| vda_errors | -1 |
| vda_read | 262144 |
| vda_read_req | 112 |
| vda_write | 5606400 |
| vda_write_req | 376 |
| vnet0_rx | 63343 |
| vnet0_rx_drop | 0 |
| vnet0_rx_errors | 0 |
| vnet0_rx_packets | 431 |
| vnet0_tx | 4905 |
| vnet0_tx_drop | 0 |
| vnet0_tx_errors | 0 |
| vnet0_tx_packets | 45 |
+------------------+------------+
28 Lay of the Land
Operations Guide (Release Version: 15.0.0)
Network Inspection
To see which fixed IP networks are configured in your cloud, you can use the openstack command-line client
to get the IP ranges:
$ openstack subnet list
+--------------------------------------+----------------+-----------------------------------
,→---+-----------------+
| ID | Name | Network
,→ | Subnet |
+--------------------------------------+----------------+-----------------------------------
,→---+-----------------+
| 346806ee-a53e-44fd-968a-ddb2bcd2ba96 | public_subnet | 0bf90de6-fc0f-4dba-b80d-
,→96670dfb331a | 172.24.4.224/28 |
| f939a1e4-3dc3-4540-a9f6-053e6f04918f | private_subnet | 1f7f429e-c38e-47ba-8acf-
,→c44e3f5e8d71 | 10.0.0.0/24 |
+--------------------------------------+----------------+-----------------------------------
,→---+-----------------+
The OpenStack command-line client can provide some additional details:
# openstack compute service list
+----+------------------+------------+----------+---------+-------+-------------------------
,→---+
| Id | Binary | Host | Zone | Status | State | Updated At
,→ |
+----+------------------+------------+----------+---------+-------+-------------------------
,→---+
| 1 | nova-consoleauth | controller | internal | enabled | up | 2016-08-18T12:16:53.
,→000000 |
| 2 | nova-scheduler | controller | internal | enabled | up | 2016-08-18T12:16:59.
,→000000 |
| 3 | nova-conductor | controller | internal | enabled | up | 2016-08-18T12:16:52.
,→000000 |
| 7 | nova-compute | controller | nova | enabled | up | 2016-08-18T12:16:58.
,→000000 |
+----+------------------+------------+----------+---------+-------+-------------------------
,→---+
This output shows that two networks are configured, each network containing 255 IPs (a /24 subnet). The first
network has been assigned to a certain project, while the second network is still open for assignment. You can
assign this network manually; otherwise, it is automatically assigned when a project launches its first instance.
To find out whether any floating IPs are available in your cloud, run:
# openstack floating ip list
+--------------------------------------+---------------------+------------------+-----------
,→---------------------------+
| ID | Floating IP Address | Fixed IP Address | Port
,→ |
+--------------------------------------+---------------------+------------------+-----------
,→---------------------------+
| 340cb36d-6a52-4091-b256-97b6e61cbb20 | 172.24.4.227 | 10.2.1.8 | 1fec8fb8-
,→7a8c-44c2-acd8-f10e2e6cd326 |
| 8b1bfc0c-7a91-4da0-b3cc-4acae26cbdec | 172.24.4.228 | None | None
,→ |
+--------------------------------------+---------------------+------------------+-----------
,→---------------------------+
Lay of the Land 29
Operations Guide (Release Version: 15.0.0)
Here, two floating IPs are available. The first has been allocated to a project, while the other is unallocated.
Users and Projects
To see a list of projects that have been added to the cloud, run:
$ openstack project list
+----------------------------------+--------------------+
| ID | Name |
+----------------------------------+--------------------+
| 422c17c0b26f4fbe9449f37a5621a5e6 | alt_demo |
| 5dc65773519248f3a580cfe28ba7fa3f | demo |
| 9faa845768224258808fc17a1bb27e5e | admin |
| a733070a420c4b509784d7ea8f6884f7 | invisible_to_admin |
| aeb3e976e7794f3f89e4a7965db46c1e | service |
+----------------------------------+--------------------+
To see a list of users, run:
$ openstack user list
+----------------------------------+----------+
| ID | Name |
+----------------------------------+----------+
| 5837063598694771aedd66aa4cddf0b8 | demo |
| 58efd9d852b74b87acc6efafaf31b30e | cinder |
| 6845d995a57a441f890abc8f55da8dfb | glance |
| ac2d15a1205f46d4837d5336cd4c5f5a | alt_demo |
| d8f593c3ae2b47289221f17a776a218b | admin |
| d959ec0a99e24df0b7cb106ff940df20 | nova |
+----------------------------------+----------+
Note: Sometimes a user and a group have a one-to-one mapping. This happens for standard system accounts,
such as cinder, glance, nova, and swift, or when only one user is part of a group.
Running Instances
To see a list of running instances, run:
$ openstack server list --all-projects
+--------------------------------------+------+--------+---------------------+------------+
| ID | Name | Status | Networks | Image Name |
+--------------------------------------+------+--------+---------------------+------------+
| 495b4f5e-0b12-4c5a-b4e0-4326dee17a5a | vm1 | ACTIVE | public=172.24.4.232 | cirros |
| e83686f9-16e8-45e6-911d-48f75cb8c0fb | vm2 | ACTIVE | private=10.0.0.7 | cirros |
+--------------------------------------+------+--------+---------------------+------------+
Unfortunately, this command does not tell you various details about the running instances, such as what compute
node the instance is running on, what flavor the instance is, and so on. You can use the following command to
view details about individual instances:
30 Lay of the Land
Operations Guide (Release Version: 15.0.0)
$ openstack server show <uuid>
For example:
# openstack server show 81db556b-8aa5-427d-a95c-2a9a6972f630
+--------------------------------------+----------------------------------------------------
,→------+
| Field | Value
,→ |
+--------------------------------------+----------------------------------------------------
,→------+
| OS-DCF:diskConfig | AUTO
,→ |
| OS-EXT-AZ:availability_zone | nova
,→ |
| OS-EXT-SRV-ATTR:host | c02.example.com
,→ |
| OS-EXT-SRV-ATTR:hypervisor_hostname | c02.example.com
,→ |
| OS-EXT-SRV-ATTR:instance_name | instance-00000001
,→ |
| OS-EXT-STS:power_state | Running
,→ |
| OS-EXT-STS:task_state | None
,→ |
| OS-EXT-STS:vm_state | active
,→ |
| OS-SRV-USG:launched_at | 2016-10-19T15:18:09.000000
,→ |
| OS-SRV-USG:terminated_at | None
,→ |
| accessIPv4 |
,→ |
| accessIPv6 |
,→ |
| addresses | private=10.0.0.7
,→ |
| config_drive |
,→ |
| created | 2016-10-19T15:17:46Z
,→ |
| flavor | m1.tiny (1)
,→ |
| hostId |
,→2b57e2b7a839508337fb55695b8f6e65aa881460a20449a76352040b |
| id | e83686f9-16e8-45e6-911d-48f75cb8c0fb
,→ |
| image | cirros (9fef3b2d-c35d-4b61-bea8-09cc6dc41829)
,→ |
| key_name | None
,→ |
| name | test
,→ |
| os-extended-volumes:volumes_attached | []
,→ |
| progress | 0
,→ |
Lay of the Land 31
Operations Guide (Release Version: 15.0.0)
| project_id | 1eaaf6ede7a24e78859591444abf314a
,→ |
| properties |
,→ |
| security_groups | [{u'name': u'default'}]
,→ |
| status | ACTIVE
,→ |
| updated | 2016-10-19T15:18:58Z
,→ |
| user_id | 7aaa9b5573ce441b98dae857a82ecc68
,→ |
+--------------------------------------+----------------------------------------------------
,→------+
This output shows that an instance named devstack was created from an Ubuntu 12.04 image using a flavor
of m1.small and is hosted on the compute node c02.example.com.
Summary
We hope you have enjoyed this quick tour of your working environment, including how to interact with your
cloud and extract useful information. From here, you can use the OpenStack Administrator Guide as your
reference for all of the command-line functionality in your cloud.
Managing Projects and Users
Managing Projects
Users must be associated with at least one project, though they may belong to many. Therefore, you should add
at least one project before adding users.
Adding Projects
To create a project through the OpenStack dashboard:
1. Log in as an administrative user.
2. Select the Identity tab in the left navigation bar.
3. Under Identity tab, click Projects.
4. Click the Create Project button.
You are prompted for a project name and an optional, but recommended, description. Select the check box at
the bottom of the form to enable this project. By default, it is enabled, as shown below:
32 Managing Projects and Users
Operations Guide (Release Version: 15.0.0)
It is also possible to add project members and adjust the project quotas. We’ll discuss those actions later, but in
practice, it can be quite convenient to deal with all these operations at one time.
To add a project through the command line, you must use the OpenStack command line client.
# openstack project create demo --domain default
This command creates a project named demo. Optionally, you can add a description string by appending --
description PROJECT_DESCRIPTION, which can be very useful. You can also create a project in a disabled
state by appending --disable to the command. By default, projects are created in an enabled state.
Quotas
To prevent system capacities from being exhausted without notification, you can set up quotas. Quotas are
operational limits. For example, the number of gigabytes allowed per tenant can be controlled to ensure that
a single tenant cannot consume all of the disk space. Quotas are currently enforced at the tenant (or project)
level, rather than the user level.
Warning: Because without sensible quotas a single tenant could use up all the available resources, default
quotas are shipped with OpenStack. You should pay attention to which quota settings make sense for your
hardware capabilities.
Using the command-line interface, you can manage quotas for the OpenStack Compute service and the Block
Storage service.
Typically, default values are changed because a tenant requires more than the OpenStack default of 10 volumes
per tenant, or more than the OpenStack default of 1 TB of disk space on a compute node.
Managing Projects and Users 33
Operations Guide (Release Version: 15.0.0)
Note: To view all tenants, run:
$ openstack project list
+---------------------------------+----------+
| ID | Name |
+---------------------------------+----------+
| a981642d22c94e159a4a6540f70f9f8 | admin |
| 934b662357674c7b9f5e4ec6ded4d0e | tenant01 |
| 7bc1dbfd7d284ec4a856ea1eb82dca8 | tenant02 |
| 9c554aaef7804ba49e1b21cbd97d218 | services |
+---------------------------------+----------+
Set Image Quotas
You can restrict a project’s image storage by total number of bytes. Currently, this quota is applied cloud-wide,
so if you were to set an Image quota limit of 5 GB, then all projects in your cloud will be able to store only 5
GB of images and snapshots.
To enable this feature, edit the /etc/glance/glance-api.conf file, and under the [DEFAULT] section, add:
user_storage_quota = <bytes>
For example, to restrict a project’s image storage to 5 GB, do this:
user_storage_quota = 5368709120
Note: There is a configuration option in /etc/glance/glance-api.conf that limits the number of members
allowed per image, called image_member_quota, set to 128 by default. That setting is a different quota from
the storage quota.
Set Compute Service Quotas
As an administrative user, you can update the Compute service quotas for an existing tenant, as well as update
the quota defaults for a new tenant. See Compute quota descriptions.
34 Managing Projects and Users
Operations Guide (Release Version: 15.0.0)
Table 3: Compute quota descriptions
Quota Description Property name
Fixed IPs Number of fixed IP addresses allowed per project. This number
must be equal to or greater than the number of allowed instances.
fixed-ips
Floating IPs Number of floating IP addresses allowed per project. floating-ips
Injected file
content bytes
Number of content bytes allowed per injected file. injected-filecontent-bytes
Injected file
path bytes
Number of bytes allowed per injected file path. injected-filepath-bytes
Injected files Number of injected files allowed per project. injected-files
Instances Number of instances allowed per project. instances
Key pairs Number of key pairs allowed per user. key-pairs
Metadata
items
Number of metadata items allowed per instance. metadata-items
RAM Megabytes of instance RAM allowed per project. ram
Security
group rules
Number of security group rules per project. securitygroup-rules
Security
groups
Number of security groups per project. securitygroups
VCPUs Number of instance cores allowed per project. cores
Server
Groups
Number of server groups per project. server_groups
Server Group
Members
Number of servers per server group. server_group_members
View and update compute quotas for a tenant (project)
As an administrative user, you can use the nova quota-* commands, which are provided by the pythonnovaclient
package, to view and update tenant quotas.
To view and update default quota values
1. List all default quotas for all tenants, as follows:
$ nova quota-defaults
For example:
$ nova quota-defaults
+-----------------------------+-------+
| Quota | Limit |
+-----------------------------+-------+
| instances | 10 |
| cores | 20 |
| ram | 51200 |
| floating_ips | 10 |
| fixed_ips | -1 |
| metadata_items | 128 |
| injected_files | 5 |
| injected_file_content_bytes | 10240 |
| injected_file_path_bytes | 255 |
Managing Projects and Users 35
Operations Guide (Release Version: 15.0.0)
| key_pairs | 100 |
| security_groups | 10 |
| security_group_rules | 20 |
| server_groups | 10 |
| server_group_members | 10 |
+-----------------------------+-------+
2. Update a default value for a new tenant, as follows:
$ nova quota-class-update default key value
For example:
$ nova quota-class-update default --instances 15
To view quota values for a tenant (project)
1. Place the tenant ID in a variable:
$ tenant=$(openstack project list | awk '/tenantName/ {print $2}')
2. List the currently set quota values for a tenant, as follows:
$ nova quota-show --tenant $tenant
For example:
$ nova quota-show --tenant $tenant
+-----------------------------+-------+
| Quota | Limit |
+-----------------------------+-------+
| instances | 10 |
| cores | 20 |
| ram | 51200 |
| floating_ips | 10 |
| fixed_ips | -1 |
| metadata_items | 128 |
| injected_files | 5 |
| injected_file_content_bytes | 10240 |
| injected_file_path_bytes | 255 |
| key_pairs | 100 |
| security_groups | 10 |
| security_group_rules | 20 |
| server_groups | 10 |
| server_group_members | 10 |
+-----------------------------+-------+
To update quota values for a tenant (project)
1. Obtain the tenant ID, as follows:
$ tenant=$(openstack project list | awk '/tenantName/ {print $2}')
2. Update a particular quota value, as follows:
# nova quota-update --quotaName quotaValue tenantID
36 Managing Projects and Users
Operations Guide (Release Version: 15.0.0)
For example:
# nova quota-update --floating-ips 20 $tenant
# nova quota-show --tenant $tenant
+-----------------------------+-------+
| Quota | Limit |
+-----------------------------+-------+
| instances | 10 |
| cores | 20 |
| ram | 51200 |
| floating_ips | 20 |
| fixed_ips | -1 |
| metadata_items | 128 |
| injected_files | 5 |
| injected_file_content_bytes | 10240 |
| injected_file_path_bytes | 255 |
| key_pairs | 100 |
| security_groups | 10 |
| security_group_rules | 20 |
| server_groups | 10 |
| server_group_members | 10 |
+-----------------------------+-------+
Note: To view a list of options for the nova quota-update command, run:
$ nova help quota-update
Set Object Storage Quotas
There are currently two categories of quotas for Object Storage:
Container quotas Limit the total size (in bytes) or number of objects that can be stored in a single container.
Account quotas Limit the total size (in bytes) that a user has available in the Object Storage service.
To take advantage of either container quotas or account quotas, your Object Storage proxy server must have
container_quotas or account_quotas (or both) added to the [pipeline:main] pipeline. Each quota type
also requires its own section in the proxy-server.conf file:
[pipeline:main]
pipeline = catch_errors [...] slo dlo account_quotas proxy-server
[filter:account_quotas]
use = egg:swift#account_quotas
[filter:container_quotas]
use = egg:swift#container_quotas
To view and update Object Storage quotas, use the swift command provided by the python-swiftclient
package. Any user included in the project can view the quotas placed on their project. To update Object Storage
quotas on a project, you must have the role of ResellerAdmin in the project that the quota is being applied to.
To view account quotas placed on a project:
Managing Projects and Users 37
Operations Guide (Release Version: 15.0.0)
$ swift stat
Account: AUTH_b36ed2d326034beba0a9dd1fb19b70f9
Containers: 0
Objects: 0
Bytes: 0
Meta Quota-Bytes: 214748364800
X-Timestamp: 1351050521.29419
Content-Type: text/plain; charset=utf-8
Accept-Ranges: bytes
To apply or update account quotas on a project:
$ swift post -m quota-bytes:
<bytes>
For example, to place a 5 GB quota on an account:
$ swift post -m quota-bytes:
5368709120
To verify the quota, run the swift stat command again:
$ swift stat
Account: AUTH_b36ed2d326034beba0a9dd1fb19b70f9
Containers: 0
Objects: 0
Bytes: 0
Meta Quota-Bytes: 5368709120
X-Timestamp: 1351541410.38328
Content-Type: text/plain; charset=utf-8
Accept-Ranges: bytes
Set Block Storage Quotas
As an administrative user, you can update the Block Storage service quotas for a tenant, as well as update the
quota defaults for a new tenant. See Table: Block Storage quota descriptions.
Table 4: Table: Block Storage quota descriptions
Property name Description
gigabytes Number of volume gigabytes allowed per tenant
snapshots Number of Block Storage snapshots allowed per tenant.
volumes Number of Block Storage volumes allowed per tenant
View and update Block Storage quotas for a tenant (project)
As an administrative user, you can use the cinder quota-* commands, which are provided by the pythoncinderclient
package, to view and update tenant quotas.
To view and update default Block Storage quota values
1. List all default quotas for all tenants, as follows:
38 Managing Projects and Users
Operations Guide (Release Version: 15.0.0)
$ cinder quota-defaults tenantID
2. Obtain the tenant ID, as follows:
$ tenant=$(openstack project list | awk '/tenantName/ {print $2}')
For example:
$ cinder quota-defaults $tenant
+-----------+-------+
| Property | Value |
+-----------+-------+
| gigabytes | 1000 |
| snapshots | 10 |
| volumes | 10 |
+-----------+-------+
3. To update a default value for a new tenant, update the property in the /etc/cinder/cinder.conf file.
To view Block Storage quotas for a tenant (project)
1. View quotas for the tenant, as follows:
# cinder quota-show tenantID
For example:
# cinder quota-show $tenant
+-----------+-------+
| Property | Value |
+-----------+-------+
| gigabytes | 1000 |
| snapshots | 10 |
| volumes | 10 |
+-----------+-------+
To update Block Storage quotas for a tenant (project)
1. Place the tenant ID in a variable:
$ tenant=$(openstack project list | awk '/tenantName/ {print $2}')
2. Update a particular quota value, as follows:
# cinder quota-update --quotaName NewValue tenantID
For example:
# cinder quota-update --volumes 15 $tenant
# cinder quota-show $tenant
+-----------+-------+
| Property | Value |
+-----------+-------+
| gigabytes | 1000 |
| snapshots | 10 |
| volumes | 15 |
+-----------+-------+
Managing Projects and Users 39
Operations Guide (Release Version: 15.0.0)
User Management
The OpenStack Dashboard provides a graphical interface to manage users. This section describes user management
with the Dashboard.
You can also manage projects, users, and roles from the command-line clients.
In addition, many sites write custom tools for local needs to enforce local policies and provide levels of selfservice
to users that are not currently available with packaged tools.
Creating New Users
To create a user, you need the following information:
• Username
• Description
• Email address
• Password
• Primary project
• Role
• Enabled
Username and email address are self-explanatory, though your site may have local conventions you should
observe. The primary project is simply the first project the user is associated with and must exist prior to
creating the user. Role is almost always going to be “member.” Out of the box, OpenStack comes with two
roles defined:
member A typical user
admin An administrative super user, which has full permissions across all projects and should be used with
great care
It is possible to define other roles, but doing so is uncommon.
Once you’ve gathered this information, creating the user in the dashboard is just another web form similar to
what we’ve seen before and can be found by clicking the Users link in the Identity navigation bar and then
clicking the Create User button at the top right.
Modifying users is also done from this Users page. If you have a large number of users, this page can get quite
crowded. The Filter search box at the top of the page can be used to limit the users listing. A form very similar
to the user creation dialog can be pulled up by selecting Edit from the actions drop-down menu at the end of
the line for the user you are modifying.
Associating Users with Projects
Many sites run with users being associated with only one project. This is a more conservative and simpler
choice both for administration and for users. Administratively, if a user reports a problem with an instance or
quota, it is obvious which project this relates to. Users needn’t worry about what project they are acting in if
they are only in one project. However, note that, by default, any user can affect the resources of any other user
within their project. It is also possible to associate users with multiple projects if that makes sense for your
organization.
40 Managing Projects and Users
Operations Guide (Release Version: 15.0.0)
Associating existing users with an additional project or removing them from an older project is done from
the Projects page of the dashboard by selecting Manage Members from the Actions column, as shown in the
screenshot below.
From this view, you can do a number of useful things, as well as a few dangerous ones.
The first column of this form, named All Users, includes a list of all the users in your cloud who are not already
associated with this project. The second column shows all the users who are. These lists can be quite long, but
they can be limited by typing a substring of the username you are looking for in the filter field at the top of the
column.
From here, click the + icon to add users to the project. Click the - to remove them.
The dangerous possibility comes with the ability to change member roles. This is the dropdown list below the
username in the Project Members list. In virtually all cases, this value should be set to Member. This example
purposefully shows an administrative user where this value is admin.
Managing Projects and Users 41
Operations Guide (Release Version: 15.0.0)
Warning: The admin is global, not per project, so granting a user the admin role in any project gives the
user administrative rights across the whole cloud.
Typical use is to only create administrative users in a single project, by convention the admin project, which
is created by default during cloud setup. If your administrative users also use the cloud to launch and manage
instances, it is strongly recommended that you use separate user accounts for administrative access and normal
operations and that they be in distinct projects.
Customizing Authorization
The default authorization settings allow administrative users only to create resources on behalf of a different
project. OpenStack handles two kinds of authorization policies:
Operation based Policies specify access criteria for specific operations, possibly with fine-grained control
over specific attributes.
Resource based Whether access to a specific resource might be granted or not according to the permissions
configured for the resource (currently available only for the network resource). The actual authorization
policies enforced in an OpenStack service vary from deployment to deployment.
The policy engine reads entries from the policy.json file. The actual location of this file might vary from
distribution to distribution: for nova, it is typically in /etc/nova/policy.json. You can update entries while
the system is running, and you do not have to restart services. Currently, the only way to update such policies
is to edit the policy file.
The OpenStack service’s policy engine matches a policy directly. A rule indicates evaluation of the elements
of such policies. For instance, in a compute:create: "rule:admin_or_owner" statement, the policy is
compute:create, and the rule is admin_or_owner.
Policies are triggered by an OpenStack policy engine whenever one of them matches an OpenStack API operation
or a specific attribute being used in a given operation. For instance, the engine tests the create:compute
policy every time a user sends a POST /v2/{tenant_id}/servers request to the OpenStack Compute API
server. Policies can be also related to specific API extensions. For instance, if a user needs an extension like
compute_extension:rescue, the attributes defined by the provider extensions trigger the rule test for that
operation.
An authorization policy can be composed by one or more rules. If more rules are specified, evaluation policy is
successful if any of the rules evaluates successfully; if an API operation matches multiple policies, then all the
policies must evaluate successfully. Also, authorization rules are recursive. Once a rule is matched, the rule(s)
can be resolved to another rule, until a terminal rule is reached. These are the rules defined:
Role-based rules Evaluate successfully if the user submitting the request has the specified role. For instance,
"role:admin" is successful if the user submitting the request is an administrator.
Field-based rules Evaluate successfully if a field of the resource specified in the current request matches a
specific value. For instance, "field:networks:shared=True" is successful if the attribute shared of
the network resource is set to true.
Generic rules Compare an attribute in the resource with an attribute extracted from the user’s security
credentials and evaluates successfully if the comparison is successful. For instance, "tenant_id:%(tenant_id)s"
is successful if the tenant identifier in the resource is equal to the tenant
identifier of the user submitting the request.
Here are snippets of the default nova policy.json file:
42 Managing Projects and Users
Operations Guide (Release Version: 15.0.0)
{
"context_is_admin": "role:admin",
"admin_or_owner": "is_admin:True", "project_id:%(project_id)s", ~~~~(1)~~~~
"default": "rule:admin_or_owner", ~~~~(2)~~~~
"compute:create": "",
"compute:create:attach_network": "",
"compute:create:attach_volume": "",
"compute:get_all": "",
"admin_api": "is_admin:True",
"compute_extension:accounts": "rule:admin_api",
"compute_extension:admin_actions": "rule:admin_api",
"compute_extension:admin_actions:pause": "rule:admin_or_owner",
"compute_extension:admin_actions:unpause": "rule:admin_or_owner",
...
"compute_extension:admin_actions:migrate": "rule:admin_api",
"compute_extension:aggregates": "rule:admin_api",
"compute_extension:certificates": "",
...
"compute_extension:flavorextraspecs": "",
"compute_extension:flavormanage": "rule:admin_api", ~~~~(3)~~~~
}
1. Shows a rule that evaluates successfully if the current user is an administrator or the owner of the resource
specified in the request (tenant identifier is equal).
2. Shows the default policy, which is always evaluated if an API operation does not match any of the policies
in policy.json.
3. Shows a policy restricting the ability to manipulate flavors to administrators using the Admin API only.
In some cases, some operations should be restricted to administrators only. Therefore, as a further example, let
us consider how this sample policy file could be modified in a scenario where we enable users to create their
own flavors:
"compute_extension:flavormanage": "",
Users Who Disrupt Other Users
Users on your cloud can disrupt other users, sometimes intentionally and maliciously and other times by accident.
Understanding the situation allows you to make a better decision on how to handle the disruption.
For example, a group of users have instances that are utilizing a large amount of compute resources for very
compute-intensive tasks. This is driving the load up on compute nodes and affecting other users. In this situation,
review your user use cases. You may find that high compute scenarios are common, and should then plan
for proper segregation in your cloud, such as host aggregation or regions.
Another example is a user consuming a very large amount of bandwidth. Again, the key is to understand what
the user is doing. If she naturally needs a high amount of bandwidth, you might have to limit her transmission
rate as to not affect other users or move her to an area with more bandwidth available. On the other hand,
maybe her instance has been hacked and is part of a botnet launching DDOS attacks. Resolution of this issue
is the same as though any other server on your network has been hacked. Contact the user and give her time to
respond. If she doesn’t respond, shut down the instance.
A final example is if a user is hammering cloud resources repeatedly. Contact the user and learn what he is
trying to do. Maybe he doesn’t understand that what he’s doing is inappropriate, or maybe there is an issue with
Managing Projects and Users 43
Operations Guide (Release Version: 15.0.0)
the resource he is trying to access that is causing his requests to queue or lag.
Summary
One key element of systems administration that is often overlooked is that end users are the reason systems
administrators exist. Don’t go the BOFH route and terminate every user who causes an alert to go off. Work
with users to understand what they’re trying to accomplish and see how your environment can better assist
them in achieving their goals. Meet your users needs by organizing your users into projects, applying policies,
managing quotas, and working with them.
An OpenStack cloud does not have much value without users. This chapter covers topics that relate to managing
users, projects, and quotas. This chapter describes users and projects as described by version 2 of the OpenStack
Identity API.
Projects or Tenants?
In OpenStack user interfaces and documentation, a group of users is referred to as a project or tenant. These
terms are interchangeable.
The initial implementation of OpenStack Compute had its own authentication system and used the term
project. When authentication moved into the OpenStack Identity (keystone) project, it used the term tenant
to refer to a group of users. Because of this legacy, some of the OpenStack tools refer to projects and some
refer to tenants.
Tip: This guide uses the term project, unless an example shows interaction with a tool that uses the term
tenant.
User-Facing Operations
This guide is for OpenStack operators and does not seek to be an exhaustive reference for users, but as an operator,
you should have a basic understanding of how to use the cloud facilities. This chapter looks at OpenStack
from a basic user perspective, which helps you understand your users’ needs and determine, when you get a
trouble ticket, whether it is a user issue or a service issue. The main concepts covered are images, flavors,
security groups, block storage, shared file system storage, and instances.
Images
OpenStack images can often be thought of as “virtual machine templates.” Images can also be standard installation
media such as ISO images. Essentially, they contain bootable file systems that are used to launch
instances.
Adding Images
Several pre-made images exist and can easily be imported into the Image service. A common image to add is
the CirrOS image, which is very small and used for testing purposes. To add this image, simply do:
44 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
$ wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img
$ openstack image create --file cirros-0.3.5-x86_64-disk.img \
--public --container-format bare \
--disk-format qcow2 "cirros image"
The openstack image create command provides a large set of options for working with your image. For
example, the --min-disk option is useful for images that require root disks of a certain size (for example, large
Windows images). To view these options, run:
$ openstack help image create
Run the following command to view the properties of existing images:
$ openstack image show IMAGE_NAME_OR_UUID
Adding Signed Images
To provide a chain of trust from an end user to the Image service, and the Image service to Compute, an end user
can import signed images that can be initially verified in the Image service, and later verified in the Compute
service. Appropriate Image service properties need to be set to enable this signature feature.
Note: Prior to the steps below, an asymmetric keypair and certificate must be generated. In this example, these
are called private_key.pem and new_cert.crt, respectively, and both reside in the current directory. Also note
that the image in this example is cirros-0.3.5-x86_64-disk.img, but any image can be used.
The following are steps needed to create the signature used for the signed images:
1. Retrieve image for upload
$ wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img
2. Use private key to create a signature of the image
Note: The following implicit values are being used to create the signature in this example:
• Signature hash method = SHA-256
• Signature key type = RSA-PSS
Note: The following options are currently supported:
• Signature hash methods: SHA-224, SHA-256, SHA-384, and SHA-512
• Signature key types: DSA, ECC_SECT571K1, ECC_SECT409K1, ECC_SECT571R1,
ECC_SECT409R1, ECC_SECP521R1, ECC_SECP384R1, and RSA-PSS
Generate signature of image and convert it to a base64 representation:
$ openssl dgst -sha256 -sign private_key.pem -sigopt rsa_padding_mode:pss \
-out image-file.signature cirros-0.3.5-x86_64-disk.img
User-Facing Operations 45
Operations Guide (Release Version: 15.0.0)
$ base64 -w 0 image-file.signature > signature_64
$ cat signature_64
'c4br5f3FYQV6Nu20cRUSnx75R/
,→VcW3diQdsUN2nhPw+UcQRDoGx92hwMgRxzFYeUyydRTWCcUS2ZLudPR9X7rM
THFInA54Zj1TwEIbJTkHwlqbWBMU4+k5IUIjXxHO6RuH3Z5f/SlSt7ajsNVXaIclWqIw5YvEkgXTIEuDPE+C4=
,→'
Note:
• Using Image API v1 requires ‘-w 0’ above, since multiline image properties are not supported.
• Image API v2 supports multiline properties, so this option is not required for v2 but it can still be
used.
3. Create context
$ python
>>> from keystoneclient.v3 import client
>>> keystone_client = client.Client(username='demo',
user_domain_name='Default',
password='password',
project_name='demo',
auth_url='http://localhost:5000/v3')
>>> from oslo_context import context
>>> context = context.RequestContext(auth_token=keystone_client.auth_token,
tenant=keystone_client.project_id)
4. Encode certificate in DER format
>>> from cryptography import x509 as cryptography_x509
>>> from cryptography.hazmat import backends
>>> from cryptography.hazmat.primitives import serialization
>>> with open("new_cert.crt", "rb") as cert_file:
>>> cert = cryptography_x509.load_pem_x509_certificate(
cert_file.read(),
backend=backends.default_backend()
)
>>> certificate_der = cert.public_bytes(encoding=serialization.Encoding.DER)
5. Upload Certificate in DER format to Castellan
>>> from castellan.common.objects import x_509
>>> from castellan import key_manager
>>> castellan_cert = x_509.X509(certificate_der)
>>> key_API = key_manager.API()
>>> cert_uuid = key_API.store(context, castellan_cert)
>>> cert_uuid
u'62a33f41-f061-44ba-9a69-4fc247d3bfce'
6. Upload Image to Image service, with Signature Metadata
Note: The following signature properties are used:
• img_signature uses the signature called signature_64
46 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
• img_signature_certificate_uuid uses the value from cert_uuid in section 5 above
• img_signature_hash_method matches ‘SHA-256’ in section 2 above
• img_signature_key_type matches ‘RSA-PSS’ in section 2 above
$ . openrc demo
$ export OS_IMAGE_API_VERSION=2
$ openstack image create --property name=cirrosSignedImage_goodSignature \
--property is-public=true --container-format bare --disk-format qcow2 \
--property img_signature='c4br5f3FYQV6Nu20cRUSnx75R/
,→VcW3diQdsUN2nhPw+UcQRDoGx92hwMgRxzFYeUyydRTWCcUS2ZLudPR9X7rMTHFInA54Zj1TwEIbJTkHwlqbWBMU4+k5IUIjXxHO6RuH3Z5fSlSt7ajsNVXaIclWqIw5YvEkgXTIEuDPE+C4=
,→' \
--property img_signature_certificate_uuid='62a33f41-f061-44ba-9a69-4fc247d3bfce' \
--property img_signature_hash_method='SHA-256' \
--property img_signature_key_type='RSA-PSS' < ~/cirros-0.3.5-x86_64-disk.img
Note: The maximum image signature character limit is 255.
7. Verify the Keystone URL
Note: The default Keystone configuration assumes that Keystone is in the local host, and it uses http:/
/localhost:5000/v3 as the endpoint URL, which is specified in glance-api.conf and nova-api.
conf files:
[barbican]
auth_endpoint = http://localhost:5000/v3
Note: If Keystone is located remotely instead, edit the glance-api.conf and nova.conf files. In the
[barbican] section, configre the auth_endpoint option:
[barbican]
auth_endpoint = https://192.168.245.9:5000/v3
8. Signature verification will occur when Compute boots the signed image
Note: nova-compute servers first need to be updated by the following steps:
• Ensure that cryptsetup is installed, and ensure that pythin-barbicanclient Python package is
installed
• Set up the Key Manager service by editing /etc/nova/nova.conf and adding the entries in the codeblock
below
• The flag verify_glance_signatures enables Compute to automatically validate signed instances
prior to its launch. This validation feature is enabled when the value is set to TRUE
[key_manager]
api_class = castellan.key_manager.barbican_key_manager.BarbicanKeyManager
User-Facing Operations 47
Operations Guide (Release Version: 15.0.0)
[glance]
verify_glance_signatures = TRUE
Note: The api_class [keymgr] is deprecated as of Newton, so it should not be included in this release
or beyond.
Sharing Images Between Projects
In a multi-tenant cloud environment, users sometimes want to share their personal images or snapshots with
other projects. This can be done on the command line with the glance tool by the owner of the image.
To share an image or snapshot with another project, do the following:
1. Obtain the UUID of the image:
$ openstack image list
2. Obtain the UUID of the project with which you want to share your image, let’s call it target project. Unfortunately,
non-admin users are unable to use the openstack command to do this. The easiest solution
is to obtain the UUID either from an administrator of the cloud or from a user located in the target project.
3. Once you have both pieces of information, run the openstack image add project command:
$ openstack image add project IMAGE_NAME_OR_UUID PROJECT_NAME_OR_UUID
For example:
$ openstack image add project 733d1c44-a2ea-414b-aca7-69decf20d810 \
771ed149ef7e4b2b88665cc1c98f77ca
4. You now need to act in the target project scope.
Note: You will not see the shared image yet. Therefore the sharing needs to be accepted.
To accept the sharing, you need to update the member status:
$ glance member-update IMAGE_UUID PROJECT_UUID accepted
For example:
$ glance member-update 733d1c44-a2ea-414b-aca7-69decf20d810 \
771ed149ef7e4b2b88665cc1c98f77ca accepted
Project 771ed149ef7e4b2b88665cc1c98f77ca will now have access to image 733d1c44-a2ea-
414b-aca7-69decf20d810.
Tip: You can explicitly ask for pending member status to view shared images not yet accepted:
$ glance image-list --member-status pending
48 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
Deleting Images
To delete an image, just execute:
$ openstack image delete IMAGE_NAME_OR_UUID
Caution: Generally, deleting an image does not affect instances or snapshots that were based on the image.
However, some drivers may require the original image to be present to perform a migration. For example,
XenAPI live-migrate will work fine if the image is deleted, but libvirt will fail.
Other CLI Options
A full set of options can be found using:
$ glance help
or the Command-Line Interface Reference.
The Image service and the Database
The only thing the Image service does not store in a database is the image itself. The Image service database
has two main tables:
• images
• image_properties
Working directly with the database and SQL queries can provide you with custom lists and reports of images.
Technically, you can update properties about images through the database, although this is not generally recommended.
Example Image service Database Queries
One interesting example is modifying the table of images and the owner of that image. This can be easily done
if you simply display the unique ID of the owner. This example goes one step further and displays the readable
name of the owner:
mysql> select glance.images.id,
glance.images.name, keystone.tenant.name, is_public from
glance.images inner join keystone.tenant on
glance.images.owner=keystone.tenant.id;
Another example is displaying all properties for a certain image:
mysql> select name, value from
image_properties where id = <image_id>
User-Facing Operations 49
Operations Guide (Release Version: 15.0.0)
Flavors
Virtual hardware templates are called “flavors” in OpenStack, defining sizes for RAM, disk, number of cores,
and so on. The default install provides five flavors.
These are configurable by admin users (the rights may also be delegated to other users by redefining the access
controls for compute_extension:flavormanage in /etc/nova/policy.json on the nova-api server). To
get the list of available flavors on your system, run:
$ openstack flavor list
+----+-----------+-------+------+-----------+-------+-----------+
| ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public |
+----+-----------+-------+------+-----------+-------+-----------+
| 1 | m1.tiny | 512 | 1 | 0 | 1 | True |
| 2 | m1.small | 2048 | 20 | 0 | 1 | True |
| 3 | m1.medium | 4096 | 40 | 0 | 2 | True |
| 4 | m1.large | 8192 | 80 | 0 | 4 | True |
| 5 | m1.xlarge | 16384 | 160 | 0 | 8 | True |
+----+-----------+-------+------+-----------+-------+-----------+
The openstack flavor create command allows authorized users to create new flavors. Additional flavor
manipulation commands can be shown with the following command:
$ openstack help | grep flavor
Flavors define a number of parameters, resulting in the user having a choice of what type of virtual machine to
run—just like they would have if they were purchasing a physical server. Table. Flavor parameters lists the
elements that can be set. Note in particular extra_specs, which can be used to define free-form characteristics,
giving a lot of flexibility beyond just the size of RAM, CPU, and Disk.
50 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
Table 5: Table. Flavor parameters
Column Description
ID Unique ID (integer or UUID) for the flavor.
Name A descriptive name, such as xx.size_name, is conventional but not required, though some
third-party tools may rely on it.
Memory_MB
Virtual machine memory in megabytes.
Disk Virtual root disk size in gigabytes. This is an ephemeral disk the base image is copied into. You
don’t use it when you boot from a persistent volume. The “0” size is a special case that uses the
native base image size as the size of the ephemeral root volume.
EphemeralSpecifies the size of a secondary ephemeral data disk. This is an empty, unformatted disk and
exists only for the life of the instance.
Swap Optional swap space allocation for the instance.
VCPUs Number of virtual CPUs presented to the instance.
RXTX_Factor Optional property that allows created servers to have a different bandwidth cap from that
defined in the network they are attached to. This factor is multiplied by the rxtx_base property
of the network. Default value is 1.0 (that is, the same as the attached network).
Is_Public Boolean value that indicates whether the flavor is available to all users or private. Private
flavors do not get the current tenant assigned to them. Defaults to True.
extra_specs
Additional optional restrictions on which compute nodes the flavor can run on. This is
implemented as key-value pairs that must match against the corresponding key-value pairs on
compute nodes. Can be used to implement things like special resources (such as flavors that can
run only on compute nodes with GPU hardware).
Private Flavors
A user might need a custom flavor that is uniquely tuned for a project she is working on. For example, the user
might require 128 GB of memory. If you create a new flavor as described above, the user would have access
to the custom flavor, but so would all other tenants in your cloud. Sometimes this sharing isn’t desirable. In
this scenario, allowing all users to have access to a flavor with 128 GB of memory might cause your cloud to
reach full capacity very quickly. To prevent this, you can restrict access to the custom flavor using the nova
flavor-access-add command:
$ nova flavor-access-add FLAVOR_ID PROJECT_ID
To view a flavor’s access list, do the following:
$ nova flavor-access-list [--flavor FLAVOR_ID]
Tip: Once access to a flavor has been restricted, no other projects besides the ones granted explicit access will
be able to see the flavor. This includes the admin project. Make sure to add the admin project in addition to the
original project.
It’s also helpful to allocate a specific numeric range for custom and private flavors. On UNIX-based systems,
nonsystem accounts usually have a UID starting at 500. A similar approach can be taken with custom flavors.
This helps you easily identify which flavors are custom, private, and public for the entire cloud.
User-Facing Operations 51
Operations Guide (Release Version: 15.0.0)
How Do I Modify an Existing Flavor?
The OpenStack dashboard simulates the ability to modify a flavor by deleting an existing flavor and creating a
new one with the same name.
Security Groups
A common new-user issue with OpenStack is failing to set an appropriate security group when launching an
instance. As a result, the user is unable to contact the instance on the network.
Security groups are sets of IP filter rules that are applied to an instance’s networking. They are project specific,
and project members can edit the default rules for their group and add new rules sets. All projects have a “default”
security group, which is applied to instances that have no other security group defined. Unless changed,
this security group denies all incoming traffic.
Tip: As noted in the previous chapter, the number of rules per security group is controlled by the
quota_security_group_rules, and the number of allowed security groups per project is controlled by the
quota_security_groups quota.
End-User Configuration of Security Groups
Security groups for the current project can be found on the OpenStack dashboard under Access & Security.
To see details of an existing group, select the Edit Security Group action for that security group. Obviously,
modifying existing groups can be done from this edit interface. There is a Create Security Group button on
the main Access & Security page for creating new groups. We discuss the terms used in these fields when we
explain the command-line equivalents.
Setting with openstack command
If your environment is using Neutron, you can configure security groups settings using the openstack command.
Get a list of security groups for the project you are acting in, by using following command:
$ openstack security group list
+------------------------+---------+------------------------+-------------------------+
| ID | Name | Description | Project |
+------------------------+---------+------------------------+-------------------------+
| 3bef30ed-442d-4cf1 | default | Default security group | 35e3820f7490493ca9e3a5e |
| -b84d-2ba50a395599 | | | 685393298 |
| aaf1d0b7-98a0-41a3-ae1 | default | Default security group | 32e9707393c34364923edf8 |
| 6-a58b94503289 | | | f5029cbfe |
+------------------------+---------+------------------------+-------------------------+
To view the details of a security group:
$ openstack security group show 3bef30ed-442d-4cf1-b84d-2ba50a395599
+-----------------+-------------------------------------------------------------------------
,→------------------------------------------------------------------------------------------
,→---------------------+
| Field | Value
,→
,→ |
+-----------------+-------------------------------------------------------------------------
,→------------------------------------------------------------------------------------------
,→---------------------+
52 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
| created_at | 2016-11-08T21:55:19Z
,→
,→ |
| description | Default security group
,→
,→ |
| id | 3bef30ed-442d-4cf1-b84d-2ba50a395599
,→
,→ |
| name | default
,→
,→ |
| project_id | 35e3820f7490493ca9e3a5e685393298
,→
,→ |
| project_id | 35e3820f7490493ca9e3a5e685393298
,→
,→ |
| revision_number | 1
,→
,→ |
| rules | created_at='2016-11-08T21:55:19Z', direction='egress', ethertype='IPv6',
,→ id='1dca4cac-d4f2-46f5-b757-d53c01a87bdf', project_id='35e3820f7490493ca9e3a5e685393298',
,→ |
| | revision_number='1', updated_at='2016-11-08T21:55:19Z'
,→
,→ |
| | created_at='2016-11-08T21:55:19Z', direction='egress', ethertype='IPv4',
,→ id='2d83d6f2-424e-4b7c-b9c4-1ede89c00aab', project_id='35e3820f7490493ca9e3a5e685393298',
,→ |
| | revision_number='1', updated_at='2016-11-08T21:55:19Z'
,→
,→ |
| | created_at='2016-11-08T21:55:19Z', direction='ingress', ethertype='IPv4
,→', id='62b7d1eb-b98d-4707-a29f-6df379afdbaa', project_id='35e3820f7490493ca9e3a5e685393298
,→', remote_group_id |
| | ='3bef30ed-442d-4cf1-b84d-2ba50a395599', revision_number='1', updated_
,→at='2016-11-08T21:55:19Z'
,→ |
| | created_at='2016-11-08T21:55:19Z', direction='ingress', ethertype='IPv6
,→', id='f0d4b8d6-32d4-4f93-813d-3ede9d698fbb', project_id='35e3820f7490493ca9e3a5e685393298
,→', remote_group_id |
| | ='3bef30ed-442d-4cf1-b84d-2ba50a395599', revision_number='1', updated_
,→at='2016-11-08T21:55:19Z'
,→ |
| updated_at | 2016-11-08T21:55:19Z
,→
,→ |
+-----------------+-------------------------------------------------------------------------
,→------------------------------------------------------------------------------------------
,→---------------------+
These rules are all “allow” type rules, as the default is deny. This example shows the full port range for all
protocols allowed from all IPs. This section describes the most common security group rule parameters:
direction The direction in which the security group rule is applied. Valid values are ingress or egress.
remote_ip_prefix This attribute value matches the specified IP prefix as the source IP address of the IP packet.
User-Facing Operations 53
Operations Guide (Release Version: 15.0.0)
protocol The protocol that is matched by the security group rule. Valid values are null, tcp, udp, icmp, and
icmpv6.
port_range_min The minimum port number in the range that is matched by the security group rule. If the
protocol is TCP or UDP, this value must be less than or equal to the port_range_max attribute value.
If the protocol is ICMP or ICMPv6, this value must be an ICMP or ICMPv6 type, respectively.
port_range_max The maximum port number in the range that is matched by the security group rule. The
port_range_min attribute constrains the port_range_max attribute. If the protocol is ICMP or
ICMPv6, this value must be an ICMP or ICMPv6 type, respectively.
ethertype Must be IPv4 or IPv6, and addresses represented in CIDR must match the ingress or egress rules.
When adding a new security group, you should pick a descriptive but brief name. This name shows up in brief
descriptions of the instances that use it where the longer description field often does not. Seeing that an instance
is using security group http is much easier to understand than bobs_group or secgrp1.
This example creates a security group that allows web traffic anywhere on the Internet. We’ll call this group
global_http, which is clear and reasonably concise, encapsulating what is allowed and from where. From
the command line, do:
$ openstack security group create global_http --description "allow web traffic from the
,→Internet"
Created a new security_group:
+-----------------+-------------------------------------------------------------------------
,→------------------------------------------------------------------------------------------
,→---------------------+
| Field | Value
,→
,→ |
+-----------------+-------------------------------------------------------------------------
,→------------------------------------------------------------------------------------------
,→---------------------+
| created_at | 2016-11-10T16:09:18Z
,→
,→ |
| description | allow web traffic from the Internet
,→
,→ |
| headers |
,→
,→ |
| id | 70675447-1b92-4102-a7ea-6a3ca99d2290
,→
,→ |
| name | global_http
,→
,→ |
| project_id | 32e9707393c34364923edf8f5029cbfe
,→
,→ |
| project_id | 32e9707393c34364923edf8f5029cbfe
,→
,→ |
| revision_number | 1
,→
,→ |
| rules | created_at='2016-11-10T16:09:18Z', direction='egress', ethertype='IPv4',
,→ id='e440b13a-e74f-4700-a36f-9ecc0de76612', project_id='32e9707393c34364923edf8f5029cbfe',
,→ |
54 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
| | revision_number='1', updated_at='2016-11-10T16:09:18Z'
,→
,→ |
| | created_at='2016-11-10T16:09:18Z', direction='egress', ethertype='IPv6',
,→ id='0debf8cb-9f1d-45e5-98db-ee169c0715fe', project_id='32e9707393c34364923edf8f5029cbfe',
,→ |
| | revision_number='1', updated_at='2016-11-10T16:09:18Z'
,→
,→ |
| updated_at | 2016-11-10T16:09:18Z
,→
,→ |
+-----------------+-------------------------------------------------------------------------
,→------------------------------------------------------------------------------------------
,→---------------------+
Immediately after create, the security group has only an allow egress rule. To make it do what we want, we
need to add some rules:
$ openstack security group rule create --help
usage: openstack security group rule create [-h]
[-f {json,shell,table,value,yaml}]
[-c COLUMN]
[--max-width <integer>]
[--noindent] [--prefix PREFIX]
[--remote-ip <ip-address> | --remote-group
,→<group>]
[--dst-port <port-range>]
[--icmp-type <icmp-type>]
[--icmp-code <icmp-code>]
[--protocol <protocol>]
[--ingress | --egress]
[--ethertype <ethertype>]
[--project <project>]
[--project-domain <project-domain>]
<group>
$ openstack security group rule create --ingress --ethertype IPv4 \
--protocol tcp --remote-ip 0.0.0.0/0 global_http
Created a new security group rule:
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| created_at | 2016-11-10T16:12:27Z |
| description | |
| direction | ingress |
| ethertype | IPv4 |
| headers | |
| id | 694d30b1-1c4d-4bb8-acbe-7f1b3de2b20f |
| port_range_max | None |
| port_range_min | None |
| project_id | 32e9707393c34364923edf8f5029cbfe |
| project_id | 32e9707393c34364923edf8f5029cbfe |
| protocol | tcp |
| remote_group_id | None |
| remote_ip_prefix | 0.0.0.0/0 |
User-Facing Operations 55
Operations Guide (Release Version: 15.0.0)
| revision_number | 1 |
| security_group_id | 70675447-1b92-4102-a7ea-6a3ca99d2290 |
| updated_at | 2016-11-10T16:12:27Z |
+-------------------+--------------------------------------+
Despite only outputting the newly added rule, this operation is additive:
$ openstack security group show global_http
+-----------------+-------------------------------------------------------------------------
,→------------------------------------------------------------------------------------------
,→---------------------+
| Field | Value
,→
,→ |
+-----------------+-------------------------------------------------------------------------
,→------------------------------------------------------------------------------------------
,→---------------------+
| created_at | 2016-11-10T16:09:18Z
,→
,→ |
| description | allow web traffic from the Internet
,→
,→ |
| id | 70675447-1b92-4102-a7ea-6a3ca99d2290
,→
,→ |
| name | global_http
,→
,→ |
| project_id | 32e9707393c34364923edf8f5029cbfe
,→
,→ |
| project_id | 32e9707393c34364923edf8f5029cbfe
,→
,→ |
| revision_number | 2
,→
,→ |
| rules | created_at='2016-11-10T16:09:18Z', direction='egress', ethertype='IPv6',
,→ id='0debf8cb-9f1d-45e5-98db-ee169c0715fe', project_id='32e9707393c34364923edf8f5029cbfe',
,→ |
| | revision_number='1', updated_at='2016-11-10T16:09:18Z'
,→
,→ |
| | created_at='2016-11-10T16:12:27Z', direction='ingress', ethertype='IPv4
,→', id='694d30b1-1c4d-4bb8-acbe-7f1b3de2b20f', project_id='32e9707393c34364923edf8f5029cbfe
,→', protocol='tcp', |
| | remote_ip_prefix='0.0.0.0/0', revision_number='1', updated_at='2016-11-
,→10T16:12:27Z'
,→ |
| | created_at='2016-11-10T16:09:18Z', direction='egress', ethertype='IPv4',
,→ id='e440b13a-e74f-4700-a36f-9ecc0de76612', project_id='32e9707393c34364923edf8f5029cbfe',
,→ |
| | revision_number='1', updated_at='2016-11-10T16:09:18Z'
,→
,→ |
| updated_at | 2016-11-10T16:12:27Z
,→
,→ |
56 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
+-----------------+-------------------------------------------------------------------------
,→------------------------------------------------------------------------------------------
,→---------------------+
The inverse operation is called openstack security group rule delete, specifying security-group-rule
ID. Whole security groups can be removed with openstack security group delete.
To create security group rules for a cluster of instances, use RemoteGroups.
RemoteGroups are a dynamic way of defining the CIDR of allowed sources. The user specifies a RemoteGroup
(security group name) and then all the users’ other instances using the specified RemoteGroup are selected
dynamically. This dynamic selection alleviates the need for individual rules to allow each new member of the
cluster.
The code is similar to the above example of openstack security group rule create. To use RemoteGroup,
specify --remote-group instead of --remote-ip. For example:
$ openstack security group rule create --ingress \
--ethertype IPv4 --protocol tcp \
--remote-group global_http cluster
The “cluster” rule allows SSH access from any other instance that uses the global-http group.
Block Storage
OpenStack volumes are persistent block-storage devices that may be attached and detached from instances, but
they can be attached to only one instance at a time. Similar to an external hard drive, they do not provide shared
storage in the way a network file system or object store does. It is left to the operating system in the instance to
put a file system on the block device and mount it, or not.
As with other removable disk technology, it is important that the operating system is not trying to make use of
the disk before removing it. On Linux instances, this typically involves unmounting any file systems mounted
from the volume. The OpenStack volume service cannot tell whether it is safe to remove volumes from an
instance, so it does what it is told. If a user tells the volume service to detach a volume from an instance while it
is being written to, you can expect some level of file system corruption as well as faults from whatever process
within the instance was using the device.
There is nothing OpenStack-specific in being aware of the steps needed to access block devices from within the
instance operating system, potentially formatting them for first use and being cautious when removing them.
What is specific is how to create new volumes and attach and detach them from instances. These operations
can all be done from the Volumes page of the dashboard or by using the openstack command-line client.
To add new volumes, you need only a volume size in gigabytes. Either put these into the Create Volume web
form or use the command line:
$ openstack volume create volume1 --size 10
This creates a 10 GB volume. To list existing volumes and the instances they are connected to, if any:
$ openstack volume list
+--------------------------------------+--------------+--------+------+-------------+
| ID | Display Name | Status | Size | Attached to |
+--------------------------------------+--------------+--------+------+-------------+
| 6cf4114a-56b2-476b-acf7-7359d8334aa2 | volume1 | error | 10 | |
+--------------------------------------+--------------+--------+------+-------------+
User-Facing Operations 57
Operations Guide (Release Version: 15.0.0)
OpenStack Block Storage also allows creating snapshots of volumes. Remember that this is a block-level
snapshot that is crash consistent, so it is best if the volume is not connected to an instance when the snapshot is
taken and second best if the volume is not in use on the instance it is attached to. If the volume is under heavy
use, the snapshot may have an inconsistent file system. In fact, by default, the volume service does not take a
snapshot of a volume that is attached to an image, though it can be forced to. To take a volume snapshot, either
select Create Snapshot from the actions column next to the volume name on the dashboard Volumes page, or
run this from the command line:
$ openstack help snapshot create
usage: openstack snapshot create [-h] [-f {json,shell,table,value,yaml}]
[-c COLUMN] [--max-width <integer>]
[--noindent] [--prefix PREFIX]
[--name <name>] [--description <description>]
[--force] [--property <key=value>]
<volume>
Create new snapshot
positional arguments:
<volume> Volume to snapshot (name or ID)
optional arguments:
-h, --help show this help message and exit
--name <name> Name of the snapshot
--description <description>
Description of the snapshot
--force Create a snapshot attached to an instance. Default is
False
--property <key=value>
Set a property to this snapshot (repeat option to set
multiple properties)
output formatters:
output formatter options
-f {json,shell,table,value,yaml}, --format {json,shell,table,value,yaml}
the output format, defaults to table
-c COLUMN, --column COLUMN
specify the column(s) to include, can be repeated
table formatter:
--max-width <integer>
Maximum display width, <1 to disable. You can also use
the CLIFF_MAX_TERM_WIDTH environment variable, but the
parameter takes precedence.
json formatter:
--noindent whether to disable indenting the JSON
shell formatter:
a format a UNIX shell can parse (variable="value")
--prefix PREFIX add a prefix to all variable names
Note: For more information about updating Block Storage volumes (for example, resizing or transferring),
58 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
see the OpenStack End User Guide.
Block Storage Creation Failures
If a user tries to create a volume and the volume immediately goes into an error state, the best way to troubleshoot
is to grep the cinder log files for the volume’s UUID. First try the log files on the cloud controller, and then try
the storage node where the volume was attempted to be created:
# grep 903b85d0-bacc-4855-a261-10843fc2d65b /var/log/cinder/*.log
Shared File Systems Service
Similar to Block Storage, the Shared File System is a persistent storage, called share, that can be used in multitenant
environments. Users create and mount a share as a remote file system on any machine that allows
mounting shares, and has network access to share exporter. This share can then be used for storing, sharing,
and exchanging files. The default configuration of the Shared File Systems service depends on the back-end
driver the admin chooses when starting the Shared File Systems service. For more information about existing
back-end drivers, see Share Backends of Shared File Systems service Developer Guide. For example, in
case of OpenStack Block Storage based back-end is used, the Shared File Systems service cares about everything,
including VMs, networking, keypairs, and security groups. Other configurations require more detailed
knowledge of shares functionality to set up and tune specific parameters and modes of shares functioning.
Shares are a remote mountable file system, so users can mount a share to multiple hosts, and have it accessed
from multiple hosts by multiple users at a time. With the Shared File Systems service, you can perform a large
number of operations with shares:
• Create, update, delete, and force-delete shares
• Change access rules for shares, reset share state
• Specify quotas for existing users or tenants
• Create share networks
• Define new share types
• Perform operations with share snapshots: create, change name, create a share from a snapshot, delete
• Operate with consistency groups
• Use security services
For more information on share management see Share management of chapter “Shared File Systems” in OpenStack
Administrator Guide. As to Security services, you should remember that different drivers support different
authentication methods, while generic driver does not support Security Services at all (see section Security
services of chapter “Shared File Systems” in OpenStack Administrator Guide).
You can create a share in a network, list shares, and show information for, update, and delete a specified share.
You can also create snapshots of shares (see Share snapshots of chapter “Shared File Systems” in OpenStack
Administrator Guide).
There are default and specific share types that allow you to filter or choose back-ends before you create a share.
Functions and behaviour of share type is similar to Block Storage volume type (see Share types of chapter
“Shared File Systems” in OpenStack Administrator Guide).
User-Facing Operations 59
Operations Guide (Release Version: 15.0.0)
To help users keep and restore their data, Shared File Systems service provides a mechanism to create and
operate snapshots (see Share snapshots of chapter “Shared File Systems” in OpenStack Administrator Guide).
A security service stores configuration information for clients for authentication and authorization. Inside
Manila a share network can be associated with up to three security types (for detailed information see Security
services of chapter “Shared File Systems” in OpenStack Administrator Guide):
• LDAP
• Kerberos
• Microsoft Active Directory
Shared File Systems service differs from the principles implemented in Block Storage. Shared File Systems
service can work in two modes:
• Without interaction with share networks, in so called “no share servers” mode.
• Interacting with share networks.
Networking service is used by the Shared File Systems service to directly operate with share servers. For
switching interaction with Networking service on, create a share specifying a share network. To use “share
servers” mode even being out of OpenStack, a network plugin called StandaloneNetworkPlugin is used. In this
case, provide network information in the configuration: IP range, network type, and segmentation ID. Also you
can add security services to a share network (see section “Networking” of chapter “Shared File Systems” in
OpenStack Administrator Guide).
The main idea of consistency groups is to enable you to create snapshots at the exact same point in time from
multiple file system shares. Those snapshots can be then used for restoring all shares that were associated
with the consistency group (see section “Consistency groups” of chapter “Shared File Systems” in OpenStack
Administrator Guide).
Shared File System storage allows administrators to set limits and quotas for specific tenants and users. Limits
are the resource limitations that are allowed for each tenant or user. Limits consist of:
• Rate limits
• Absolute limits
Rate limits control the frequency at which users can issue specific API requests. Rate limits are configured by
administrators in a config file. Also, administrator can specify quotas also known as max values of absolute
limits per tenant. Whereas users can see only the amount of their consumed resources. Administrator can
specify rate limits or quotas for the following resources:
• Max amount of space available for all shares
• Max number of shares
• Max number of shared networks
• Max number of share snapshots
• Max total amount of all snapshots
• Type and number of API calls that can be made in a specific time interval
User can see his rate limits and absolute limits by running commands manila rate-limits and manila
absolute-limits respectively. For more details on limits and quotas see Quotas and limits of “Share management”
section of OpenStack Administrator Guide document.
This section lists several of the most important Use Cases that demonstrate the main functions and abilities of
Shared File Systems service:
60 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
• Create share
• Operating with a share
• Manage access to shares
• Create snapshots
• Create a share network
• Manage a share network
Note: Shared File Systems service cannot warn you beforehand if it is safe to write a specific large amount of
data onto a certain share or to remove a consistency group if it has a number of shares assigned to it. In such
a potentially erroneous situations, if a mistake happens, you can expect some error message or even failing of
shares or consistency groups into an incorrect status. You can also expect some level of system corruption if a
user tries to unmount an unmanaged share while a process is using it for data transfer.
Create Share
In this section, we examine the process of creating a simple share. It consists of several steps:
• Check if there is an appropriate share type defined in the Shared File Systems service
• If such a share type does not exist, an Admin should create it using manila type-create command
before other users are able to use it
• Using a share network is optional. However if you need one, check if there is an appropriate network
defined in Shared File Systems service by using manila share-network-list command. For the
information on creating a share network, see Create a Share Network below in this chapter.
• Create a public share using manila create.
• Make sure that the share has been created successfully and is ready to use (check the share status and see
the share export location)
Below is the same whole procedure described step by step and in more detail.
Note: Before you start, make sure that Shared File Systems service is installed on your OpenStack cluster and
is ready to use.
By default, there are no share types defined in Shared File Systems service, so you can check if a required one
has been already created:
$ manila type-list
+------+--------+-----------+-----------+----------------------------------+----------------
,→------+
| ID | Name | Visibility| is_default| required_extra_specs | optional_extra_
,→specs |
+------+--------+-----------+-----------+----------------------------------+----------------
,→------+
| c0...| default| public | YES | driver_handles_share_servers:True| snapshot_
,→support:True|
+------+--------+-----------+-----------+----------------------------------+----------------
,→------+
User-Facing Operations 61
Operations Guide (Release Version: 15.0.0)
If the share types list is empty or does not contain a type you need, create the required share type using this
command:
$ manila type-create netapp1 False --is_public True
This command will create a public share with the following parameters: name = netapp1,
spec_driver_handles_share_servers = False
You can now create a public share with my_share_net network, default share type, NFS shared file systems
protocol, and 1 GB size:
$ manila create nfs 1 --name "Share1" --description "My first share" \
--share-type default --share-network my_share_net --metadata aim=testing --public
+-----------------------------+--------------------------------------+
| Property | Value |
+-----------------------------+--------------------------------------+
| status | creating |
| share_type_name | default |
| description | My first share |
| availability_zone | None |
| share_network_id | 9c187d23-7e1d-4d91-92d0-77ea4b9b9496 |
| share_server_id | None |
| host | |
| access_rules_status | active |
| snapshot_id | None |
| is_public | True |
| task_state | None |
| snapshot_support | True |
| id | edd82179-587e-4a87-9601-f34b2ca47e5b |
| size | 1 |
| name | Share1 |
| share_type | e031d5e9-f113-491a-843f-607128a5c649 |
| has_replicas | False |
| replication_type | None |
| created_at | 2016-03-20T00:00:00.000000 |
| share_proto | NFS |
| consistency_group_id | None |
| source_cgsnapshot_member_id | None |
| project_id | e81908b1bfe8468abb4791eae0ef6dd9 |
| metadata | {u'aim': u'testing'} |
+-----------------------------+--------------------------------------+
To confirm that creation has been successful, see the share in the share list:
$ manila list
+----+-------+-----+------------+-----------+-------------------------------+---------------
,→-------+
| ID | Name | Size| Share Proto| Share Type| Export location | Host
,→ |
+----+-------+-----+------------+-----------+-------------------------------+---------------
,→-------+
| a..| Share1| 1 | NFS | c0086... | 10.254.0.3:/shares/share-2d5..|
,→manila@generic1#GEN..|
+----+-------+-----+------------+-----------+-------------------------------+---------------
,→-------+
Check the share status and see the share export location. After creation, the share status should become available:
62 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
$ manila show Share1
+-----------------------------+-------------------------------------------------------------
,→---------+
| Property | Value
,→ |
+-----------------------------+-------------------------------------------------------------
,→---------+
| status | available
,→ |
| share_type_name | default
,→ |
| description | My first share
,→ |
| availability_zone | nova
,→ |
| share_network_id | 9c187d23-7e1d-4d91-92d0-77ea4b9b9496
,→ |
| export_locations |
,→ |
| | path = 10.254.0.3:/shares/share-18cb05be-eb69-4cb2-810f-
,→91c75ef30f90 |
| | preferred = False
,→ |
| | is_admin_only = False
,→ |
| | id = d6a82c0d-36b0-438b-bf34-63f3932ddf4e
,→ |
| | share_instance_id = 18cb05be-eb69-4cb2-810f-91c75ef30f90
,→ |
| | path = 10.0.0.3:/shares/share-18cb05be-eb69-4cb2-810f-
,→91c75ef30f90 |
| | preferred = False
,→ |
| | is_admin_only = True
,→ |
| | id = 51672666-06b8-4741-99ea-64f2286f52e2
,→ |
| | share_instance_id = 18cb05be-eb69-4cb2-810f-91c75ef30f90
,→ |
| share_server_id | ea8b3a93-ab41-475e-9df1-0f7d49b8fa54
,→ |
| host | manila@generic1#GENERIC1
,→ |
| access_rules_status | active
,→ |
| snapshot_id | None
,→ |
| is_public | True
,→ |
| task_state | None
,→ |
| snapshot_support | True
,→ |
| id | e7364bcc-3821-49bf-82d6-0c9f0276d4ce
,→ |
| size | 1
,→ |
User-Facing Operations 63
Operations Guide (Release Version: 15.0.0)
| name | Share1
,→ |
| share_type | e031d5e9-f113-491a-843f-607128a5c649
,→ |
| has_replicas | False
,→ |
| replication_type | None
,→ |
| created_at | 2016-03-20T00:00:00.000000
,→ |
| share_proto | NFS
,→ |
| consistency_group_id | None
,→ |
| source_cgsnapshot_member_id | None
,→ |
| project_id | e81908b1bfe8468abb4791eae0ef6dd9
,→ |
| metadata | {u'aim': u'testing'}
,→ |
+-----------------------------+-------------------------------------------------------------
,→---------+
The value is_public defines the level of visibility for the share: whether other tenants can or cannot see the
share. By default, the share is private. Now you can mount the created share like a remote file system and use
it for your purposes.
Note: See Share Management of “Shared File Systems” section of OpenStack Administrator Guide document
for the details on share management operations.
Manage Access To Shares
Currently, you have a share and would like to control access to this share for other users. For this, you have to
perform a number of steps and operations. Before getting to manage access to the share, pay attention to the
following important parameters. To grant or deny access to a share, specify one of these supported share access
levels:
• rw: read and write (RW) access. This is the default value.
• ro: read-only (RO) access.
Additionally, you should also specify one of these supported authentication methods:
• ip: authenticates an instance through its IP address. A valid format is XX.XX.XX.XX
orXX.XX.XX.XX/XX. For example 0.0.0.0/0.
• cert: authenticates an instance through a TLS certificate. Specify the TLS identity as the IDENTKEY.
A valid value is any string up to 64 characters long in the common name (CN) of the certificate. The
meaning of a string depends on its interpretation.
• user: authenticates by a specified user or group name. A valid value is an alphanumeric string that can
contain some special characters and is from 4 to 32 characters long.
64 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
Note: Do not mount a share without an access rule! This can lead to an exception.
Allow access to the share with IP access type and 10.254.0.4 IP address:
$ manila access-allow Share1 ip 10.254.0.4 --access-level rw
+--------------+--------------------------------------+
| Property | Value |
+--------------+--------------------------------------+
| share_id | 7bcd888b-681b-4836-ac9c-c3add4e62537 |
| access_type | ip |
| access_to | 10.254.0.4 |
| access_level | rw |
| state | new |
| id | de715226-da00-4cfc-b1ab-c11f3393745e |
+--------------+--------------------------------------+
Mount the Share:
$ sudo mount -v -t nfs 10.254.0.5:/shares/share-5789ddcf-35c9-4b64-a28a-7f6a4a574b6a /mnt/
Then check if the share mounted successfully and according to the specified access rules:
$ manila access-list Share1
+--------------------------------------+-------------+------------+--------------+--------+
| id | access type | access to | access level | state |
+--------------------------------------+-------------+------------+--------------+--------+
| 4f391c6b-fb4f-47f5-8b4b-88c5ec9d568a | user | demo | rw | error |
| de715226-da00-4cfc-b1ab-c11f3393745e | ip | 10.254.0.4 | rw | active |
+--------------------------------------+-------------+------------+--------------+--------+
Note: Different share features are supported by different share drivers. In these examples there was used
generic (Cinder as a back-end) driver that does not support user and cert authentication methods.
Tip: For the details of features supported by different drivers see Manila share features support mapping of
Manila Developer Guide document.
Manage Shares
There are several other useful operations you would perform when working with shares.
Update Share
To change the name of a share, or update its description, or level of visibility for other tenants, use this command:
$ manila update Share1 --description "My first share. Updated" --is-public False
Check the attributes of the updated Share1:
User-Facing Operations 65
Operations Guide (Release Version: 15.0.0)
$ manila show Share1
+-----------------------------+-------------------------------------------------------------
,→---------+
| Property | Value
,→ |
+-----------------------------+-------------------------------------------------------------
,→---------+
| status | available
,→ |
| share_type_name | default
,→ |
| description | My first share. Updated
,→ |
| availability_zone | nova
,→ |
| share_network_id | 9c187d23-7e1d-4d91-92d0-77ea4b9b9496
,→ |
| export_locations |
,→ |
| | path = 10.254.0.3:/shares/share-18cb05be-eb69-4cb2-810f-
,→91c75ef30f90 |
| | preferred = False
,→ |
| | is_admin_only = False
,→ |
| | id = d6a82c0d-36b0-438b-bf34-63f3932ddf4e
,→ |
| | share_instance_id = 18cb05be-eb69-4cb2-810f-91c75ef30f90
,→ |
| | path = 10.0.0.3:/shares/share-18cb05be-eb69-4cb2-810f-
,→91c75ef30f90 |
| | preferred = False
,→ |
| | is_admin_only = True
,→ |
| | id = 51672666-06b8-4741-99ea-64f2286f52e2
,→ |
| | share_instance_id = 18cb05be-eb69-4cb2-810f-91c75ef30f90
,→ |
| share_server_id | ea8b3a93-ab41-475e-9df1-0f7d49b8fa54
,→ |
| host | manila@generic1#GENERIC1
,→ |
| access_rules_status | active
,→ |
| snapshot_id | None
,→ |
| is_public | False
,→ |
| task_state | None
,→ |
| snapshot_support | True
,→ |
| id | e7364bcc-3821-49bf-82d6-0c9f0276d4ce
,→ |
| size | 1
,→ |
66 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
| name | Share1
,→ |
| share_type | e031d5e9-f113-491a-843f-607128a5c649
,→ |
| has_replicas | False
,→ |
| replication_type | None
,→ |
| created_at | 2016-03-20T00:00:00.000000
,→ |
| share_proto | NFS
,→ |
| consistency_group_id | None
,→ |
| source_cgsnapshot_member_id | None
,→ |
| project_id | e81908b1bfe8468abb4791eae0ef6dd9
,→ |
| metadata | {u'aim': u'testing'}
,→ |
+-----------------------------+-------------------------------------------------------------
,→---------+
Reset Share State
Sometimes a share may appear and then hang in an erroneous or a transitional state. Unprivileged users do not
have the appropriate access rights to correct this situation. However, having cloud administrator’s permissions,
you can reset the share’s state by using
$ manila reset-state [–state state] share_name
command to reset share state, where state indicates which state to assign the share to. Options include:
available, error, creating, deleting, error_deleting states.
After running
$ manila reset-state Share2 --state deleting
check the share’s status:
$ manila show Share2
+-----------------------------+-------------------------------------------+
| Property | Value |
+-----------------------------+-------------------------------------------+
| status | deleting |
| share_type_name | default |
| description | share from a snapshot. |
| availability_zone | nova |
| share_network_id | 5c3cbabb-f4da-465f-bc7f-fadbe047b85a |
| export_locations | [] |
| share_server_id | 41b7829d-7f6b-4c96-aea5-d106c2959961 |
| host | manila@generic1#GENERIC1 |
| snapshot_id | 962e8126-35c3-47bb-8c00-f0ee37f42ddd |
| is_public | False |
| task_state | None |
User-Facing Operations 67
Operations Guide (Release Version: 15.0.0)
| snapshot_support | True |
| id | b6b0617c-ea51-4450-848e-e7cff69238c7 |
| size | 1 |
| name | Share2 |
| share_type | c0086582-30a6-4060-b096-a42ec9d66b86 |
| created_at | 2015-09-25T06:25:50.000000 |
| export_location | 10.254.0.3:/shares/share-1dc2a471-3d47-...|
| share_proto | NFS |
| consistency_group_id | None |
| source_cgsnapshot_member_id | None |
| project_id | 20787a7ba11946adad976463b57d8a2f |
| metadata | {u'source': u'snapshot'} |
+-----------------------------+-------------------------------------------+
Delete Share
If you do not need a share any more, you can delete it using manila delete share_name_or_ID command
like:
$ manila delete Share2
Note: If you specified the consistency group while creating a share, you should provide the –consistency-group
parameter to delete the share:
$ manila delete ba52454e-2ea3-47fa-a683-3176a01295e6 --consistency-group \
ffee08d9-c86c-45e5-861e-175c731daca2
Sometimes it appears that a share hangs in one of transitional states (i.e. creating, deleting, managing,
unmanaging, extending, and shrinking). In that case, to delete it, you need manila force-delete
share_name_or_ID command and administrative permissions to run it:
$ manila force-delete b6b0617c-ea51-4450-848e-e7cff69238c7
Tip: For more details and additional information about other cases, features, API commands etc, see Share
Management of “Shared File Systems” section of OpenStack Administrator Guide document.
Create Snapshots
The Shared File Systems service provides a mechanism of snapshots to help users to restore their own data. To
create a snapshot, use manila snapshot-create command like:
$ manila snapshot-create Share1 --name Snapshot1 --description "Snapshot of Share1"
+-------------------+--------------------------------------+
| Property | Value |
+-------------------+--------------------------------------+
| status | creating |
| share_id | e7364bcc-3821-49bf-82d6-0c9f0276d4ce |
| description | Snapshot of Share1 |
| created_at | 2016-03-20T00:00:00.000000 |
68 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
| share_proto | NFS |
| provider_location | None |
| id | a96cf025-92d1-4012-abdd-bb0f29e5aa8f |
| size | 1 |
| share_size | 1 |
| name | Snapshot1 |
+-------------------+--------------------------------------+
Then, if needed, update the name and description of the created snapshot:
$ manila snapshot-rename Snapshot1 Snapshot_1 --description "Snapshot of Share1. Updated."
To make sure that the snapshot is available, run:
$ manila snapshot-show Snapshot1
+-------------------+--------------------------------------+
| Property | Value |
+-------------------+--------------------------------------+
| status | available |
| share_id | e7364bcc-3821-49bf-82d6-0c9f0276d4ce |
| description | Snapshot of Share1 |
| created_at | 2016-03-30T10:53:19.000000 |
| share_proto | NFS |
| provider_location | 3ca7a3b2-9f9f-46af-906f-6a565bf8ee37 |
| id | a96cf025-92d1-4012-abdd-bb0f29e5aa8f |
| size | 1 |
| share_size | 1 |
| name | Snapshot1 |
+-------------------+--------------------------------------+
Tip: For more details and additional information on snapshots, see Share Snapshots of “Shared File Systems”
section of “OpenStack Administrator Guide” document.
Create a Share Network
To control a share network, Shared File Systems service requires interaction with Networking service to manage
share servers on its own. If the selected driver runs in a mode that requires such kind of interaction, you need
to specify the share network when a share is created. For the information on share creation, see Create Share
earlier in this chapter. Initially, check the existing share networks type list by:
$ manila share-network-list
+--------------------------------------+--------------+
| id | name |
+--------------------------------------+--------------+
+--------------------------------------+--------------+
If share network list is empty or does not contain a required network, just create, for example, a share network
with a private network and subnetwork.
$ manila share-network-create --neutron-net-id 5ed5a854-21dc-4ed3-870a-117b7064eb21 \
--neutron-subnet-id 74dcfb5a-b4d7-4855-86f5-a669729428dc --name my_share_net \
--description "My first share network"
+-------------------+--------------------------------------+
User-Facing Operations 69
Operations Guide (Release Version: 15.0.0)
| Property | Value |
+-------------------+--------------------------------------+
| name | my_share_net |
| segmentation_id | None |
| created_at | 2015-09-24T12:06:32.602174 |
| neutron_subnet_id | 74dcfb5a-b4d7-4855-86f5-a669729428dc |
| updated_at | None |
| network_type | None |
| neutron_net_id | 5ed5a854-21dc-4ed3-870a-117b7064eb21 |
| ip_version | None |
| nova_net_id | None |
| cidr | None |
| project_id | 20787a7ba11946adad976463b57d8a2f |
| id | 5c3cbabb-f4da-465f-bc7f-fadbe047b85a |
| description | My first share network |
+-------------------+--------------------------------------+
The segmentation_id, cidr, ip_version, and network_type share network attributes are automatically
set to the values determined by the network provider.
Then check if the network became created by requesting the networks list once again:
$ manila share-network-list
+--------------------------------------+--------------+
| id | name |
+--------------------------------------+--------------+
| 5c3cbabb-f4da-465f-bc7f-fadbe047b85a | my_share_net |
+--------------------------------------+--------------+
Finally, to create a share that uses this share network, get to Create Share use case described earlier in this
chapter.
Tip: See Share Networks of “Shared File Systems” section of OpenStack Administrator Guide document for
more details.
Manage a Share Network
There is a pair of useful commands that help manipulate share networks. To start, check the network list:
$ manila share-network-list
+--------------------------------------+--------------+
| id | name |
+--------------------------------------+--------------+
| 5c3cbabb-f4da-465f-bc7f-fadbe047b85a | my_share_net |
+--------------------------------------+--------------+
If you configured the back-end with driver_handles_share_servers = True (with the share servers) and
had already some operations in the Shared File Systems service, you can see manila_service_network in
the neutron list of networks. This network was created by the share driver for internal usage.
$ openstack network list
+--------------+------------------------+------------------------------------+
| ID | Name | Subnets |
70 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
+--------------+------------------------+------------------------------------+
| 3b5a629a-e...| manila_service_network | 4f366100-50... 10.254.0.0/28 |
| bee7411d-d...| public | 884a6564-01... 2001:db8::/64 |
| | | e6da81fa-55... 172.24.4.0/24 |
| 5ed5a854-2...| private | 74dcfb5a-bd... 10.0.0.0/24 |
| | | cc297be2-51... fd7d:177d:a48b::/64 |
+--------------+------------------------+------------------------------------+
You also can see detailed information about the share network including network_type, segmentation_id
fields:
$ openstack network show manila_service_network
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | True |
| availability_zone_hints | |
| availability_zones | nova |
| created_at | 2016-03-20T00:00:00 |
| description | |
| id | ef5282ab-dbf9-4d47-91d4-b0cc9b164567 |
| ipv4_address_scope | |
| ipv6_address_scope | |
| mtu | 1450 |
| name | manila_service_network |
| port_security_enabled | True |
| provider:network_type | vxlan |
| provider:physical_network | |
| provider:segmentation_id | 1047 |
| router:external | False |
| shared | False |
| status | ACTIVE |
| subnets | aba49c7d-c7eb-44b9-9c8f-f6112b05a2e0 |
| tags | |
| tenant_id | f121b3ee03804266af2959e56671b24a |
| updated_at | 2016-03-20T00:00:00 |
+---------------------------+--------------------------------------+
You also can add and remove the security services to the share network.
Tip: For details, see subsection Security Services of “Shared File Systems” section of OpenStack Administrator
Guide document.
Instances
Instances are the running virtual machines within an OpenStack cloud. This section deals with how to work
with them and their underlying images, their network properties, and how they are represented in the database.
Starting Instances
To launch an instance, you need to select an image, a flavor, and a name. The name needn’t be unique, but your
life will be simpler if it is because many tools will use the name in place of the UUID so long as the name is
User-Facing Operations 71
Operations Guide (Release Version: 15.0.0)
unique. You can start an instance from the dashboard from the Launch Instance button on the Instances page
or by selecting the Launch action next to an image or a snapshot on the Images page.
On the command line, do this:
$ openstack server create --flavor FLAVOR --image IMAGE_NAME_OR_ID
There are a number of optional items that can be specified. You should read the rest of this section before trying
to start an instance, but this is the base command that later details are layered upon.
To delete instances from the dashboard, select the Delete Instance action next to the instance on the Instances
page.
Note: In releases prior to Mitaka, select the equivalent Terminate instance action.
From the command line, do this:
$ openstack server delete INSTANCE_ID
It is important to note that powering off an instance does not terminate it in the OpenStack sense.
Instance Boot Failures
If an instance fails to start and immediately moves to an error state, there are a few different ways to track down
what has gone wrong. Some of these can be done with normal user access, while others require access to your
log server or compute nodes.
The simplest reasons for nodes to fail to launch are quota violations or the scheduler being unable to find a
suitable compute node on which to run the instance. In these cases, the error is apparent when you run a
openstack server show on the faulted instance:
$ openstack server show test-instance
+--------------------------------------+----------------------------------------------------
,→-----------------------------------------------------------------------------------+
| Field | Value
,→ |
+--------------------------------------+----------------------------------------------------
,→-----------------------------------------------------------------------------------+
| OS-DCF:diskConfig | AUTO
,→ |
| OS-EXT-AZ:availability_zone | nova
,→ |
| OS-EXT-SRV-ATTR:host | None
,→ |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None
,→ |
| OS-EXT-SRV-ATTR:instance_name | instance-0000000a
,→ |
| OS-EXT-STS:power_state | NOSTATE
,→ |
| OS-EXT-STS:task_state | None
,→ |
| OS-EXT-STS:vm_state | error
,→ |
| OS-SRV-USG:launched_at | None
,→ |
72 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
| OS-SRV-USG:terminated_at | None
,→ |
| accessIPv4 |
,→ |
| accessIPv6 |
,→ |
| addresses |
,→ |
| config_drive |
,→ |
| created | 2016-11-23T07:51:53Z
,→ |
| fault | {u'message': u'Build of instance 6ec42311-a121-
,→4887-aece-48fb93a4a098 aborted: Failed to allocate the network(s), not rescheduling.', |
| | u'code': 500, u'details': u' File "/usr/lib/
,→python2.7/site-packages/nova/compute/manager.py", line 1779, in |
| | _do_build_and_run_instance\n filter_
,→properties)\n File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1960,
,→ in |
| | _build_and_run_instance\n reason=msg)\n', u
,→'created': u'2016-11-23T07:57:04Z'} |
| flavor | m1.tiny (1)
,→ |
| hostId |
,→ |
| id | 6ec42311-a121-4887-aece-48fb93a4a098
,→ |
| image | cirros (9fef3b2d-c35d-4b61-bea8-09cc6dc41829)
,→ |
| key_name | None
,→ |
| name | test-instance
,→ |
| os-extended-volumes:volumes_attached | []
,→ |
| project_id | 5669caad86a04256994cdf755df4d3c1
,→ |
| properties |
,→ |
| status | ERROR
,→ |
| updated | 2016-11-23T07:57:04Z
,→ |
| user_id | c36cec73b0e44876a4478b1e6cd749bb
,→ |
+--------------------------------------+----------------------------------------------------
,→-----------------------------------------------------------------------------------+
In this case, looking at the fault message shows NoValidHost, indicating that the scheduler was unable to
match the instance requirements.
If openstack server show does not sufficiently explain the failure, searching for the instance UUID in the
nova-compute.log on the compute node it was scheduled on or the nova-scheduler.log on your scheduler
hosts is a good place to start looking for lower-level problems.
Using openstack server show as an admin user will show the compute node the instance was scheduled on
as hostId. If the instance failed during scheduling, this field is blank.
User-Facing Operations 73
Operations Guide (Release Version: 15.0.0)
Using Instance-Specific Data
There are two main types of instance-specific data: metadata and user data.
Instance metadata
For Compute, instance metadata is a collection of key-value pairs associated with an instance. Compute reads
and writes to these key-value pairs any time during the instance lifetime, from inside and outside the instance,
when the end user uses the Compute API to do so. However, you cannot query the instance-associated key-value
pairs with the metadata service that is compatible with the Amazon EC2 metadata service.
For an example of instance metadata, users can generate and register SSH keys using the openstack keypair
create command:
$ openstack keypair create mykey > mykey.pem
This creates a key named mykey, which you can associate with instances. The file mykey.pem is the private
key, which should be saved to a secure location because it allows root access to instances the mykey key is
associated with.
Use this command to register an existing key with OpenStack:
$ openstack keypair create --public-key mykey.pub mykey
Note: You must have the matching private key to access instances associated with this key.
To associate a key with an instance on boot, add --key-name mykey to your command line. For example:
$ openstack server create --image ubuntu-cloudimage --flavor 2 \
--key-name mykey myimage
When booting a server, you can also add arbitrary metadata so that you can more easily identify it among other
running instances. Use the --property option with a key-value pair, where you can make up the string for
both the key and the value. For example, you could add a description and also the creator of the server:
$ openstack server create --image=test-image --flavor=1 \
--property description='Small test image' smallimage
When viewing the server information, you can see the metadata included on the metadata line:
$ openstack server show smallimage
+--------------------------------------+----------------------------------------------------
,→------+
| Field | Value
,→ |
+--------------------------------------+----------------------------------------------------
,→------+
| OS-DCF:diskConfig | MANUAL
,→ |
| OS-EXT-AZ:availability_zone | nova
,→ |
| OS-EXT-SRV-ATTR:host | rdo-newton.novalocal
,→ |
74 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
| OS-EXT-SRV-ATTR:hypervisor_hostname | rdo-newton.novalocal
,→ |
| OS-EXT-SRV-ATTR:instance_name | instance-00000002
,→ |
| OS-EXT-STS:power_state | Running
,→ |
| OS-EXT-STS:task_state | None
,→ |
| OS-EXT-STS:vm_state | active
,→ |
| OS-SRV-USG:launched_at | 2016-12-07T11:20:08.000000
,→ |
| OS-SRV-USG:terminated_at | None
,→ |
| accessIPv4 |
,→ |
| accessIPv6 |
,→ |
| addresses | public=172.24.4.227
,→ |
| config_drive |
,→ |
| created | 2016-12-07T11:17:44Z
,→ |
| flavor | m1.tiny (1)
,→ |
| hostId |
,→aca973d5b7981faaf8c713a0130713bbc1e64151be65c8dfb53039f7 |
| id | 4f7c6b2c-f27e-4ccd-a606-6bfc9d7c0d91
,→ |
| image | cirros (01bcb649-45d7-4e3d-8a58-1fcc87816907)
,→ |
| key_name | None
,→ |
| name | smallimage
,→ |
| os-extended-volumes:volumes_attached | []
,→ |
| progress | 0
,→ |
| project_id | 2daf82a578e9437cab396c888ff0ca57
,→ |
| properties | description='Small test image'
,→ |
| security_groups | [{u'name': u'default'}]
,→ |
| status | ACTIVE
,→ |
| updated | 2016-12-07T11:20:08Z
,→ |
| user_id | 8cbea24666ae49bbb8c1641f9b12d2d2
,→ |
+--------------------------------------+----------------------------------------------------
,→------+
User-Facing Operations 75
Operations Guide (Release Version: 15.0.0)
Instance user data
The user-data key is a special key in the metadata service that holds a file that cloud-aware applications within
the guest instance can access. For example, cloudinit is an open source package from Ubuntu, but available in
most distributions, that handles early initialization of a cloud instance that makes use of this user data.
This user data can be put in a file on your local system and then passed in at instance creation with the flag
--user-data <user-data-file>.
For example
$ openstack server create --image ubuntu-cloudimage --flavor 1 \
--user-data mydata.file mydatainstance
To understand the difference between user data and metadata, realize that user data is created before an instance
is started. User data is accessible from within the instance when it is running. User data can be used to store
configuration, a script, or anything the tenant wants.
File injection
Arbitrary local files can also be placed into the instance file system at creation time by using the --file <dstpath=src-path>
option. You may store up to five files.
For example, let’s say you have a special authorized_keys file named special_authorized_keysfile that for
some reason you want to put on the instance instead of using the regular SSH key injection. In this case, you
can use the following command:
$ openstack server create --image ubuntu-cloudimage --flavor 1 \
--file /root/.ssh/authorized_keys=special_authorized_keysfile \
authkeyinstance
Associating Security Groups
Security groups, as discussed earlier, are typically required to allow network traffic to an instance, unless the
default security group for a project has been modified to be more permissive.
Adding security groups is typically done on instance boot. When launching from the dashboard, you do this
on the Access & Security tab of the Launch Instance dialog. When launching from the command line, append
--security-groups with a comma-separated list of security groups.
It is also possible to add and remove security groups when an instance is running. Currently this is only available
through the command-line tools. Here is an example:
$ openstack server add security group SERVER SECURITY_GROUP_NAME_OR_ID
$ openstack server remove security group SERVER SECURITY_GROUP_NAME_OR_ID
Floating IPs
Where floating IPs are configured in a deployment, each project will have a limited number of floating IPs
controlled by a quota. However, these need to be allocated to the project from the central pool prior to their
use—usually by the administrator of the project. To allocate a floating IP to a project, use the Allocate IP To
76 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
Project button on the Floating IPs tab of the Access & Security page of the dashboard. The command line can
also be used:
$ openstack floating ip create NETWORK_NAME_OR_ID
Once allocated, a floating IP can be assigned to running instances from the dashboard either by selecting Associate
from the actions drop-down next to the IP on the Floating IPs tab of the Access & Security page or
by making this selection next to the instance you want to associate it with on the Instances page. The inverse
action, Dissociate Floating IP, is available from the Floating IPs tab of the Access & Security page and from
the Instances page.
To associate or disassociate a floating IP with a server from the command line, use the following commands:
$ openstack server add floating ip SERVER IP_ADDRESS
$ openstack server remove floating ip SERVER IP_ADDRESS
Attaching Block Storage
You can attach block storage to instances from the dashboard on the Volumes page. Click the Manage Attachments
action next to the volume you want to attach.
To perform this action from command line, run the following command:
$ openstack server add volume SERVER VOLUME_NAME_OR_ID --device DEVICE
You can also specify block deviceblock device mapping at instance boot time through the nova command-line
client with this option set:
--block-device-mapping <dev-name=mapping>
The block device mapping format is <dev-name>=<id>:<type>:<size(GB)>:<delete-on-terminate>,
where:
dev-name A device name where the volume is attached in the system at /dev/dev_name
id The ID of the volume to boot from, as shown in the output of openstack volume list
type Either snap, which means that the volume was created from a snapshot, or anything other than snap (a
blank string is valid). In the preceding example, the volume was not created from a snapshot, so we leave
this field blank in our following example.
size (GB) The size of the volume in gigabytes. It is safe to leave this blank and have the Compute Service infer
the size.
delete-on-terminate A boolean to indicate whether the volume should be deleted when the instance is terminated.
True can be specified as True or 1. False can be specified as False or 0.
The following command will boot a new instance and attach a volume at the same time. The volume of ID 13
will be attached as /dev/vdc. It is not a snapshot, does not specify a size, and will not be deleted when the
instance is terminated:
$ openstack server create --image 4042220e-4f5e-4398-9054-39fbd75a5dd7 \
--flavor 2 --key-name mykey --block-device-mapping vdc=13:::0 \
boot-with-vol-test
User-Facing Operations 77
Operations Guide (Release Version: 15.0.0)
If you have previously prepared block storage with a bootable file system image, it is even possible to boot
from persistent block storage. The following command boots an image from the specified volume. It is similar
to the previous command, but the image is omitted and the volume is now attached as /dev/vda:
$ openstck server create --flavor 2 --key-name mykey \
--block-device-mapping vda=13:::0 boot-from-vol-test
Read more detailed instructions for launching an instance from a bootable volume in the OpenStack End User
Guide.
To boot normally from an image and attach block storage, map to a device other than vda. You can find
instructions for launching an instance and attaching a volume to the instance and for copying the image to the
attached volume in the OpenStack End User Guide.
Taking Snapshots
The OpenStack snapshot mechanism allows you to create new images from running instances. This is very
convenient for upgrading base images or for taking a published image and customizing it for local use. To
snapshot a running instance to an image using the CLI, do this:
$ openstack image create IMAGE_NAME --volume VOLUME_NAME_OR_ID
The dashboard interface for snapshots can be confusing because the snapshots and images are displayed in the
Images page. However, an instance snapshot is an image. The only difference between an image that you upload
directly to the Image Service and an image that you create by snapshot is that an image created by snapshot has
additional properties in the glance database. These properties are found in the image_properties table and
include:
Name Value
image_type snapshot
instance_uuid <uuid of instance that was snapshotted>
base_image_ref <uuid of original image of instance that was snapshotted>
image_location snapshot
Live Snapshots
Live snapshots is a feature that allows users to snapshot the running virtual machines without pausing them.
These snapshots are simply disk-only snapshots. Snapshotting an instance can now be performed with no
downtime (assuming QEMU 1.3+ and libvirt 1.0+ are used).
Note: If you use libvirt version 1.2.2, you may experience intermittent problems with live snapshot creation.
To effectively disable the libvirt live snapshotting, until the problem is resolved, add the below setting to
nova.conf.
[workarounds]
disable_libvirt_livesnapshot = True
Ensuring Snapshots of Linux Guests Are Consistent
The following section is from Sébastien Han’s “OpenStack: Perform Consistent Snapshots” blog entry.
78 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
A snapshot captures the state of the file system, but not the state of the memory. Therefore, to ensure your
snapshot contains the data that you want, before your snapshot you need to ensure that:
• Running programs have written their contents to disk
• The file system does not have any “dirty” buffers: where programs have issued the command to write to
disk, but the operating system has not yet done the write
To ensure that important services have written their contents to disk (such as databases), we recommend that
you read the documentation for those applications to determine what commands to issue to have them sync their
contents to disk. If you are unsure how to do this, the safest approach is to simply stop these running services
normally.
To deal with the “dirty” buffer issue, we recommend using the sync command before snapshotting:
# sync
Running sync writes dirty buffers (buffered blocks that have been modified but not written yet to the disk block)
to disk.
Just running sync is not enough to ensure that the file system is consistent. We recommend that you use the
fsfreeze tool, which halts new access to the file system, and create a stable image on disk that is suitable for
snapshotting. The fsfreeze tool supports several file systems, including ext3, ext4, and XFS. If your virtual
machine instance is running on Ubuntu, install the util-linux package to get fsfreeze:
Note: In the very common case where the underlying snapshot is done via LVM, the filesystem freeze is
automatically handled by LVM.
# apt-get install util-linux
If your operating system doesn’t have a version of fsfreeze available, you can use xfs_freeze instead, which
is available on Ubuntu in the xfsprogs package. Despite the “xfs” in the name, xfs_freeze also works on ext3
and ext4 if you are using a Linux kernel version 2.6.29 or greater, since it works at the virtual file system (VFS)
level starting at 2.6.29. The xfs_freeze version supports the same command-line arguments as fsfreeze.
Consider the example where you want to take a snapshot of a persistent block storage volume, detected by the
guest operating system as /dev/vdb and mounted on /mnt. The fsfreeze command accepts two arguments:
-f Freeze the system
-u Thaw (unfreeze) the system
To freeze the volume in preparation for snapshotting, you would do the following, as root, inside the instance:
# fsfreeze -f /mnt
You must mount the file system before you run the fsfreeze command.
When the fsfreeze -f command is issued, all ongoing transactions in the file system are allowed to complete,
new write system calls are halted, and other calls that modify the file system are halted. Most importantly, all
dirty data, metadata, and log information are written to disk.
Once the volume has been frozen, do not attempt to read from or write to the volume, as these operations hang.
The operating system stops every I/O operation and any I/O attempts are delayed until the file system has been
unfrozen.
User-Facing Operations 79
Operations Guide (Release Version: 15.0.0)
Once you have issued the fsfreeze command, it is safe to perform the snapshot. For example, if the volume
of your instance was named mon-volume and you wanted to snapshot it to an image named mon-snapshot,
you could now run the following:
$ openstack image create mon-snapshot --volume mon-volume
When the snapshot is done, you can thaw the file system with the following command, as root, inside of the
instance:
# fsfreeze -u /mnt
If you want to back up the root file system, you can’t simply run the preceding command because it will freeze
the prompt. Instead, run the following one-liner, as root, inside the instance:
# fsfreeze -f / && read x; fsfreeze -u /
After this command it is common practice to call openstack image create from your workstation, and once
done press enter in your instance shell to unfreeze it. Obviously you could automate this, but at least it will let
you properly synchronize.
Ensuring Snapshots of Windows Guests Are Consistent
Obtaining consistent snapshots of Windows VMs is conceptually similar to obtaining consistent snapshots of
Linux VMs, although it requires additional utilities to coordinate with a Windows-only subsystem designed to
facilitate consistent backups.
Windows XP and later releases include a Volume Shadow Copy Service (VSS) which provides a framework so
that compliant applications can be consistently backed up on a live filesystem. To use this framework, a VSS
requestor is run that signals to the VSS service that a consistent backup is needed. The VSS service notifies
compliant applications (called VSS writers) to quiesce their data activity. The VSS service then tells the copy
provider to create a snapshot. Once the snapshot has been made, the VSS service unfreezes VSS writers and
normal I/O activity resumes.
QEMU provides a guest agent that can be run in guests running on KVM hypervisors. This guest agent, on
Windows VMs, coordinates with the Windows VSS service to facilitate a workflow which ensures consistent
snapshots. This feature requires at least QEMU 1.7. The relevant guest agent commands are:
guest-file-flush Write out “dirty” buffers to disk, similar to the Linux sync operation.
guest-fsfreeze Suspend I/O to the disks, similar to the Linux fsfreeze -f operation.
guest-fsfreeze-thaw Resume I/O to the disks, similar to the Linux fsfreeze -u operation.
To obtain snapshots of a Windows VM these commands can be scripted in sequence: flush the filesystems,
freeze the filesystems, snapshot the filesystems, then unfreeze the filesystems. As with scripting similar workflows
against Linux VMs, care must be used when writing such a script to ensure error handling is thorough
and filesystems will not be left in a frozen state.
Instances in the Database
While instance information is stored in a number of database tables, the table you most likely need to look at
in relation to user instances is the instances table.
The instances table carries most of the information related to both running and deleted instances. It has a
bewildering array of fields; for an exhaustive list, look at the database. These are the most useful fields for
operators looking to form queries:
80 User-Facing Operations
Operations Guide (Release Version: 15.0.0)
• The deleted field is set to 1 if the instance has been deleted and NULL if it has not been deleted. This
field is important for excluding deleted instances from your queries.
• The uuid field is the UUID of the instance and is used throughout other tables in the database as a
foreign key. This ID is also reported in logs, the dashboard, and command-line tools to uniquely identify
an instance.
• A collection of foreign keys are available to find relations to the instance. The most useful of these —
user_id and project_id are the UUIDs of the user who launched the instance and the project it was
launched in.
• The host field tells which compute node is hosting the instance.
• The hostname field holds the name of the instance when it is launched. The display-name is initially
the same as hostname but can be reset using the nova rename command.
A number of time-related fields are useful for tracking when state changes happened on an instance:
• created_at
• updated_at
• deleted_at
• scheduled_at
• launched_at
• terminated_at
Good Luck!
This section was intended as a brief introduction to some of the most useful of many OpenStack commands.
For an exhaustive list, please refer to the OpenStack Administrator Guide. We hope your users remain happy
and recognize your hard work! (For more hard work, turn the page to the next chapter, where we discuss the
system-facing operations: maintenance, failures and debugging.)
Maintenance, Failures, and Debugging
Cloud Controller and Storage Proxy Failures and Maintenance
The cloud controller and storage proxy are very similar to each other when it comes to expected and unexpected
downtime. One of each server type typically runs in the cloud, which makes them very noticeable when they
are not running.
For the cloud controller, the good news is if your cloud is using the FlatDHCP multi-host HA network mode,
existing instances and volumes continue to operate while the cloud controller is offline. For the storage proxy,
however, no storage traffic is possible until it is back up and running.
Planned Maintenance
One way to plan for cloud controller or storage proxy maintenance is to simply do it off-hours, such as at 1 a.m.
or 2 a.m. This strategy affects fewer users. If your cloud controller or storage proxy is too important to have
unavailable at any point in time, you must look into high-availability options.
Maintenance, Failures, and Debugging 81
Operations Guide (Release Version: 15.0.0)
Rebooting a Cloud Controller or Storage Proxy
All in all, just issue the reboot command. The operating system cleanly shuts down services and then automatically
reboots. If you want to be very thorough, run your backup jobs just before you reboot.
After a cloud controller reboots, ensure that all required services were successfully started. The following
commands use ps and grep to determine if nova, glance, and keystone are currently running:
# ps aux | grep nova-
# ps aux | grep glance-
# ps aux | grep keystone
# ps aux | grep cinder
Also check that all services are functioning. The following set of commands sources the openrc file, then runs
some basic glance, nova, and openstack commands. If the commands work as expected, you can be confident
that those services are in working condition:
# . openrc
# openstack image list
# openstack server list
# openstack project list
For the storage proxy, ensure that the Object Storage service has resumed:
# ps aux | grep swift
Also check that it is functioning:
# swift stat
Total Cloud Controller Failure
The cloud controller could completely fail if, for example, its motherboard goes bad. Users will immediately
notice the loss of a cloud controller since it provides core functionality to your cloud environment. If your
infrastructure monitoring does not alert you that your cloud controller has failed, your users definitely will.
Unfortunately, this is a rough situation. The cloud controller is an integral part of your cloud. If you have only
one controller, you will have many missing services if it goes down.
To avoid this situation, create a highly available cloud controller cluster. This is outside the scope of this
document, but you can read more in the OpenStack High Availability Guide.
The next best approach is to use a configuration-management tool, such as Puppet, to automatically build a
cloud controller. This should not take more than 15 minutes if you have a spare server available. After the
controller rebuilds, restore any backups taken (see Backup and Recovery).
Also, in practice, the nova-compute services on the compute nodes do not always reconnect cleanly to rabbitmq
hosted on the controller when it comes back up after a long reboot; a restart on the nova services on the compute
nodes is required.
Compute Node Failures and Maintenance
Sometimes a compute node either crashes unexpectedly or requires a reboot for maintenance reasons.
82 Maintenance, Failures, and Debugging
Operations Guide (Release Version: 15.0.0)
Planned Maintenance
If you need to reboot a compute node due to planned maintenance, such as a software or hardware upgrade,
perform the following steps:
1. Disable scheduling of new VMs to the node, optionally providing a reason comment:
# nova service-disable --reason maintenance c01.example.com nova-compute
2. Verify that all hosted instances have been moved off the node:
• If your cloud is using a shared storage:
(a) Get a list of instances that need to be moved:
# openstack server list --host c01.example.com --all-projects
(b) Migrate all instances one by one:
# openstack server migrate <uuid> --live c02.example.com
• If your cloud is not using a shared storage, run:
# openstack server migrate <uuid> --live --block-migration c02.example.com
3. Stop the nova-compute service:
# stop nova-compute
If you use a configuration-management system, such as Puppet, that ensures the nova-compute service
is always running, you can temporarily move the init files:
# mkdir /root/tmp
# mv /etc/init/nova-compute.conf /root/tmp
# mv /etc/init.d/nova-compute /root/tmp
4. Shut down your compute node, perform the maintenance, and turn the node back on.
5. Start the nova-compute service:
# start nova-compute
You can re-enable the nova-compute service by undoing the commands:
# mv /root/tmp/nova-compute.conf /etc/init
# mv /root/tmp/nova-compute /etc/init.d/
6. Enable scheduling of VMs to the node:
# nova service-enable c01.example.com nova-compute
7. Optionally, migrate the instances back to their original compute node.
After a Compute Node Reboots
When you reboot a compute node, first verify that it booted successfully. This includes ensuring that the novacompute
service is running:
Maintenance, Failures, and Debugging 83
Operations Guide (Release Version: 15.0.0)
# ps aux | grep nova-compute
# status nova-compute
Also ensure that it has successfully connected to the AMQP server:
# grep AMQP /var/log/nova/nova-compute.log
2013-02-26 09:51:31 12427 INFO nova.openstack.common.rpc.common [-] Connected to AMQP
,→server on 199.116.232.36:5672
After the compute node is successfully running, you must deal with the instances that are hosted on that compute
node because none of them are running. Depending on your SLA with your users or customers, you might have
to start each instance and ensure that they start correctly.
Instances
You can create a list of instances that are hosted on the compute node by performing the following command:
# openstack server list --host c01.example.com --all-projects
After you have the list, you can use the openstack command to start each instance:
# openstack server reboot <server>
Note: Any time an instance shuts down unexpectedly, it might have problems on boot. For example, the
instance might require an fsck on the root partition. If this happens, the user can use the dashboard VNC
console to fix this.
If an instance does not boot, meaning virsh list never shows the instance as even attempting to boot, do the
following on the compute node:
# tail -f /var/log/nova/nova-compute.log
Try executing the openstack server reboot command again. You should see an error message about why
the instance was not able to boot.
In most cases, the error is the result of something in libvirt’s XML file (/etc/libvirt/qemu/instancexxxxxxxx.xml)
that no longer exists. You can enforce re-creation of the XML file as well as rebooting the
instance by running the following command:
# openstack server reboot --hard <server>
Inspecting and Recovering Data from Failed Instances
In some scenarios, instances are running but are inaccessible through SSH and do not respond to any command.
The VNC console could be displaying a boot failure or kernel panic error messages. This could be an indication
of file system corruption on the VM itself. If you need to recover files or inspect the content of the instance,
qemu-nbd can be used to mount the disk.
84 Maintenance, Failures, and Debugging
Operations Guide (Release Version: 15.0.0)
Warning: If you access or view the user’s content and data, get approval first!
To access the instance’s disk (/var/lib/nova/instances/instance-xxxxxx/disk), use the following
steps:
1. Suspend the instance using the virsh command.
2. Connect the qemu-nbd device to the disk.
3. Mount the qemu-nbd device.
4. Unmount the device after inspecting.
5. Disconnect the qemu-nbd device.
6. Resume the instance.
If you do not follow last three steps, OpenStack Compute cannot manage the instance any longer. It fails to
respond to any command issued by OpenStack Compute, and it is marked as shut down.
Once you mount the disk file, you should be able to access it and treat it as a collection of normal directories
with files and a directory structure. However, we do not recommend that you edit or touch any files because
this could change the access control lists (ACLs) that are used to determine which accounts can perform what
operations on files and directories. Changing ACLs can make the instance unbootable if it is not already.
1. Suspend the instance using the virsh command, taking note of the internal ID:
# virsh list
Id Name State
----------------------------------
1 instance-00000981 running
2 instance-000009f5 running
30 instance-0000274a running
# virsh suspend 30
Domain 30 suspended
2. Find the ID for each instance by listing the server IDs using the following command:
# openstack server list
+--------------------------------------+-------+---------+----------------------------
,→-+------------+
| ID | Name | Status | Networks
,→ | Image Name |
+--------------------------------------+-------+---------+----------------------------
,→-+------------+
| 2da14c5c-de6d-407d-a7d2-2dd0862b9967 | try3 | ACTIVE | finance-internal=10.10.0.4
,→ | |
| 223f4860-722a-44a0-bac7-f73f58beec7b | try2 | ACTIVE | finance-internal=10.10.0.
,→13 | |
+--------------------------------------+-------+---------+----------------------------
,→-+------------+
3. Connect the qemu-nbd device to the disk:
# cd /var/lib/nova/instances/instance-0000274a
# ls -lh
total 33M
Maintenance, Failures, and Debugging 85
Operations Guide (Release Version: 15.0.0)
-rw-rw---- 1 libvirt-qemu kvm 6.3K Oct 15 11:31 console.log
-rw-r--r-- 1 libvirt-qemu kvm 33M Oct 15 22:06 disk
-rw-r--r-- 1 libvirt-qemu kvm 384K Oct 15 22:06 disk.local
-rw-rw-r-- 1 nova nova 1.7K Oct 15 11:30 libvirt.xml
# qemu-nbd -c /dev/nbd0 `pwd`/disk
4. Mount the qemu-nbd device.
The qemu-nbd device tries to export the instance disk’s different partitions as separate devices. For
example, if vda is the disk and vda1 is the root partition, qemu-nbd exports the device as /dev/nbd0 and
/dev/nbd0p1, respectively:
# mount /dev/nbd0p1 /mnt/
You can now access the contents of /mnt, which correspond to the first partition of the instance’s disk.
To examine the secondary or ephemeral disk, use an alternate mount point if you want both primary and
secondary drives mounted at the same time:
# umount /mnt
# qemu-nbd -c /dev/nbd1 `pwd`/disk.local
# mount /dev/nbd1 /mnt/
# ls -lh /mnt/
total 76K
lrwxrwxrwx. 1 root root 7 Oct 15 00:44 bin -> usr/bin
dr-xr-xr-x. 4 root root 4.0K Oct 15 01:07 boot
drwxr-xr-x. 2 root root 4.0K Oct 15 00:42 dev
drwxr-xr-x. 70 root root 4.0K Oct 15 11:31 etc
drwxr-xr-x. 3 root root 4.0K Oct 15 01:07 home
lrwxrwxrwx. 1 root root 7 Oct 15 00:44 lib -> usr/lib
lrwxrwxrwx. 1 root root 9 Oct 15 00:44 lib64 -> usr/lib64
drwx------. 2 root root 16K Oct 15 00:42 lost+found
drwxr-xr-x. 2 root root 4.0K Feb 3 2012 media
drwxr-xr-x. 2 root root 4.0K Feb 3 2012 mnt
drwxr-xr-x. 2 root root 4.0K Feb 3 2012 opt
drwxr-xr-x. 2 root root 4.0K Oct 15 00:42 proc
dr-xr-x---. 3 root root 4.0K Oct 15 21:56 root
drwxr-xr-x. 14 root root 4.0K Oct 15 01:07 run
lrwxrwxrwx. 1 root root 8 Oct 15 00:44 sbin -> usr/sbin
drwxr-xr-x. 2 root root 4.0K Feb 3 2012 srv
drwxr-xr-x. 2 root root 4.0K Oct 15 00:42 sys
drwxrwxrwt. 9 root root 4.0K Oct 15 16:29 tmp
drwxr-xr-x. 13 root root 4.0K Oct 15 00:44 usr
drwxr-xr-x. 17 root root 4.0K Oct 15 00:44 var
5. Once you have completed the inspection, unmount the mount point and release the qemu-nbd device:
# umount /mnt
# qemu-nbd -d /dev/nbd0
/dev/nbd0 disconnected
6. Resume the instance using virsh:
# virsh list
Id Name State
----------------------------------
1 instance-00000981 running
86 Maintenance, Failures, and Debugging
Operations Guide (Release Version: 15.0.0)
2 instance-000009f5 running
30 instance-0000274a paused
# virsh resume 30
Domain 30 resumed
Managing floating IP addresses between instances
In an elastic cloud environment using the Public_AGILE network, each instance has a publicly accessible IPv4
& IPv6 address. It does not support the concept of OpenStack floating IP addresses that can easily be attached,
removed, and transferred between instances. However, there is a workaround using neutron ports which contain
the IPv4 & IPv6 address.
Create a port that can be reused
1. Create a port on the Public_AGILE network:
$ openstack port create port1 --network Public_AGILE
Created a new port:
+-----------------------+------------------------------------------------------+
| Field | Value |
+-----------------------+------------------------------------------------------+
| admin_state_up | UP |
| allowed_address_pairs | |
| binding_host_id | None |
| binding_profile | None |
| binding_vif_details | None |
| binding_vif_type | None |
| binding_vnic_type | normal |
| created_at | 2017-02-26T14:23:18Z |
| description | |
| device_id | |
| device_owner | |
| dns_assignment | None |
| dns_name | None |
| extra_dhcp_opts | |
| fixed_ips | ip_address='96.118.182.106', |
| | subnet_id='4279c70a-7218-4c7e-94e5-7bd4c045644e' |
| | ip_address='2001:558:fc0b:100:f816:3eff:fefb:45fb', |
| | subnet_id='11d8087b-6288-4129-95ff-42c3df0c1df0' |
| id | 3871bf29-e963-4701-a7dd-8888dbaab375 |
| ip_address | None |
| mac_address | fa:16:3e:e2:09:e0 |
| name | port1 |
| network_id | f41bd921-3a59-49c4-aa95-c2e4496a4b56 |
| option_name | None |
| option_value | None |
| port_security_enabled | True |
| project_id | 52f0574689f14c8a99e7ca22c4eb572 |
| qos_policy_id | None |
| revision_number | 6 |
| security_groups | 20d96891-0055-428a-8fa6-d5aed25f0dc6 |
| status | DOWN |
| subnet_id | None |
Maintenance, Failures, and Debugging 87
Operations Guide (Release Version: 15.0.0)
| updated_at | 2017-02-26T14:23:19Z |
+-----------------------+------------------------------------------------------+
2. If you know the fully qualified domain name (FQDN) that will be assigned to the IP address, assign the
port with the same name:
$ openstack port create "example-fqdn-01.sys.example.com" --network Public_AGILE
Created a new port:
+-----------------------+------------------------------------------------------+
| Field | Value |
+-----------------------+------------------------------------------------------+
| admin_state_up | UP |
| allowed_address_pairs | |
| binding_host_id | None |
| binding_profile | None |
| binding_vif_details | None |
| binding_vif_type | None |
| binding_vnic_type | normal |
| created_at | 2017-02-26T14:24:16Z |
| description | |
| device_id | |
| device_owner | |
| dns_assignment | None |
| dns_name | None |
| extra_dhcp_opts | |
| fixed_ips | ip_address='96.118.182.107', |
| | subnet_id='4279c70a-7218-4c7e-94e5-7bd4c045644e' |
| | ip_address='2001:558:fc0b:100:f816:3eff:fefb:65fc', |
| | subnet_id='11d8087b-6288-4129-95ff-42c3df0c1df0' |
| id | 731c3b28-3753-4e63-bae3-b58a52d6ccca |
| ip_address | None |
| mac_address | fa:16:3e:fb:65:fc |
| name | example-fqdn-01.sys.example.com |
| network_id | f41bd921-3a59-49c4-aa95-c2e4496a4b56 |
| option_name | None |
| option_value | None |
| port_security_enabled | True |
| project_id | 52f0574689f14c8a99e7ca22c4eb5720 |
| qos_policy_id | None |
| revision_number | 6 |
| security_groups | 20d96891-0055-428a-8fa6-d5aed25f0dc6 |
| status | DOWN |
| subnet_id | None |
| updated_at | 2017-02-26T14:24:17Z |
+-----------------------+------------------------------------------------------+
3. Use the port when creating an instance:
$ openstack server create --flavor m1.medium --image ubuntu.qcow2 \
--key-name team_key --nic port-id=PORT_ID \
"example-fqdn-01.sys.example.com"
4. Verify the instance has the correct IP address:
+--------------------------------------+----------------------------------------------
,→------------+
88 Maintenance, Failures, and Debugging
Operations Guide (Release Version: 15.0.0)
| Field | Value
,→ |
+--------------------------------------+----------------------------------------------
,→------------+
| OS-DCF:diskConfig | MANUAL
,→ |
| OS-EXT-AZ:availability_zone | nova
,→ |
| OS-EXT-SRV-ATTR:host | os_compute-1
,→ |
| OS-EXT-SRV-ATTR:hypervisor_hostname | os_compute.ece.example.com
,→ |
| OS-EXT-SRV-ATTR:instance_name | instance-00012b82
,→ |
| OS-EXT-STS:power_state | Running
,→ |
| OS-EXT-STS:task_state | None
,→ |
| OS-EXT-STS:vm_state | active
,→ |
| OS-SRV-USG:launched_at | 2016-11-30T08:55:27.000000
,→ |
| OS-SRV-USG:terminated_at | None
,→ |
| accessIPv4 |
,→ |
| accessIPv6 |
,→ |
| addresses | public=172.24.4.236
,→ |
| config_drive |
,→ |
| created | 2016-11-30T08:55:14Z
,→ |
| flavor | m1.medium (103)
,→ |
| hostId |
,→aca973d5b7981faaf8c713a0130713bbc1e64151be65c8dfb53039f7 |
| id | f91bd761-6407-46a6-b5fd-11a8a46e4983
,→ |
| image | Example Cloud Ubuntu 14.04 x86_64 v2.5
,→(fb49d7e1-273b-...|
| key_name | team_key
,→ |
| name | example-fqdn-01.sys.example.com
,→ |
| os-extended-volumes:volumes_attached | []
,→ |
| progress | 0
,→ |
| project_id | 2daf82a578e9437cab396c888ff0ca57
,→ |
| properties |
,→ |
| security_groups | [{u'name': u'default'}]
,→ |
| status | ACTIVE
,→ |
Maintenance, Failures, and Debugging 89
Operations Guide (Release Version: 15.0.0)
| updated | 2016-11-30T08:55:27Z
,→ |
| user_id | 8cbea24666ae49bbb8c1641f9b12d2d2
,→ |
+--------------------------------------+----------------------------------------------
,→------------+
5. Check the port connection using the netcat utility:
$ nc -v -w 2 96.118.182.107 22
Ncat: Version 7.00 ( https://nmap.org/ncat )
Ncat: Connected to 96.118.182.107:22.
SSH-2.0-OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.6
Detach a port from an instance
1. Find the port corresponding to the instance. For example:
$ openstack port list | grep -B1 96.118.182.107
| 731c3b28-3753-4e63-bae3-b58a52d6ccca | example-fqdn-01.sys.example.com |
,→fa:16:3e:fb:65:fc | ip_address='96.118.182.107', subnet_id='4279c70a-7218-4c7e-94e5-
,→7bd4c045644e' |
2. Run the openstack port set command to remove the port from the instance:
$ openstack port set 731c3b28-3753-4e63-bae3-b58a52d6ccca \
--device "" --device-owner "" --no-binding-profile
3. Delete the instance and create a new instance using the --nic port-id option.
Retrieve an IP address when an instance is deleted before detaching a port
The following procedure is a possible workaround to retrieve an IP address when an instance has been deleted
with the port still attached:
1. Launch several neutron ports:
$ for i in {0..10}; do openstack port create --network Public_AGILE \
ip-recovery; done
2. Check the ports for the lost IP address and update the name:
$ openstack port set 731c3b28-3753-4e63-bae3-b58a52d6ccca \
--name "don't delete"
3. Delete the ports that are not needed:
$ for port in $(openstack port list | grep -i ip-recovery | \
awk '{print $2}'); do openstack port delete $port; done
4. If you still cannot find the lost IP address, repeat these steps again.
Volumes
If the affected instances also had attached volumes, first generate a list of instance and volume UUIDs:
90 Maintenance, Failures, and Debugging
Operations Guide (Release Version: 15.0.0)
mysql> select nova.instances.uuid as instance_uuid,
cinder.volumes.id as volume_uuid, cinder.volumes.status,
cinder.volumes.attach_status, cinder.volumes.mountpoint,
cinder.volumes.display_name from cinder.volumes
inner join nova.instances on cinder.volumes.instance_uuid=nova.instances.uuid
where nova.instances.host = 'c01.example.com';
You should see a result similar to the following:
+--------------+------------+-------+--------------+-----------+--------------+
|instance_uuid |volume_uuid |status |attach_status |mountpoint | display_name |
+--------------+------------+-------+--------------+-----------+--------------+
|9b969a05 |1f0fbf36 |in-use |attached |/dev/vdc | test |
+--------------+------------+-------+--------------+-----------+--------------+
1 row in set (0.00 sec)
Next, manually detach and reattach the volumes, where X is the proper mount point:
# openstack server remove volume <instance_uuid> <volume_uuid>
# openstack server add volume <instance_uuid> <volume_uuid> --device /dev/vdX
Be sure that the instance has successfully booted and is at a login screen before doing the above.
Total Compute Node Failure
Compute nodes can fail the same way a cloud controller can fail. A motherboard failure or some other type
of hardware failure can cause an entire compute node to go offline. When this happens, all instances running
on that compute node will not be available. Just like with a cloud controller failure, if your infrastructure
monitoring does not detect a failed compute node, your users will notify you because of their lost instances.
If a compute node fails and won’t be fixed for a few hours (or at all), you can relaunch all instances that are
hosted on the failed node if you use shared storage for /var/lib/nova/instances.
To do this, generate a list of instance UUIDs that are hosted on the failed node by running the following query
on the nova database:
mysql> select uuid from instances
where host = 'c01.example.com' and deleted = 0;
Next, update the nova database to indicate that all instances that used to be hosted on c01.example.com are now
hosted on c02.example.com:
mysql> update instances set host = 'c02.example.com'
where host = 'c01.example.com' and deleted = 0;
If you’re using the Networking service ML2 plug-in, update the Networking service database to indicate that
all ports that used to be hosted on c01.example.com are now hosted on c02.example.com:
mysql> update ml2_port_bindings set host = 'c02.example.com'
where host = 'c01.example.com';
mysql> update ml2_port_binding_levels set host = 'c02.example.com'
where host = 'c01.example.com';
After that, use the openstack command to reboot all instances that were on c01.example.com while regenerating
their XML files at the same time:
Maintenance, Failures, and Debugging 91
Operations Guide (Release Version: 15.0.0)
# openstack server reboot --hard <server>
Finally, reattach volumes using the same method described in the section Volumes.
/var/lib/nova/instances
It’s worth mentioning this directory in the context of failed compute nodes. This directory contains the libvirt
KVM file-based disk images for the instances that are hosted on that compute node. If you are not running your
cloud in a shared storage environment, this directory is unique across all compute nodes.
/var/lib/nova/instances contains two types of directories.
The first is the _base directory. This contains all the cached base images from glance for each unique image
that has been launched on that compute node. Files ending in _20 (or a different number) are the ephemeral
base images.
The other directories are titled instance-xxxxxxxx. These directories correspond to instances running on
that compute node. The files inside are related to one of the files in the _base directory. They’re essentially
differential-based files containing only the changes made from the original _base directory.
All files and directories in /var/lib/nova/instances are uniquely named. The files in _base are uniquely
titled for the glance image that they are based on, and the directory names instance-xxxxxxxx are uniquely
titled for that particular instance. For example, if you copy all data from /var/lib/nova/instances on one
compute node to another, you do not overwrite any files or cause any damage to images that have the same
unique name, because they are essentially the same file.
Although this method is not documented or supported, you can use it when your compute node is permanently
offline but you have instances locally stored on it.
Storage Node Failures and Maintenance
Because of the high redundancy of Object Storage, dealing with object storage node issues is a lot easier than
dealing with compute node issues.
Rebooting a Storage Node
If a storage node requires a reboot, simply reboot it. Requests for data hosted on that node are redirected to
other copies while the server is rebooting.
Shutting Down a Storage Node
If you need to shut down a storage node for an extended period of time (one or more days), consider removing
the node from the storage ring. For example:
# swift-ring-builder account.builder remove <ip address of storage node>
# swift-ring-builder container.builder remove <ip address of storage node>
# swift-ring-builder object.builder remove <ip address of storage node>
# swift-ring-builder account.builder rebalance
# swift-ring-builder container.builder rebalance
# swift-ring-builder object.builder rebalance
92 Maintenance, Failures, and Debugging
Operations Guide (Release Version: 15.0.0)
Next, redistribute the ring files to the other nodes:
# for i in s01.example.com s02.example.com s03.example.com
> do
> scp *.ring.gz $i:/etc/swift
> done
These actions effectively take the storage node out of the storage cluster.
When the node is able to rejoin the cluster, just add it back to the ring. The exact syntax you use to add a
node to your swift cluster with swift-ring-builder heavily depends on the original options used when you
originally created your cluster. Please refer back to those commands.
Replacing a Swift Disk
If a hard drive fails in an Object Storage node, replacing it is relatively easy. This assumes that your Object
Storage environment is configured correctly, where the data that is stored on the failed drive is also replicated
to other drives in the Object Storage environment.
This example assumes that /dev/sdb has failed.
First, unmount the disk:
# umount /dev/sdb
Next, physically remove the disk from the server and replace it with a working disk.
Ensure that the operating system has recognized the new disk:
# dmesg | tail
You should see a message about /dev/sdb.
Because it is recommended to not use partitions on a swift disk, simply format the disk as a whole:
# mkfs.xfs /dev/sdb
Finally, mount the disk:
# mount -a
Swift should notice the new disk and that no data exists. It then begins replicating the data to the disk from the
other existing replicas.
Handling a Complete Failure
A common way of dealing with the recovery from a full system failure, such as a power outage of a data center,
is to assign each service a priority, and restore in order. Table. Example service restoration priority list shows
an example.
Maintenance, Failures, and Debugging 93
Operations Guide (Release Version: 15.0.0)
Table 6: Table. Example service restoration priority list
Priority Services
1 Internal network connectivity
2 Backing storage services
3 Public network connectivity for user virtual machines
4 nova-compute, cinder hosts
5 User virtual machines
10 Message queue and database services
15 Keystone services
20 cinder-scheduler
21 Image Catalog and Delivery services
22 nova-scheduler services
98 cinder-api
99 nova-api services
100 Dashboard node
Use this example priority list to ensure that user-affected services are restored as soon as possible, but not
before a stable environment is in place. Of course, despite being listed as a single-line item, each step requires
significant work. For example, just after starting the database, you should check its integrity, or, after starting
the nova services, you should verify that the hypervisor matches the database and fix any mismatches.
Configuration Management
Maintaining an OpenStack cloud requires that you manage multiple physical servers, and this number might
grow over time. Because managing nodes manually is error prone, we strongly recommend that you use a
configuration-management tool. These tools automate the process of ensuring that all your nodes are configured
properly and encourage you to maintain your configuration information (such as packages and configuration
options) in a version-controlled repository.
Note: Several configuration-management tools are available, and this guide does not recommend a specific
one. The most popular ones in the OpenStack community are:
• Puppet, with available OpenStack Puppet modules
• Ansible, with OpenStack Ansible
• Chef, with available OpenStack Chef recipes
Other newer configuration tools include Juju and Salt; and more mature configuration management tools include
CFEngine and Bcfg2.
Working with Hardware
As for your initial deployment, you should ensure that all hardware is appropriately burned in before adding it
to production. Run software that uses the hardware to its limits—maxing out RAM, CPU, disk, and network.
Many options are available, and normally double as benchmark software, so you also get a good idea of the
performance of your system.
94 Maintenance, Failures, and Debugging
Operations Guide (Release Version: 15.0.0)
Adding a Compute Node
If you find that you have reached or are reaching the capacity limit of your computing resources, you should
plan to add additional compute nodes. Adding more nodes is quite easy. The process for adding compute nodes
is the same as when the initial compute nodes were deployed to your cloud: use an automated deployment
system to bootstrap the bare-metal server with the operating system and then have a configuration-management
system install and configure OpenStack Compute. Once the Compute service has been installed and configured
in the same way as the other compute nodes, it automatically attaches itself to the cloud. The cloud controller
notices the new node(s) and begins scheduling instances to launch there.
If your OpenStack Block Storage nodes are separate from your compute nodes, the same procedure still applies
because the same queuing and polling system is used in both services.
We recommend that you use the same hardware for new compute and block storage nodes. At the very least,
ensure that the CPUs are similar in the compute nodes to not break live migration.
Adding an Object Storage Node
Adding a new object storage node is different from adding compute or block storage nodes. You still want
to initially configure the server by using your automated deployment and configuration-management systems.
After that is done, you need to add the local disks of the object storage node into the object storage ring. The
exact command to do this is the same command that was used to add the initial disks to the ring. Simply rerun
this command on the object storage proxy server for all disks on the new object storage node. Once this has
been done, rebalance the ring and copy the resulting ring files to the other storage nodes.
Note: If your new object storage node has a different number of disks than the original nodes have, the command
to add the new node is different from the original commands. These parameters vary from environment
to environment.
Replacing Components
Failures of hardware are common in large-scale deployments such as an infrastructure cloud. Consider your
processes and balance time saving against availability. For example, an Object Storage cluster can easily live
with dead disks in it for some period of time if it has sufficient capacity. Or, if your compute installation is not
full, you could consider live migrating instances off a host with a RAM failure until you have time to deal with
the problem.
Databases
Almost all OpenStack components have an underlying database to store persistent information. Usually this
database is MySQL. Normal MySQL administration is applicable to these databases. OpenStack does not configure
the databases out of the ordinary. Basic administration includes performance tweaking, high availability,
backup, recovery, and repairing. For more information, see a standard MySQL administration guide.
You can perform a couple of tricks with the database to either more quickly retrieve information or fix a data
inconsistency error—for example, an instance was terminated, but the status was not updated in the database.
These tricks are discussed throughout this book.
Maintenance, Failures, and Debugging 95
Operations Guide (Release Version: 15.0.0)
Database Connectivity
Review the component’s configuration file to see how each OpenStack component accesses its corresponding
database. Look for a connection option. The following command uses grep to display the SQL connection
string for nova, glance, cinder, and keystone:
# grep -hE "connection ?=" \
/etc/nova/nova.conf /etc/glance/glance-*.conf \
/etc/cinder/cinder.conf /etc/keystone/keystone.conf \
/etc/neutron/neutron.conf
connection = mysql+pymysql://nova:password@cloud.example.com/nova
connection = mysql+pymysql://glance:password@cloud.example.com/glance
connection = mysql+pymysql://glance:password@cloud.example.com/glance
connection = mysql+pymysql://cinder:password@cloud.example.com/cinder
connection = mysql+pymysql://keystone:password@cloud.example.com/keystone
connection = mysql+pymysql://neutron:password@cloud.example.com/neutron
The connection strings take this format:
mysql+pymysql:// <username> : <password> @ <hostname> / <database name>
Performance and Optimizing
As your cloud grows, MySQL is utilized more and more. If you suspect that MySQL might be becoming
a bottleneck, you should start researching MySQL optimization. The MySQL manual has an entire section
dedicated to this topic: Optimization Overview.
RabbitMQ troubleshooting
This section provides tips on resolving common RabbitMQ issues.
RabbitMQ service hangs
It is quite common for the RabbitMQ service to hang when it is restarted or stopped. Therefore, it is highly
recommended that you manually restart RabbitMQ on each controller node.
Note: The RabbitMQ service name may vary depending on your operating system or vendor who supplies
your RabbitMQ service.
1. Restart the RabbitMQ service on the first controller node. The service rabbitmq-server restart
command may not work in certain situations, so it is best to use:
# service rabbitmq-server stop
# service rabbitmq-server start
2. If the service refuses to stop, then run the pkill command to stop the service, then restart the service:
# pkill -KILL -u rabbitmq
# service rabbitmq-server start
96 Maintenance, Failures, and Debugging
Operations Guide (Release Version: 15.0.0)
3. Verify RabbitMQ processes are running:
# ps -ef | grep rabbitmq
# rabbitmqctl list_queues
# rabbitmqctl list_queues 2>&1 | grep -i error
4. If there are errors, run the cluster_status command to make sure there are no partitions:
# rabbitmqctl cluster_status
For more information, see RabbitMQ documentation.
5. Go back to the first step and try restarting the RabbitMQ service again. If you still have errors, remove the
contents in the /var/lib/rabbitmq/mnesia/ directory between stopping and starting the RabbitMQ
service.
6. If there are no errors, restart the RabbitMQ service on the next controller node.
Since the Liberty release, OpenStack services will automatically recover from a RabbitMQ outage. You should
only consider restarting OpenStack services after checking if RabbitMQ heartbeat functionality is enabled, and
if OpenStack services are not picking up messages from RabbitMQ queues.
RabbitMQ alerts
If you receive alerts for RabbitMQ, take the following steps to troubleshoot and resolve the issue:
1. Determine which servers the RabbitMQ alarms are coming from.
2. Attempt to boot a nova instance in the affected environment.
3. If you cannot launch an instance, continue to troubleshoot the issue.
4. Log in to each of the controller nodes for the affected environment, and check the /var/log/rabbitmq
log files for any reported issues.
5. Look for connection issues identified in the log files.
6. For each controller node in your environment, view the /etc/init.d directory to check it contains
nova*, cinder*, neutron*, or glance*. Also check RabbitMQ message queues that are growing without
being consumed which will indicate which OpenStack service is affected. Restart the affected OpenStack
service.
7. For each compute node your environment, view the /etc/init.d directory and check if it contains
nova*, cinder*, neutron*, or glance*, Also check RabbitMQ message queues that are growing without
being consumed which will indicate which OpenStack services are affected. Restart the affected OpenStack
services.
8. Open OpenStack Dashboard and launch an instance. If the instance launches, the issue is resolved.
9. If you cannot launch an instance, check the /var/log/rabbitmq log files for reported connection issues.
10. Restart the RabbitMQ service on all of the controller nodes:
# service rabbitmq-server stop
# service rabbitmq-server start
Maintenance, Failures, and Debugging 97
Operations Guide (Release Version: 15.0.0)
Note: This step applies if you have already restarted only the OpenStack components, and cannot
connect to the RabbitMQ service.
11. Repeat steps 7-8.
Excessive database management memory consumption
Since the Liberty release, OpenStack with RabbitMQ 3.4.x or 3.6.x has an issue with the management database
consuming the memory allocated to RabbitMQ. This is caused by statistics collection and processing. When
a single node with RabbitMQ reaches its memory threshold, all exchange and queue processing is halted until
the memory alarm recovers.
To address this issue:
1. Check memory consumption:
# rabbitmqctl status
2. Edit the /etc/rabbitmq/rabbitmq.config configuration file, and change the collect_statistics_interval
parameter between 30000-60000 milliseconds. Alternatively you
can turn off statistics collection by setting collect_statistics parameter to “none”.
File descriptor limits when scaling a cloud environment
A cloud environment that is scaled to a certain size will require the file descriptor limits to be adjusted.
Run the rabbitmqctl status to view the current file descriptor limits:
"{file_descriptors,
[{total_limit,3996},
{total_used,135},
{sockets_limit,3594},
{sockets_used,133}]},"
Adjust the appropriate limits in the /etc/security/limits.conf configuration file.
HDWMY
Here’s a quick list of various to-do items for each hour, day, week, month, and year. Please note that these tasks
are neither required nor definitive but helpful ideas:
Hourly
• Check your monitoring system for alerts and act on them.
• Check your ticket queue for new tickets.
98 Maintenance, Failures, and Debugging
Operations Guide (Release Version: 15.0.0)
Daily
• Check for instances in a failed or weird state and investigate why.
• Check for security patches and apply them as needed.
Weekly
• Check cloud usage:
– User quotas
– Disk space
– Image usage
– Large instances
– Network usage (bandwidth and IP usage)
• Verify your alert mechanisms are still working.
Monthly
• Check usage and trends over the past month.
• Check for user accounts that should be removed.
• Check for operator accounts that should be removed.
Quarterly
• Review usage and trends over the past quarter.
• Prepare any quarterly reports on usage and statistics.
• Review and plan any necessary cloud additions.
• Review and plan any major OpenStack upgrades.
Semiannually
• Upgrade OpenStack.
• Clean up after an OpenStack upgrade (any unused or new services to be aware of?).
Determining Which Component Is Broken
OpenStack’s collection of different components interact with each other strongly. For example, uploading
an image requires interaction from nova-api, glance-api, glance-registry, keystone, and potentially
swift-proxy. As a result, it is sometimes difficult to determine exactly where problems lie. Assisting in this
is the purpose of this section.
Maintenance, Failures, and Debugging 99
Operations Guide (Release Version: 15.0.0)
Tailing Logs
The first place to look is the log file related to the command you are trying to run. For example, if openstack
server list is failing, try tailing a nova log file and running the command again:
Terminal 1:
# tail -f /var/log/nova/nova-api.log
Terminal 2:
# openstack server list
Look for any errors or traces in the log file. For more information, see Logging and Monitoring.
If the error indicates that the problem is with another component, switch to tailing that component’s log file.
For example, if nova cannot access glance, look at the glance-api log:
Terminal 1:
# tail -f /var/log/glance/api.log
Terminal 2:
# openstack server list
Wash, rinse, and repeat until you find the core cause of the problem.
Running Daemons on the CLI
Unfortunately, sometimes the error is not apparent from the log files. In this case, switch tactics and use a
different command; maybe run the service directly on the command line. For example, if the glance-api
service refuses to start and stay running, try launching the daemon from the command line:
# sudo -u glance -H glance-api
This might print the error and cause of the problem.
Note: The -H flag is required when running the daemons with sudo because some daemons will write files
relative to the user’s home directory, and this write may fail if -H is left off.
Tip: Example of Complexity
One morning, a compute node failed to run any instances. The log files were a bit vague, claiming that a certain
instance was unable to be started. This ended up being a red herring because the instance was simply the first
instance in alphabetical order, so it was the first instance that nova-compute would touch.
Further troubleshooting showed that libvirt was not running at all. This made more sense. If libvirt wasn’t
running, then no instance could be virtualized through KVM. Upon trying to start libvirt, it would silently die
immediately. The libvirt logs did not explain why.
Next, the libvirtd daemon was run on the command line. Finally a helpful error message: it could not
connect to d-bus. As ridiculous as it sounds, libvirt, and thus nova-compute, relies on d-bus and somehow
100 Maintenance, Failures, and Debugging
Operations Guide (Release Version: 15.0.0)
d-bus crashed. Simply starting d-bus set the entire chain back on track, and soon everything was back up and
running.
What to do when things are running slowly
When you are getting slow responses from various services, it can be hard to know where to start looking. The
first thing to check is the extent of the slowness: is it specific to a single service, or varied among different
services? If your problem is isolated to a specific service, it can temporarily be fixed by restarting the service,
but that is often only a fix for the symptom and not the actual problem.
This is a collection of ideas from experienced operators on common things to look at that may be the cause of
slowness. It is not, however, designed to be an exhaustive list.
OpenStack Identity service
If OpenStack Identity service is responding slowly, it could be due to the token table getting large. This can be
fixed by running the keystone-manage token_flush command.
Additionally, for Identity-related issues, try the tips in SQL back end.
OpenStack Image service
OpenStack Image service can be slowed down by things related to the Identity service, but the Image service
itself can be slowed down if connectivity to the back-end storage in use is slow or otherwise problematic. For
example, your back-end NFS server might have gone down.
OpenStack Block Storage service
OpenStack Block Storage service is similar to the Image service, so start by checking Identity-related services,
and the back-end storage. Additionally, both the Block Storage and Image services rely on AMQP and SQL
functionality, so consider these when debugging.
OpenStack Compute service
Services related to OpenStack Compute are normally fairly fast and rely on a couple of backend services:
Identity for authentication and authorization), and AMQP for interoperability. Any slowness related to services
is normally related to one of these. Also, as with all other services, SQL is used extensively.
OpenStack Networking service
Slowness in the OpenStack Networking service can be caused by services that it relies upon, but it can also
be related to either physical or virtual networking. For example: network namespaces that do not exist or
are not tied to interfaces correctly; DHCP daemons that have hung or are not running; a cable being physically
disconnected; a switch not being configured correctly. When debugging Networking service problems, begin by
verifying all physical networking functionality (switch configuration, physical cabling, etc.). After the physical
networking is verified, check to be sure all of the Networking services are running (neutron-server, neutrondhcp-agent,
etc.), then check on AMQP and SQL back ends.
Maintenance, Failures, and Debugging 101
Operations Guide (Release Version: 15.0.0)
AMQP broker
Regardless of which AMQP broker you use, such as RabbitMQ, there are common issues which not only slow
down operations, but can also cause real problems. Sometimes messages queued for services stay on the queues
and are not consumed. This can be due to dead or stagnant services and can be commonly cleared up by either
restarting the AMQP-related services or the OpenStack service in question.
SQL back end
Whether you use SQLite or an RDBMS (such as MySQL), SQL interoperability is essential to a functioning
OpenStack environment. A large or fragmented SQLite file can cause slowness when using files as a back end.
A locked or long-running query can cause delays for most RDBMS services. In this case, do not kill the query
immediately, but look into it to see if it is a problem with something that is hung, or something that is just taking
a long time to run and needs to finish on its own. The administration of an RDBMS is outside the scope of this
document, but it should be noted that a properly functioning RDBMS is essential to most OpenStack services.
Uninstalling
While we’d always recommend using your automated deployment system to reinstall systems from scratch,
sometimes you do need to remove OpenStack from a system the hard way. Here’s how:
• Remove all packages.
• Remove remaining files.
• Remove databases.
These steps depend on your underlying distribution, but in general you should be looking for purge commands
in your package manager, like aptitude purge ~c $package. Following this, you can look for orphaned
files in the directories referenced throughout this guide. To uninstall the database properly, refer to the manual
appropriate for the product in use.
Downtime, whether planned or unscheduled, is a certainty when running a cloud. This chapter aims to provide
useful information for dealing proactively, or reactively, with these occurrences.
Network Troubleshooting
Network troubleshooting can be challenging. A network issue may cause problems at any point in the cloud.
Using a logical troubleshooting procedure can help mitigate the issue and isolate where the network issue is.
This chapter aims to give you the information you need to identify any issues for nova-network or OpenStack
Networking (neutron) with Linux Bridge or Open vSwitch.
Using ip a to Check Interface States
On compute nodes and nodes running nova-network, use the following command to see information about
interfaces, including information about IPs, VLANs, and whether your interfaces are up:
# ip a
If you are encountering any sort of networking difficulty, one good initial troubleshooting step is to make sure
that your interfaces are up. For example:
102 Network Troubleshooting
Operations Guide (Release Version: 15.0.0)
$ ip a | grep state
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP
qlen 1000
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast
master br100 state UP qlen 1000
4: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN
5: br100: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP
You can safely ignore the state of virbr0, which is a default bridge created by libvirt and not used by OpenStack.
Visualizing nova-network Traffic in the Cloud
If you are logged in to an instance and ping an external host, for example, Google, the ping packet takes the
route shown in Figure. Traffic route for ping packet.
Fig. 2: Figure. Traffic route for ping packet
1. The instance generates a packet and places it on the virtual Network Interface Card (NIC) inside the
instance, such as eth0.
2. The packet transfers to the virtual NIC of the compute host, such as, vnet1. You can find out what vnet
NIC is being used by looking at the /etc/libvirt/qemu/instance-xxxxxxxx.xml file.
3. From the vnet NIC, the packet transfers to a bridge on the compute node, such as br100.
If you run FlatDHCPManager, one bridge is on the compute node. If you run VlanManager, one bridge
exists for each VLAN.
To see which bridge the packet will use, run the command:
$ brctl show
Look for the vnet NIC. You can also reference nova.conf and look for the flat_interface_bridge
option.
Network Troubleshooting 103
Operations Guide (Release Version: 15.0.0)
4. The packet transfers to the main NIC of the compute node. You can also see this NIC in the brctl
output, or you can find it by referencing the flat_interface option in nova.conf.
5. After the packet is on this NIC, it transfers to the compute node’s default gateway. The packet is now
most likely out of your control at this point. The diagram depicts an external gateway. However, in the
default configuration with multi-host, the compute host is the gateway.
Reverse the direction to see the path of a ping reply. From this path, you can see that a single packet travels
across four different NICs. If a problem occurs with any of these NICs, a network issue occurs.
Visualizing OpenStack Networking Service Traffic in the Cloud
OpenStack Networking has many more degrees of freedom than nova-network does because of its pluggable
back end. It can be configured with open source or vendor proprietary plug-ins that control software defined
networking (SDN) hardware or plug-ins that use Linux native facilities on your hosts, such as Open vSwitch or
Linux Bridge.
The networking chapter of the OpenStack Administrator Guide shows a variety of networking scenarios and
their connection paths. The purpose of this section is to give you the tools to troubleshoot the various components
involved however they are plumbed together in your environment.
For this example, we will use the Open vSwitch (OVS) back end. Other back-end plug-ins will have very different
flow paths. OVS is the most popularly deployed network driver, according to the April 2016 OpenStack
User Survey. We’ll describe each step in turn, with Figure. Neutron network paths for reference.
1. The instance generates a packet and places it on the virtual NIC inside the instance, such as eth0.
2. The packet transfers to a Test Access Point (TAP) device on the compute host, such as tap690466bc-92.
You can find out what TAP is being used by looking at the /etc/libvirt/qemu/instance-xxxxxxxx.
xml file.
The TAP device name is constructed using the first 11 characters of the port ID (10 hex digits plus an
included ‘-‘), so another means of finding the device name is to use the neutron command. This returns
a pipe-delimited list, the first item of which is the port ID. For example, to get the port ID associated
with IP address 10.0.0.10, do this:
# openstack port list | grep 10.0.0.10 | cut -d \| -f 2
ff387e54-9e54-442b-94a3-aa4481764f1d
Taking the first 11 characters, we can construct a device name of tapff387e54-9e from this output.
104 Network Troubleshooting
Operations Guide (Release Version: 15.0.0)
Fig. 3: Figure. Neutron network paths
3. The TAP device is connected to the integration bridge, br-int. This bridge connects all the instance
TAP devices and any other bridges on the system. In this example, we have int-br-eth1 and patchtun.
int-br-eth1 is one half of a veth pair connecting to the bridge br-eth1, which handles VLAN
networks trunked over the physical Ethernet device eth1. patch-tun is an Open vSwitch internal port
that connects to the br-tun bridge for GRE networks.
The TAP devices and veth devices are normal Linux network devices and may be inspected with the
usual tools, such as ip and tcpdump. Open vSwitch internal devices, such as patch-tun, are only
visible within the Open vSwitch environment. If you try to run tcpdump -i patch-tun, it will raise
an error, saying that the device does not exist.
It is possible to watch packets on internal interfaces, but it does take a little bit of networking gymnastics.
First you need to create a dummy network device that normal Linux tools can see. Then you need to add
it to the bridge containing the internal interface you want to snoop on. Finally, you need to tell Open
vSwitch to mirror all traffic to or from the internal port onto this dummy port. After all this, you can then
run tcpdump on the dummy interface and see the traffic on the internal port.
Network Troubleshooting 105
Operations Guide (Release Version: 15.0.0)
To capture packets from the patch-tun internal interface on integration bridge, br-int:
(a) Create and bring up a dummy interface, snooper0:
# ip link add name snooper0 type dummy
# ip link set dev snooper0 up
(b) Add device snooper0 to bridge br-int:
# ovs-vsctl add-port br-int snooper0
(c) Create mirror of patch-tun to snooper0 (returns UUID of mirror port):
# ovs-vsctl -- set Bridge br-int mirrors=@m -- --id=@snooper0 \
get Port snooper0 -- --id=@patch-tun get Port patch-tun \
-- --id=@m create Mirror name=mymirror select-dst-port=@patch-tun \
select-src-port=@patch-tun output-port=@snooper0 select_all=1
(d) Profit. You can now see traffic on patch-tun by running tcpdump -i snooper0.
(e) Clean up by clearing all mirrors on br-int and deleting the dummy interface:
# ovs-vsctl clear Bridge br-int mirrors
# ovs-vsctl del-port br-int snooper0
# ip link delete dev snooper0
On the integration bridge, networks are distinguished using internal VLANs regardless of how the networking
service defines them. This allows instances on the same host to communicate directly without
transiting the rest of the virtual, or physical, network. These internal VLAN IDs are based on the order
they are created on the node and may vary between nodes. These IDs are in no way related to the
segmentation IDs used in the network definition and on the physical wire.
VLAN tags are translated between the external tag defined in the network settings, and internal tags in
several places. On the br-int, incoming packets from the int-br-eth1 are translated from external
tags to internal tags. Other translations also happen on the other bridges and will be discussed in those
sections.
To discover which internal VLAN tag is in use for a given external VLAN by using the ovs-ofctl
command
(a) Find the external VLAN tag of the network you’re interested in. This is the
provider:segmentation_id as returned by the networking service:
# neutron net-show --fields provider:segmentation_id <network name>
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| provider:network_type | vlan |
| provider:segmentation_id | 2113 |
+---------------------------+--------------------------------------+
(b) Grep for the provider:segmentation_id, 2113 in this case, in the output of ovs-ofctl dumpflows
br-int:
# ovs-ofctl dump-flows br-int | grep vlan=2113
cookie=0x0, duration=173615.481s, table=0, n_packets=7676140,
n_bytes=444818637, idle_age=0, hard_age=65534, priority=3,
in_port=1,dl_vlan=2113 actions=mod_vlan_vid:7,NORMAL
106 Network Troubleshooting
Operations Guide (Release Version: 15.0.0)
Here you can see packets received on port ID 1 with the VLAN tag 2113 are modified to have the
internal VLAN tag 7. Digging a little deeper, you can confirm that port 1 is in fact int-br-eth1:
# ovs-ofctl show br-int
OFPT_FEATURES_REPLY (xid=0x2): dpid:000022bc45e1914b
n_tables:254, n_buffers:256
capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS
ARP_MATCH_IP
actions: OUTPUT SET_VLAN_VID SET_VLAN_PCP STRIP_VLAN SET_DL_SRC
SET_DL_DST SET_NW_SRC SET_NW_DST SET_NW_TOS SET_TP_SRC
SET_TP_DST ENQUEUE
1(int-br-eth1): addr:c2:72:74:7f:86:08
config: 0
state: 0
current: 10GB-FD COPPER
speed: 10000 Mbps now, 0 Mbps max
2(patch-tun): addr:fa:24:73:75:ad:cd
config: 0
state: 0
speed: 0 Mbps now, 0 Mbps max
3(tap9be586e6-79): addr:fe:16:3e:e6:98:56
config: 0
state: 0
current: 10MB-FD COPPER
speed: 10 Mbps now, 0 Mbps max
LOCAL(br-int): addr:22:bc:45:e1:91:4b
config: 0
state: 0
speed: 0 Mbps now, 0 Mbps max
OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0
4. The next step depends on whether the virtual network is configured to use 802.1q VLAN tags or GRE:
(a) VLAN-based networks exit the integration bridge via veth interface int-br-eth1 and arrive on
the bridge br-eth1 on the other member of the veth pair phy-br-eth1. Packets on this interface
arrive with internal VLAN tags and are translated to external tags in the reverse of the process
described above:
# ovs-ofctl dump-flows br-eth1 | grep 2113
cookie=0x0, duration=184168.225s, table=0, n_packets=0, n_bytes=0,
idle_age=65534, hard_age=65534, priority=4,in_port=1,dl_vlan=7
actions=mod_vlan_vid:2113,NORMAL
Packets, now tagged with the external VLAN tag, then exit onto the physical network via eth1.
The Layer2 switch this interface is connected to must be configured to accept traffic with the VLAN
ID used. The next hop for this packet must also be on the same layer-2 network.
(b) GRE-based networks are passed with patch-tun to the tunnel bridge br-tun on interface patchint.
This bridge also contains one port for each GRE tunnel peer, so one for each compute node
and network node in your network. The ports are named sequentially from gre-1 onward.
Matching gre-<n> interfaces to tunnel endpoints is possible by looking at the Open vSwitch state:
# ovs-vsctl show | grep -A 3 -e Port\ \"grePort
"gre-1"
Interface "gre-1"
Network Troubleshooting 107
Operations Guide (Release Version: 15.0.0)
type: gre
options: {in_key=flow, local_ip="10.10.128.21",
out_key=flow, remote_ip="10.10.128.16"}
In this case, gre-1 is a tunnel from IP 10.10.128.21, which should match a local interface on this
node, to IP 10.10.128.16 on the remote side.
These tunnels use the regular routing tables on the host to route the resulting GRE packet, so
there is no requirement that GRE endpoints are all on the same layer-2 network, unlike VLAN
encapsulation.
All interfaces on the br-tun are internal to Open vSwitch. To monitor traffic on them, you need
to set up a mirror port as described above for patch-tun in the br-int bridge.
All translation of GRE tunnels to and from internal VLANs happens on this bridge.
To discover which internal VLAN tag is in use for a GRE tunnel by using the ovs-ofctl command
(a) Find the provider:segmentation_id of the network you’re interested in. This is the same field
used for the VLAN ID in VLAN-based networks:
# neutron net-show --fields provider:segmentation_id <network name>
+--------------------------+-------+
| Field | Value |
+--------------------------+-------+
| provider:network_type | gre |
| provider:segmentation_id | 3 |
+--------------------------+-------+
(b) Grep for 0x<provider:segmentation_id>, 0x3 in this case, in the output of ovs-ofctl dumpflows
br-tun:
# ovs-ofctl dump-flows br-tun|grep 0x3
cookie=0x0, duration=380575.724s, table=2, n_packets=1800,
n_bytes=286104, priority=1,tun_id=0x3
actions=mod_vlan_vid:1,resubmit(,10)
cookie=0x0, duration=715.529s, table=20, n_packets=5,
n_bytes=830, hard_timeout=300,priority=1,
vlan_tci=0x0001/0x0fff,dl_dst=fa:16:3e:a6:48:24
actions=load:0->NXM_OF_VLAN_TCI[],
load:0x3->NXM_NX_TUN_ID[],output:53
cookie=0x0, duration=193729.242s, table=21, n_packets=58761,
n_bytes=2618498, dl_vlan=1 actions=strip_vlan,set_tunnel:0x3,
output:4,output:58,output:56,output:11,output:12,output:47,
output:13,output:48,output:49,output:44,output:43,output:45,
output:46,output:30,output:31,output:29,output:28,output:26,
output:27,output:24,output:25,output:32,output:19,output:21,
output:59,output:60,output:57,output:6,output:5,output:20,
output:18,output:17,output:16,output:15,output:14,output:7,
output:9,output:8,output:53,output:10,output:3,output:2,
output:38,output:37,output:39,output:40,output:34,output:23,
output:36,output:35,output:22,output:42,output:41,output:54,
output:52,output:51,output:50,output:55,output:33
Here, you see three flows related to this GRE tunnel. The first is the translation from inbound
packets with this tunnel ID to internal VLAN ID 1. The second shows a unicast flow to output port
53 for packets destined for MAC address fa:16:3e:a6:48:24. The third shows the translation from
108 Network Troubleshooting
Operations Guide (Release Version: 15.0.0)
the internal VLAN representation to the GRE tunnel ID flooded to all output ports. For further
details of the flow descriptions, see the man page for ovs-ofctl. As in the previous VLAN
example, numeric port IDs can be matched with their named representations by examining the
output of ovs-ofctl show br-tun.
5. The packet is then received on the network node. Note that any traffic to the l3-agent or dhcp-agent will
be visible only within their network namespace. Watching any interfaces outside those namespaces, even
those that carry the network traffic, will only show broadcast packets like Address Resolution Protocols
(ARPs), but unicast traffic to the router or DHCP address will not be seen. See Dealing with Network
Namespaces for detail on how to run commands within these namespaces.
Alternatively, it is possible to configure VLAN-based networks to use external routers rather than the
l3-agent shown here, so long as the external router is on the same VLAN:
(a) VLAN-based networks are received as tagged packets on a physical network interface, eth1 in
this example. Just as on the compute node, this interface is a member of the br-eth1 bridge.
(b) GRE-based networks will be passed to the tunnel bridge br-tun, which behaves just like the GRE
interfaces on the compute node.
6. Next, the packets from either input go through the integration bridge, again just as on the compute node.
7. The packet then makes it to the l3-agent. This is actually another TAP device within the router’s network
namespace. Router namespaces are named in the form qrouter-<router-uuid>. Running ip a within
the namespace will show the TAP device name, qr-e6256f7d-31 in this example:
# ip netns exec qrouter-e521f9d0-a1bd-4ff4-bc81-78a60dd88fe5 ip a | grep state
10: qr-e6256f7d-31: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue
state UNKNOWN
11: qg-35916e1f-36: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500
qdisc pfifo_fast state UNKNOWN qlen 500
28: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN
8. The qg-<n> interface in the l3-agent router namespace sends the packet on to its next hop through device
eth2 on the external bridge br-ex. This bridge is constructed similarly to br-eth1 and may be inspected
in the same way.
9. This external bridge also includes a physical network interface, eth2 in this example, which finally lands
the packet on the external network destined for an external router or destination.
10. DHCP agents running on OpenStack networks run in namespaces similar to the l3-agents. DHCP namespaces
are named qdhcp-<uuid> and have a TAP device on the integration bridge. Debugging of DHCP
issues usually involves working inside this network namespace.
Finding a Failure in the Path
Use ping to quickly find where a failure exists in the network path. In an instance, first see whether you can
ping an external host, such as google.com. If you can, then there shouldn’t be a network problem at all.
If you can’t, try pinging the IP address of the compute node where the instance is hosted. If you can ping this
IP, then the problem is somewhere between the compute node and that compute node’s gateway.
If you can’t ping the IP address of the compute node, the problem is between the instance and the compute
node. This includes the bridge connecting the compute node’s main NIC with the vnet NIC of the instance.
One last test is to launch a second instance and see whether the two instances can ping each other. If they can,
the issue might be related to the firewall on the compute node.
Network Troubleshooting 109
Operations Guide (Release Version: 15.0.0)
tcpdump
One great, although very in-depth, way of troubleshooting network issues is to use tcpdump. We recommended
using tcpdump at several points along the network path to correlate where a problem might be. If you prefer
working with a GUI, either live or by using a tcpdump capture, check out Wireshark.
For example, run the following command:
# tcpdump -i any -n -v 'icmp[icmptype] = icmp-echoreply or icmp[icmptype] = icmp-echo'
Run this on the command line of the following areas:
1. An external server outside of the cloud
2. A compute node
3. An instance running on that compute node
In this example, these locations have the following IP addresses:
Instance
10.0.2.24
203.0.113.30
Compute Node
10.0.0.42
203.0.113.34
External Server
1.2.3.4
Next, open a new shell to the instance and then ping the external host where tcpdump is running. If the network
path to the external server and back is fully functional, you see something like the following:
On the external server:
12:51:42.020227 IP (tos 0x0, ttl 61, id 0, offset 0, flags [DF],
proto ICMP (1), length 84)
203.0.113.30 > 1.2.3.4: ICMP echo request, id 24895, seq 1, length 64
12:51:42.020255 IP (tos 0x0, ttl 64, id 8137, offset 0, flags [none],
proto ICMP (1), length 84)
1.2.3.4 > 203.0.113.30: ICMP echo reply, id 24895, seq 1,
length 64
On the compute node:
12:51:42.019519 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF],
proto ICMP (1), length 84)
10.0.2.24 > 1.2.3.4: ICMP echo request, id 24895, seq 1, length 64
12:51:42.019519 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF],
proto ICMP (1), length 84)
10.0.2.24 > 1.2.3.4: ICMP echo request, id 24895, seq 1, length 64
12:51:42.019545 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF],
proto ICMP (1), length 84)
203.0.113.30 > 1.2.3.4: ICMP echo request, id 24895, seq 1, length 64
12:51:42.019780 IP (tos 0x0, ttl 62, id 8137, offset 0, flags [none],
proto ICMP (1), length 84)
1.2.3.4 > 203.0.113.30: ICMP echo reply, id 24895, seq 1, length 64
12:51:42.019801 IP (tos 0x0, ttl 61, id 8137, offset 0, flags [none],
proto ICMP (1), length 84)
1.2.3.4 > 10.0.2.24: ICMP echo reply, id 24895, seq 1, length 64
110 Network Troubleshooting
Operations Guide (Release Version: 15.0.0)
12:51:42.019807 IP (tos 0x0, ttl 61, id 8137, offset 0, flags [none],
proto ICMP (1), length 84)
1.2.3.4 > 10.0.2.24: ICMP echo reply, id 24895, seq 1, length 64
On the instance:
12:51:42.020974 IP (tos 0x0, ttl 61, id 8137, offset 0, flags [none],
proto ICMP (1), length 84)
1.2.3.4 > 10.0.2.24: ICMP echo reply, id 24895, seq 1, length 64
Here, the external server received the ping request and sent a ping reply. On the compute node, you can see that
both the ping and ping reply successfully passed through. You might also see duplicate packets on the compute
node, as seen above, because tcpdump captured the packet on both the bridge and outgoing interface.
iptables
Through nova-network or neutron, OpenStack Compute automatically manages iptables, including forwarding
packets to and from instances on a compute node, forwarding floating IP traffic, and managing security
group rules. In addition to managing the rules, comments (if supported) will be inserted in the rules to help
indicate the purpose of the rule.
The following comments are added to the rule set as appropriate:
• Perform source NAT on outgoing traffic.
• Default drop rule for unmatched traffic.
• Direct traffic from the VM interface to the security group chain.
• Jump to the VM specific chain.
• Direct incoming traffic from VM to the security group chain.
• Allow traffic from defined IP/MAC pairs.
• Drop traffic without an IP/MAC allow rule.
• Allow DHCP client traffic.
• Prevent DHCP Spoofing by VM.
• Send unmatched traffic to the fallback chain.
• Drop packets that are not associated with a state.
• Direct packets associated with a known session to the RETURN chain.
• Allow IPv6 ICMP traffic to allow RA packets.
Run the following command to view the current iptables configuration:
# iptables-save
Note: If you modify the configuration, it reverts the next time you restart nova-network or neutron-server.
You must use OpenStack to manage iptables.
Network Troubleshooting 111
Operations Guide (Release Version: 15.0.0)
Network Configuration in the Database for nova-network
With nova-network, the nova database table contains a few tables with networking information:
fixed_ips Contains each possible IP address for the subnet(s) added to Compute. This table is related to the
instances table by way of the fixed_ips.instance_uuid column.
floating_ips Contains each floating IP address that was added to Compute. This table is related to the
fixed_ips table by way of the floating_ips.fixed_ip_id column.
instances Not entirely network specific, but it contains information about the instance that is utilizing the
fixed_ip and optional floating_ip.
From these tables, you can see that a floating IP is technically never directly related to an instance; it must
always go through a fixed IP.
Manually Disassociating a Floating IP
Sometimes an instance is terminated but the floating IP was not correctly disassociated from that instance.
Because the database is in an inconsistent state, the usual tools to disassociate the IP no longer work. To fix
this, you must manually update the database.
First, find the UUID of the instance in question:
mysql> select uuid from instances where hostname = 'hostname';
Next, find the fixed IP entry for that UUID:
mysql> select * from fixed_ips where instance_uuid = '<uuid>';
You can now get the related floating IP entry:
mysql> select * from floating_ips where fixed_ip_id = '<fixed_ip_id>';
And finally, you can disassociate the floating IP:
mysql> update floating_ips set fixed_ip_id = NULL, host = NULL where
fixed_ip_id = '<fixed_ip_id>';
You can optionally also deallocate the IP from the user’s pool:
mysql> update floating_ips set project_id = NULL where
fixed_ip_id = '<fixed_ip_id>';
Debugging DHCP Issues with nova-network
One common networking problem is that an instance boots successfully but is not reachable because it failed to
obtain an IP address from dnsmasq, which is the DHCP server that is launched by the nova-network service.
The simplest way to identify that this is the problem with your instance is to look at the console output of your
instance. If DHCP failed, you can retrieve the console log by doing:
$ openstack console log show <instance name or uuid>
112 Network Troubleshooting
Operations Guide (Release Version: 15.0.0)
If your instance failed to obtain an IP through DHCP, some messages should appear in the console. For example,
for the Cirros image, you see output that looks like the following:
udhcpc (v1.17.2) started
Sending discover...
Sending discover...
Sending discover...
No lease, forking to background
starting DHCP forEthernet interface eth0 [ [1;32mOK[0;39m ]
cloud-setup: checking http://169.254.169.254/2009-04-04/meta-data/instance-id
wget: can't connect to remote host (169.254.169.254): Network is
unreachable
After you establish that the instance booted properly, the task is to figure out where the failure is.
A DHCP problem might be caused by a misbehaving dnsmasq process. First, debug by checking logs and then
restart the dnsmasq processes only for that project (tenant). In VLAN mode, there is a dnsmasq process for
each tenant. Once you have restarted targeted dnsmasq processes, the simplest way to rule out dnsmasq causes
is to kill all of the dnsmasq processes on the machine and restart nova-network. As a last resort, do this as
root:
# killall dnsmasq
# restart nova-network
Note: Use openstack-nova-network on RHEL/CentOS/Fedora but nova-network on Ubuntu/Debian.
Several minutes after nova-network is restarted, you should see new dnsmasq processes running:
# ps aux | grep dnsmasq
nobody 3735 0.0 0.0 27540 1044 ? S 15:40 0:00 /usr/sbin/dnsmasq --strict-order
--bind-interfaces --conf-file=
--domain=novalocal --pid-file=/var/lib/nova/networks/nova-br100.pid
--listen-address=192.168.100.1 --except-interface=lo
--dhcp-range=set:'novanetwork',192.168.100.2,static,120s
--dhcp-lease-max=256
--dhcp-hostsfile=/var/lib/nova/networks/nova-br100.conf
--dhcp-script=/usr/bin/nova-dhcpbridge --leasefile-ro
root 3736 0.0 0.0 27512 444 ? S 15:40 0:00 /usr/sbin/dnsmasq --strict-order
--bind-interfaces --conf-file=
--domain=novalocal --pid-file=/var/lib/nova/networks/nova-br100.pid
--listen-address=192.168.100.1 --except-interface=lo
--dhcp-range=set:'novanetwork',192.168.100.2,static,120s
--dhcp-lease-max=256
--dhcp-hostsfile=/var/lib/nova/networks/nova-br100.conf
--dhcp-script=/usr/bin/nova-dhcpbridge --leasefile-ro
If your instances are still not able to obtain IP addresses, the next thing to check is whether dnsmasq is seeing the
DHCP requests from the instance. On the machine that is running the dnsmasq process, which is the compute
host if running in multi-host mode, look at /var/log/syslog to see the dnsmasq output. If dnsmasq is seeing
the request properly and handing out an IP, the output looks like this:
Feb 27 22:01:36 mynode dnsmasq-dhcp[2438]: DHCPDISCOVER(br100) fa:16:3e:56:0b:6f
Feb 27 22:01:36 mynode dnsmasq-dhcp[2438]: DHCPOFFER(br100) 192.168.100.3
fa:16:3e:56:0b:6f
Feb 27 22:01:36 mynode dnsmasq-dhcp[2438]: DHCPREQUEST(br100) 192.168.100.3
Network Troubleshooting 113
Operations Guide (Release Version: 15.0.0)
fa:16:3e:56:0b:6f
Feb 27 22:01:36 mynode dnsmasq-dhcp[2438]: DHCPACK(br100) 192.168.100.3
fa:16:3e:56:0b:6f test
If you do not see the DHCPDISCOVER, a problem exists with the packet getting from the instance to the machine
running dnsmasq. If you see all of the preceding output and your instances are still not able to obtain IP
addresses, then the packet is able to get from the instance to the host running dnsmasq, but it is not able to make
the return trip.
You might also see a message such as this:
Feb 27 22:01:36 mynode dnsmasq-dhcp[25435]: DHCPDISCOVER(br100)
fa:16:3e:78:44:84 no address available
This may be a dnsmasq and/or nova-network related issue. (For the preceding example, the problem happened
to be that dnsmasq did not have any more IP addresses to give away because there were no more fixed IPs
available in the OpenStack Compute database.)
If there’s a suspicious-looking dnsmasq log message, take a look at the command-line arguments to the dnsmasq
processes to see if they look correct:
$ ps aux | grep dnsmasq
The output looks something like the following:
108 1695 0.0 0.0 25972 1000 ? S Feb26 0:00 /usr/sbin/dnsmasq
-u libvirt-dnsmasq
--strict-order --bind-interfaces
--pid-file=/var/run/libvirt/network/default.pid --conf-file=
--except-interface lo --listen-address 192.168.122.1
--dhcp-range 192.168.122.2,192.168.122.254
--dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases
--dhcp-lease-max=253 --dhcp-no-override
nobody 2438 0.0 0.0 27540 1096 ? S Feb26 0:00 /usr/sbin/dnsmasq
--strict-order --bind-interfaces --conf-file=
--domain=novalocal --pid-file=/var/lib/nova/networks/nova-br100.pid
--listen-address=192.168.100.1
--except-interface=lo
--dhcp-range=set:'novanetwork',192.168.100.2,static,120s
--dhcp-lease-max=256
--dhcp-hostsfile=/var/lib/nova/networks/nova-br100.conf
--dhcp-script=/usr/bin/nova-dhcpbridge --leasefile-ro
root 2439 0.0 0.0 27512 472 ? S Feb26 0:00 /usr/sbin/dnsmasq --strict-order
--bind-interfaces --conf-file=
--domain=novalocal --pid-file=/var/lib/nova/networks/nova-br100.pid
--listen-address=192.168.100.1
--except-interface=lo
--dhcp-range=set:'novanetwork',192.168.100.2,static,120s
--dhcp-lease-max=256
--dhcp-hostsfile=/var/lib/nova/networks/nova-br100.conf
--dhcp-script=/usr/bin/nova-dhcpbridge --leasefile-ro
The output shows three different dnsmasq processes. The dnsmasq process that has the DHCP subnet range
of 192.168.122.0 belongs to libvirt and can be ignored. The other two dnsmasq processes belong to novanetwork.
The two processes are actually related—one is simply the parent process of the other. The arguments
of the dnsmasq processes should correspond to the details you configured nova-network with.
114 Network Troubleshooting
Operations Guide (Release Version: 15.0.0)
If the problem does not seem to be related to dnsmasq itself, at this point use tcpdump on the interfaces to
determine where the packets are getting lost.
DHCP traffic uses UDP. The client sends from port 68 to port 67 on the server. Try to boot a new instance and
then systematically listen on the NICs until you identify the one that isn’t seeing the traffic. To use tcpdump to
listen to ports 67 and 68 on br100, you would do:
# tcpdump -i br100 -n port 67 or port 68
You should be doing sanity checks on the interfaces using command such as ip a and brctl show to ensure
that the interfaces are actually up and configured the way that you think that they are.
Debugging DNS Issues
If you are able to use SSH to log into an instance, but it takes a very long time (on the order of a minute) to get
a prompt, then you might have a DNS issue. The reason a DNS issue can cause this problem is that the SSH
server does a reverse DNS lookup on the IP address that you are connecting from. If DNS lookup isn’t working
on your instances, then you must wait for the DNS reverse lookup timeout to occur for the SSH login process
to complete.
When debugging DNS issues, start by making sure that the host where the dnsmasq process for that instance
runs is able to correctly resolve. If the host cannot resolve, then the instances won’t be able to either.
A quick way to check whether DNS is working is to resolve a hostname inside your instance by using the host
command. If DNS is working, you should see:
$ host openstack.org
openstack.org has address 174.143.194.225
openstack.org mail is handled by 10 mx1.emailsrvr.com.
openstack.org mail is handled by 20 mx2.emailsrvr.com.
If you’re running the Cirros image, it doesn’t have the “host” program installed, in which case you can use ping
to try to access a machine by hostname to see whether it resolves. If DNS is working, the first line of ping
would be:
$ ping openstack.org
PING openstack.org (174.143.194.225): 56 data bytes
If the instance fails to resolve the hostname, you have a DNS problem. For example:
$ ping openstack.org
ping: bad address 'openstack.org'
In an OpenStack cloud, the dnsmasq process acts as the DNS server for the instances in addition to acting as the
DHCP server. A misbehaving dnsmasq process may be the source of DNS-related issues inside the instance.
As mentioned in the previous section, the simplest way to rule out a misbehaving dnsmasq process is to kill
all the dnsmasq processes on the machine and restart nova-network. However, be aware that this command
affects everyone running instances on this node, including tenants that have not seen the issue. As a last resort,
as root:
# killall dnsmasq
# restart nova-network
After the dnsmasq processes start again, check whether DNS is working.
Network Troubleshooting 115
Operations Guide (Release Version: 15.0.0)
If restarting the dnsmasq process doesn’t fix the issue, you might need to use tcpdump to look at the packets
to trace where the failure is. The DNS server listens on UDP port 53. You should see the DNS request on the
bridge (such as, br100) of your compute node. Let’s say you start listening with tcpdump on the compute node:
# tcpdump -i br100 -n -v udp port 53
tcpdump: listening on br100, link-type EN10MB (Ethernet), capture size 65535 bytes
Then, if you use SSH to log into your instance and try ping openstack.org, you should see something like:
16:36:18.807518 IP (tos 0x0, ttl 64, id 56057, offset 0, flags [DF],
proto UDP (17), length 59)
192.168.100.4.54244 > 192.168.100.1.53: 2+ A? openstack.org. (31)
16:36:18.808285 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF],
proto UDP (17), length 75)
192.168.100.1.53 > 192.168.100.4.54244: 2 1/0/0 openstack.org. A
174.143.194.225 (47)
Troubleshooting Open vSwitch
Open vSwitch, as used in the previous OpenStack Networking examples is a full-featured multilayer virtual
switch licensed under the open source Apache 2.0 license. Full documentation can be found at the project’s
website. In practice, given the preceding configuration, the most common issues are being sure that the required
bridges (br-int, br-tun, and br-ex) exist and have the proper ports connected to them.
The Open vSwitch driver should and usually does manage this automatically, but it is useful to know how to
do this by hand with the ovs-vsctl command. This command has many more subcommands than we will use
here; see the man page or use ovs-vsctl --help for the full listing.
To list the bridges on a system, use ovs-vsctl list-br. This example shows a compute node that has an
internal bridge and a tunnel bridge. VLAN networks are trunked through the eth1 network interface:
# ovs-vsctl list-br
br-int
br-tun
eth1-br
Working from the physical interface inwards, we can see the chain of ports and bridges. First, the bridge eth1-
br, which contains the physical network interface eth1 and the virtual interface phy-eth1-br:
# ovs-vsctl list-ports eth1-br
eth1
phy-eth1-br
Next, the internal bridge, br-int, contains int-eth1-br, which pairs with phy-eth1-br to connect to the
physical network shown in the previous bridge, patch-tun, which is used to connect to the GRE tunnel bridge
and the TAP devices that connect to the instances currently running on the system:
# ovs-vsctl list-ports br-int
int-eth1-br
patch-tun
tap2d782834-d1
tap690466bc-92
tap8a864970-2d
116 Network Troubleshooting
Operations Guide (Release Version: 15.0.0)
The tunnel bridge, br-tun, contains the patch-int interface and gre-<N> interfaces for each peer it connects
to via GRE, one for each compute and network node in your cluster:
# ovs-vsctl list-ports br-tun
patch-int
gre-1
.
.
.
gre-<N>
If any of these links are missing or incorrect, it suggests a configuration error. Bridges can be added with ovsvsctl
add-br, and ports can be added to bridges with ovs-vsctl add-port. While running these by hand
can be useful debugging, it is imperative that manual changes that you intend to keep be reflected back into
your configuration files.
Dealing with Network Namespaces
Linux network namespaces are a kernel feature the networking service uses to support multiple isolated layer-2
networks with overlapping IP address ranges. The support may be disabled, but it is on by default. If it is enabled
in your environment, your network nodes will run their dhcp-agents and l3-agents in isolated namespaces.
Network interfaces and traffic on those interfaces will not be visible in the default namespace.
To see whether you are using namespaces, run ip netns:
# ip netns
qdhcp-e521f9d0-a1bd-4ff4-bc81-78a60dd88fe5
qdhcp-a4d00c60-f005-400e-a24c-1bf8b8308f98
qdhcp-fe178706-9942-4600-9224-b2ae7c61db71
qdhcp-0a1d0a27-cffa-4de3-92c5-9d3fd3f2e74d
qrouter-8a4ce760-ab55-4f2f-8ec5-a2e858ce0d39
L3-agent router namespaces are named qrouter-<router_uuid>, and dhcp-agent name spaces are named
qdhcp-<net_uuid>. This output shows a network node with four networks running dhcp-agents, one of which
is also running an l3-agent router. It’s important to know which network you need to be working in. A list of
existing networks and their UUIDs can be obtained by running openstack network list with administrative
credentials.
Once you’ve determined which namespace you need to work in, you can use any of the debugging tools mention
earlier by prefixing the command with ip netns exec <namespace>. For example, to see what network
interfaces exist in the first qdhcp namespace returned above, do this:
# ip netns exec qdhcp-e521f9d0-a1bd-4ff4-bc81-78a60dd88fe5 ip a
10: tape6256f7d-31: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN
link/ether fa:16:3e:aa:f7:a1 brd ff:ff:ff:ff:ff:ff
inet 10.0.1.100/24 brd 10.0.1.255 scope global tape6256f7d-31
inet 169.254.169.254/16 brd 169.254.255.255 scope global tape6256f7d-31
inet6 fe80::f816:3eff:feaa:f7a1/64 scope link
valid_lft forever preferred_lft forever
28: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
Network Troubleshooting 117
Operations Guide (Release Version: 15.0.0)
From this you see that the DHCP server on that network is using the tape6256f7d-31 device and has an IP
address of 10.0.1.100. Seeing the address 169.254.169.254, you can also see that the dhcp-agent is running
a metadata-proxy service. Any of the commands mentioned previously in this chapter can be run in the same
way. It is also possible to run a shell, such as bash, and have an interactive session within the namespace. In
the latter case, exiting the shell returns you to the top-level default namespace.
Assign a lost IPv4 address back to a project
1. Using administrator credentials, confirm the lost IP address is still available:
# openstack server list --all-project | grep 'IP-ADDRESS'
2. Create a port:
$ openstack port create --network NETWORK_ID PORT_NAME
3. Update the new port with the IPv4 address:
# openstack subnet list
# neutron port-update PORT_NAME --request-format=json --fixed-ips \
type=dict list=true subnet_id=NETWORK_ID_IPv4_SUBNET_ID \
ip_address=IP_ADDRESS subnet_id=NETWORK_ID_IPv6_SUBNET_ID
# openstack port show PORT-NAME
Tools for automated neutron diagnosis
easyOVS is a useful tool when it comes to operating your OpenvSwitch bridges and iptables on your OpenStack
platform. It automatically associates the virtual ports with the VM MAC/IP, VLAN tag and namespace
information, as well as the iptables rules for VMs.
Don is another convenient network analysis and diagnostic system that provides a completely automated service
for verifying and diagnosing the networking functionality provided by OVS.
Additionally, you can refer to neutron debug for more options.
Logging and Monitoring
Logging
Where Are the Logs?
Most services use the convention of writing their log files to subdirectories of the /var/log directory, as
listed in Table OpenStack log locations.
118 Logging and Monitoring
Operations Guide (Release Version: 15.0.0)
Table 7: Table OpenStack log locations
Node type Service Log location
Cloud
controller
nova-* /var/log/nova
Cloud
controller
glance-* /var/log/glance
Cloud
controller
cinder-* /var/log/cinder
Cloud
controller
keystone-* /var/log/keystone
Cloud
controller
neutron-* /var/log/neutron
Cloud
controller
horizon /var/log/apache2/
All nodes misc (swift, dnsmasq) /var/log/syslog
Compute
nodes
libvirt /var/log/libvirt/libvirtd.log
Compute
nodes
Console (boot up messages) for
VM instances:
/var/lib/nova/instances/instance-
<instance
id>/console.log
Block Storage
nodes
cinder-volume /var/log/cinder/cinder-volume.log
Reading the Logs
OpenStack services use the standard logging levels, at increasing severity: TRACE, DEBUG, INFO, AUDIT,
WARNING, ERROR, and CRITICAL. That is, messages only appear in the logs if they are more “severe” than
the particular log level, with DEBUG allowing all log statements through. For example, TRACE is logged
only if the software has a stack trace, while INFO is logged for every message including those that are only for
information.
To disable DEBUG-level logging, edit /etc/nova/nova.conf file as follows:
debug=false
Keystone is handled a little differently. To modify the logging level, edit the /etc/keystone/logging.conf
file and look at the logger_root and handler_file sections.
Logging for horizon is configured in /etc/openstack_dashboard/local_settings.py. Because horizon
is a Django web application, it follows the Django Logging framework conventions.
The first step in finding the source of an error is typically to search for a CRITICAL, or ERROR message in
the log starting at the bottom of the log file.
Here is an example of a log message with the corresponding ERROR (Python traceback) immediately following:
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server [req-c0b38ace-2586-48ce-9336-
,→6233efa1f035 6c9808c2c5044e1388a83a74da9364d5 e07f5395c
2eb428cafc41679e7deeab1 - default default] Exception during message handling
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server Traceback (most recent call
,→last):
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server File "/openstack/venvs/
,→cinder-14.0.0/lib/python2.7/site-packages/oslo_messaging/rpc/server.py", line 133, in _
,→process_incoming
Logging and Monitoring 119
Operations Guide (Release Version: 15.0.0)
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server res = self.dispatcher.
,→dispatch(message)
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server File "/openstack/venvs/
,→cinder-14.0.0/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py", line 150, in
,→dispatch
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server return self._do_
,→dispatch(endpoint, method, ctxt, args)
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server File "/openstack/venvs/
,→cinder-14.0.0/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py", line 121, in
,→_do_dispatch
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server result = func(ctxt, **new_
,→args)
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server File "/openstack/venvs/
,→cinder-14.0.0/lib/python2.7/site-packages/cinder/volume/manager.py", line 4366, in create_
,→volume
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server allow_reschedule=allow_
,→reschedule, volume=volume)
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server File "/openstack/venvs/
,→cinder-14.0.0/lib/python2.7/site-packages/cinder/volume/manager.py", line 634, in create_
,→volume
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server _run_flow()
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server File "/openstack/venvs/
,→cinder-14.0.0/lib/python2.7/site-packages/cinder/volume/manager.py", line 626, in _run_
,→flow
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server flow_engine.run()
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server File "/openstack/venvs/
,→cinder-14.0.0/lib/python2.7/site-packages/taskflow/engines/action_engine/engine.py", line
,→247, in run
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server for _state in self.run_
,→iter(timeout=timeout):
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server File "/openstack/venvs/
,→cinder-14.0.0/lib/python2.7/site-packages/taskflow/engines/action_engine/engine.py", line
,→340, in run_iter
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server failure.Failure.reraise_
,→if_any(er_failures)
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server File "/openstack/venvs/
,→cinder-14.0.0/lib/python2.7/site-packages/taskflow/types/failure.py", line 336, in
,→reraise_if_any
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server failures[0].reraise()
2017-01-18 15:54:00.467 32552 ERROR oslo_messaging.rpc.server File "/openstack/venvs/
,→cinder-14.0.0/lib/python2.7/site-packages/taskflow/types/failure.py", line 343, in reraise
In this example, cinder-volumes failed to start and has provided a stack trace, since its volume back end
has been unable to set up the storage volume—probably because the LVM volume that is expected from the
configuration does not exist.
Here is an example error log:
2013-02-25 20:26:33 6619 ERROR nova.openstack.common.rpc.common [-] AMQP server on
,→localhost:5672 is unreachable:
[Errno 111] ECONNREFUSED. Trying again in 23 seconds.
In this error, a nova service has failed to connect to the RabbitMQ server because it got a connection refused
error.
120 Logging and Monitoring
Operations Guide (Release Version: 15.0.0)
Tracing Instance Requests
When an instance fails to behave properly, you will often have to trace activity associated with that instance
across the log files of various nova-* services and across both the cloud controller and compute nodes.
The typical way is to trace the UUID associated with an instance across the service logs.
Consider the following example:
$ openstack server list
+--------------------------------+--------+--------+--------------------------+------------+
| ID | Name | Status | Networks | Image Name |
+--------------------------------+--------+--------+--------------------------+------------+
| fafed8-4a46-413b-b113-f1959ffe | cirros | ACTIVE | novanetwork=192.168.100.3| cirros |
+--------------------------------------+--------+--------+--------------------+------------+
Here, the ID associated with the instance is faf7ded8-4a46-413b-b113-f19590746ffe. If you search for
this string on the cloud controller in the /var/log/nova-*.log files, it appears in nova-api.log and novascheduler.log.
If you search for this on the compute nodes in /var/log/nova-*.log, it appears in novacompute.log.
If no ERROR or CRITICAL messages appear, the most recent log entry that reports this may
provide a hint about what has gone wrong.
Adding Custom Logging Statements
If there is not enough information in the existing logs, you may need to add your own custom logging statements
to the nova-* services.
The source files are located in /usr/lib/python2.7/dist-packages/nova.
To add logging statements, the following line should be near the top of the file. For most files, these should
already be there:
from nova.openstack.common import log as logging
LOG = logging.getLogger(__name__)
To add a DEBUG logging statement, you would do:
LOG.debug("This is a custom debugging statement")
You may notice that all the existing logging messages are preceded by an underscore and surrounded by parentheses,
for example:
LOG.debug(_("Logging statement appears here"))
This formatting is used to support translation of logging messages into different languages using the gettext
internationalization library. You don’t need to do this for your own custom log messages. However, if you
want to contribute the code back to the OpenStack project that includes logging statements, you must surround
your log messages with underscores and parentheses.
RabbitMQ Web Management Interface or rabbitmqctl
Aside from connection failures, RabbitMQ log files are generally not useful for debugging OpenStack related
issues. Instead, we recommend you use the RabbitMQ web management interface. Enable it on your cloud
controller:
Logging and Monitoring 121
Operations Guide (Release Version: 15.0.0)
# /usr/lib/rabbitmq/bin/rabbitmq-plugins enable rabbitmq_management
# service rabbitmq-server restart
The RabbitMQ web management interface is accessible on your cloud controller at http://localhost:55672.
Note: Ubuntu 12.04 installs RabbitMQ version 2.7.1, which uses port 55672. RabbitMQ versions 3.0 and
above use port 15672 instead. You can check which version of RabbitMQ you have running on your local
Ubuntu machine by doing:
$ dpkg -s rabbitmq-server | grep "Version:"
Version: 2.7.1-0ubuntu4
An alternative to enabling the RabbitMQ web management interface is to use the rabbitmqctl commands.
For example, rabbitmqctl list_queues| grep cinder displays any messages left in the queue. If there
are messages, it’s a possible sign that cinder services didn’t connect properly to rabbitmq and might have to be
restarted.
Items to monitor for RabbitMQ include the number of items in each of the queues and the processing time
statistics for the server.
Centrally Managing Logs
Because your cloud is most likely composed of many servers, you must check logs on each of those servers to
properly piece an event together. A better solution is to send the logs of all servers to a central location so that
they can all be accessed from the same area.
The choice of central logging engine will be dependent on the operating system in use as well as any organizational
requirements for logging tools.
Syslog choices
There are a large number of syslogs engines available, each have differing capabilities and configuration requirements.
rsyslog
A number of operating systems use rsyslog as the default logging service. Since it is natively able to send logs to
a remote location, you do not have to install anything extra to enable this feature, just modify the configuration
file. In doing this, consider running your logging over a management network or using an encrypted VPN to
avoid interception.
rsyslog client configuration
To begin, configure all OpenStack components to log to the syslog log file in addition to their standard log file
location. Also, configure each component to log to a different syslog facility. This makes it easier to split the
logs into individual components on the central server:
122 Logging and Monitoring
Operations Guide (Release Version: 15.0.0)
nova.conf:
use_syslog=True
syslog_log_facility=LOG_LOCAL0
glance-api.conf and glance-registry.conf:
use_syslog=True
syslog_log_facility=LOG_LOCAL1
cinder.conf:
use_syslog=True
syslog_log_facility=LOG_LOCAL2
keystone.conf:
use_syslog=True
syslog_log_facility=LOG_LOCAL3
By default, Object Storage logs to syslog.
Next, create /etc/rsyslog.d/client.conf with the following line:
*.* @192.168.1.10
This instructs rsyslog to send all logs to the IP listed. In this example, the IP points to the cloud controller.
rsyslog server configuration
Designate a server as the central logging server. The best practice is to choose a server that is solely dedicated
to this purpose. Create a file called /etc/rsyslog.d/server.conf with the following contents:
# Enable UDP
$ModLoad imudp
# Listen on 192.168.1.10 only
$UDPServerAddress 192.168.1.10
# Port 514
$UDPServerRun 514
# Create logging templates for nova
$template NovaFile,"/var/log/rsyslog/%HOSTNAME%/nova.log"
$template NovaAll,"/var/log/rsyslog/nova.log"
# Log everything else to syslog.log
$template DynFile,"/var/log/rsyslog/%HOSTNAME%/syslog.log"
*.* ?DynFile
# Log various openstack components to their own individual file
local0.* ?NovaFile
local0.* ?NovaAll
& ~
This example configuration handles the nova service only. It first configures rsyslog to act as a server that runs
on port 514. Next, it creates a series of logging templates. Logging templates control where received logs are
stored. Using the last example, a nova log from c01.example.com goes to the following locations:
Logging and Monitoring 123
Operations Guide (Release Version: 15.0.0)
• /var/log/rsyslog/c01.example.com/nova.log
• /var/log/rsyslog/nova.log
This is useful, as logs from c02.example.com go to:
• /var/log/rsyslog/c02.example.com/nova.log
• /var/log/rsyslog/nova.log
This configuration will result in a separate log file for each compute node as well as an aggregated log file that
contains nova logs from all nodes.
Monitoring
There are two types of monitoring: watching for problems and watching usage trends. The former ensures that
all services are up and running, creating a functional cloud. The latter involves monitoring resource usage over
time in order to make informed decisions about potential bottlenecks and upgrades.
Process Monitoring
A basic type of alert monitoring is to simply check and see whether a required process is running. For example,
ensure that the nova-api service is running on the cloud controller:
# ps aux | grep nova-api
nova 12786 0.0 0.0 37952 1312 ? Ss Feb11 0:00 su -s /bin/sh -c exec nova-api
--config-file=/etc/nova/nova.conf nova
nova 12787 0.0 0.1 135764 57400 ? S Feb11 0:01 /usr/bin/python
/usr/bin/nova-api --config-file=/etc/nova/nova.conf
nova 12792 0.0 0.0 96052 22856 ? S Feb11 0:01 /usr/bin/python
/usr/bin/nova-api --config-file=/etc/nova/nova.conf
nova 12793 0.0 0.3 290688 115516 ? S Feb11 1:23 /usr/bin/python
/usr/bin/nova-api --config-file=/etc/nova/nova.conf
nova 12794 0.0 0.2 248636 77068 ? S Feb11 0:04 /usr/bin/python
/usr/bin/nova-api --config-file=/etc/nova/nova.conf
root 24121 0.0 0.0 11688 912 pts/5 S+ 13:07 0:00 grep nova-api
The OpenStack processes that should be monitored depend on the specific configuration of the environment,
but can include:
Compute service (nova)
• nova-api
• nova-scheduler
• nova-conductor
• nova-novncproxy
• nova-compute
Block Storage service (cinder)
• cinder-volume
• cinder-api
• cinder-scheduler
124 Logging and Monitoring
Operations Guide (Release Version: 15.0.0)
Networking service (neutron)
• neutron-api
• neutron-server
• neutron-openvswitch-agent
• neutron-dhcp-agent
• neutron-l3-agent
• neutron-metadata-agent
Image service (glance)
• glance-api
• glance-registry
Identity service (keystone)
The keystone processes are run within Apache as WSGI applications.
Resource Alerting
Resource alerting provides notifications when one or more resources are critically low. While the monitoring
thresholds should be tuned to your specific OpenStack environment, monitoring resource usage is not specific
to OpenStack at all—any generic type of alert will work fine.
Some of the resources that you want to monitor include:
• Disk usage
• Server load
• Memory usage
• Network I/O
• Available vCPUs
Telemetry Service
The Telemetry service (ceilometer) collects metering and event data relating to OpenStack services. Data collected
by the Telemetry service could be used for billing. Depending on deployment configuration, collected
data may be accessible to users based on the deployment configuration. The Telemetry service provides a
REST API documented at ceilometer V2 Web API. You can read more about the module in the OpenStack
Administrator Guide or in the developer documentation.
OpenStack Specific Resources
Resources such as memory, disk, and CPU are generic resources that all servers (even non-OpenStack servers)
have and are important to the overall health of the server. When dealing with OpenStack specifically, these
resources are important for a second reason: ensuring that enough are available to launch instances. There are
a few ways you can see OpenStack resource usage. The first is through the nova command:
Logging and Monitoring 125
Operations Guide (Release Version: 15.0.0)
# openstack usage list
This command displays a list of how many instances a tenant has running and some light usage statistics about
the combined instances. This command is useful for a quick overview of your cloud, but it doesn’t really get
into a lot of details.
Next, the nova database contains three tables that store usage information.
The nova.quotas and nova.quota_usages tables store quota information. If a tenant’s quota is different
from the default quota settings, its quota is stored in the nova.quotas table. For example:
mysql> select project_id, resource, hard_limit from quotas;
+----------------------------------+-----------------------------+------------+
| project_id | resource | hard_limit |
+----------------------------------+-----------------------------+------------+
| 628df59f091142399e0689a2696f5baa | metadata_items | 128 |
| 628df59f091142399e0689a2696f5baa | injected_file_content_bytes | 10240 |
| 628df59f091142399e0689a2696f5baa | injected_files | 5 |
| 628df59f091142399e0689a2696f5baa | gigabytes | 1000 |
| 628df59f091142399e0689a2696f5baa | ram | 51200 |
| 628df59f091142399e0689a2696f5baa | floating_ips | 10 |
| 628df59f091142399e0689a2696f5baa | instances | 10 |
| 628df59f091142399e0689a2696f5baa | volumes | 10 |
| 628df59f091142399e0689a2696f5baa | cores | 20 |
+----------------------------------+-----------------------------+------------+
The nova.quota_usages table keeps track of how many resources the tenant currently has in use:
mysql> select project_id, resource, in_use from quota_usages where project_id like '628%';
+----------------------------------+--------------+--------+
| project_id | resource | in_use |
+----------------------------------+--------------+--------+
| 628df59f091142399e0689a2696f5baa | instances | 1 |
| 628df59f091142399e0689a2696f5baa | ram | 512 |
| 628df59f091142399e0689a2696f5baa | cores | 1 |
| 628df59f091142399e0689a2696f5baa | floating_ips | 1 |
| 628df59f091142399e0689a2696f5baa | volumes | 2 |
| 628df59f091142399e0689a2696f5baa | gigabytes | 12 |
| 628df59f091142399e0689a2696f5baa | images | 1 |
+----------------------------------+--------------+--------+
By comparing a tenant’s hard limit with their current resource usage, you can see their usage percentage. For
example, if this tenant is using 1 floating IP out of 10, then they are using 10 percent of their floating IP quota.
Rather than doing the calculation manually, you can use SQL or the scripting language of your choice and create
a formatted report:
+----------------------------------+------------+-------------+---------------+
| some_tenant |
+-----------------------------------+------------+------------+---------------+
| Resource | Used | Limit | |
+-----------------------------------+------------+------------+---------------+
| cores | 1 | 20 | 5 % |
| floating_ips | 1 | 10 | 10 % |
| gigabytes | 12 | 1000 | 1 % |
| images | 1 | 4 | 25 % |
| injected_file_content_bytes | 0 | 10240 | 0 % |
| injected_file_path_bytes | 0 | 255 | 0 % |
126 Logging and Monitoring
Operations Guide (Release Version: 15.0.0)
| injected_files | 0 | 5 | 0 % |
| instances | 1 | 10 | 10 % |
| key_pairs | 0 | 100 | 0 % |
| metadata_items | 0 | 128 | 0 % |
| ram | 512 | 51200 | 1 % |
| reservation_expire | 0 | 86400 | 0 % |
| security_group_rules | 0 | 20 | 0 % |
| security_groups | 0 | 10 | 0 % |
| volumes | 2 | 10 | 20 % |
+-----------------------------------+------------+------------+---------------+
The preceding information was generated by using a custom script that can be found on GitHub.
Note: This script is specific to a certain OpenStack installation and must be modified to fit your environment.
However, the logic should easily be transferable.
Intelligent Alerting
Intelligent alerting can be thought of as a form of continuous integration for operations. For example, you
can easily check to see whether the Image service is up and running by ensuring that the glance-api and
glance-registry processes are running or by seeing whether glace-api is responding on port 9292.
But how can you tell whether images are being successfully uploaded to the Image service? Maybe the disk
that Image service is storing the images on is full or the S3 back end is down. You could naturally check this
by doing a quick image upload:
#!/bin/bash
#
# assumes that reasonable credentials have been stored at
# /root/auth
. /root/openrc
wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img
openstack image create --name='cirros image' --public \
--container-format=bare --disk-format=qcow2 \
--file cirros-0.3.5-x86_64-disk.img
By taking this script and rolling it into an alert for your monitoring system (such as Nagios), you now have an
automated way of ensuring that image uploads to the Image Catalog are working.
Note: You must remove the image after each test. Even better, test whether you can successfully delete an
image from the Image service.
Intelligent alerting takes considerably more time to plan and implement than the other alerts described in this
chapter. A good outline to implement intelligent alerting is:
• Review common actions in your cloud.
• Create ways to automatically test these actions.
• Roll these tests into an alerting system.
Logging and Monitoring 127
Operations Guide (Release Version: 15.0.0)
Some other examples for Intelligent Alerting include:
• Can instances launch and be destroyed?
• Can users be created?
• Can objects be stored and deleted?
• Can volumes be created and destroyed?
Trending
Trending can give you great insight into how your cloud is performing day to day. You can learn, for example,
if a busy day was simply a rare occurrence or if you should start adding new compute nodes.
Trending takes a slightly different approach than alerting. While alerting is interested in a binary result (whether
a check succeeds or fails), trending records the current state of something at a certain point in time. Once enough
points in time have been recorded, you can see how the value has changed over time.
All of the alert types mentioned earlier can also be used for trend reporting. Some other trend examples include:
• The number of instances on each compute node
• The types of flavors in use
• The number of volumes in use
• The number of Object Storage requests each hour
• The number of nova-api requests each hour
• The I/O statistics of your storage services
As an example, recording nova-api usage can allow you to track the need to scale your cloud controller. By
keeping an eye on nova-api requests, you can determine whether you need to spawn more nova-api processes
or go as far as introducing an entirely new server to run nova-api. To get an approximate count of the requests,
look for standard INFO messages in /var/log/nova/nova-api.log:
# grep INFO /var/log/nova/nova-api.log | wc
You can obtain further statistics by looking for the number of successful requests:
# grep " 200 " /var/log/nova/nova-api.log | wc
By running this command periodically and keeping a record of the result, you can create a trending report over
time that shows whether your nova-api usage is increasing, decreasing, or keeping steady.
A tool such as collectd can be used to store this information. While collectd is out of the scope of this book,
a good starting point would be to use collectd to store the result as a COUNTER data type. More information
can be found in collectd’s documentation.
Monitoring Tools
Nagios
Nagios is an open source monitoring service. It is capable of executing arbitrary commands to check the status of
server and network services, remotely executing arbitrary commands directly on servers, and allowing servers
128 Logging and Monitoring
Operations Guide (Release Version: 15.0.0)
to push notifications back in the form of passive monitoring. Nagios has been around since 1999. Although
newer monitoring services are available, Nagios is a tried-and-true systems administration staple.
You can create automated alerts for critical processes by using Nagios and NRPE. For example, to ensure that
the nova-compute process is running on the compute nodes, create an alert on your Nagios server:
define service {
host_name c01.example.com
check_command check_nrpe_1arg!check_nova-compute
use generic-service
notification_period 24x7
contact_groups sysadmins
service_description nova-compute
}
On the Compute node, create the following NRPE configuration:
command[check_nova-compute]=/usr/lib/nagios/plugins/check_procs -c 1: -a nova-compute
Nagios checks that at least one nova-compute service is running at all times.
For resource alerting, for example, monitor disk capacity on a compute node with Nagios, add the following to
your Nagios configuration:
define service {
host_name c01.example.com
check_command check_nrpe!check_all_disks!20% 10%
use generic-service
contact_groups sysadmins
service_description Disk
}
On the compute node, add the following to your NRPE configuration:
command[check_all_disks]=/usr/lib/nagios/plugins/check_disk -w $ARG1$ -c $ARG2$ -e
Nagios alerts you with a WARNING when any disk on the compute node is 80 percent full and CRITICAL when
90 percent is full.
StackTach
StackTach is a tool that collects and reports the notifications sent by nova. Notifications are essentially the
same as logs but can be much more detailed. Nearly all OpenStack components are capable of generating notifications
when significant events occur. Notifications are messages placed on the OpenStack queue (generally
RabbitMQ) for consumption by downstream systems. An overview of notifications can be found at System
Usage Data.
To enable nova to send notifications, add the following to the nova.conf configuration file:
notification_topics=monitor
notification_driver=messagingv2
Once nova is sending notifications, install and configure StackTach. StackTach works for queue consumption
and pipeline processing are configured to read these notifications from RabbitMQ servers and store them in a
Logging and Monitoring 129
Operations Guide (Release Version: 15.0.0)
database. Users can inquire on instances, requests, and servers by using the browser interface or commandline
tool, Stacky. Since StackTach is relatively new and constantly changing, installation instructions quickly
become outdated. Refer to the StackTach Git repository for instructions as well as a demostration video. Additional
details on the latest developments can be discovered at the official page
Logstash
Logstash is a high performance indexing and search engine for logs. Logs from Jenkins test runs are sent to
logstash where they are indexed and stored. Logstash facilitates reviewing logs from multiple sources in a
single test run, searching for errors or particular events within a test run, and searching for log event trends
across test runs.
There are four major layers in Logstash setup which are:
• Log Pusher
• Log Indexer
• ElasticSearch
• Kibana
Each layer scales horizontally. As the number of logs grows you can add more log pushers, more Logstash
indexers, and more ElasticSearch nodes.
Logpusher is a pair of Python scripts that first listens to Jenkins build events, then converts them into Gearman
jobs. Gearman provides a generic application framework to farm out work to other machines or processes that
are better suited to do the work. It allows you to do work in parallel, to load balance processing, and to call
functions between languages. Later, Logpusher performs Gearman jobs to push log files into logstash. Logstash
indexer reads these log events, filters them to remove unwanted lines, collapse multiple events together, and
parses useful information before shipping them to ElasticSearch for storage and indexing. Kibana is a logstash
oriented web client for ElasticSearch.
Summary
For stable operations, you want to detect failure promptly and determine causes efficiently. With a distributed
system, it’s even more important to track the right items to meet a service-level target. Learning where these
logs are located in the file system or API gives you an advantage. This chapter also showed how to read,
interpret, and manipulate information from OpenStack services so that you can monitor effectively.
As an OpenStack cloud is composed of so many different services, there are a large number of log files. This
chapter aims to assist you in locating and working with them and describes other ways to track the status of
your deployment.
Backup and Recovery
Standard backup best practices apply when creating your OpenStack backup policy. For example, how often to
back up your data is closely related to how quickly you need to recover from data loss.
Note: If you cannot have any data loss at all, you should also focus on a highly available deployment. The
OpenStack High Availability Guide offers suggestions for elimination of a single point of failure that could
130 Backup and Recovery
Operations Guide (Release Version: 15.0.0)
cause system downtime. While it is not a completely prescriptive document, it offers methods and techniques
for avoiding downtime and data loss.
Other backup considerations include:
• How many backups to keep?
• Should backups be kept off-site?
• How often should backups be tested?
Just as important as a backup policy is a recovery policy (or at least recovery testing).
What to Back Up
While OpenStack is composed of many components and moving parts, backing up the critical data is quite
simple.
This chapter describes only how to back up configuration files and databases that the various OpenStack components
need to run. This chapter does not describe how to back up objects inside Object Storage or data contained
inside Block Storage. Generally these areas are left for users to back up on their own.
Database Backups
The example OpenStack architecture designates the cloud controller as the MySQL server. This MySQL server
hosts the databases for nova, glance, cinder, and keystone. With all of these databases in one place, it’s very
easy to create a database backup:
# mysqldump --opt --all-databases > openstack.sql
If you only want to backup a single database, you can instead run:
# mysqldump --opt nova > nova.sql
where nova is the database you want to back up.
You can easily automate this process by creating a cron job that runs the following script once per day:
#!/bin/bash
backup_dir="/var/lib/backups/mysql"
filename="${backup_dir}/mysql-`hostname`-`eval date +%Y%m%d`.sql.gz"
# Dump the entire MySQL database
/usr/bin/mysqldump --opt --all-databases | gzip > $filename
# Delete backups older than 7 days
find $backup_dir -ctime +7 -type f -delete
This script dumps the entire MySQL database and deletes any backups older than seven days.
File System Backups
This section discusses which files and directories should be backed up regularly, organized by service.
Backup and Recovery 131
Operations Guide (Release Version: 15.0.0)
Compute
The /etc/nova directory on both the cloud controller and compute nodes should be regularly backed up.
/var/log/nova does not need to be backed up if you have all logs going to a central area. It is highly recommended
to use a central logging server or back up the log directory.
/var/lib/nova is another important directory to back up. The exception to this is the /var/lib/nova/
instances subdirectory on compute nodes. This subdirectory contains the KVM images of running instances.
You would want to back up this directory only if you need to maintain backup copies of all instances. Under
most circumstances, you do not need to do this, but this can vary from cloud to cloud and your service levels.
Also be aware that making a backup of a live KVM instance can cause that instance to not boot properly if it is
ever restored from a backup.
Image Catalog and Delivery
/etc/glance and /var/log/glance follow the same rules as their nova counterparts.
/var/lib/glance should also be backed up. Take special notice of /var/lib/glance/images. If you are
using a file-based back end of glance, /var/lib/glance/images is where the images are stored and care
should be taken.
There are two ways to ensure stability with this directory. The first is to make sure this directory is run on a
RAID array. If a disk fails, the directory is available. The second way is to use a tool such as rsync to replicate
the images to another server:
# rsync -az --progress /var/lib/glance/images backup-server:/var/lib/glance/images/
Identity
/etc/keystone and /var/log/keystone follow the same rules as other components.
/var/lib/keystone, although it should not contain any data being used, can also be backed up just in case.
Block Storage
/etc/cinder and /var/log/cinder follow the same rules as other components.
/var/lib/cinder should also be backed up.
Networking
/etc/neutron and /var/log/neutron follow the same rules as other components.
/var/lib/neutron should also be backed up.
Object Storage
/etc/swift is very important to have backed up. This directory contains the swift configuration files as well
as the ring files and ring builder files, which if lost, render the data on your cluster inaccessible. A best practice
is to copy the builder files to all storage nodes along with the ring files. Multiple backup copies are spread
throughout your storage cluster.
132 Backup and Recovery
Operations Guide (Release Version: 15.0.0)
Telemetry
Back up the /etc/ceilometer directory containing Telemetry configuration files.
Orchestration
Back up HOT template yaml files, and the /etc/heat/ directory containing Orchestration configuration files.
Recovering Backups
Recovering backups is a fairly simple process. To begin, first ensure that the service you are recovering is not
running. For example, to do a full recovery of nova on the cloud controller, first stop all nova services:
# stop nova-api
# stop nova-consoleauth
# stop nova-novncproxy
# stop nova-objectstore
# stop nova-scheduler
Now you can import a previously backed-up database:
# mysql nova < nova.sql
You can also restore backed-up nova directories:
# mv /etc/nova{,.orig}
# cp -a /path/to/backup/nova /etc/
Once the files are restored, start everything back up:
# start mysql
# for i in nova-api nova-consoleauth nova-novncproxy \
nova-objectstore nova-scheduler
> do
> start $i
> done
Other services follow the same process, with their respective directories and databases.
Summary
Backup and subsequent recovery is one of the first tasks system administrators learn. However, each system
has different items that need attention. By taking care of your database, image service, and appropriate file
system locations, you can be assured that you can handle any event requiring recovery.
Customization
Provision an instance
To help understand how OpenStack works, this section describes the end-to-end process and interaction of
components when provisioning an instance on OpenStack.
Customization 133
Operations Guide (Release Version: 15.0.0)
Provision an instance
Create an OpenStack Development Environment
To create a development environment, you can use DevStack. DevStack is essentially a collection of shell
scripts and configuration files that builds an OpenStack development environment for you. You use it to create
such an environment for developing a new feature.
For more information on installing DevStack, see the DevStack website.
Customizing Object Storage (Swift) Middleware
OpenStack Object Storage, known as swift when reading the code, is based on the Python Paste framework.
The best introduction to its architecture is A Do-It-Yourself Framework. Because of the swift project’s use of
this framework, you are able to add features to a project by placing some custom code in a project’s pipeline
without having to change any of the core code.
Imagine a scenario where you have public access to one of your containers, but what you really want is to
restrict access to that to a set of IPs based on a whitelist. In this example, we’ll create a piece of middleware for
swift that allows access to a container from only a set of IP addresses, as determined by the container’s metadata
items. Only those IP addresses that you explicitly whitelist using the container’s metadata will be able to access
the container.
134 Customization
Operations Guide (Release Version: 15.0.0)
Warning: This example is for illustrative purposes only. It should not be used as a container IP whitelist
solution without further development and extensive security testing.
When you join the screen session that stack.sh starts with screen -r stack, you see a screen for each
service running, which can be a few or several, depending on how many services you configured DevStack to
run.
The asterisk * indicates which screen window you are viewing. This example shows we are viewing the key
(for keystone) screen window:
0$ shell 1$ key* 2$ horizon 3$ s-proxy 4$ s-object 5$ s-container 6$ s-account
The purpose of the screen windows are as follows:
shell A shell where you can get some work done
key* The keystone service
horizon The horizon dashboard web application
s-{name} The swift services
To create the middleware and plug it in through Paste configuration:
All of the code for OpenStack lives in /opt/stack. Go to the swift directory in the shell screen and edit your
middleware module.
1. Change to the directory where Object Storage is installed:
$ cd /opt/stack/swift
2. Create the ip_whitelist.py Python source code file:
$ vim swift/common/middleware/ip_whitelist.py
3. Copy the code as shown below into ip_whitelist.py. The following code is a middleware example
that restricts access to a container based on IP address as explained at the beginning of the section.
Middleware passes the request on to another application. This example uses the swift “swob” library
to wrap Web Server Gateway Interface (WSGI) requests and responses into objects for swift to interact
with. When you’re done, save and close the file.
# vim: tabstop=4 shiftwidth=4 softtabstop=4
# Copyright (c) 2014 OpenStack Foundation
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
Customization 135
Operations Guide (Release Version: 15.0.0)
import socket
from swift.common.utils import get_logger
from swift.proxy.controllers.base import get_container_info
from swift.common.swob import Request, Response
class IPWhitelistMiddleware(object):
"""
IP Whitelist Middleware
Middleware that allows access to a container from only a set of IP
addresses as determined by the container's metadata items that start
with the prefix 'allow'. E.G. allow-dev=192.168.0.20
"""
def __init__(self, app, conf, logger=None):
self.app = app
if logger:
self.logger = logger
else:
self.logger = get_logger(conf, log_route='ip_whitelist')
self.deny_message = conf.get('deny_message', "IP Denied")
self.local_ip = socket.gethostbyname(socket.gethostname())
def __call__(self, env, start_response):
"""
WSGI entry point.
Wraps env in swob.Request object and passes it down.
:param env: WSGI environment dictionary
:param start_response: WSGI callable
"""
req = Request(env)
try:
version, account, container, obj = req.split_path(1, 4, True)
except ValueError:
return self.app(env, start_response)
container_info = get_container_info(
req.environ, self.app, swift_source='IPWhitelistMiddleware')
remote_ip = env['REMOTE_ADDR']
self.logger.debug("Remote IP: %(remote_ip)s",
{'remote_ip': remote_ip})
meta = container_info['meta']
allow = {k:v for k,v in meta.iteritems() if k.startswith('allow')}
allow_ips = set(allow.values())
allow_ips.add(self.local_ip)
self.logger.debug("Allow IPs: %(allow_ips)s",
{'allow_ips': allow_ips})
if remote_ip in allow_ips:
return self.app(env, start_response)
136 Customization
Operations Guide (Release Version: 15.0.0)
else:
self.logger.debug(
"IP %(remote_ip)s denied access to Account=%(account)s "
"Container=%(container)s. Not in %(allow_ips)s", locals())
return Response(
status=403,
body=self.deny_message,
request=req)(env, start_response)
def filter_factory(global_conf, **local_conf):
"""
paste.deploy app factory for creating WSGI proxy apps.
"""
conf = global_conf.copy()
conf.update(local_conf)
def ip_whitelist(app):
return IPWhitelistMiddleware(app, conf)
return ip_whitelist
There is a lot of useful information in env and conf that you can use to decide what to do with the
request. To find out more about what properties are available, you can insert the following log statement
into the __init__ method:
self.logger.debug("conf = %(conf)s", locals())
and the following log statement into the __call__ method:
self.logger.debug("env = %(env)s", locals())
4. To plug this middleware into the swift Paste pipeline, you edit one configuration file, /etc/swift/
proxy-server.conf:
$ vim /etc/swift/proxy-server.conf
5. Find the [filter:ratelimit] section in /etc/swift/proxy-server.conf, and copy in the following
configuration section after it:
[filter:ip_whitelist]
paste.filter_factory = swift.common.middleware.ip_whitelist:filter_factory
# You can override the default log routing for this filter here:
# set log_name = ratelimit
# set log_facility = LOG_LOCAL0
# set log_level = INFO
# set log_headers = False
# set log_address = /dev/log
deny_message = You shall not pass!
6. Find the [pipeline:main] section in /etc/swift/proxy-server.conf, and add ip_whitelist
after ratelimit to the list like so. When you’re done, save and close the file:
[pipeline:main]
pipeline = catch_errors gatekeeper healthcheck proxy-logging cache bulk tempurl
,→ratelimit ip_whitelist ...
Customization 137
Operations Guide (Release Version: 15.0.0)
7. Restart the swift proxy service to make swift use your middleware. Start by switching to the swiftproxy
screen:
(a) Press Ctrl+A followed by 3.
(b) Press Ctrl+C to kill the service.
(c) Press Up Arrow to bring up the last command.
(d) Press Enter to run it.
8. Test your middleware with the swift CLI. Start by switching to the shell screen and finish by switching
back to the swift-proxy screen to check the log output:
(a) Press Ctrl+A followed by 0.
(b) Make sure you’re in the devstack directory:
$ cd /root/devstack
(c) Source openrc to set up your environment variables for the CLI:
$ . openrc
(d) Create a container called middleware-test:
$ swift post middleware-test
(e) Press Ctrl+A followed by 3 to check the log output.
9. Among the log statements you’ll see the lines:
proxy-server Remote IP: my.instance.ip.address (txn: ...)
proxy-server Allow IPs: set(['my.instance.ip.address']) (txn: ...)
These two statements are produced by our middleware and show that the request was sent from our
DevStack instance and was allowed.
10. Test the middleware from outside DevStack on a remote machine that has access to your DevStack
instance:
(a) Install the keystone and swift clients on your local machine:
# pip install python-keystoneclient python-swiftclient
(b) Attempt to list the objects in the middleware-test container:
$ swift --os-auth-url=http://my.instance.ip.address:5000/v2.0/ \
--os-region-name=RegionOne --os-username=demo:demo \
--os-password=devstack list middleware-test
Container GET failed: http://my.instance.ip.address:8080/v1/AUTH_.../
middleware-test?format=json 403 Forbidden You shall not pass!
11. Press Ctrl+A followed by 3 to check the log output. Look at the swift log statements again, and among
the log statements, you’ll see the lines:
proxy-server Authorizing from an overriding middleware (i.e: tempurl) (txn: ...)
proxy-server ... IPWhitelistMiddleware
proxy-server Remote IP: my.local.ip.address (txn: ...)
138 Customization
Operations Guide (Release Version: 15.0.0)
proxy-server Allow IPs: set(['my.instance.ip.address']) (txn: ...)
proxy-server IP my.local.ip.address denied access to Account=AUTH_... \
Container=None. Not in set(['my.instance.ip.address']) (txn: ...)
Here we can see that the request was denied because the remote IP address wasn’t in the set of allowed
IPs.
12. Back in your DevStack instance on the shell screen, add some metadata to your container to allow the
request from the remote machine:
(a) Press Ctrl+A followed by 0.
(b) Add metadata to the container to allow the IP:
$ swift post --meta allow-dev:my.local.ip.address middleware-test
(c) Now try the command from Step 10 again and it succeeds. There are no objects in the container,
so there is nothing to list; however, there is also no error to report.
Warning: Functional testing like this is not a replacement for proper unit and integration
testing, but it serves to get you started.
You can follow a similar pattern in other projects that use the Python Paste framework. Simply create a middleware
module and plug it in through configuration. The middleware runs in sequence as part of that project’s
pipeline and can call out to other services as necessary. No project core code is touched. Look for a pipeline
value in the project’s conf or ini configuration files in /etc/<project> to identify projects that use Paste.
When your middleware is done, we encourage you to open source it and let the community know on the OpenStack
mailing list. Perhaps others need the same functionality. They can use your code, provide feedback, and
possibly contribute. If enough support exists for it, perhaps you can propose that it be added to the official swift
middleware.
Customizing the OpenStack Compute (nova) Scheduler
Many OpenStack projects allow for customization of specific features using a driver architecture. You can
write a driver that conforms to a particular interface and plug it in through configuration. For example, you
can easily plug in a new scheduler for Compute. The existing schedulers for Compute are feature full and well
documented at Scheduling. However, depending on your user’s use cases, the existing schedulers might not
meet your requirements. You might need to create a new scheduler.
To create a scheduler, you must inherit from the class nova.scheduler.driver.Scheduler. Of the five
methods that you can override, you must override the two methods marked with an asterisk (*) below:
• update_service_capabilities
• hosts_up
• group_hosts
• * schedule_run_instance
• * select_destinations
To demonstrate customizing OpenStack, we’ll create an example of a Compute scheduler that randomly places
an instance on a subset of hosts, depending on the originating IP address of the request and the prefix of the
Customization 139
Operations Guide (Release Version: 15.0.0)
hostname. Such an example could be useful when you have a group of users on a subnet and you want all of
their instances to start within some subset of your hosts.
Warning: This example is for illustrative purposes only. It should not be used as a scheduler for Compute
without further development and testing.
When you join the screen session that stack.sh starts with screen -r stack, you are greeted with many
screen windows:
0$ shell* 1$ key 2$ horizon ... 9$ n-api ... 14$ n-sch ...
shell A shell where you can get some work done
key The keystone service
horizon The horizon dashboard web application
n-{name} The nova services
n-sch The nova scheduler service
To create the scheduler and plug it in through configuration
1. The code for OpenStack lives in /opt/stack, so go to the nova directory and edit your scheduler module.
Change to the directory where nova is installed:
$ cd /opt/stack/nova
2. Create the ip_scheduler.py Python source code file:
$ vim nova/scheduler/ip_scheduler.py
3. The code shown below is a driver that will schedule servers to hosts based on IP address as explained
at the beginning of the section. Copy the code into ip_scheduler.py. When you are done, save and
close the file.
# vim: tabstop=4 shiftwidth=4 softtabstop=4
# Copyright (c) 2014 OpenStack Foundation
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""
IP Scheduler implementation
"""
140 Customization
Operations Guide (Release Version: 15.0.0)
import random
from oslo_config import cfg
from nova.compute import rpcapi as compute_rpcapi
from nova import exception
from nova.openstack.common import log as logging
from nova.openstack.common.gettextutils import _
from nova.scheduler import driver
CONF = cfg.CONF
CONF.import_opt('compute_topic', 'nova.compute.rpcapi')
LOG = logging.getLogger(__name__)
class IPScheduler(driver.Scheduler):
"""
Implements Scheduler as a random node selector based on
IP address and hostname prefix.
"""
def __init__(self, *args, **kwargs):
super(IPScheduler, self).__init__(*args, **kwargs)
self.compute_rpcapi = compute_rpcapi.ComputeAPI()
def _filter_hosts(self, request_spec, hosts, filter_properties,
hostname_prefix):
"""Filter a list of hosts based on hostname prefix."""
hosts = [host for host in hosts if host.startswith(hostname_prefix)]
return hosts
def _schedule(self, context, topic, request_spec, filter_properties):
"""Picks a host that is up at random."""
elevated = context.elevated()
hosts = self.hosts_up(elevated, topic)
if not hosts:
msg = _("Is the appropriate service running?")
raise exception.NoValidHost(reason=msg)
remote_ip = context.remote_address
if remote_ip.startswith('10.1'):
hostname_prefix = 'doc'
elif remote_ip.startswith('10.2'):
hostname_prefix = 'ops'
else:
hostname_prefix = 'dev'
hosts = self._filter_hosts(request_spec, hosts, filter_properties,
hostname_prefix)
if not hosts:
msg = _("Could not find another compute")
raise exception.NoValidHost(reason=msg)
host = random.choice(hosts)
LOG.debug("Request from %(remote_ip)s scheduled to %(host)s" % locals())
Customization 141
Operations Guide (Release Version: 15.0.0)
return host
def select_destinations(self, context, request_spec, filter_properties):
"""Selects random destinations."""
num_instances = request_spec['num_instances']
# NOTE(timello): Returns a list of dicts with 'host', 'nodename' and
# 'limits' as keys for compatibility with filter_scheduler.
dests = []
for i in range(num_instances):
host = self._schedule(context, CONF.compute_topic,
request_spec, filter_properties)
host_state = dict(host=host, nodename=None, limits=None)
dests.append(host_state)
if len(dests) < num_instances:
raise exception.NoValidHost(reason='')
return dests
def schedule_run_instance(self, context, request_spec,
admin_password, injected_files,
requested_networks, is_first_time,
filter_properties, legacy_bdm_in_spec):
"""Create and run an instance or instances."""
instance_uuids = request_spec.get('instance_uuids')
for num, instance_uuid in enumerate(instance_uuids):
request_spec['instance_properties']['launch_index'] = num
try:
host = self._schedule(context, CONF.compute_topic,
request_spec, filter_properties)
updated_instance = driver.instance_update_db(context,
instance_uuid)
self.compute_rpcapi.run_instance(context,
instance=updated_instance, host=host,
requested_networks=requested_networks,
injected_files=injected_files,
admin_password=admin_password,
is_first_time=is_first_time,
request_spec=request_spec,
filter_properties=filter_properties,
legacy_bdm_in_spec=legacy_bdm_in_spec)
except Exception as ex:
# NOTE(vish): we don't reraise the exception here to make sure
# that all instances in the request get set to
# error properly
driver.handle_schedule_error(context, ex, instance_uuid,
request_spec)
There is a lot of useful information in context, request_spec, and filter_properties that you
can use to decide where to schedule the instance. To find out more about what properties are available,
you can insert the following log statements into the schedule_run_instance method of the scheduler
above:
LOG.debug("context = %(context)s" % {'context': context.__dict__})
LOG.debug("request_spec = %(request_spec)s" % locals())
LOG.debug("filter_properties = %(filter_properties)s" % locals())
142 Customization
Operations Guide (Release Version: 15.0.0)
4. To plug this scheduler into nova, edit one configuration file, /etc/nova/nova.conf:
$ vim /etc/nova/nova.conf
5. Find the scheduler_driver config and change it like so:
scheduler_driver=nova.scheduler.ip_scheduler.IPScheduler
6. Restart the nova scheduler service to make nova use your scheduler. Start by switching to the n-sch
screen:
(a) Press Ctrl+A followed by 9.
(b) Press Ctrl+A followed by N until you reach the n-sch screen.
(c) Press Ctrl+C to kill the service.
(d) Press Up Arrow to bring up the last command.
(e) Press Enter to run it.
7. Test your scheduler with the nova CLI. Start by switching to the shell screen and finish by switching
back to the n-sch screen to check the log output:
(a) Press Ctrl+A followed by 0.
(b) Make sure you are in the devstack directory:
$ cd /root/devstack
(c) Source openrc to set up your environment variables for the CLI:
$ . openrc
(d) Put the image ID for the only installed image into an environment variable:
$ IMAGE_ID=`openstack image list | egrep cirros | egrep -v "kernel|ramdisk" | awk
,→'{print $2}'`
(e) Boot a test server:
$ openstack server create --flavor 1 --image $IMAGE_ID scheduler-test
8. Switch back to the n-sch screen. Among the log statements, you’ll see the line:
2014-01-23 19:57:47.262 DEBUG nova.scheduler.ip_scheduler
[req-... demo demo] Request from xx.xx.xx.xx scheduled to devstack-havana
_schedule /opt/stack/nova/nova/scheduler/ip_scheduler.py:76
Warning: Functional testing like this is not a replacement for proper unit and integration testing, but it
serves to get you started.
A similar pattern can be followed in other projects that use the driver architecture. Simply create a module and
class that conform to the driver interface and plug it in through configuration. Your code runs when that feature
is used and can call out to other services as necessary. No project core code is touched. Look for a “driver” value
in the project’s .conf configuration files in /etc/<project> to identify projects that use a driver architecture.
Customization 143
Operations Guide (Release Version: 15.0.0)
When your scheduler is done, we encourage you to open source it and let the community know on the OpenStack
mailing list. Perhaps others need the same functionality. They can use your code, provide feedback, and possibly
contribute. If enough support exists for it, perhaps you can propose that it be added to the official Compute
schedulers.
Customizing the Dashboard (Horizon)
The dashboard is based on the Python Django web application framework. To know how to build your Dashboard,
see Building a Dashboard using Horizon.
Conclusion
When operating an OpenStack cloud, you may discover that your users can be quite demanding. If OpenStack
doesn’t do what your users need, it may be up to you to fulfill those requirements. This chapter provided you
with some options for customization and gave you the tools you need to get started.
OpenStack might not do everything you need it to do out of the box. To add a new feature, you can follow
different paths.
To take the first path, you can modify the OpenStack code directly. Learn how to contribute, follow the Developer’s
Guide, make your changes, and contribute them back to the upstream OpenStack project. This path
is recommended if the feature you need requires deep integration with an existing project. The community is
always open to contributions and welcomes new functionality that follows the feature-development guidelines.
This path still requires you to use DevStack for testing your feature additions, so this chapter walks you through
the DevStack environment.
For the second path, you can write new features and plug them in using changes to a configuration file. If the
project where your feature would need to reside uses the Python Paste framework, you can create middleware
for it and plug it in through configuration. There may also be specific ways of customizing a project, such as
creating a new scheduler driver for Compute or a custom tab for the dashboard.
This chapter focuses on the second path for customizing OpenStack by providing two examples for writing
new features. The first example shows how to modify Object Storage service (swift) middleware to add a new
feature, and the second example provides a new scheduler feature for Compute service (nova). To customize
OpenStack this way you need a development environment. The best way to get an environment up and running
quickly is to run DevStack within your cloud.
Advanced Configuration
OpenStack is intended to work well across a variety of installation flavors, from very small private clouds
to large public clouds. To achieve this, the developers add configuration options to their code that allow the
behavior of the various components to be tweaked depending on your needs. Unfortunately, it is not possible
to cover all possible deployments with the default configuration values.
At the time of writing, OpenStack has more than 3,000 configuration options. You can see them documented
at the OpenStack Configuration Reference. This chapter cannot hope to document all of these, but we do try to
introduce the important concepts so that you know where to go digging for more information.
144 Advanced Configuration
Operations Guide (Release Version: 15.0.0)
Differences Between Various Drivers
Many OpenStack projects implement a driver layer, and each of these drivers will implement its own configuration
options. For example, in OpenStack Compute (nova), there are various hypervisor drivers implemented—
libvirt, xenserver, hyper-v, and vmware, for example. Not all of these hypervisor drivers have the same features,
and each has different tuning requirements.
Note: The currently implemented hypervisors are listed on the OpenStack Configuration Reference. You can
see a matrix of the various features in OpenStack Compute (nova) hypervisor drivers at the Hypervisor support
matrix page.
The point we are trying to make here is that just because an option exists doesn’t mean that option is relevant
to your driver choices. Normally, the documentation notes which drivers the configuration applies to.
Implementing Periodic Tasks
Another common concept across various OpenStack projects is that of periodic tasks. Periodic tasks are much
like cron jobs on traditional Unix systems, but they are run inside an OpenStack process. For example, when
OpenStack Compute (nova) needs to work out what images it can remove from its local cache, it runs a periodic
task to do this.
Periodic tasks are important to understand because of limitations in the threading model that OpenStack uses.
OpenStack uses cooperative threading in Python, which means that if something long and complicated is running,
it will block other tasks inside that process from running unless it voluntarily yields execution to another
cooperative thread.
A tangible example of this is the nova-compute process. In order to manage the image cache with libvirt,
nova-compute has a periodic process that scans the contents of the image cache. Part of this scan is calculating
a checksum for each of the images and making sure that checksum matches what nova-compute expects it to
be. However, images can be very large, and these checksums can take a long time to generate. At one point,
before it was reported as a bug and fixed, nova-compute would block on this task and stop responding to RPC
requests. This was visible to users as failure of operations such as spawning or deleting instances.
The take away from this is if you observe an OpenStack process that appears to “stop” for a while and then
continue to process normally, you should check that periodic tasks aren’t the problem. One way to do this is
to disable the periodic tasks by setting their interval to zero. Additionally, you can configure how often these
periodic tasks run—in some cases, it might make sense to run them at a different frequency from the default.
The frequency is defined separately for each periodic task. Therefore, to disable every periodic task in OpenStack
Compute (nova), you would need to set a number of configuration options to zero. The current list of
configuration options you would need to set to zero are:
• bandwidth_poll_interval
• sync_power_state_interval
• heal_instance_info_cache_interval
• host_state_interval
• image_cache_manager_interval
• reclaim_instance_interval
• volume_usage_poll_interval
Advanced Configuration 145
Operations Guide (Release Version: 15.0.0)
• shelved_poll_interval
• shelved_offload_time
• instance_delete_interval
To set a configuration option to zero, include a line such as image_cache_manager_interval=0 in your
nova.conf file.
This list will change between releases, so please refer to your configuration guide for up-to-date information.
Specific Configuration Topics
This section covers specific examples of configuration options you might consider tuning. It is by no means an
exhaustive list.
Security Configuration for Compute, Networking, and Storage
The OpenStack Security Guide provides a deep dive into securing an OpenStack cloud, including SSL/TLS,
key management, PKI and certificate management, data transport and privacy concerns, and compliance.
High Availability
The OpenStack High Availability Guide offers suggestions for elimination of a single point of failure that could
cause system downtime. While it is not a completely prescriptive document, it offers methods and techniques
for avoiding downtime and data loss.
Enabling IPv6 Support
You can follow the progress being made on IPV6 support by watching the neutron IPv6 Subteam at work.
By modifying your configuration setup, you can set up IPv6 when using nova-network for networking, and a
tested setup is documented for FlatDHCP and a multi-host configuration. The key is to make nova-network
think a radvd command ran successfully. The entire configuration is detailed in a Cybera blog post, “An IPv6
enabled cloud”.
Geographical Considerations for Object Storage
Support for global clustering of object storage servers is available for all supported releases. You would implement
these global clusters to ensure replication across geographic areas in case of a natural disaster and also to
ensure that users can write or access their objects more quickly based on the closest data center. You configure a
default region with one zone for each cluster, but be sure your network (WAN) can handle the additional request
and response load between zones as you add more zones and build a ring that handles more zones. Refer to
Geographically Distributed Clusters in the documentation for additional information.
Upgrades
With the exception of Object Storage, upgrading from one version of OpenStack to another can take a great
deal of effort. This chapter provides some guidance on the operational aspects that you should consider for
performing an upgrade for an OpenStack environment.
146 Upgrades
Operations Guide (Release Version: 15.0.0)
Pre-upgrade considerations
Upgrade planning
• Thoroughly review the release notes to learn about new, updated, and deprecated features. Find incompatibilities
between versions.
• Consider the impact of an upgrade to users. The upgrade process interrupts management of your environment
including the dashboard. If you properly prepare for the upgrade, existing instances, networking,
and storage should continue to operate. However, instances might experience intermittent network interruptions.
• Consider the approach to upgrading your environment. You can perform an upgrade with operational
instances, but this is a dangerous approach. You might consider using live migration to temporarily relocate
instances to other compute nodes while performing upgrades. However, you must ensure database
consistency throughout the process; otherwise your environment might become unstable. Also, don’t
forget to provide sufficient notice to your users, including giving them plenty of time to perform their
own backups.
• Consider adopting structure and options from the service configuration files and merging them with existing
configuration files. The OpenStack Configuration Reference contains new, updated, and deprecated
options for most services.
• Like all major system upgrades, your upgrade could fail for one or more reasons. You can prepare for this
situation by having the ability to roll back your environment to the previous release, including databases,
configuration files, and packages. We provide an example process for rolling back your environment in
Rolling back a failed upgrade.
• Develop an upgrade procedure and assess it thoroughly by using a test environment similar to your
production environment.
Pre-upgrade testing environment
The most important step is the pre-upgrade testing. If you are upgrading immediately after release of a new
version, undiscovered bugs might hinder your progress. Some deployers prefer to wait until the first point
release is announced. However, if you have a significant deployment, you might follow the development and
testing of the release to ensure that bugs for your use cases are fixed.
Each OpenStack cloud is different even if you have a near-identical architecture as described in this guide. As
a result, you must still test upgrades between versions in your environment using an approximate clone of your
environment.
However, that is not to say that it needs to be the same size or use identical hardware as the production environment.
It is important to consider the hardware and scale of the cloud that you are upgrading. The following
tips can help you minimise the cost:
Use your own cloud The simplest place to start testing the next version of OpenStack is by setting up a new
environment inside your own cloud. This might seem odd, especially the double virtualization used in
running compute nodes. But it is a sure way to very quickly test your configuration.
Use a public cloud Consider using a public cloud to test the scalability limits of your cloud controller configuration.
Most public clouds bill by the hour, which means it can be inexpensive to perform even a test
with many nodes.
Upgrades 147
Operations Guide (Release Version: 15.0.0)
Make another storage endpoint on the same system If you use an external storage plug-in or shared file system
with your cloud, you can test whether it works by creating a second share or endpoint. This allows
you to test the system before entrusting the new version on to your storage.
Watch the network Even at smaller-scale testing, look for excess network packets to determine whether something
is going horribly wrong in inter-component communication.
To set up the test environment, you can use one of several methods:
• Do a full manual install by using the Installation Tutorials and Guides for your platform. Review the
final configuration files and installed packages.
• Create a clone of your automated configuration infrastructure with changed package repository URLs.
Alter the configuration until it works.
Either approach is valid. Use the approach that matches your experience.
An upgrade pre-testing system is excellent for getting the configuration to work. However, it is important to
note that the historical use of the system and differences in user interaction can affect the success of upgrades.
If possible, we highly recommend that you dump your production database tables and test the upgrade in your
development environment using this data. Several MySQL bugs have been uncovered during database migrations
because of slight table differences between a fresh installation and tables that migrated from one version
to another. This will have impact on large real datasets, which you do not want to encounter during a production
outage.
Artificial scale testing can go only so far. After your cloud is upgraded, you must pay careful attention to the
performance aspects of your cloud.
Upgrade Levels
Upgrade levels are a feature added to OpenStack Compute since the Grizzly release to provide version locking
on the RPC (Message Queue) communications between the various Compute services.
This functionality is an important piece of the puzzle when it comes to live upgrades and is conceptually similar
to the existing API versioning that allows OpenStack services of different versions to communicate without
issue.
Without upgrade levels, an X+1 version Compute service can receive and understand X version RPC messages,
but it can only send out X+1 version RPC messages. For example, if a nova-conductor process has been
upgraded to X+1 version, then the conductor service will be able to understand messages from X version novacompute
processes, but those compute services will not be able to understand messages sent by the conductor
service.
During an upgrade, operators can add configuration options to nova.conf which lock the version of RPC
messages and allow live upgrading of the services without interruption caused by version mismatch. The configuration
options allow the specification of RPC version numbers if desired, but release name alias are also
supported. For example:
[upgrade_levels]
compute=X+1
conductor=X+1
scheduler=X+1
148 Upgrades
Operations Guide (Release Version: 15.0.0)
will keep the RPC version locked across the specified services to the RPC version used in X+1. As all instances
of a particular service are upgraded to the newer version, the corresponding line can be removed from nova.
conf.
Using this functionality, ideally one would lock the RPC version to the OpenStack version being upgraded
from on nova-compute nodes, to ensure that, for example X+1 version nova-compute processes will continue
to work with X version nova-conductor processes while the upgrade completes. Once the upgrade of novacompute
processes is complete, the operator can move onto upgrading nova-conductor and remove the version
locking for nova-compute in nova.conf.
Upgrade process
This section describes the process to upgrade a basic OpenStack deployment based on the basic two-node
architecture in the Installation Tutorials and Guides. All nodes must run a supported distribution of Linux with
a recent kernel and the current release packages.
Service specific upgrade instructions
Refer to the following upgrade notes for information on upgrading specific OpenStack services:
• Networking service (neutron) upgrades
• Compute service (nova) upgrades
• Identity service (keystone) upgrades
• Block Storage service (cinder) upgrades
• Image service (glance) zero downtime database upgrades
• Image service (glance) rolling upgrades
• Bare Metal service (ironic) upgrades
• Object Storage service (swift) upgrades
• Telemetry service (ceilometer) upgrades
Prerequisites
• Perform some cleaning of the environment prior to starting the upgrade process to ensure a consistent
state. For example, instances not fully purged from the system after deletion might cause indeterminate
behavior.
• For environments using the OpenStack Networking service (neutron), verify the release version of the
database. For example:
# su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf \
--config-file /etc/neutron/plugins/ml2/ml2_conf.ini current" neutron
Perform a backup
1. Save the configuration files on all nodes. For example:
Upgrades 149
Operations Guide (Release Version: 15.0.0)
# for i in keystone glance nova neutron openstack-dashboard cinder heat ceilometer; \
do mkdir $i-RELEASE_NAME; \
done
# for i in keystone glance nova neutron openstack-dashboard cinder heat ceilometer; \
do cp -r /etc/$i/* $i-RELEASE_NAME/; \
done
Note: You can modify this example script on each node to handle different services.
2. Make a full database backup of your production data. Since the Kilo release, database downgrades are
not supported, and restoring from backup is the only method available to retrieve a previous database
version.
# mysqldump -u root -p --opt --add-drop-database --all-databases > RELEASE_NAME-db-
,→backup.sql
Note: Consider updating your SQL server configuration as described in the Installation Tutorials and
Guides.
Manage repositories
On all nodes:
1. Remove the repository for the previous release packages.
2. Add the repository for the new release packages.
3. Update the repository database.
Upgrade packages on each node
Depending on your specific configuration, upgrading all packages might restart or break services supplemental
to your OpenStack environment. For example, if you use the TGT iSCSI framework for Block Storage volumes
and the upgrade includes new packages for it, the package manager might restart the TGT iSCSI services and
impact connectivity to volumes.
If the package manager prompts you to update configuration files, reject the changes. The package manager
appends a suffix to newer versions of configuration files. Consider reviewing and adopting content from these
files.
Note: You may need to explicitly install the ipset package if your distribution does not install it as a dependency.
Update services
To update a service on each node, you generally modify one or more configuration files, stop the service,
synchronize the database schema, and start the service. Some services require different steps. We recommend
150 Upgrades
Operations Guide (Release Version: 15.0.0)
verifying operation of each service before proceeding to the next service.
The order you should upgrade services, and any changes from the general upgrade process is described below:
Controller node
1. Identity service - Clear any expired tokens before synchronizing the database.
2. Image service
3. Compute service, including networking components.
4. Networking service
5. Block Storage service
6. Dashboard - In typical environments, updating Dashboard only requires restarting the Apache HTTP
service.
7. Orchestration service
8. Telemetry service - In typical environments, updating the Telemetry service only requires restarting the
service.
9. Compute service - Edit the configuration file and restart the service.
10. Networking service - Edit the configuration file and restart the service.
Storage nodes
• Block Storage service - Updating the Block Storage service only requires restarting the service.
Compute nodes
• Networking service - Edit the configuration file and restart the service.
Final steps
On all distributions, you must perform some final tasks to complete the upgrade process.
1. Decrease DHCP timeouts by modifying the /etc/nova/nova.conf file on the compute nodes back to
the original value for your environment.
2. Update all .ini files to match passwords and pipelines as required for the OpenStack release in your
environment.
3. After migration, users see different results from openstack image list and glance image-list.
To ensure users see the same images in the list commands, edit the /etc/glance/policy.json file
and /etc/nova/policy.json file to contain "context_is_admin": "role:admin", which limits
access to private images for projects.
4. Verify proper operation of your environment. Then, notify your users that their cloud is operating normally
again.
Rolling back a failed upgrade
This section provides guidance for rolling back to a previous release of OpenStack. All distributions follow a
similar procedure.
Upgrades 151
Operations Guide (Release Version: 15.0.0)
Warning: Rolling back your environment should be the final course of action since you are likely to lose
any data added since the backup.
A common scenario is to take down production management services in preparation for an upgrade, completed
part of the upgrade process, and discovered one or more problems not encountered during testing. As a consequence,
you must roll back your environment to the original “known good” state. You also made sure that
you did not make any state changes after attempting the upgrade process; no new instances, networks, storage
volumes, and so on. Any of these new resources will be in a frozen state after the databases are restored from
backup.
Within this scope, you must complete these steps to successfully roll back your environment:
1. Roll back configuration files.
2. Restore databases from backup.
3. Roll back packages.
You should verify that you have the requisite backups to restore. Rolling back upgrades is a tricky process
because distributions tend to put much more effort into testing upgrades than downgrades. Broken downgrades
take significantly more effort to troubleshoot and, resolve than broken upgrades. Only you can weigh the risks
of trying to push a failed upgrade forward versus rolling it back. Generally, consider rolling back as the very
last option.
The following steps described for Ubuntu have worked on at least one production environment, but they might
not work for all environments.
To perform a rollback
1. Stop all OpenStack services.
2. Copy contents of configuration backup directories that you created during the upgrade process back to
/etc/<service> directory.
3. Restore databases from the RELEASE_NAME-db-backup.sql backup file that you created with the
mysqldump command during the upgrade process:
# mysql -u root -p < RELEASE_NAME-db-backup.sql
4. Downgrade OpenStack packages.
Warning: Downgrading packages is by far the most complicated step; it is highly dependent on the
distribution and the overall administration of the system.
(a) Determine which OpenStack packages are installed on your system. Use the dpkg --getselections
command. Filter for OpenStack packages, filter again to omit packages explicitly
marked in the deinstall state, and save the final output to a file. For example, the following
command covers a controller node with keystone, glance, nova, neutron, and cinder:
# dpkg --get-selections | grep -e keystone -e glance -e nova -e neutron \
-e cinder | grep -v deinstall | tee openstack-selections
cinder-api install
cinder-common install
cinder-scheduler install
cinder-volume install
152 Upgrades
Operations Guide (Release Version: 15.0.0)
glance install
glance-api install
glance-common install
glance-registry install
neutron-common install
neutron-dhcp-agent install
neutron-l3-agent install
neutron-lbaas-agent install
neutron-metadata-agent install
neutron-plugin-openvswitch install
neutron-plugin-openvswitch-agent install
neutron-server install
nova-api install
nova-common install
nova-conductor install
nova-consoleauth install
nova-novncproxy install
nova-objectstore install
nova-scheduler install
python-cinder install
python-cinderclient install
python-glance install
python-glanceclient install
python-keystone install
python-keystoneclient install
python-neutron install
python-neutronclient install
python-nova install
python-novaclient install
Note: Depending on the type of server, the contents and order of your package list might vary
from this example.
(b) You can determine the package versions available for reversion by using the apt-cache policy
command. For example:
# apt-cache policy nova-common
nova-common:
Installed: 2:14.0.1-0ubuntu1~cloud0
Candidate: 2:14.0.1-0ubuntu1~cloud0
Version table:
*** 2:14.0.1-0ubuntu1~cloud0 500
500 http://ubuntu-cloud.archive.canonical.com/ubuntu xenial-updates/newton/
,→main amd64 Packages
100 /var/lib/dpkg/status
2:13.1.2-0ubuntu2 500
500 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages
2:13.0.0-0ubuntu2 500
500 http://archive.ubuntu.com/ubuntu xenial/main amd64 Packages
Note: If you removed the release repositories, you must first reinstall them and run the apt-get
Upgrades 153
Operations Guide (Release Version: 15.0.0)
update command.
The command output lists the currently installed version of the package, newest candidate version,
and all versions along with the repository that contains each version. Look for the appropriate
release version— 2:14.0.1-0ubuntu1~cloud0 in this case. The process of manually picking
through this list of packages is rather tedious and prone to errors. You should consider using a
script to help with this process. For example:
# for i in `cut -f 1 openstack-selections | sed 's/neutron/;'`;
do echo -n $i ;apt-cache policy $i | grep -B 1 RELEASE_NAME |
grep -v Packages | awk '{print "="$1}';done | tr '\n' ' ' |
tee openstack-RELEASE_NAME-versions
cinder-api=2:9.0.0-0ubuntu1~cloud0
cinder-common=2:9.0.0-0ubuntu1~cloud0
cinder-scheduler=2:9.0.0-0ubuntu1~cloud0
cinder-volume=2:9.0.0-0ubuntu1~cloud0
glance=2:13.0.0-0ubuntu1~cloud0
glance-api=2:13.0.0-0ubuntu1~cloud0 500
glance-common=2:13.0.0-0ubuntu1~cloud0 500
glance-registry=2:13.0.0-0ubuntu1~cloud0 500
neutron-common=2:9.0.0-0ubuntu1~cloud0
neutron-dhcp-agent=2:9.0.0-0ubuntu1~cloud0
neutron-l3-agent=2:9.0.0-0ubuntu1~cloud0
neutron-lbaas-agent=2:9.0.0-0ubuntu1~cloud0
neutron-metadata-agent=2:9.0.0-0ubuntu1~cloud0
neutron-server=2:9.0.0-0ubuntu1~cloud0
nova-api=2:14.0.1-0ubuntu1~cloud0
nova-common=2:14.0.1-0ubuntu1~cloud0
nova-conductor=2:14.0.1-0ubuntu1~cloud0
nova-consoleauth=2:14.0.1-0ubuntu1~cloud0
nova-novncproxy=2:14.0.1-0ubuntu1~cloud0
nova-objectstore=2:14.0.1-0ubuntu1~cloud0
nova-scheduler=2:14.0.1-0ubuntu1~cloud0
python-cinder=2:9.0.0-0ubuntu1~cloud0
python-cinderclient=1:1.9.0-0ubuntu1~cloud0
python-glance=2:13.0.0-0ubuntu1~cloud0
python-glanceclient=1:2.5.0-0ubuntu1~cloud0
python-neutron=2:9.0.0-0ubuntu1~cloud0
python-neutronclient=1:6.0.0-0ubuntu1~cloud0
python-nova=2:14.0.1-0ubuntu1~cloud0
python-novaclient=2:6.0.0-0ubuntu1~cloud0
python-openstackclient=3.2.0-0ubuntu2~cloud0
(c) Use the apt-get install command to install specific versions of each package by specifying
<package-name>=<version>. The script in the previous step conveniently created a list of package=version
pairs for you:
# apt-get install `cat openstack-RELEASE_NAME-versions`
This step completes the rollback procedure. You should remove the upgrade release repository and
run apt-get update to prevent accidental upgrades until you solve whatever issue caused you to
roll back your environment.
154 Upgrades
Operations Guide (Release Version: 15.0.0)
Appendix
Use Cases
This appendix contains a small selection of use cases from the community, with more technical detail than usual.
Further examples can be found on the OpenStack website.
NeCTAR
Who uses it: researchers from the Australian publicly funded research sector. Use is across a wide variety of
disciplines, with the purpose of instances ranging from running simple web servers to using hundreds of cores
for high-throughput computing.
Deployment
Using OpenStack Compute cells, the NeCTAR Research Cloud spans eight sites with approximately 4,000
cores per site.
Each site runs a different configuration, as a resource cells in an OpenStack Compute cells setup. Some sites
span multiple data centers, some use off compute node storage with a shared file system, and some use on
compute node storage with a non-shared file system. Each site deploys the Image service with an Object
Storage back end. A central Identity, dashboard, and Compute API service are used. A login to the dashboard
triggers a SAML login with Shibboleth, which creates an account in the Identity service with an SQL back end.
An Object Storage Global Cluster is used across several sites.
Compute nodes have 24 to 48 cores, with at least 4 GB of RAM per core and approximately 40 GB of ephemeral
storage per core.
All sites are based on Ubuntu 14.04, with KVM as the hypervisor. The OpenStack version in use is typically
the current stable version, with 5 to 10 percent back-ported code from trunk and modifications.
Resources
• OpenStack.org case study
• NeCTAR-RC GitHub
• NeCTAR website
MIT CSAIL
Who uses it: researchers from the MIT Computer Science and Artificial Intelligence Lab.
Deployment
The CSAIL cloud is currently 64 physical nodes with a total of 768 physical cores and 3,456 GB of RAM.
Persistent data storage is largely outside the cloud on NFS, with cloud resources focused on compute resources.
There are more than 130 users in more than 40 projects, typically running 2,000–2,500 vCPUs in 300 to 400
instances.
Appendix 155
Operations Guide (Release Version: 15.0.0)
We initially deployed on Ubuntu 12.04 with the Essex release of OpenStack using FlatDHCP multi-host networking.
The software stack is still Ubuntu 12.04 LTS, but now with OpenStack Havana from the Ubuntu Cloud Archive.
KVM is the hypervisor, deployed using FAI and Puppet for configuration management. The FAI and Puppet
combination is used lab-wide, not only for OpenStack. There is a single cloud controller node, which also acts
as network controller, with the remainder of the server hardware dedicated to compute nodes.
Host aggregates and instance-type extra specs are used to provide two different resource allocation ratios. The
default resource allocation ratios we use are 4:1 CPU and 1.5:1 RAM. Compute-intensive workloads use instance
types that require non-oversubscribed hosts where cpu_ratio and ram_ratio are both set to 1.0. Since
we have hyper-threading enabled on our compute nodes, this provides one vCPU per CPU thread, or two vCPUs
per physical core.
With our upgrade to Grizzly in August 2013, we moved to OpenStack Networking, neutron (quantum at the
time). Compute nodes have two-gigabit network interfaces and a separate management card for IPMI management.
One network interface is used for node-to-node communications. The other is used as a trunk port
for OpenStack managed VLANs. The controller node uses two bonded 10g network interfaces for its public IP
communications. Big pipes are used here because images are served over this port, and it is also used to connect
to iSCSI storage, back-ending the image storage and database. The controller node also has a gigabit interface
that is used in trunk mode for OpenStack managed VLAN traffic. This port handles traffic to the dhcp-agent
and metadata-proxy.
We approximate the older nova-network multi-host HA setup by using “provider VLAN networks” that connect
instances directly to existing publicly addressable networks and use existing physical routers as their default
gateway. This means that if our network controller goes down, running instances still have their network
available, and no single Linux host becomes a traffic bottleneck. We are able to do this because we have a
sufficient supply of IPv4 addresses to cover all of our instances and thus don’t need NAT and don’t use floating
IP addresses. We provide a single generic public network to all projects and additional existing VLANs on a
project-by-project basis as needed. Individual projects are also allowed to create their own private GRE based
networks.
Resources
• CSAIL homepage
DAIR
Who uses it: DAIR is an integrated virtual environment that leverages the CANARIE network to develop and
test new information communication technology (ICT) and other digital technologies. It combines such digital
infrastructure as advanced networking and cloud computing and storage to create an environment for developing
and testing innovative ICT applications, protocols, and services; performing at-scale experimentation for
deployment; and facilitating a faster time to market.
Deployment
DAIR is hosted at two different data centers across Canada: one in Alberta and the other in Quebec. It consists
of a cloud controller at each location, although, one is designated the “master” controller that is in charge of
central authentication and quotas. This is done through custom scripts and light modifications to OpenStack.
DAIR is currently running Havana.
156 Appendix
Operations Guide (Release Version: 15.0.0)
For Object Storage, each region has a swift environment.
A NetApp appliance is used in each region for both block storage and instance storage. There are future plans
to move the instances off the NetApp appliance and onto a distributed file system such as Ceph or GlusterFS.
VlanManager is used extensively for network management. All servers have two bonded 10GbE NICs that are
connected to two redundant switches. DAIR is set up to use single-node networking where the cloud controller
is the gateway for all instances on all compute nodes. Internal OpenStack traffic (for example, storage traffic)
does not go through the cloud controller.
Resources
• DAIR homepage
CERN
Who uses it: researchers at CERN (European Organization for Nuclear Research) conducting high-energy
physics research.
Deployment
The environment is largely based on Scientific Linux 6, which is Red Hat compatible. We use KVM as our
primary hypervisor, although tests are ongoing with Hyper-V on Windows Server 2008.
We use the Puppet Labs OpenStack modules to configure Compute, Image service, Identity, and dashboard.
Puppet is used widely for instance configuration, and Foreman is used as a GUI for reporting and instance
provisioning.
Users and groups are managed through Active Directory and imported into the Identity service using
LDAP. CLIs are available for nova and Euca2ools to do this.
There are three clouds currently running at CERN, totaling about 4,700 compute nodes, with approximately
120,000 cores. The CERN IT cloud aims to expand to 300,000 cores by 2015.
Resources
• “OpenStack in Production: A tale of 3 OpenStack Clouds”
• “Review of CERN Data Centre Infrastructure”
• “CERN Cloud Infrastructure User Guide”
Tales From the Cryp^H^H^H^H Cloud
Herein lies a selection of tales from OpenStack cloud operators. Read, and learn from their wisdom.
Appendix 157
Operations Guide (Release Version: 15.0.0)
Double VLAN
I was on-site in Kelowna, British Columbia, Canada setting up a new OpenStack cloud. The deployment was
fully automated: Cobbler deployed the OS on the bare metal, bootstrapped it, and Puppet took over from there.
I had run the deployment scenario so many times in practice and took for granted that everything was working.
On my last day in Kelowna, I was in a conference call from my hotel. In the background, I was fooling around
on the new cloud. I launched an instance and logged in. Everything looked fine. Out of boredom, I ran ps aux
and all of the sudden the instance locked up.
Thinking it was just a one-off issue, I terminated the instance and launched a new one. By then, the conference
call ended and I was off to the data center.
At the data center, I was finishing up some tasks and remembered the lock-up. I logged into the new instance
and ran ps aux again. It worked. Phew. I decided to run it one more time. It locked up.
After reproducing the problem several times, I came to the unfortunate conclusion that this cloud did indeed
have a problem. Even worse, my time was up in Kelowna and I had to return back to Calgary.
Where do you even begin troubleshooting something like this? An instance that just randomly locks up when
a command is issued. Is it the image? Nope—it happens on all images. Is it the compute node? Nope—all
nodes. Is the instance locked up? No! New SSH connections work just fine!
We reached out for help. A networking engineer suggested it was an MTU issue. Great! MTU! Something to
go on! What’s MTU and why would it cause a problem?
MTU is maximum transmission unit. It specifies the maximum number of bytes that the interface accepts
for each packet. If two interfaces have two different MTUs, bytes might get chopped off and weird things
happen—such as random session lockups.
Note: Not all packets have a size of 1500. Running the ls command over SSH might only create a single
packets less than 1500 bytes. However, running a command with heavy output, such as ps aux requires several
packets of 1500 bytes.
OK, so where is the MTU issue coming from? Why haven’t we seen this in any other deployment? What’s new
in this situation? Well, new data center, new uplink, new switches, new model of switches, new servers, first
time using this model of servers… so, basically everything was new. Wonderful. We toyed around with raising
the MTU at various areas: the switches, the NICs on the compute nodes, the virtual NICs in the instances, we
even had the data center raise the MTU for our uplink interface. Some changes worked, some didn’t. This line
of troubleshooting didn’t feel right, though. We shouldn’t have to be changing the MTU in these areas.
As a last resort, our network admin (Alvaro) and myself sat down with four terminal windows, a pencil, and a
piece of paper. In one window, we ran ping. In the second window, we ran tcpdump on the cloud controller.
In the third, tcpdump on the compute node. And the forth had tcpdump on the instance. For background, this
cloud was a multi-node, non-multi-host setup.
One cloud controller acted as a gateway to all compute nodes. VlanManager was used for the network config.
This means that the cloud controller and all compute nodes had a different VLAN for each OpenStack project.
We used the -s option of ping to change the packet size. We watched as sometimes packets would fully return,
sometimes they’d only make it out and never back in, and sometimes the packets would stop at a random point.
We changed tcpdump to start displaying the hex dump of the packet. We pinged between every combination
of outside, controller, compute, and instance.
Finally, Alvaro noticed something. When a packet from the outside hits the cloud controller, it should not be
configured with a VLAN. We verified this as true. When the packet went from the cloud controller to the
158 Appendix
Operations Guide (Release Version: 15.0.0)
compute node, it should only have a VLAN if it was destined for an instance. This was still true. When the
ping reply was sent from the instance, it should be in a VLAN. True. When it came back to the cloud controller
and on its way out to the Internet, it should no longer have a VLAN. False. Uh oh. It looked as though the
VLAN part of the packet was not being removed.
That made no sense.
While bouncing this idea around in our heads, I was randomly typing commands on the compute node:
$ ip a
…
10: vlan100@vlan20: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br100
,→state UP
…
“Hey Alvaro, can you run a VLAN on top of a VLAN?”
“If you did, you’d add an extra 4 bytes to the packet…”
Then it all made sense…
$ grep vlan_interface /etc/nova/nova.conf
vlan_interface=vlan20
In nova.conf, vlan_interface specifies what interface OpenStack should attach all VLANs to. The correct
setting should have been:
vlan_interface=bond0
As this would be the server’s bonded NIC.
vlan20 is the VLAN that the data center gave us for outgoing Internet access. It’s a correct VLAN and is also
attached to bond0.
By mistake, I configured OpenStack to attach all tenant VLANs to vlan20 instead of bond0 thereby stacking
one VLAN on top of another. This added an extra 4 bytes to each packet and caused a packet of 1504 bytes to
be sent out which would cause problems when it arrived at an interface that only accepted 1500.
As soon as this setting was fixed, everything worked.
“The Issue”
At the end of August 2012, a post-secondary school in Alberta, Canada migrated its infrastructure to an OpenStack
cloud. As luck would have it, within the first day or two of it running, one of their servers just disappeared
from the network. Blip. Gone.
After restarting the instance, everything was back up and running. We reviewed the logs and saw that at some
point, network communication stopped and then everything went idle. We chalked this up to a random occurrence.
A few nights later, it happened again.
We reviewed both sets of logs. The one thing that stood out the most was DHCP. At the time, OpenStack, by
default, set DHCP leases for one minute (it’s now two minutes). This means that every instance contacts the
cloud controller (DHCP server) to renew its fixed IP. For some reason, this instance could not renew its IP. We
correlated the instance’s logs with the logs on the cloud controller and put together a conversation:
1. Instance tries to renew IP.
Appendix 159
Operations Guide (Release Version: 15.0.0)
2. Cloud controller receives the renewal request and sends a response.
3. Instance “ignores” the response and re-sends the renewal request.
4. Cloud controller receives the second request and sends a new response.
5. Instance begins sending a renewal request to 255.255.255.255 since it hasn’t heard back from the
cloud controller.
6. The cloud controller receives the 255.255.255.255 request and sends a third response.
7. The instance finally gives up.
With this information in hand, we were sure that the problem had to do with DHCP. We thought that for some
reason, the instance wasn’t getting a new IP address and with no IP, it shut itself off from the network.
A quick Google search turned up this: DHCP lease errors in VLAN mode which further supported our DHCP
theory.
An initial idea was to just increase the lease time. If the instance only renewed once every week, the chances
of this problem happening would be tremendously smaller than every minute. This didn’t solve the problem,
though. It was just covering the problem up.
We decided to have tcpdump run on this instance and see if we could catch it in action again. Sure enough, we
did.
The tcpdump looked very, very weird. In short, it looked as though network communication stopped before
the instance tried to renew its IP. Since there is so much DHCP chatter from a one minute lease, it’s very hard
to confirm it, but even with only milliseconds difference between packets, if one packet arrives first, it arrived
first, and if that packet reported network issues, then it had to have happened before DHCP.
Additionally, this instance in question was responsible for a very, very large backup job each night. While “The
Issue” (as we were now calling it) didn’t happen exactly when the backup happened, it was close enough (a few
hours) that we couldn’t ignore it.
Further days go by and we catch The Issue in action more and more. We find that dhclient is not running
after The Issue happens. Now we’re back to thinking it’s a DHCP issue. Running /etc/init.d/networking
restart brings everything back up and running.
Ever have one of those days where all of the sudden you get the Google results you were looking for? Well,
that’s what happened here. I was looking for information on dhclient and why it dies when it can’t renew its
lease and all of the sudden I found a bunch of OpenStack and dnsmasq discussions that were identical to the
problem we were seeing!
Problem with Heavy Network IO and Dnsmasq.
instances losing IP address while running, due to No DHCPOFFER.
Seriously, Google.
This bug report was the key to everything: KVM images lose connectivity with bridged network.
It was funny to read the report. It was full of people who had some strange network problem but didn’t quite
explain it in the same way.
So it was a qemu/kvm bug.
At the same time of finding the bug report, a co-worker was able to successfully reproduce The Issue! How?
He used iperf to spew a ton of bandwidth at an instance. Within 30 minutes, the instance just disappeared
from the network.
160 Appendix
Operations Guide (Release Version: 15.0.0)
Armed with a patched qemu and a way to reproduce, we set out to see if we’ve finally solved The Issue. After
48 hours straight of hammering the instance with bandwidth, we were confident. The rest is history. You can
search the bug report for “joe” to find my comments and actual tests.
Disappearing Images
At the end of 2012, Cybera (a nonprofit with a mandate to oversee the development of cyberinfrastructure in
Alberta, Canada) deployed an updated OpenStack cloud for their DAIR project. A few days into production, a
compute node locks up. Upon rebooting the node, I checked to see what instances were hosted on that node so
I could boot them on behalf of the customer. Luckily, only one instance.
The nova reboot command wasn’t working, so I used virsh, but it immediately came back with an error
saying it was unable to find the backing disk. In this case, the backing disk is the Glance image that is copied
to /var/lib/nova/instances/_base when the image is used for the first time. Why couldn’t it find it? I
checked the directory and sure enough it was gone.
I reviewed the nova database and saw the instance’s entry in the nova.instances table. The image that the
instance was using matched what virsh was reporting, so no inconsistency there.
I checked Glance and noticed that this image was a snapshot that the user created. At least that was good
news—this user would have been the only user affected.
Finally, I checked StackTach and reviewed the user’s events. They had created and deleted several snapshots—
most likely experimenting. Although the timestamps didn’t match up, my conclusion was that they launched
their instance and then deleted the snapshot and it was somehow removed from /var/lib/nova/instances/
_base. None of that made sense, but it was the best I could come up with.
It turns out the reason that this compute node locked up was a hardware issue. We removed it from the DAIR
cloud and called Dell to have it serviced. Dell arrived and began working. Somehow or another (or a fat finger),
a different compute node was bumped and rebooted. Great.
When this node fully booted, I ran through the same scenario of seeing what instances were running so I could
turn them back on. There were a total of four. Three booted and one gave an error. It was the same error as
before: unable to find the backing disk. Seriously, what?
Again, it turns out that the image was a snapshot. The three other instances that successfully started were
standard cloud images. Was it a problem with snapshots? That didn’t make sense.
A note about DAIR’s architecture: /var/lib/nova/instances is a shared NFS mount. This means that all
compute nodes have access to it, which includes the _base directory. Another centralized area is /var/log/
rsyslog on the cloud controller. This directory collects all OpenStack logs from all compute nodes. I wondered
if there were any entries for the file that virsh is reporting:
dair-ua-c03/nova.log:Dec 19 12:10:59 dair-ua-c03
2012-12-19 12:10:59 INFO nova.virt.libvirt.imagecache
[-] Removing base file:
/var/lib/nova/instances/_base/7b4783508212f5d242cbf9ff56fb8d33b4ce6166_10
Ah-hah! So OpenStack was deleting it. But why?
A feature was introduced in Essex to periodically check and see if there were any _base files not in use. If there
were, OpenStack Compute would delete them. This idea sounds innocent enough and has some good qualities
to it. But how did this feature end up turned on? It was disabled by default in Essex. As it should be. It was
decided to be turned on in Folsom. I cannot emphasize enough that:
Actions which delete things should not be enabled by default.
Appendix 161
Operations Guide (Release Version: 15.0.0)
Disk space is cheap these days. Data recovery is not.
Secondly, DAIR’s shared /var/lib/nova/instances directory contributed to the problem. Since all compute
nodes have access to this directory, all compute nodes periodically review the _base directory. If there is only
one instance using an image, and the node that the instance is on is down for a few minutes, it won’t be able to
mark the image as still in use. Therefore, the image seems like it’s not in use and is deleted. When the compute
node comes back online, the instance hosted on that node is unable to start.
The Valentine’s Day Compute Node Massacre
Although the title of this story is much more dramatic than the actual event, I don’t think, or hope, that I’ll have
the opportunity to use “Valentine’s Day Massacre” again in a title.
This past Valentine’s Day, I received an alert that a compute node was no longer available in the cloud—
meaning,
$ openstack compute service list
showed this particular node in a down state.
I logged into the cloud controller and was able to both ping and SSH into the problematic compute node which
seemed very odd. Usually if I receive this type of alert, the compute node has totally locked up and would be
inaccessible.
After a few minutes of troubleshooting, I saw the following details:
• A user recently tried launching a CentOS instance on that node
• This user was the only user on the node (new node)
• The load shot up to 8 right before I received the alert
• The bonded 10gb network device (bond0) was in a DOWN state
• The 1gb NIC was still alive and active
I looked at the status of both NICs in the bonded pair and saw that neither was able to communicate with the
switch port. Seeing as how each NIC in the bond is connected to a separate switch, I thought that the chance of
a switch port dying on each switch at the same time was quite improbable. I concluded that the 10gb dual port
NIC had died and needed replaced. I created a ticket for the hardware support department at the data center
where the node was hosted. I felt lucky that this was a new node and no one else was hosted on it yet.
An hour later I received the same alert, but for another compute node. Crap. OK, now there’s definitely a
problem going on. Just like the original node, I was able to log in by SSH. The bond0 NIC was DOWN but the
1gb NIC was active.
And the best part: the same user had just tried creating a CentOS instance. What?
I was totally confused at this point, so I texted our network admin to see if he was available to help. He logged
in to both switches and immediately saw the problem: the switches detected spanning tree packets coming from
the two compute nodes and immediately shut the ports down to prevent spanning tree loops:
Feb 15 01:40:18 SW-1 Stp: %SPANTREE-4-BLOCK_BPDUGUARD: Received BPDU packet on Port-
,→Channel35 with BPDU guard enabled. Disabling interface. (source mac fa:16:3e:24:e7:22)
Feb 15 01:40:18 SW-1 Ebra: %ETH-4-ERRDISABLE: bpduguard error detected on Port-Channel35.
Feb 15 01:40:18 SW-1 Mlag: %MLAG-4-INTF_INACTIVE_LOCAL: Local interface Port-Channel35 is
,→link down. MLAG 35 is inactive.
Feb 15 01:40:18 SW-1 Ebra: %LINEPROTO-5-UPDOWN: Line protocol on Interface Port-Channel35
,→(Server35), changed state to down
162 Appendix
Operations Guide (Release Version: 15.0.0)
Feb 15 01:40:19 SW-1 Stp: %SPANTREE-6-INTERFACE_DEL: Interface Port-Channel35 has been
,→removed from instance MST0
Feb 15 01:40:19 SW-1 Ebra: %LINEPROTO-5-UPDOWN: Line protocol on Interface Ethernet35
,→(Server35), changed state to down
He re-enabled the switch ports and the two compute nodes immediately came back to life.
Unfortunately, this story has an open ending... we’re still looking into why the CentOS image was sending
out spanning tree packets. Further, we’re researching a proper way on how to mitigate this from happening.
It’s a bigger issue than one might think. While it’s extremely important for switches to prevent spanning tree
loops, it’s very problematic to have an entire compute node be cut from the network when this happens. If a
compute node is hosting 100 instances and one of them sends a spanning tree packet, that instance has effectively
DDOS’d the other 99 instances.
This is an ongoing and hot topic in networking circles —especially with the raise of virtualization and virtual
switches.
Down the Rabbit Hole
Users being able to retrieve console logs from running instances is a boon for support—many times they can
figure out what’s going on inside their instance and fix what’s going on without bothering you. Unfortunately,
sometimes overzealous logging of failures can cause problems of its own.
A report came in: VMs were launching slowly, or not at all. Cue the standard checks—nothing on the Nagios,
but there was a spike in network towards the current master of our RabbitMQ cluster. Investigation started, but
soon the other parts of the queue cluster were leaking memory like a sieve. Then the alert came in—the master
Rabbit server went down and connections failed over to the slave.
At that time, our control services were hosted by another team and we didn’t have much debugging information
to determine what was going on with the master, and we could not reboot it. That team noted that it failed
without alert, but managed to reboot it. After an hour, the cluster had returned to its normal state and we went
home for the day.
Continuing the diagnosis the next morning was kick started by another identical failure. We quickly got the
message queue running again, and tried to work out why Rabbit was suffering from so much network traffic.
Enabling debug logging on nova-api quickly brought understanding. A tail -f /var/log/nova/novaapi.log
was scrolling by faster than we’d ever seen before. CTRL+C on that and we could plainly see the
contents of a system log spewing failures over and over again - a system log from one of our users’ instances.
After finding the instance ID we headed over to /var/lib/nova/instances to find the console.log:
adm@cc12:/var/lib/nova/instances/instance-00000e05# wc -l console.log
92890453 console.log
adm@cc12:/var/lib/nova/instances/instance-00000e05# ls -sh console.log
5.5G console.log
Sure enough, the user had been periodically refreshing the console log page on the dashboard and the 5G file
was traversing the Rabbit cluster to get to the dashboard.
We called them and asked them to stop for a while, and they were happy to abandon the horribly broken VM.
After that, we started monitoring the size of console logs.
To this day, the issue doesn’t have a permanent resolution, but we look forward to the discussion at the next
summit.
Appendix 163
Operations Guide (Release Version: 15.0.0)
Havana Haunted by the Dead
Felix Lee of Academia Sinica Grid Computing Centre in Taiwan contributed this story.
I just upgraded OpenStack from Grizzly to Havana 2013.2-2 using the RDO repository and everything was
running pretty well—except the EC2 API.
I noticed that the API would suffer from a heavy load and respond slowly to particular EC2 requests such as
RunInstances.
Output from /var/log/nova/nova-api.log on Havana:
2014-01-10 09:11:45.072 129745 INFO nova.ec2.wsgi.server
[req-84d16d16-3808-426b-b7af-3b90a11b83b0
0c6e7dba03c24c6a9bce299747499e8a 7052bd6714e7460caeb16242e68124f9]
117.103.103.29 "GET
/services/Cloud?AWSAccessKeyId=[something]&Action=RunInstances&ClientToken=[something]&
,→ImageId=ami-00000001&InstanceInitiatedShutdownBehavior=terminate...
HTTP/1.1" status: 200 len: 1109 time: 138.5970151
This request took over two minutes to process, but executed quickly on another co-existing Grizzly deployment
using the same hardware and system configuration.
Output from /var/log/nova/nova-api.log on Grizzly:
2014-01-08 11:15:15.704 INFO nova.ec2.wsgi.server
[req-ccac9790-3357-4aa8-84bd-cdaab1aa394e
ebbd729575cb404081a45c9ada0849b7 8175953c209044358ab5e0ec19d52c37]
117.103.103.29 "GET
/services/Cloud?AWSAccessKeyId=[something]&Action=RunInstances&ClientToken=[something]&
,→ImageId=ami-00000007&InstanceInitiatedShutdownBehavior=terminate...
HTTP/1.1" status: 200 len: 931 time: 3.9426181
While monitoring system resources, I noticed a significant increase in memory consumption while the EC2
API processed this request. I thought it wasn’t handling memory properly—possibly not releasing memory.
If the API received several of these requests, memory consumption quickly grew until the system ran out of
RAM and began using swap. Each node has 48 GB of RAM and the “nova-api” process would consume all
of it within minutes. Once this happened, the entire system would become unusably slow until I restarted the
nova-api service.
So, I found myself wondering what changed in the EC2 API on Havana that might cause this to happen. Was
it a bug or a normal behavior that I now need to work around?
After digging into the nova (OpenStack Compute) code, I noticed two areas in api/ec2/cloud.py potentially
impacting my system:
instances = self.compute_api.get_all(context,
search_opts=search_opts,
sort_dir='asc')
sys_metas = self.compute_api.get_all_system_metadata(
context, search_filts=[{'key': ['EC2_client_token']},
{'value': [client_token]}])
Since my database contained many records—over 1 million metadata records and over 300,000 instance records
in “deleted” or “errored” states—each search took a long time. I decided to clean up the database by first
archiving a copy for backup and then performing some deletions using the MySQL client. For example, I ran
the following SQL command to remove rows of instances deleted for over a year:
164 Appendix
Operations Guide (Release Version: 15.0.0)
mysql> delete from nova.instances where deleted=1 and terminated_at < (NOW() - INTERVAL 1
,→YEAR);
Performance increased greatly after deleting the old records and my new deployment continues to behave well.
Working with Roadmaps
The good news: OpenStack has unprecedented transparency when it comes to providing information about
what’s coming up. The bad news: each release moves very quickly. The purpose of this appendix is to highlight
some of the useful pages to track, and take an educated guess at what is coming up in the next release and perhaps
further afield.
OpenStack follows a six month release cycle, typically releasing in April/May and October/November each
year. At the start of each cycle, the community gathers in a single location for a design summit. At the summit,
the features for the coming releases are discussed, prioritized, and planned. The below figure shows an example
release cycle, with dates showing milestone releases, code freeze, and string freeze dates, along with an example
of when the summit occurs. Milestones are interim releases within the cycle that are available as packages for
download and testing. Code freeze is putting a stop to adding new features to the release. String freeze is putting
a stop to changing any strings within the source code.
Information Available to You
There are several good sources of information available that you can use to track your OpenStack development
desires.
Release notes are maintained on the OpenStack wiki, and also shown here:
Series Status Releases Date
Liberty Under Development 2015.2 Oct, 2015
Kilo Current stable release, security-supported 2015.1 Apr 30, 2015
Juno Security-supported 2014.2 Oct 16, 2014
Icehouse End-of-life 2014.1 Apr 17, 2014
2014.1.1 Jun 9, 2014
2014.1.2 Aug 8, 2014
2014.1.3 Oct 2, 2014
Havana End-of-life 2013.2 Apr 4, 2013
2013.2.1 Dec 16, 2013
Continued on next page
Appendix 165
Operations Guide (Release Version: 15.0.0)
Table 8 – continued from previous page
Series Status Releases Date
2013.2.2 Feb 13, 2014
2013.2.3 Apr 3, 2014
2013.2.4 Sep 22, 2014
2013.2.1 Dec 16, 2013
Grizzly End-of-life 2013.1 Apr 4, 2013
2013.1.1 May 9, 2013
2013.1.2 Jun 6, 2013
2013.1.3 Aug 8, 2013
2013.1.4 Oct 17, 2013
2013.1.5 Mar 20, 2015
Folsom End-of-life 2012.2 Sep 27, 2012
2012.2.1 Nov 29, 2012
2012.2.2 Dec 13, 2012
2012.2.3 Jan 31, 2013
2012.2.4 Apr 11, 2013
Essex End-of-life 2012.1 Apr 5, 2012
2012.1.1 Jun 22, 2012
2012.1.2 Aug 10, 2012
2012.1.3 Oct 12, 2012
Diablo Deprecated 2011.3 Sep 22, 2011
2011.3.1 Jan 19, 2012
Cactus Deprecated 2011.2 Apr 15, 2011
Bexar Deprecated 2011.1 Feb 3, 2011
Austin Deprecated 2010.1 Oct 21, 2010
Here are some other resources:
• A breakdown of current features under development, with their target milestone
• A list of all features, including those not yet under development
• Rough-draft design discussions (“etherpads”) from the last design summit
• List of individual code changes under review
Influencing the Roadmap
OpenStack truly welcomes your ideas (and contributions) and highly values feedback from real-world users of
the software. By learning a little about the process that drives feature development, you can participate and
perhaps get the additions you desire.
Feature requests typically start their life in Etherpad, a collaborative editing tool, which is used to take coordinating
notes at a design summit session specific to the feature. This then leads to the creation of a blueprint on
the Launchpad site for the particular project, which is used to describe the feature more formally. Blueprints
are then approved by project team members, and development can begin.
Therefore, the fastest way to get your feature request up for consideration is to create an Etherpad with your
ideas and propose a session to the design summit. If the design summit has already passed, you may also create
a blueprint directly. Read this blog post about how to work with blueprints the perspective of Victoria Martínez,
a developer intern.
166 Appendix
Operations Guide (Release Version: 15.0.0)
The roadmap for the next release as it is developed can be seen at Releases.
To determine the potential features going in to future releases, or to look at features implemented previously, take
a look at the existing blueprints such as OpenStack Compute (nova) Blueprints, OpenStack Identity (keystone)
Blueprints, and release notes.
Aside from the direct-to-blueprint pathway, there is another very well-regarded mechanism to influence the
development roadmap: the user survey. Found at OpenStack User Survey, it allows you to provide details of
your deployments and needs, anonymously by default. Each cycle, the user committee analyzes the results and
produces a report, including providing specific information to the technical committee and project team leads.
Aspects to Watch
You want to keep an eye on the areas improving within OpenStack. The best way to “watch” roadmaps for each
project is to look at the blueprints that are being approved for work on milestone releases. You can also learn
from PTL webinars that follow the OpenStack summits twice a year.
Driver Quality Improvements
A major quality push has occurred across drivers and plug-ins in Block Storage, Compute, and Networking.
Particularly, developers of Compute and Networking drivers that require proprietary or hardware products are
now required to provide an automated external testing system for use during the development process.
Easier Upgrades
One of the most requested features since OpenStack began (for components other than Object Storage, which
tends to “just work”): easier upgrades. In all recent releases internal messaging communication is versioned,
meaning services can theoretically drop back to backward-compatible behavior. This allows you to run later
versions of some components, while keeping older versions of others.
In addition, database migrations are now tested with the Turbo Hipster tool. This tool tests database migration
performance on copies of real-world user databases.
These changes have facilitated the first proper OpenStack upgrade guide, found in Upgrades, and will continue
to improve in the next release.
Deprecation of Nova Network
With the introduction of the full software-defined networking stack provided by OpenStack Networking (neutron)
in the Folsom release, development effort on the initial networking code that remains part of the Compute
component has gradually lessened. While many still use nova-network in production, there has been a longterm
plan to remove the code in favor of the more flexible and full-featured OpenStack Networking.
An attempt was made to deprecate nova-network during the Havana release, which was aborted due to the
lack of equivalent functionality (such as the FlatDHCP multi-host high-availability mode mentioned in this
guide), lack of a migration path between versions, insufficient testing, and simplicity when used for the more
straightforward use cases nova-network traditionally supported. Though significant effort has been made to
address these concerns, nova-network was not be deprecated in the Juno release. In addition, to a limited
degree, patches to nova-network have again begin to be accepted, such as adding a per-network settings
feature and SR-IOV support in Juno.
Appendix 167
Operations Guide (Release Version: 15.0.0)
This leaves you with an important point of decision when designing your cloud. OpenStack Networking is
robust enough to use with a small number of limitations (performance issues in some scenarios, only basic high
availability of layer 3 systems) and provides many more features than nova-network. However, if you do not
have the more complex use cases that can benefit from fuller software-defined networking capabilities, or are
uncomfortable with the new concepts introduced, nova-network may continue to be a viable option for the
next 12 months.
Similarly, if you have an existing cloud and are looking to upgrade from nova-network to OpenStack Networking,
you should have the option to delay the upgrade for this period of time. However, each release of
OpenStack brings significant new innovation, and regardless of your use of networking methodology, it is
likely best to begin planning for an upgrade within a reasonable timeframe of each release.
As mentioned, there’s currently no way to cleanly migrate from nova-network to neutron. We recommend
that you keep a migration in mind and what that process might involve for when a proper migration path is
released.
Distributed Virtual Router
One of the long-time complaints surrounding OpenStack Networking was the lack of high availability for the
layer 3 components. The Juno release introduced Distributed Virtual Router (DVR), which aims to solve this
problem.
Early indications are that it does do this well for a base set of scenarios, such as using the ML2 plug-in with
Open vSwitch, one flat external network and VXLAN tenant networks. However, it does appear that there are
problems with the use of VLANs, IPv6, Floating IPs, high north-south traffic scenarios and large numbers of
compute nodes. It is expected these will improve significantly with the next release, but bug reports on specific
issues are highly desirable.
Replacement of Open vSwitch Plug-in with Modular Layer 2
The Modular Layer 2 plug-in is a framework allowing OpenStack Networking to simultaneously utilize the
variety of layer-2 networking technologies found in complex real-world data centers. It currently works with
the existing Open vSwitch, Linux Bridge, and Hyper-V L2 agents and is intended to replace and deprecate the
monolithic plug-ins associated with those L2 agents.
New API Versions
The third version of the Compute API was broadly discussed and worked on during the Havana and Icehouse
release cycles. Current discussions indicate that the V2 API will remain for many releases, and the next iteration
of the API will be denoted v2.1 and have similar properties to the existing v2.0, rather than an entirely new
v3 API. This is a great time to evaluate all API and provide comments while the next generation APIs are
being defined. A new working group was formed specifically to improve OpenStack APIs and create design
guidelines, which you are welcome to join.
OpenStack on OpenStack (TripleO)
This project continues to improve and you may consider using it for greenfield deployments, though according
to the latest user survey results it remains to see widespread uptake.
168 Appendix
Operations Guide (Release Version: 15.0.0)
Data processing service for OpenStack (sahara)
A much-requested answer to big data problems, a dedicated team has been making solid progress on a Hadoopas-a-Service
project.
Bare metal Deployment (ironic)
The bare-metal deployment has been widely lauded, and development continues. The Juno release brought the
OpenStack Bare metal drive into the Compute project, and it was aimed to deprecate the existing bare-metal
driver in Kilo. If you are a current user of the bare metal driver, a particular blueprint to follow is Deprecate
the bare metal driver
Database as a Service (trove)
The OpenStack community has had a database-as-a-service tool in development for some time, and we saw
the first integrated release of it in Icehouse. From its release it was able to deploy database servers out of the
box in a highly available way, initially supporting only MySQL. Juno introduced support for Mongo (including
clustering), PostgreSQL and Couchbase, in addition to replication functionality for MySQL. In Kilo, more
advanced clustering capability was delivered, in addition to better integration with other OpenStack components
such as Networking.
Message Service (zaqar)
A service to provide queues of messages and notifications was released.
DNS service (designate)
A long requested service, to provide the ability to manipulate DNS entries associated with OpenStack resources
has gathered a following. The designate project was also released.
Scheduler Improvements
Both Compute and Block Storage rely on schedulers to determine where to place virtual machines or volumes.
In Havana, the Compute scheduler underwent significant improvement, while in Icehouse it was the scheduler
in Block Storage that received a boost. Further down the track, an effort started this cycle that aims to create
a holistic scheduler covering both will come to fruition. Some of the work that was done in Kilo can be found
under the Gantt project.
Block Storage Improvements
Block Storage is considered a stable project, with wide uptake and a long track record of quality drivers. The
team has discussed many areas of work at the summits, including better error reporting, automated discovery,
and thin provisioning features.
Appendix 169
Operations Guide (Release Version: 15.0.0)
Toward a Python SDK
Though many successfully use the various python-*client code as an effective SDK for interacting with OpenStack,
consistency between the projects and documentation availability waxes and wanes. To combat this, an
effort to improve the experience has started. Cross-project development efforts in OpenStack have a checkered
history, such as the unified client project having several false starts. However, the early signs for the SDK
project are promising, and we expect to see results during the Juno cycle.
Resources
OpenStack
• OpenStack Installation Tutorial for openSUSE and SUSE Linux Enterprise Server
• OpenStack Installation Tutorial for Red Hat Enterprise Linux and CentOS
• OpenStack Installation Tutorial for Ubuntu Server
• OpenStack Administrator Guide
• OpenStack Cloud Computing Cookbook (Packt Publishing)
Cloud (General)
• “The NIST Definition of Cloud Computing”
Python
• Dive Into Python (Apress)
Networking
• TCP/IP Illustrated, Volume 1: The Protocols, 2/E (Pearson)
• The TCP/IP Guide (No Starch Press)
• “A tcpdump Tutorial and Primer”
Systems Administration
• UNIX and Linux Systems Administration Handbook (Prentice Hall)
Virtualization
• The Book of Xen (No Starch Press)
170 Appendix
Operations Guide (Release Version: 15.0.0)
Configuration Management
• Puppet Labs Documentation
• Pro Puppet (Apress)
Community support
The following resources are available to help you run and use OpenStack. The OpenStack community constantly
improves and adds to the main features of OpenStack, but if you have any questions, do not hesitate to ask. Use
the following resources to get OpenStack support and troubleshoot your installations.
Documentation
For the available OpenStack documentation, see docs.openstack.org.
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at
OpenStack Documentation Mailing List, join our IRC channel #openstack-doc on the freenode IRC network,
or report a bug.
The following books explain how to install an OpenStack cloud and its associated components:
• Installation Tutorial for openSUSE Leap 42.2 and SUSE Linux Enterprise Server 12 SP2
• Installation Tutorial for Red Hat Enterprise Linux 7 and CentOS 7
• Installation Tutorial for Ubuntu 16.04 (LTS)
The following books explain how to configure and run an OpenStack cloud:
• Architecture Design Guide
• Administrator Guide
• Configuration Reference
• Operations Guide
• Networking Guide
• High Availability Guide
• Security Guide
• Virtual Machine Image Guide
The following books explain how to use the OpenStack Dashboard and command-line clients:
• End User Guide
• Command-Line Interface Reference
The following documentation provides reference and guidance information for the OpenStack APIs:
• API Guide
The following guide provides how to contribute to OpenStack documentation:
• Documentation Contributor Guide
Appendix 171
Operations Guide (Release Version: 15.0.0)
ask.openstack.org
During the set up or testing of OpenStack, you might have questions about how a specific task is completed or
be in a situation where a feature does not work correctly. Use the ask.openstack.org site to ask questions and
get answers. When you visit the Ask OpenStack site, scan the recently asked questions to see whether your
question has already been answered. If not, ask a new question. Be sure to give a clear, concise summary in the
title and provide as much detail as possible in the description. Paste in your command output or stack traces,
links to screen shots, and any other information which might be useful.
OpenStack mailing lists
A great way to get answers and insights is to post your question or problematic scenario to the OpenStack
mailing list. You can learn from and help others who might have similar issues. To subscribe or view the
archives, go to the general OpenStack mailing list. If you are interested in the other mailing lists for specific
projects or development, refer to Mailing Lists.
The OpenStack wiki
The OpenStack wiki contains a broad range of topics but some of the information can be difficult to find or is a
few pages deep. Fortunately, the wiki search feature enables you to search by title or content. If you search for
specific information, such as about networking or OpenStack Compute, you can find a large amount of relevant
material. More is being added all the time, so be sure to check back often. You can find the search box in the
upper-right corner of any OpenStack wiki page.
The Launchpad Bugs area
The OpenStack community values your set up and testing efforts and wants your feedback. To log a bug, you
must sign up for a Launchpad account. You can view existing bugs and report bugs in the Launchpad Bugs
area. Use the search feature to determine whether the bug has already been reported or already been fixed. If it
still seems like your bug is unreported, fill out a bug report.
Some tips:
• Give a clear, concise summary.
• Provide as much detail as possible in the description. Paste in your command output or stack traces, links
to screen shots, and any other information which might be useful.
• Be sure to include the software and package versions that you are using, especially
if you are using a development branch, such as, "Kilo release" vs git commit
bc79c3ecc55929bac585d04a03475b72e06a3208.
• Any deployment-specific information is helpful, such as whether you are using Ubuntu 14.04 or are
performing a multi-node installation.
The following Launchpad Bugs areas are available:
• Bugs: OpenStack Block Storage (cinder)
• Bugs: OpenStack Compute (nova)
• Bugs: OpenStack Dashboard (horizon)
• Bugs: OpenStack Identity (keystone)
172 Appendix
Operations Guide (Release Version: 15.0.0)
• Bugs: OpenStack Image service (glance)
• Bugs: OpenStack Networking (neutron)
• Bugs: OpenStack Object Storage (swift)
• Bugs: Application catalog (murano)
• Bugs: Bare metal service (ironic)
• Bugs: Clustering service (senlin)
• Bugs: Container Infrastructure Management service (magnum)
• Bugs: Data processing service (sahara)
• Bugs: Database service (trove)
• Bugs: Deployment service (fuel)
• Bugs: DNS service (designate)
• Bugs: Key Manager Service (barbican)
• Bugs: Monitoring (monasca)
• Bugs: Orchestration (heat)
• Bugs: Rating (cloudkitty)
• Bugs: Shared file systems (manila)
• Bugs: Telemetry (ceilometer)
• Bugs: Telemetry v3 (gnocchi)
• Bugs: Workflow service (mistral)
• Bugs: Messaging service (zaqar)
• Bugs: OpenStack API Documentation (developer.openstack.org)
• Bugs: OpenStack Documentation (docs.openstack.org)
The OpenStack IRC channel
The OpenStack community lives in the #openstack IRC channel on the Freenode network. You can hang out, ask
questions, or get immediate feedback for urgent and pressing issues. To install an IRC client or use a browserbased
client, go to https://webchat.freenode.net/. You can also use Colloquy (Mac OS X), mIRC (Windows),
or XChat (Linux). When you are in the IRC channel and want to share code or command output, the generally
accepted method is to use a Paste Bin. The OpenStack project has one at Paste. Just paste your longer amounts
of text or logs in the web form and you get a URL that you can paste into the channel. The OpenStack IRC
channel is #openstack on irc.freenode.net. You can find a list of all OpenStack IRC channels on the IRC
page on the wiki.
Documentation feedback
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at
OpenStack Documentation Mailing List, or report a bug.
Appendix 173
Operations Guide (Release Version: 15.0.0)
OpenStack distribution packages
The following Linux distributions provide community-supported packages for OpenStack:
• Debian: https://wiki.debian.org/OpenStack
• CentOS, Fedora, and Red Hat Enterprise Linux: https://www.rdoproject.org/
• openSUSE and SUSE Linux Enterprise Server: https://en.opensuse.org/Portal:OpenStack
• Ubuntu: https://wiki.ubuntu.com/ServerTeam/CloudArchive
Glossary
This glossary offers a list of terms and definitions to define a vocabulary for OpenStack-related concepts.
To add to OpenStack glossary, clone the openstack/openstack-manuals repository and update the source file
doc/common/glossary.rst through the OpenStack contribution process.
0-9
6to4 A mechanism that allows IPv6 packets to be transmitted over an IPv4 network, providing a strategy for
migrating to IPv6.
A
absolute limit Impassable limits for guest VMs. Settings include total RAM size, maximum number of vCPUs,
and maximum disk size.
access control list (ACL) A list of permissions attached to an object. An ACL specifies which users or system
processes have access to objects. It also defines which operations can be performed on specified objects.
Each entry in a typical ACL specifies a subject and an operation. For instance, the ACL entry (Alice,
delete) for a file gives Alice permission to delete the file.
access key Alternative term for an Amazon EC2 access key. See EC2 access key.
account The Object Storage context of an account. Do not confuse with a user account from an authentication
service, such as Active Directory, /etc/passwd, OpenLDAP, OpenStack Identity, and so on.
account auditor Checks for missing replicas and incorrect or corrupted objects in a specified Object Storage
account by running queries against the back-end SQLite database.
account database A SQLite database that contains Object Storage accounts and related metadata and that the
accounts server accesses.
account reaper An Object Storage worker that scans for and deletes account databases and that the account
server has marked for deletion.
account server Lists containers in Object Storage and stores container information in the account database.
account service An Object Storage component that provides account services such as list, create, modify, and
audit. Do not confuse with OpenStack Identity service, OpenLDAP, or similar user-account services.
accounting The Compute service provides accounting information through the event notification and system
usage data facilities.
Active Directory Authentication and identity service by Microsoft, based on LDAP. Supported in OpenStack.
174 Appendix
Operations Guide (Release Version: 15.0.0)
active/active configuration In a high-availability setup with an active/active configuration, several systems
share the load together and if one fails, the load is distributed to the remaining systems.
active/passive configuration In a high-availability setup with an active/passive configuration, systems are set
up to bring additional resources online to replace those that have failed.
address pool A group of fixed and/or floating IP addresses that are assigned to a project and can be used by
or assigned to the VM instances in a project.
Address Resolution Protocol (ARP) The protocol by which layer-3 IP addresses are resolved into layer-2
link local addresses.
admin API A subset of API calls that are accessible to authorized administrators and are generally not accessible
to end users or the public Internet. They can exist as a separate service (keystone) or can be a
subset of another API (nova).
admin server In the context of the Identity service, the worker process that provides access to the admin API.
administrator The person responsible for installing, configuring, and managing an OpenStack cloud.
Advanced Message Queuing Protocol (AMQP) The open standard messaging protocol used by OpenStack
components for intra-service communications, provided by RabbitMQ, Qpid, or ZeroMQ.
Advanced RISC Machine (ARM) Lower power consumption CPU often found in mobile and embedded
devices. Supported by OpenStack.
alert The Compute service can send alerts through its notification system, which includes a facility to create
custom notification drivers. Alerts can be sent to and displayed on the dashboard.
allocate The process of taking a floating IP address from the address pool so it can be associated with a fixed
IP on a guest VM instance.
Amazon Kernel Image (AKI) Both a VM container format and disk format. Supported by Image service.
Amazon Machine Image (AMI) Both a VM container format and disk format. Supported by Image service.
Amazon Ramdisk Image (ARI) Both a VM container format and disk format. Supported by Image service.
Anvil A project that ports the shell script-based project named DevStack to Python.
aodh Part of the OpenStack Telemetry service; provides alarming functionality.
Apache The Apache Software Foundation supports the Apache community of open-source software projects.
These projects provide software products for the public good.
Apache License 2.0 All OpenStack core projects are provided under the terms of the Apache License 2.0
license.
Apache Web Server The most common web server software currently used on the Internet.
API endpoint The daemon, worker, or service that a client communicates with to access an API. API endpoints
can provide any number of services, such as authentication, sales data, performance meters, Compute
VM commands, census data, and so on.
API extension Custom modules that extend some OpenStack core APIs.
API extension plug-in Alternative term for a Networking plug-in or Networking API extension.
API key Alternative term for an API token.
API server Any node running a daemon or worker that provides an API endpoint.
Appendix 175
Operations Guide (Release Version: 15.0.0)
API token Passed to API requests and used by OpenStack to verify that the client is authorized to run the
requested operation.
API version In OpenStack, the API version for a project is part of the URL. For example, example.com/
nova/v1/foobar.
applet A Java program that can be embedded into a web page.
Application Catalog service (murano) The project that provides an application catalog service so that users
can compose and deploy composite environments on an application abstraction level while managing
the application lifecycle.
Application Programming Interface (API) A collection of specifications used to access a service, application,
or program. Includes service calls, required parameters for each call, and the expected return
values.
application server A piece of software that makes available another piece of software over a network.
Application Service Provider (ASP) Companies that rent specialized applications that help businesses and
organizations provide additional services with lower cost.
arptables Tool used for maintaining Address Resolution Protocol packet filter rules in the Linux kernel firewall
modules. Used along with iptables, ebtables, and ip6tables in Compute to provide firewall services
for VMs.
associate The process associating a Compute floating IP address with a fixed IP address.
Asynchronous JavaScript and XML (AJAX) A group of interrelated web development techniques used on
the client-side to create asynchronous web applications. Used extensively in horizon.
ATA over Ethernet (AoE) A disk storage protocol tunneled within Ethernet.
attach The process of connecting a VIF or vNIC to a L2 network in Networking. In the context of Compute,
this process connects a storage volume to an instance.
attachment (network) Association of an interface ID to a logical port. Plugs an interface into a port.
auditing Provided in Compute through the system usage data facility.
auditor A worker process that verifies the integrity of Object Storage objects, containers, and accounts. Auditors
is the collective term for the Object Storage account auditor, container auditor, and object auditor.
Austin The code name for the initial release of OpenStack. The first design summit took place in Austin,
Texas, US.
auth node Alternative term for an Object Storage authorization node.
authentication The process that confirms that the user, process, or client is really who they say they are
through private key, secret token, password, fingerprint, or similar method.
authentication token A string of text provided to the client after authentication. Must be provided by the user
or process in subsequent requests to the API endpoint.
AuthN The Identity service component that provides authentication services.
authorization The act of verifying that a user, process, or client is authorized to perform an action.
authorization node An Object Storage node that provides authorization services.
AuthZ The Identity component that provides high-level authorization services.
Auto ACK Configuration setting within RabbitMQ that enables or disables message acknowledgment. Enabled
by default.
176 Appendix
Operations Guide (Release Version: 15.0.0)
auto declare A Compute RabbitMQ setting that determines whether a message exchange is automatically
created when the program starts.
availability zone An Amazon EC2 concept of an isolated area that is used for fault tolerance. Do not confuse
with an OpenStack Compute zone or cell.
AWS CloudFormation template AWS CloudFormation allows Amazon Web Services (AWS) users to create
and manage a collection of related resources. The Orchestration service supports a CloudFormationcompatible
format (CFN).
B
back end Interactions and processes that are obfuscated from the user, such as Compute volume mount, data
transmission to an iSCSI target by a daemon, or Object Storage object integrity checks.
back-end catalog The storage method used by the Identity service catalog service to store and retrieve information
about API endpoints that are available to the client. Examples include an SQL database, LDAP
database, or KVS back end.
back-end store The persistent data store used to save and retrieve information for a service, such as lists of
Object Storage objects, current state of guest VMs, lists of user names, and so on. Also, the method that
the Image service uses to get and store VM images. Options include Object Storage, locally mounted
file system, RADOS block devices, VMware datastore, and HTTP.
Backup, Restore, and Disaster Recovery service (freezer) The project that provides integrated tooling for
backing up, restoring, and recovering file systems, instances, or database backups.
bandwidth The amount of available data used by communication resources, such as the Internet. Represents
the amount of data that is used to download things or the amount of data available to download.
barbican Code name of the Key Manager service.
bare An Image service container format that indicates that no container exists for the VM image.
Bare Metal service (ironic) The OpenStack service that provides a service and associated libraries capable
of managing and provisioning physical machines in a security-aware and fault-tolerant manner.
base image An OpenStack-provided image.
Bell-LaPadula model A security model that focuses on data confidentiality and controlled access to classified
information. This model divides the entities into subjects and objects. The clearance of a subject is
compared to the classification of the object to determine if the subject is authorized for the specific
access mode. The clearance or classification scheme is expressed in terms of a lattice.
Benchmark service (rally) OpenStack project that provides a framework for performance analysis and benchmarking
of individual OpenStack components as well as full production OpenStack cloud deployments.
Bexar A grouped release of projects related to OpenStack that came out in February of 2011. It included only
Compute (nova) and Object Storage (swift). Bexar is the code name for the second release of OpenStack.
The design summit took place in San Antonio, Texas, US, which is the county seat for Bexar county.
binary Information that consists solely of ones and zeroes, which is the language of computers.
bit A bit is a single digit number that is in base of 2 (either a zero or one). Bandwidth usage is measured in
bits per second.
bits per second (BPS) The universal measurement of how quickly data is transferred from place to place.
Appendix 177
Operations Guide (Release Version: 15.0.0)
block device A device that moves data in the form of blocks. These device nodes interface the devices, such
as hard disks, CD-ROM drives, flash drives, and other addressable regions of memory.
block migration A method of VM live migration used by KVM to evacuate instances from one host to another
with very little downtime during a user-initiated switchover. Does not require shared storage. Supported
by Compute.
Block Storage API An API on a separate endpoint for attaching, detaching, and creating block storage for
compute VMs.
Block Storage service (cinder) The OpenStack service that implement services and libraries to provide ondemand,
self-service access to Block Storage resources via abstraction and automation on top of other
block storage devices.
BMC (Baseboard Management Controller) The intelligence in the IPMI architecture, which is a specialized
micro-controller that is embedded on the motherboard of a computer and acts as a server. Manages the
interface between system management software and platform hardware.
bootable disk image A type of VM image that exists as a single, bootable file.
Bootstrap Protocol (BOOTP) A network protocol used by a network client to obtain an IP address from a
configuration server. Provided in Compute through the dnsmasq daemon when using either the FlatDHCP
manager or VLAN manager network manager.
Border Gateway Protocol (BGP) The Border Gateway Protocol is a dynamic routing protocol that connects
autonomous systems. Considered the backbone of the Internet, this protocol connects disparate networks
to form a larger network.
browser Any client software that enables a computer or device to access the Internet.
builder file Contains configuration information that Object Storage uses to reconfigure a ring or to re-create
it from scratch after a serious failure.
bursting The practice of utilizing a secondary environment to elastically build instances on-demand when the
primary environment is resource constrained.
button class A group of related button types within horizon. Buttons to start, stop, and suspend VMs are in
one class. Buttons to associate and disassociate floating IP addresses are in another class, and so on.
byte Set of bits that make up a single character; there are usually 8 bits to a byte.
C
cache pruner A program that keeps the Image service VM image cache at or below its configured maximum
size.
Cactus An OpenStack grouped release of projects that came out in the spring of 2011. It included Compute
(nova), Object Storage (swift), and the Image service (glance). Cactus is a city in Texas, US and is the
code name for the third release of OpenStack. When OpenStack releases went from three to six months
long, the code name of the release changed to match a geography nearest the previous summit.
CALL One of the RPC primitives used by the OpenStack message queue software. Sends a message and
waits for a response.
capability Defines resources for a cell, including CPU, storage, and networking. Can apply to the specific
services within a cell or a whole cell.
capacity cache A Compute back-end database table that contains the current workload, amount of free RAM,
and number of VMs running on each host. Used to determine on which host a VM starts.
178 Appendix
Operations Guide (Release Version: 15.0.0)
capacity updater A notification driver that monitors VM instances and updates the capacity cache as needed.
CAST One of the RPC primitives used by the OpenStack message queue software. Sends a message and does
not wait for a response.
catalog A list of API endpoints that are available to a user after authentication with the Identity service.
catalog service An Identity service that lists API endpoints that are available to a user after authentication
with the Identity service.
ceilometer Part of the OpenStack Telemetry service; gathers and stores metrics from other OpenStack services.
cell Provides logical partitioning of Compute resources in a child and parent relationship. Requests are passed
from parent cells to child cells if the parent cannot provide the requested resource.
cell forwarding A Compute option that enables parent cells to pass resource requests to child cells if the parent
cannot provide the requested resource.
cell manager The Compute component that contains a list of the current capabilities of each host within the
cell and routes requests as appropriate.
CentOS A Linux distribution that is compatible with OpenStack.
Ceph Massively scalable distributed storage system that consists of an object store, block store, and POSIXcompatible
distributed file system. Compatible with OpenStack.
CephFS The POSIX-compliant file system provided by Ceph.
certificate authority (CA) In cryptography, an entity that issues digital certificates. The digital certificate
certifies the ownership of a public key by the named subject of the certificate. This enables others
(relying parties) to rely upon signatures or assertions made by the private key that corresponds to the
certified public key. In this model of trust relationships, a CA is a trusted third party for both the subject
(owner) of the certificate and the party relying upon the certificate. CAs are characteristic of many public
key infrastructure (PKI) schemes. In OpenStack, a simple certificate authority is provided by Compute
for cloudpipe VPNs and VM image decryption.
Challenge-Handshake Authentication Protocol (CHAP) An iSCSI authentication method supported by
Compute.
chance scheduler A scheduling method used by Compute that randomly chooses an available host from the
pool.
changes since A Compute API parameter that downloads changes to the requested item since your last request,
instead of downloading a new, fresh set of data and comparing it against the old data.
Chef An operating system configuration management tool supporting OpenStack deployments.
child cell If a requested resource such as CPU time, disk storage, or memory is not available in the parent
cell, the request is forwarded to its associated child cells. If the child cell can fulfill the request, it does.
Otherwise, it attempts to pass the request to any of its children.
cinder Codename for Block Storage service.
CirrOS A minimal Linux distribution designed for use as a test image on clouds such as OpenStack.
Cisco neutron plug-in A Networking plug-in for Cisco devices and technologies, including UCS and Nexus.
cloud architect A person who plans, designs, and oversees the creation of clouds.
Cloud Auditing Data Federation (CADF) Cloud Auditing Data Federation (CADF) is a specification for
audit event data. CADF is supported by OpenStack Identity.
Appendix 179
Operations Guide (Release Version: 15.0.0)
cloud computing A model that enables access to a shared pool of configurable computing resources, such as
networks, servers, storage, applications, and services, that can be rapidly provisioned and released with
minimal management effort or service provider interaction.
cloud controller Collection of Compute components that represent the global state of the cloud; talks to services,
such as Identity authentication, Object Storage, and node/storage workers through a queue.
cloud controller node A node that runs network, volume, API, scheduler, and image services. Each service
may be broken out into separate nodes for scalability or availability.
Cloud Data Management Interface (CDMI) SINA standard that defines a RESTful API for managing objects
in the cloud, currently unsupported in OpenStack.
Cloud Infrastructure Management Interface (CIMI) An in-progress specification for cloud management.
Currently unsupported in OpenStack.
cloud-init A package commonly installed in VM images that performs initialization of an instance after boot
using information that it retrieves from the metadata service, such as the SSH public key and user data.
cloudadmin One of the default roles in the Compute RBAC system. Grants complete system access.
Cloudbase-Init A Windows project providing guest initialization features, similar to cloud-init.
cloudpipe A compute service that creates VPNs on a per-project basis.
cloudpipe image A pre-made VM image that serves as a cloudpipe server. Essentially, OpenVPN running on
Linux.
Clustering service (senlin) The project that implements clustering services and libraries for the management
of groups of homogeneous objects exposed by other OpenStack services.
command filter Lists allowed commands within the Compute rootwrap facility.
Common Internet File System (CIFS) A file sharing protocol. It is a public or open variation of the original
Server Message Block (SMB) protocol developed and used by Microsoft. Like the SMB protocol, CIFS
runs at a higher level and uses the TCP/IP protocol.
Common Libraries (oslo) The project that produces a set of python libraries containing code shared by OpenStack
projects. The APIs provided by these libraries should be high quality, stable, consistent, documented
and generally applicable.
community project A project that is not officially endorsed by the OpenStack Foundation. If the project is
successful enough, it might be elevated to an incubated project and then to a core project, or it might be
merged with the main code trunk.
compression Reducing the size of files by special encoding, the file can be decompressed again to its original
content. OpenStack supports compression at the Linux file system level but does not support compression
for things such as Object Storage objects or Image service VM images.
Compute API (Nova API) The nova-api daemon provides access to nova services. Can communicate with
other APIs, such as the Amazon EC2 API.
compute controller The Compute component that chooses suitable hosts on which to start VM instances.
compute host Physical host dedicated to running compute nodes.
compute node A node that runs the nova-compute daemon that manages VM instances that provide a wide
range of services, such as web applications and analytics.
180 Appendix
Operations Guide (Release Version: 15.0.0)
Compute service (nova) The OpenStack core project that implements services and associated libraries to
provide massively-scalable, on-demand, self-service access to compute resources, including bare metal,
virtual machines, and containers.
compute worker The Compute component that runs on each compute node and manages the VM instance
lifecycle, including run, reboot, terminate, attach/detach volumes, and so on. Provided by the novacompute
daemon.
concatenated object A set of segment objects that Object Storage combines and sends to the client.
conductor In Compute, conductor is the process that proxies database requests from the compute process.
Using conductor improves security because compute nodes do not need direct access to the database.
congress Code name for the Governance service.
consistency window The amount of time it takes for a new Object Storage object to become accessible to all
clients.
console log Contains the output from a Linux VM console in Compute.
container Organizes and stores objects in Object Storage. Similar to the concept of a Linux directory but
cannot be nested. Alternative term for an Image service container format.
container auditor Checks for missing replicas or incorrect objects in specified Object Storage containers
through queries to the SQLite back-end database.
container database A SQLite database that stores Object Storage containers and container metadata. The
container server accesses this database.
container format A wrapper used by the Image service that contains a VM image and its associated metadata,
such as machine state, OS disk size, and so on.
Container Infrastructure Management service (magnum) The project which provides a set of services for
provisioning, scaling, and managing container orchestration engines.
container server An Object Storage server that manages containers.
container service The Object Storage component that provides container services, such as create, delete, list,
and so on.
content delivery network (CDN) A content delivery network is a specialized network that is used to distribute
content to clients, typically located close to the client for increased performance.
controller node Alternative term for a cloud controller node.
core API Depending on context, the core API is either the OpenStack API or the main API of a specific core
project, such as Compute, Networking, Image service, and so on.
core service An official OpenStack service defined as core by DefCore Committee. Currently, consists of
Block Storage service (cinder), Compute service (nova), Identity service (keystone), Image service
(glance), Networking service (neutron), and Object Storage service (swift).
cost Under the Compute distributed scheduler, this is calculated by looking at the capabilities of each host
relative to the flavor of the VM instance being requested.
credentials Data that is only known to or accessible by a user and used to verify that the user is who he says
he is. Credentials are presented to the server during authentication. Examples include a password, secret
key, digital certificate, and fingerprint.
CRL A Certificate Revocation List (CRL) in a PKI model is a list of certificates that have been revoked. End
entities presenting these certificates should not be trusted.
Appendix 181
Operations Guide (Release Version: 15.0.0)
Cross-Origin Resource Sharing (CORS) A mechanism that allows many resources (for example, fonts,
JavaScript) on a web page to be requested from another domain outside the domain from which the
resource originated. In particular, JavaScript’s AJAX calls can use the XMLHttpRequest mechanism.
Crowbar An open source community project by SUSE that aims to provide all necessary services to quickly
deploy and manage clouds.
current workload An element of the Compute capacity cache that is calculated based on the number of build,
snapshot, migrate, and resize operations currently in progress on a given host.
customer Alternative term for project.
customization module A user-created Python module that is loaded by horizon to change the look and feel
of the dashboard.
D
daemon A process that runs in the background and waits for requests. May or may not listen on a TCP or
UDP port. Do not confuse with a worker.
Dashboard (horizon) OpenStack project which provides an extensible, unified, web-based user interface for
all OpenStack services.
data encryption Both Image service and Compute support encrypted virtual machine (VM) images (but not
instances). In-transit data encryption is supported in OpenStack using technologies such as HTTPS,
SSL, TLS, and SSH. Object Storage does not support object encryption at the application level but may
support storage that uses disk encryption.
Data loss prevention (DLP) software Software programs used to protect sensitive information and prevent
it from leaking outside a network boundary through the detection and denying of the data transportation.
Data Processing service (sahara) OpenStack project that provides a scalable data-processing stack and associated
management interfaces.
data store A database engine supported by the Database service.
database ID A unique ID given to each replica of an Object Storage database.
database replicator An Object Storage component that copies changes in the account, container, and object
databases to other nodes.
Database service (trove) An integrated project that provides scalable and reliable Cloud Database-as-aService
functionality for both relational and non-relational database engines.
deallocate The process of removing the association between a floating IP address and a fixed IP address. Once
this association is removed, the floating IP returns to the address pool.
Debian A Linux distribution that is compatible with OpenStack.
deduplication The process of finding duplicate data at the disk block, file, and/or object level to minimize
storage use—currently unsupported within OpenStack.
default panel The default panel that is displayed when a user accesses the dashboard.
default project New users are assigned to this project if no project is specified when a user is created.
default token An Identity service token that is not associated with a specific project and is exchanged for a
scoped token.
182 Appendix
Operations Guide (Release Version: 15.0.0)
delayed delete An option within Image service so that an image is deleted after a predefined number of seconds
instead of immediately.
delivery mode Setting for the Compute RabbitMQ message delivery mode; can be set to either transient or
persistent.
denial of service (DoS) Denial of service (DoS) is a short form for denial-of-service attack. This is a malicious
attempt to prevent legitimate users from using a service.
deprecated auth An option within Compute that enables administrators to create and manage users through
the nova-manage command as opposed to using the Identity service.
designate Code name for the DNS service.
Desktop-as-a-Service A platform that provides a suite of desktop environments that users access to receive
a desktop experience from any location. This may provide general use, development, or even homogeneous
testing environments.
developer One of the default roles in the Compute RBAC system and the default role assigned to a new user.
device ID Maps Object Storage partitions to physical storage devices.
device weight Distributes partitions proportionately across Object Storage devices based on the storage capacity
of each device.
DevStack Community project that uses shell scripts to quickly build complete OpenStack development environments.
DHCP agent OpenStack Networking agent that provides DHCP services for virtual networks.
Diablo A grouped release of projects related to OpenStack that came out in the fall of 2011, the fourth release
of OpenStack. It included Compute (nova 2011.3), Object Storage (swift 1.4.3), and the Image service
(glance). Diablo is the code name for the fourth release of OpenStack. The design summit took place in
the Bay Area near Santa Clara, California, US and Diablo is a nearby city.
direct consumer An element of the Compute RabbitMQ that comes to life when a RPC call is executed. It
connects to a direct exchange through a unique exclusive queue, sends the message, and terminates.
direct exchange A routing table that is created within the Compute RabbitMQ during RPC calls; one is created
for each RPC call that is invoked.
direct publisher Element of RabbitMQ that provides a response to an incoming MQ message.
disassociate The process of removing the association between a floating IP address and fixed IP and thus
returning the floating IP address to the address pool.
Discretionary Access Control (DAC) Governs the ability of subjects to access objects, while enabling users
to make policy decisions and assign security attributes. The traditional UNIX system of users, groups,
and read-write-execute permissions is an example of DAC.
disk encryption The ability to encrypt data at the file system, disk partition, or whole-disk level. Supported
within Compute VMs.
disk format The underlying format that a disk image for a VM is stored as within the Image service back-end
store. For example, AMI, ISO, QCOW2, VMDK, and so on.
dispersion In Object Storage, tools to test and ensure dispersion of objects and containers to ensure fault
tolerance.
distributed virtual router (DVR) Mechanism for highly available multi-host routing when using OpenStack
Networking (neutron).
Appendix 183
Operations Guide (Release Version: 15.0.0)
Django A web framework used extensively in horizon.
DNS record A record that specifies information about a particular domain and belongs to the domain.
DNS service (designate) OpenStack project that provides scalable, on demand, self service access to authoritative
DNS services, in a technology-agnostic manner.
dnsmasq Daemon that provides DNS, DHCP, BOOTP, and TFTP services for virtual networks.
domain An Identity API v3 entity. Represents a collection of projects, groups and users that defines administrative
boundaries for managing OpenStack Identity entities. On the Internet, separates a website from
other sites. Often, the domain name has two or more parts that are separated by dots. For example,
yahoo.com, usa.gov, harvard.edu, or mail.yahoo.com. Also, a domain is an entity or container of all
DNS-related information containing one or more records.
Domain Name System (DNS) A system by which Internet domain name-to-address and address-to-name resolutions
are determined. DNS helps navigate the Internet by translating the IP address into an address
that is easier to remember. For example, translating 111.111.111.1 into www.yahoo.com. All domains
and their components, such as mail servers, utilize DNS to resolve to the appropriate locations. DNS
servers are usually set up in a master-slave relationship such that failure of the master invokes the slave.
DNS servers might also be clustered or replicated such that changes made to one DNS server are automatically
propagated to other active servers. In Compute, the support that enables associating DNS
entries with floating IP addresses, nodes, or cells so that hostnames are consistent across reboots.
download The transfer of data, usually in the form of files, from one computer to another.
durable exchange The Compute RabbitMQ message exchange that remains active when the server restarts.
durable queue A Compute RabbitMQ message queue that remains active when the server restarts.
Dynamic Host Configuration Protocol (DHCP) A network protocol that configures devices that are connected
to a network so that they can communicate on that network by using the Internet Protocol (IP).
The protocol is implemented in a client-server model where DHCP clients request configuration data,
such as an IP address, a default route, and one or more DNS server addresses from a DHCP server. A
method to automatically configure networking for a host at boot time. Provided by both Networking and
Compute.
Dynamic HyperText Markup Language (DHTML) Pages that use HTML, JavaScript, and Cascading Style
Sheets to enable users to interact with a web page or show simple animation.
E
east-west traffic Network traffic between servers in the same cloud or data center. See also north-south traffic.
EBS boot volume An Amazon EBS storage volume that contains a bootable VM image, currently unsupported
in OpenStack.
ebtables Filtering tool for a Linux bridging firewall, enabling filtering of network traffic passing through
a Linux bridge. Used in Compute along with arptables, iptables, and ip6tables to ensure isolation of
network communications.
EC2 The Amazon commercial compute product, similar to Compute.
EC2 access key Used along with an EC2 secret key to access the Compute EC2 API.
EC2 API OpenStack supports accessing the Amazon EC2 API through Compute.
EC2 Compatibility API A Compute component that enables OpenStack to communicate with Amazon EC2.
184 Appendix
Operations Guide (Release Version: 15.0.0)
EC2 secret key Used along with an EC2 access key when communicating with the Compute EC2 API; used
to digitally sign each request.
Elastic Block Storage (EBS) The Amazon commercial block storage product.
encapsulation The practice of placing one packet type within another for the purposes of abstracting or securing
data. Examples include GRE, MPLS, or IPsec.
encryption OpenStack supports encryption technologies such as HTTPS, SSH, SSL, TLS, digital certificates,
and data encryption.
endpoint See API endpoint.
endpoint registry Alternative term for an Identity service catalog.
endpoint template A list of URL and port number endpoints that indicate where a service, such as Object
Storage, Compute, Identity, and so on, can be accessed.
entity Any piece of hardware or software that wants to connect to the network services provided by Networking,
the network connectivity service. An entity can make use of Networking by implementing a
VIF.
ephemeral image A VM image that does not save changes made to its volumes and reverts them to their
original state after the instance is terminated.
ephemeral volume Volume that does not save the changes made to it and reverts to its original state when the
current user relinquishes control.
Essex A grouped release of projects related to OpenStack that came out in April 2012, the fifth release of
OpenStack. It included Compute (nova 2012.1), Object Storage (swift 1.4.8), Image (glance), Identity
(keystone), and Dashboard (horizon). Essex is the code name for the fifth release of OpenStack. The
design summit took place in Boston, Massachusetts, US and Essex is a nearby city.
ESXi An OpenStack-supported hypervisor.
ETag MD5 hash of an object within Object Storage, used to ensure data integrity.
euca2ools A collection of command-line tools for administering VMs; most are compatible with OpenStack.
Eucalyptus Kernel Image (EKI) Used along with an ERI to create an EMI.
Eucalyptus Machine Image (EMI) VM image container format supported by Image service.
Eucalyptus Ramdisk Image (ERI) Used along with an EKI to create an EMI.
evacuate The process of migrating one or all virtual machine (VM) instances from one host to another, compatible
with both shared storage live migration and block migration.
exchange Alternative term for a RabbitMQ message exchange.
exchange type A routing algorithm in the Compute RabbitMQ.
exclusive queue Connected to by a direct consumer in RabbitMQ—Compute, the message can be consumed
only by the current connection.
extended attributes (xattr) File system option that enables storage of additional information beyond owner,
group, permissions, modification time, and so on. The underlying Object Storage file system must support
extended attributes.
extension Alternative term for an API extension or plug-in. In the context of Identity service, this is a call that
is specific to the implementation, such as adding support for OpenID.
external network A network segment typically used for instance Internet access.
Appendix 185
Operations Guide (Release Version: 15.0.0)
extra specs Specifies additional requirements when Compute determines where to start a new instance. Examples
include a minimum amount of network bandwidth or a GPU.
F
FakeLDAP An easy method to create a local LDAP directory for testing Identity and Compute. Requires
Redis.
fan-out exchange Within RabbitMQ and Compute, it is the messaging interface that is used by the scheduler
service to receive capability messages from the compute, volume, and network nodes.
federated identity A method to establish trusts between identity providers and the OpenStack cloud.
Fedora A Linux distribution compatible with OpenStack.
Fibre Channel Storage protocol similar in concept to TCP/IP; encapsulates SCSI commands and data.
Fibre Channel over Ethernet (FCoE) The fibre channel protocol tunneled within Ethernet.
fill-first scheduler The Compute scheduling method that attempts to fill a host with VMs rather than starting
new VMs on a variety of hosts.
filter The step in the Compute scheduling process when hosts that cannot run VMs are eliminated and not
chosen.
firewall Used to restrict communications between hosts and/or nodes, implemented in Compute using iptables,
arptables, ip6tables, and ebtables.
FireWall-as-a-Service (FWaaS) A Networking extension that provides perimeter firewall functionality.
fixed IP address An IP address that is associated with the same instance each time that instance boots, is
generally not accessible to end users or the public Internet, and is used for management of the instance.
Flat Manager The Compute component that gives IP addresses to authorized nodes and assumes DHCP, DNS,
and routing configuration and services are provided by something else.
flat mode injection A Compute networking method where the OS network configuration information is injected
into the VM image before the instance starts.
flat network Virtual network type that uses neither VLANs nor tunnels to segregate project traffic. Each
flat network typically requires a separate underlying physical interface defined by bridge mappings.
However, a flat network can contain multiple subnets.
FlatDHCP Manager The Compute component that provides dnsmasq (DHCP, DNS, BOOTP, TFTP) and
radvd (routing) services.
flavor Alternative term for a VM instance type.
flavor ID UUID for each Compute or Image service VM flavor or instance type.
floating IP address An IP address that a project can associate with a VM so that the instance has the same
public IP address each time that it boots. You create a pool of floating IP addresses and assign them to
instances as they are launched to maintain a consistent IP address for maintaining DNS assignment.
Folsom A grouped release of projects related to OpenStack that came out in the fall of 2012, the sixth release
of OpenStack. It includes Compute (nova), Object Storage (swift), Identity (keystone), Networking
(neutron), Image service (glance), and Volumes or Block Storage (cinder). Folsom is the code name
for the sixth release of OpenStack. The design summit took place in San Francisco, California, US and
Folsom is a nearby city.
186 Appendix
Operations Guide (Release Version: 15.0.0)
FormPost Object Storage middleware that uploads (posts) an image through a form on a web page.
freezer Code name for the Backup, Restore, and Disaster Recovery service.
front end The point where a user interacts with a service; can be an API endpoint, the dashboard, or a
command-line tool.
G
gateway An IP address, typically assigned to a router, that passes network traffic between different networks.
generic receive offload (GRO) Feature of certain network interface drivers that combines many smaller received
packets into a large packet before delivery to the kernel IP stack.
generic routing encapsulation (GRE) Protocol that encapsulates a wide variety of network layer protocols
inside virtual point-to-point links.
glance Codename for the Image service.
glance API server Alternative name for the Image API.
glance registry Alternative term for the Image service image registry.
global endpoint template The Identity service endpoint template that contains services available to all
projects.
GlusterFS A file system designed to aggregate NAS hosts, compatible with OpenStack.
gnocchi Part of the OpenStack Telemetry service; provides an indexer and time-series database.
golden image A method of operating system installation where a finalized disk image is created and then used
by all nodes without modification.
Governance service (congress) The project that provides Governance-as-a-Service across any collection of
cloud services in order to monitor, enforce, and audit policy over dynamic infrastructure.
Graphic Interchange Format (GIF) A type of image file that is commonly used for animated images on web
pages.
Graphics Processing Unit (GPU) Choosing a host based on the existence of a GPU is currently unsupported
in OpenStack.
Green Threads The cooperative threading model used by Python; reduces race conditions and only context
switches when specific library calls are made. Each OpenStack service is its own thread.
Grizzly The code name for the seventh release of OpenStack. The design summit took place in San Diego,
California, US and Grizzly is an element of the state flag of California.
Group An Identity v3 API entity. Represents a collection of users that is owned by a specific domain.
guest OS An operating system instance running under the control of a hypervisor.
H
Hadoop Apache Hadoop is an open source software framework that supports data-intensive distributed applications.
Hadoop Distributed File System (HDFS) A distributed, highly fault-tolerant file system designed to run on
low-cost commodity hardware.
Appendix 187
Operations Guide (Release Version: 15.0.0)
handover An object state in Object Storage where a new replica of the object is automatically created due to
a drive failure.
HAProxy Provides a load balancer for TCP and HTTP-based applications that spreads requests across multiple
servers.
hard reboot A type of reboot where a physical or virtual power button is pressed as opposed to a graceful,
proper shutdown of the operating system.
Havana The code name for the eighth release of OpenStack. The design summit took place in Portland,
Oregon, US and Havana is an unincorporated community in Oregon.
health monitor Determines whether back-end members of a VIP pool can process a request. A pool can
have several health monitors associated with it. When a pool has several monitors associated with it, all
monitors check each member of the pool. All monitors must declare a member to be healthy for it to stay
active.
heat Codename for the Orchestration service.
Heat Orchestration Template (HOT) Heat input in the format native to OpenStack.
high availability (HA) A high availability system design approach and associated service implementation
ensures that a prearranged level of operational performance will be met during a contractual measurement
period. High availability systems seek to minimize system downtime and data loss.
horizon Codename for the Dashboard.
horizon plug-in A plug-in for the OpenStack Dashboard (horizon).
host A physical computer, not a VM instance (node).
host aggregate A method to further subdivide availability zones into hypervisor pools, a collection of common
hosts.
Host Bus Adapter (HBA) Device plugged into a PCI slot, such as a fibre channel or network card.
hybrid cloud A hybrid cloud is a composition of two or more clouds (private, community or public) that
remain distinct entities but are bound together, offering the benefits of multiple deployment models.
Hybrid cloud can also mean the ability to connect colocation, managed and/or dedicated services with
cloud resources.
Hyper-V One of the hypervisors supported by OpenStack.
hyperlink Any kind of text that contains a link to some other site, commonly found in documents where
clicking on a word or words opens up a different website.
Hypertext Transfer Protocol (HTTP) An application protocol for distributed, collaborative, hypermedia information
systems. It is the foundation of data communication for the World Wide Web. Hypertext is
structured text that uses logical links (hyperlinks) between nodes containing text. HTTP is the protocol
to exchange or transfer hypertext.
Hypertext Transfer Protocol Secure (HTTPS) An encrypted communications protocol for secure communication
over a computer network, with especially wide deployment on the Internet. Technically, it is not a
protocol in and of itself; rather, it is the result of simply layering the Hypertext Transfer Protocol (HTTP)
on top of the TLS or SSL protocol, thus adding the security capabilities of TLS or SSL to standard HTTP
communications. Most OpenStack API endpoints and many inter-component communications support
HTTPS communication.
hypervisor Software that arbitrates and controls VM access to the actual underlying hardware.
hypervisor pool A collection of hypervisors grouped together through host aggregates.
188 Appendix
Operations Guide (Release Version: 15.0.0)
I
Icehouse The code name for the ninth release of OpenStack. The design summit took place in Hong Kong
and Ice House is a street in that city.
ID number Unique numeric ID associated with each user in Identity, conceptually similar to a Linux or LDAP
UID.
Identity API Alternative term for the Identity service API.
Identity back end The source used by Identity service to retrieve user information; an OpenLDAP server, for
example.
identity provider A directory service, which allows users to login with a user name and password. It is a
typical source of authentication tokens.
Identity service (keystone) The project that facilitates API client authentication, service discovery, distributed
multi-project authorization, and auditing. It provides a central directory of users mapped to
the OpenStack services they can access. It also registers endpoints for OpenStack services and acts as a
common authentication system.
Identity service API The API used to access the OpenStack Identity service provided through keystone.
IETF Internet Engineering Task Force (IETF) is an open standards organization that develops Internet standards,
particularly the standards pertaining to TCP/IP.
image A collection of files for a specific operating system (OS) that you use to create or rebuild a server.
OpenStack provides pre-built images. You can also create custom images, or snapshots, from servers
that you have launched. Custom images can be used for data backups or as “gold” images for additional
servers.
Image API The Image service API endpoint for management of VM images. Processes client requests for
VMs, updates Image service metadata on the registry server, and communicates with the store adapter to
upload VM images from the back-end store.
image cache Used by Image service to obtain images on the local host rather than re-downloading them from
the image server each time one is requested.
image ID Combination of a URI and UUID used to access Image service VM images through the image API.
image membership A list of projects that can access a given VM image within Image service.
image owner The project who owns an Image service virtual machine image.
image registry A list of VM images that are available through Image service.
Image service (glance) The OpenStack service that provide services and associated libraries to store, browse,
share, distribute and manage bootable disk images, other data closely associated with initializing compute
resources, and metadata definitions.
image status The current status of a VM image in Image service, not to be confused with the status of a running
instance.
image store The back-end store used by Image service to store VM images, options include Object Storage,
locally mounted file system, RADOS block devices, VMware datastore, or HTTP.
image UUID UUID used by Image service to uniquely identify each VM image.
incubated project A community project may be elevated to this status and is then promoted to a core project.
Appendix 189
Operations Guide (Release Version: 15.0.0)
Infrastructure Optimization service (watcher) OpenStack project that aims to provide a flexible and scalable
resource optimization service for multi-project OpenStack-based clouds.
Infrastructure-as-a-Service (IaaS) IaaS is a provisioning model in which an organization outsources physical
components of a data center, such as storage, hardware, servers, and networking components. A service
provider owns the equipment and is responsible for housing, operating and maintaining it. The client
typically pays on a per-use basis. IaaS is a model for providing cloud services.
ingress filtering The process of filtering incoming network traffic. Supported by Compute.
INI format The OpenStack configuration files use an INI format to describe options and their values. It
consists of sections and key value pairs.
injection The process of putting a file into a virtual machine image before the instance is started.
Input/Output Operations Per Second (IOPS) IOPS are a common performance measurement used to
benchmark computer storage devices like hard disk drives, solid state drives, and storage area networks.
instance A running VM, or a VM in a known state such as suspended, that can be used like a hardware server.
instance ID Alternative term for instance UUID.
instance state The current state of a guest VM image.
instance tunnels network A network segment used for instance traffic tunnels between compute nodes and
the network node.
instance type Describes the parameters of the various virtual machine images that are available to users;
includes parameters such as CPU, storage, and memory. Alternative term for flavor.
instance type ID Alternative term for a flavor ID.
instance UUID Unique ID assigned to each guest VM instance.
Intelligent Platform Management Interface (IPMI) IPMI is a standardized computer system interface used
by system administrators for out-of-band management of computer systems and monitoring of their operation.
In layman’s terms, it is a way to manage a computer using a direct network connection, whether
it is turned on or not; connecting to the hardware rather than an operating system or login shell.
interface A physical or virtual device that provides connectivity to another device or medium.
interface ID Unique ID for a Networking VIF or vNIC in the form of a UUID.
Internet Control Message Protocol (ICMP) A network protocol used by network devices for control messages.
For example, ping uses ICMP to test connectivity.
Internet protocol (IP) Principal communications protocol in the internet protocol suite for relaying datagrams
across network boundaries.
Internet Service Provider (ISP) Any business that provides Internet access to individuals or businesses.
Internet Small Computer System Interface (iSCSI) Storage protocol that encapsulates SCSI frames for
transport over IP networks. Supported by Compute, Object Storage, and Image service.
IP address Number that is unique to every computer system on the Internet. Two versions of the Internet
Protocol (IP) are in use for addresses: IPv4 and IPv6.
IP Address Management (IPAM) The process of automating IP address allocation, deallocation, and management.
Currently provided by Compute, melange, and Networking.
190 Appendix
Operations Guide (Release Version: 15.0.0)
ip6tables Tool used to set up, maintain, and inspect the tables of IPv6 packet filter rules in the Linux kernel.
In OpenStack Compute, ip6tables is used along with arptables, ebtables, and iptables to create firewalls
for both nodes and VMs.
ipset Extension to iptables that allows creation of firewall rules that match entire “sets” of IP addresses simultaneously.
These sets reside in indexed data structures to increase efficiency, particularly on systems
with a large quantity of rules.
iptables Used along with arptables and ebtables, iptables create firewalls in Compute. iptables are the tables
provided by the Linux kernel firewall (implemented as different Netfilter modules) and the chains and
rules it stores. Different kernel modules and programs are currently used for different protocols: iptables
applies to IPv4, ip6tables to IPv6, arptables to ARP, and ebtables to Ethernet frames. Requires root
privilege to manipulate.
ironic Codename for the Bare Metal service.
iSCSI Qualified Name (IQN) IQN is the format most commonly used for iSCSI names, which uniquely identify
nodes in an iSCSI network. All IQNs follow the pattern iqn.yyyy-mm.domain:identifier, where
‘yyyy-mm’ is the year and month in which the domain was registered, ‘domain’ is the reversed domain
name of the issuing organization, and ‘identifier’ is an optional string which makes each IQN under the
same domain unique. For example, ‘iqn.2015-10.org.openstack.408ae959bce1’.
ISO9660 One of the VM image disk formats supported by Image service.
itsec A default role in the Compute RBAC system that can quarantine an instance in any project.
J
Java A programming language that is used to create systems that involve more than one computer by way of
a network.
JavaScript A scripting language that is used to build web pages.
JavaScript Object Notation (JSON) One of the supported response formats in OpenStack.
jumbo frame Feature in modern Ethernet networks that supports frames up to approximately 9000 bytes.
Juno The code name for the tenth release of OpenStack. The design summit took place in Atlanta, Georgia,
US and Juno is an unincorporated community in Georgia.
K
Kerberos A network authentication protocol which works on the basis of tickets. Kerberos allows nodes
communication over a non-secure network, and allows nodes to prove their identity to one another in a
secure manner.
kernel-based VM (KVM) An OpenStack-supported hypervisor. KVM is a full virtualization solution for
Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V), ARM, IBM Power,
and IBM zSeries. It consists of a loadable kernel module, that provides the core virtualization infrastructure
and a processor specific module.
Key Manager service (barbican) The project that produces a secret storage and generation system capable
of providing key management for services wishing to enable encryption features.
keystone Codename of the Identity service.
Appendix 191
Operations Guide (Release Version: 15.0.0)
Kickstart A tool to automate system configuration and installation on Red Hat, Fedora, and CentOS-based
Linux distributions.
Kilo The code name for the eleventh release of OpenStack. The design summit took place in Paris, France.
Due to delays in the name selection, the release was known only as K. Because k is the unit symbol
for kilo and the kilogram reference artifact is stored near Paris in the Pavillon de Breteuil in Sèvres, the
community chose Kilo as the release name.
L
large object An object within Object Storage that is larger than 5 GB.
Launchpad The collaboration site for OpenStack.
Layer-2 (L2) agent OpenStack Networking agent that provides layer-2 connectivity for virtual networks.
Layer-2 network Term used in the OSI network architecture for the data link layer. The data link layer is
responsible for media access control, flow control and detecting and possibly correcting errors that may
occur in the physical layer.
Layer-3 (L3) agent OpenStack Networking agent that provides layer-3 (routing) services for virtual networks.
Layer-3 network Term used in the OSI network architecture for the network layer. The network layer is
responsible for packet forwarding including routing from one node to another.
Liberty The code name for the twelfth release of OpenStack. The design summit took place in Vancouver,
Canada and Liberty is the name of a village in the Canadian province of Saskatchewan.
libvirt Virtualization API library used by OpenStack to interact with many of its supported hypervisors.
Lightweight Directory Access Protocol (LDAP) An application protocol for accessing and maintaining distributed
directory information services over an IP network.
Linux Unix-like computer operating system assembled under the model of free and open-source software
development and distribution.
Linux bridge Software that enables multiple VMs to share a single physical NIC within Compute.
Linux Bridge neutron plug-in Enables a Linux bridge to understand a Networking port, interface attachment,
and other abstractions.
Linux containers (LXC) An OpenStack-supported hypervisor.
live migration The ability within Compute to move running virtual machine instances from one host to another
with only a small service interruption during switchover.
load balancer A load balancer is a logical device that belongs to a cloud account. It is used to distribute
workloads between multiple back-end systems or services, based on the criteria defined as part of its
configuration.
load balancing The process of spreading client requests between two or more nodes to improve performance
and availability.
Load-Balancer-as-a-Service (LBaaS) Enables Networking to distribute incoming requests evenly between
designated instances.
Load-balancing service (octavia) The project that aims to provide scalable, on demand, self service access
to load-balancer services, in technology-agnostic manner.
192 Appendix
Operations Guide (Release Version: 15.0.0)
Logical Volume Manager (LVM) Provides a method of allocating space on mass-storage devices that is more
flexible than conventional partitioning schemes.
M
magnum Code name for the Containers Infrastructure Management service.
management API Alternative term for an admin API.
management network A network segment used for administration, not accessible to the public Internet.
manager Logical groupings of related code, such as the Block Storage volume manager or network manager.
manifest Used to track segments of a large object within Object Storage.
manifest object A special Object Storage object that contains the manifest for a large object.
manila Codename for OpenStack Shared File Systems service.
manila-share Responsible for managing Shared File System Service devices, specifically the back-end devices.
maximum transmission unit (MTU) Maximum frame or packet size for a particular network medium. Typically
1500 bytes for Ethernet networks.
mechanism driver A driver for the Modular Layer 2 (ML2) neutron plug-in that provides layer-2 connectivity
for virtual instances. A single OpenStack installation can use multiple mechanism drivers.
melange Project name for OpenStack Network Information Service. To be merged with Networking.
membership The association between an Image service VM image and a project. Enables images to be shared
with specified projects.
membership list A list of projects that can access a given VM image within Image service.
memcached A distributed memory object caching system that is used by Object Storage for caching.
memory overcommit The ability to start new VM instances based on the actual memory usage of a host, as
opposed to basing the decision on the amount of RAM each running instance thinks it has available. Also
known as RAM overcommit.
message broker The software package used to provide AMQP messaging capabilities within Compute. Default
package is RabbitMQ.
message bus The main virtual communication line used by all AMQP messages for inter-cloud communications
within Compute.
message queue Passes requests from clients to the appropriate workers and returns the output to the client
after the job completes.
Message service (zaqar) The project that provides a messaging service that affords a variety of distributed
application patterns in an efficient, scalable and highly available manner, and to create and maintain
associated Python libraries and documentation.
Meta-Data Server (MDS) Stores CephFS metadata.
Metadata agent OpenStack Networking agent that provides metadata services for instances.
migration The process of moving a VM instance from one host to another.
mistral Code name for Workflow service.
Appendix 193
Operations Guide (Release Version: 15.0.0)
Mitaka The code name for the thirteenth release of OpenStack. The design summit took place in Tokyo,
Japan. Mitaka is a city in Tokyo.
Modular Layer 2 (ML2) neutron plug-in Can concurrently use multiple layer-2 networking technologies,
such as 802.1Q and VXLAN, in Networking.
monasca Codename for OpenStack Monitoring.
Monitor (LBaaS) LBaaS feature that provides availability monitoring using the ping command, TCP, and
HTTP/HTTPS GET.
Monitor (Mon) A Ceph component that communicates with external clients, checks data state and consistency,
and performs quorum functions.
Monitoring (monasca) The OpenStack service that provides a multi-project, highly scalable, performant,
fault-tolerant monitoring-as-a-service solution for metrics, complex event processing and logging. To
build an extensible platform for advanced monitoring services that can be used by both operators and
projects to gain operational insight and visibility, ensuring availability and stability.
multi-factor authentication Authentication method that uses two or more credentials, such as a password
and a private key. Currently not supported in Identity.
multi-host High-availability mode for legacy (nova) networking. Each compute node handles NAT and DHCP
and acts as a gateway for all of the VMs on it. A networking failure on one compute node doesn’t affect
VMs on other compute nodes.
multinic Facility in Compute that allows each virtual machine instance to have more than one VIF connected
to it.
murano Codename for the Application Catalog service.
N
Nebula Released as open source by NASA in 2010 and is the basis for Compute.
netadmin One of the default roles in the Compute RBAC system. Enables the user to allocate publicly accessible
IP addresses to instances and change firewall rules.
NetApp volume driver Enables Compute to communicate with NetApp storage devices through the NetApp
OnCommand Provisioning Manager.
network A virtual network that provides connectivity between entities. For example, a collection of virtual
ports that share network connectivity. In Networking terminology, a network is always a layer-2 network.
Network Address Translation (NAT) Process of modifying IP address information while in transit. Supported
by Compute and Networking.
network controller A Compute daemon that orchestrates the network configuration of nodes, including IP
addresses, VLANs, and bridging. Also manages routing for both public and private networks.
Network File System (NFS) A method for making file systems available over the network. Supported by
OpenStack.
network ID Unique ID assigned to each network segment within Networking. Same as network UUID.
network manager The Compute component that manages various network components, such as firewall rules,
IP address allocation, and so on.
194 Appendix
Operations Guide (Release Version: 15.0.0)
network namespace Linux kernel feature that provides independent virtual networking instances on a single
host with separate routing tables and interfaces. Similar to virtual routing and forwarding (VRF) services
on physical network equipment.
network node Any compute node that runs the network worker daemon.
network segment Represents a virtual, isolated OSI layer-2 subnet in Networking.
Network Service Header (NSH) Provides a mechanism for metadata exchange along the instantiated service
path.
Network Time Protocol (NTP) Method of keeping a clock for a host or node correct via communication with
a trusted, accurate time source.
network UUID Unique ID for a Networking network segment.
network worker The nova-network worker daemon; provides services such as giving an IP address to a
booting nova instance.
Networking API (Neutron API) API used to access OpenStack Networking. Provides an extensible architecture
to enable custom plug-in creation.
Networking service (neutron) The OpenStack project which implements services and associated libraries to
provide on-demand, scalable, and technology-agnostic network abstraction.
neutron Codename for OpenStack Networking service.
neutron API An alternative name for Networking API.
neutron manager Enables Compute and Networking integration, which enables Networking to perform network
management for guest VMs.
neutron plug-in Interface within Networking that enables organizations to create custom plug-ins for advanced
features, such as QoS, ACLs, or IDS.
Newton The code name for the fourteenth release of OpenStack. The design summit took place in Austin,
Texas, US. The release is named after “Newton House” which is located at 1013 E. Ninth St., Austin,
TX. which is listed on the National Register of Historic Places.
Nexenta volume driver Provides support for NexentaStor devices in Compute.
NFV Orchestration Service (tacker) OpenStack service that aims to implement Network Function Virtualization
(NFV) orchestration services and libraries for end-to-end life-cycle management of network
services and Virtual Network Functions (VNFs).
Nginx An HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server.
No ACK Disables server-side message acknowledgment in the Compute RabbitMQ. Increases performance
but decreases reliability.
node A VM instance that runs on a host.
non-durable exchange Message exchange that is cleared when the service restarts. Its data is not written to
persistent storage.
non-durable queue Message queue that is cleared when the service restarts. Its data is not written to persistent
storage.
non-persistent volume Alternative term for an ephemeral volume.
north-south traffic Network traffic between a user or client (north) and a server (south), or traffic into the
cloud (south) and out of the cloud (north). See also east-west traffic.
Appendix 195
Operations Guide (Release Version: 15.0.0)
nova Codename for OpenStack Compute service.
Nova API Alternative term for the Compute API.
nova-network A Compute component that manages IP address allocation, firewalls, and other network-related
tasks. This is the legacy networking option and an alternative to Networking.
O
object A BLOB of data held by Object Storage; can be in any format.
object auditor Opens all objects for an object server and verifies the MD5 hash, size, and metadata for each
object.
object expiration A configurable option within Object Storage to automatically delete objects after a specified
amount of time has passed or a certain date is reached.
object hash Unique ID for an Object Storage object.
object path hash Used by Object Storage to determine the location of an object in the ring. Maps objects to
partitions.
object replicator An Object Storage component that copies an object to remote partitions for fault tolerance.
object server An Object Storage component that is responsible for managing objects.
Object Storage API API used to access OpenStack Object Storage.
Object Storage Device (OSD) The Ceph storage daemon.
Object Storage service (swift) The OpenStack core project that provides eventually consistent and redundant
storage and retrieval of fixed digital content.
object versioning Allows a user to set a flag on an Object Storage container so that all objects within the
container are versioned.
Ocata The code name for the fifteenth release of OpenStack. The design summit will take place in Barcelona,
Spain. Ocata is a beach north of Barcelona.
Octavia Code name for the Load-balancing service.
Oldie Term for an Object Storage process that runs for a long time. Can indicate a hung process.
Open Cloud Computing Interface (OCCI) A standardized interface for managing compute, data, and network
resources, currently unsupported in OpenStack.
Open Virtualization Format (OVF) Standard for packaging VM images. Supported in OpenStack.
Open vSwitch Open vSwitch is a production quality, multilayer virtual switch licensed under the open source
Apache 2.0 license. It is designed to enable massive network automation through programmatic extension,
while still supporting standard management interfaces and protocols (for example NetFlow, sFlow,
SPAN, RSPAN, CLI, LACP, 802.1ag).
Open vSwitch (OVS) agent Provides an interface to the underlying Open vSwitch service for the Networking
plug-in.
Open vSwitch neutron plug-in Provides support for Open vSwitch in Networking.
OpenLDAP An open source LDAP server. Supported by both Compute and Identity.
196 Appendix
Operations Guide (Release Version: 15.0.0)
OpenStack OpenStack is a cloud operating system that controls large pools of compute, storage, and networking
resources throughout a data center, all managed through a dashboard that gives administrators
control while empowering their users to provision resources through a web interface. OpenStack is an
open source project licensed under the Apache License 2.0.
OpenStack code name Each OpenStack release has a code name. Code names ascend in alphabetical order:
Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, Icehouse, Juno, Kilo, Liberty, Mitaka,
Newton, Ocata, Pike, Queens, and Rocky. Code names are cities or counties near where the corresponding
OpenStack design summit took place. An exception, called the Waldon exception, is granted to
elements of the state flag that sound especially cool. Code names are chosen by popular vote.
openSUSE A Linux distribution that is compatible with OpenStack.
operator The person responsible for planning and maintaining an OpenStack installation.
optional service An official OpenStack service defined as optional by DefCore Committee. Currently, consists
of Dashboard (horizon), Telemetry service (Telemetry), Orchestration service (heat), Database service
(trove), Bare Metal service (ironic), and so on.
Orchestration service (heat) The OpenStack service which orchestrates composite cloud applications using
a declarative template format through an OpenStack-native REST API.
orphan In the context of Object Storage, this is a process that is not terminated after an upgrade, restart, or
reload of the service.
Oslo Codename for the Common Libraries project.
P
panko Part of the OpenStack Telemetry service; provides event storage.
parent cell If a requested resource, such as CPU time, disk storage, or memory, is not available in the parent
cell, the request is forwarded to associated child cells.
partition A unit of storage within Object Storage used to store objects. It exists on top of devices and is
replicated for fault tolerance.
partition index Contains the locations of all Object Storage partitions within the ring.
partition shift value Used by Object Storage to determine which partition data should reside on.
path MTU discovery (PMTUD) Mechanism in IP networks to detect end-to-end MTU and adjust packet size
accordingly.
pause A VM state where no changes occur (no changes in memory, network communications stop, etc); the
VM is frozen but not shut down.
PCI passthrough Gives guest VMs exclusive access to a PCI device. Currently supported in OpenStack
Havana and later releases.
persistent message A message that is stored both in memory and on disk. The message is not lost after a
failure or restart.
persistent volume Changes to these types of disk volumes are saved.
personality file A file used to customize a Compute instance. It can be used to inject SSH keys or a specific
network configuration.
Appendix 197
Operations Guide (Release Version: 15.0.0)
Pike The code name for the sixteenth release of OpenStack. The design summit will take place in Boston,
Massachusetts, US. The release is named after the Massachusetts Turnpike, abbreviated commonly as
the Mass Pike, which is the easternmost stretch of Interstate 90.
Platform-as-a-Service (PaaS) Provides to the consumer the ability to deploy applications through a programming
language or tools supported by the cloud platform provider. An example of Platform-as-a-Service
is an Eclipse/Java programming platform provided with no downloads required.
plug-in Software component providing the actual implementation for Networking APIs, or for Compute APIs,
depending on the context.
policy service Component of Identity that provides a rule-management interface and a rule-based authorization
engine.
policy-based routing (PBR) Provides a mechanism to implement packet forwarding and routing according
to the policies defined by the network administrator.
pool A logical set of devices, such as web servers, that you group together to receive and process traffic.
The load balancing function chooses which member of the pool handles the new requests or connections
received on the VIP address. Each VIP has one pool.
pool member An application that runs on the back-end server in a load-balancing system.
port A virtual network port within Networking; VIFs / vNICs are connected to a port.
port UUID Unique ID for a Networking port.
preseed A tool to automate system configuration and installation on Debian-based Linux distributions.
private image An Image service VM image that is only available to specified projects.
private IP address An IP address used for management and administration, not available to the public Internet.
private network The Network Controller provides virtual networks to enable compute servers to interact
with each other and with the public network. All machines must have a public and private network
interface. A private network interface can be a flat or VLAN network interface. A flat network interface
is controlled by the flat_interface with flat managers. A VLAN network interface is controlled by the
vlan_interface option with VLAN managers.
project Projects represent the base unit of “ownership” in OpenStack, in that all resources in OpenStack should
be owned by a specific project. In OpenStack Identity, a project must be owned by a specific domain.
project ID Unique ID assigned to each project by the Identity service.
project VPN Alternative term for a cloudpipe.
promiscuous mode Causes the network interface to pass all traffic it receives to the host rather than passing
only the frames addressed to it.
protected property Generally, extra properties on an Image service image to which only cloud administrators
have access. Limits which user roles can perform CRUD operations on that property. The cloud
administrator can configure any image property as protected.
provider An administrator who has access to all hosts and instances.
proxy node A node that provides the Object Storage proxy service.
proxy server Users of Object Storage interact with the service through the proxy server, which in turn looks
up the location of the requested data within the ring and returns the results to the user.
public API An API endpoint used for both service-to-service communication and end-user interactions.
198 Appendix
Operations Guide (Release Version: 15.0.0)
public image An Image service VM image that is available to all projects.
public IP address An IP address that is accessible to end-users.
public key authentication Authentication method that uses keys rather than passwords.
public network The Network Controller provides virtual networks to enable compute servers to interact with
each other and with the public network. All machines must have a public and private network interface.
The public network interface is controlled by the public_interface option.
Puppet An operating system configuration-management tool supported by OpenStack.
Python Programming language used extensively in OpenStack.
Q
QEMU Copy On Write 2 (QCOW2) One of the VM image disk formats supported by Image service.
Qpid Message queue software supported by OpenStack; an alternative to RabbitMQ.
Quality of Service (QoS) The ability to guarantee certain network or storage requirements to satisfy a Service
Level Agreement (SLA) between an application provider and end users. Typically includes performance
requirements like networking bandwidth, latency, jitter correction, and reliability as well as storage performance
in Input/Output Operations Per Second (IOPS), throttling agreements, and performance expectations
at peak load.
quarantine If Object Storage finds objects, containers, or accounts that are corrupt, they are placed in this
state, are not replicated, cannot be read by clients, and a correct copy is re-replicated.
Queens The code name for the seventeenth release of OpenStack. The design summit will take place in
Sydney, Australia. The release is named after the Queens Pound river in the South Coast region of New
South Wales.
Quick EMUlator (QEMU) QEMU is a generic and open source machine emulator and virtualizer. One of
the hypervisors supported by OpenStack, generally used for development purposes.
quota In Compute and Block Storage, the ability to set resource limits on a per-project basis.
R
RabbitMQ The default message queue software used by OpenStack.
Rackspace Cloud Files Released as open source by Rackspace in 2010; the basis for Object Storage.
RADOS Block Device (RBD) Ceph component that enables a Linux block device to be striped over multiple
distributed data stores.
radvd The router advertisement daemon, used by the Compute VLAN manager and FlatDHCP manager to
provide routing services for VM instances.
rally Codename for the Benchmark service.
RAM filter The Compute setting that enables or disables RAM overcommitment.
RAM overcommit The ability to start new VM instances based on the actual memory usage of a host, as
opposed to basing the decision on the amount of RAM each running instance thinks it has available.
Also known as memory overcommit.
Appendix 199
Operations Guide (Release Version: 15.0.0)
rate limit Configurable option within Object Storage to limit database writes on a per-account and/or percontainer
basis.
raw One of the VM image disk formats supported by Image service; an unstructured disk image.
rebalance The process of distributing Object Storage partitions across all drives in the ring; used during initial
ring creation and after ring reconfiguration.
reboot Either a soft or hard reboot of a server. With a soft reboot, the operating system is signaled to restart,
which enables a graceful shutdown of all processes. A hard reboot is the equivalent of power cycling
the server. The virtualization platform should ensure that the reboot action has completed successfully,
even in cases in which the underlying domain/VM is paused or halted/stopped.
rebuild Removes all data on the server and replaces it with the specified image. Server ID and IP addresses
remain the same.
Recon An Object Storage component that collects meters.
record Belongs to a particular domain and is used to specify information about the domain. There are several
types of DNS records. Each record type contains particular information used to describe the purpose of
that record. Examples include mail exchange (MX) records, which specify the mail server for a particular
domain; and name server (NS) records, which specify the authoritative name servers for a domain.
record ID A number within a database that is incremented each time a change is made. Used by Object
Storage when replicating.
Red Hat Enterprise Linux (RHEL) A Linux distribution that is compatible with OpenStack.
reference architecture A recommended architecture for an OpenStack cloud.
region A discrete OpenStack environment with dedicated API endpoints that typically shares only the Identity
(keystone) with other regions.
registry Alternative term for the Image service registry.
registry server An Image service that provides VM image metadata information to clients.
Reliable, Autonomic Distributed Object Store (RADOS)
A collection of components that provides object storage within Ceph. Similar to OpenStack Object
Storage.
Remote Procedure Call (RPC) The method used by the Compute RabbitMQ for intra-service communications.
replica Provides data redundancy and fault tolerance by creating copies of Object Storage objects, accounts,
and containers so that they are not lost when the underlying storage fails.
replica count The number of replicas of the data in an Object Storage ring.
replication The process of copying data to a separate physical device for fault tolerance and performance.
replicator The Object Storage back-end process that creates and manages object replicas.
request ID Unique ID assigned to each request sent to Compute.
rescue image A special type of VM image that is booted when an instance is placed into rescue mode. Allows
an administrator to mount the file systems for an instance to correct the problem.
resize Converts an existing server to a different flavor, which scales the server up or down. The original server
is saved to enable rollback if a problem occurs. All resizes must be tested and explicitly confirmed, at
which time the original server is removed.
200 Appendix
Operations Guide (Release Version: 15.0.0)
RESTful A kind of web service API that uses REST, or Representational State Transfer. REST is the style of
architecture for hypermedia systems that is used for the World Wide Web.
ring An entity that maps Object Storage data to partitions. A separate ring exists for each service, such as
account, object, and container.
ring builder Builds and manages rings within Object Storage, assigns partitions to devices, and pushes the
configuration to other storage nodes.
Rocky The code name for the eightteenth release of OpenStack. The design summit will take place in Vancouver,
Kanada. The release is named after the Rocky Mountains.
role A personality that a user assumes to perform a specific set of operations. A role includes a set of rights
and privileges. A user assuming that role inherits those rights and privileges.
Role Based Access Control (RBAC) Provides a predefined list of actions that the user can perform, such
as start or stop VMs, reset passwords, and so on. Supported in both Identity and Compute and can be
configured using the dashboard.
role ID Alphanumeric ID assigned to each Identity service role.
Root Cause Analysis (RCA) service (Vitrage) OpenStack project that aims to organize, analyze and visualize
OpenStack alarms and events, yield insights regarding the root cause of problems and deduce their
existence before they are directly detected.
rootwrap A feature of Compute that allows the unprivileged “nova” user to run a specified list of commands
as the Linux root user.
round-robin scheduler Type of Compute scheduler that evenly distributes instances among available hosts.
router A physical or virtual network device that passes network traffic between different networks.
routing key The Compute direct exchanges, fanout exchanges, and topic exchanges use this key to determine
how to process a message; processing varies depending on exchange type.
RPC driver Modular system that allows the underlying message queue software of Compute to be changed.
For example, from RabbitMQ to ZeroMQ or Qpid.
rsync Used by Object Storage to push object replicas.
RXTX cap Absolute limit on the amount of network traffic a Compute VM instance can send and receive.
RXTX quota Soft limit on the amount of network traffic a Compute VM instance can send and receive.
S
sahara Codename for the Data Processing service.
SAML assertion Contains information about a user as provided by the identity provider. It is an indication
that a user has been authenticated.
scheduler manager A Compute component that determines where VM instances should start. Uses modular
design to support a variety of scheduler types.
scoped token An Identity service API access token that is associated with a specific project.
scrubber Checks for and deletes unused VMs; the component of Image service that implements delayed
delete.
secret key String of text known only by the user; used along with an access key to make requests to the
Compute API.
Appendix 201
Operations Guide (Release Version: 15.0.0)
secure boot Process whereby the system firmware validates the authenticity of the code involved in the boot
process.
secure shell (SSH) Open source tool used to access remote hosts through an encrypted communications channel,
SSH key injection is supported by Compute.
security group A set of network traffic filtering rules that are applied to a Compute instance.
segmented object An Object Storage large object that has been broken up into pieces. The re-assembled
object is called a concatenated object.
self-service For IaaS, ability for a regular (non-privileged) account to manage a virtual infrastructure component
such as networks without involving an administrator.
SELinux Linux kernel security module that provides the mechanism for supporting access control policies.
senlin Code name for the Clustering service.
server Computer that provides explicit services to the client software running on that system, often managing
a variety of computer operations. A server is a VM instance in the Compute system. Flavor and image
are requisite elements when creating a server.
server image Alternative term for a VM image.
server UUID Unique ID assigned to each guest VM instance.
service An OpenStack service, such as Compute, Object Storage, or Image service. Provides one or more
endpoints through which users can access resources and perform operations.
service catalog Alternative term for the Identity service catalog.
Service Function Chain (SFC) For a given service, SFC is the abstracted view of the required service functions
and the order in which they are to be applied.
service ID Unique ID assigned to each service that is available in the Identity service catalog.
Service Level Agreement (SLA) Contractual obligations that ensure the availability of a service.
service project Special project that contains all services that are listed in the catalog.
service provider A system that provides services to other system entities. In case of federated identity, OpenStack
Identity is the service provider.
service registration An Identity service feature that enables services, such as Compute, to automatically register
with the catalog.
service token An administrator-defined token used by Compute to communicate securely with the Identity
service.
session back end The method of storage used by horizon to track client sessions, such as local memory, cookies,
a database, or memcached.
session persistence A feature of the load-balancing service. It attempts to force subsequent connections to a
service to be redirected to the same node as long as it is online.
session storage A horizon component that stores and tracks client session information. Implemented through
the Django sessions framework.
share A remote, mountable file system in the context of the Shared File Systems service. You can mount a
share to, and access a share from, several hosts by several users at a time.
202 Appendix
Operations Guide (Release Version: 15.0.0)
share network An entity in the context of the Shared File Systems service that encapsulates interaction with
the Networking service. If the driver you selected runs in the mode requiring such kind of interaction,
you need to specify the share network to create a share.
Shared File Systems API A Shared File Systems service that provides a stable RESTful API. The service authenticates
and routes requests throughout the Shared File Systems service. There is python-manilaclient
to interact with the API.
Shared File Systems service (manila) The service that provides a set of services for management of shared
file systems in a multi-project cloud environment, similar to how OpenStack provides block-based storage
management through the OpenStack Block Storage service project. With the Shared File Systems
service, you can create a remote file system and mount the file system on your instances. You can also
read and write data from your instances to and from your file system.
shared IP address An IP address that can be assigned to a VM instance within the shared IP group. Public IP
addresses can be shared across multiple servers for use in various high-availability scenarios. When an
IP address is shared to another server, the cloud network restrictions are modified to enable each server
to listen to and respond on that IP address. You can optionally specify that the target server network
configuration be modified. Shared IP addresses can be used with many standard heartbeat facilities,
such as keepalive, that monitor for failure and manage IP failover.
shared IP group A collection of servers that can share IPs with other members of the group. Any server in a
group can share one or more public IPs with any other server in the group. With the exception of the first
server in a shared IP group, servers must be launched into shared IP groups. A server may be a member
of only one shared IP group.
shared storage Block storage that is simultaneously accessible by multiple clients, for example, NFS.
Sheepdog Distributed block storage system for QEMU, supported by OpenStack.
Simple Cloud Identity Management (SCIM) Specification for managing identity in the cloud, currently unsupported
by OpenStack.
Simple Protocol for Independent Computing Environments (SPICE) SPICE provides remote desktop access
to guest virtual machines. It is an alternative to VNC. SPICE is supported by OpenStack.
Single-root I/O Virtualization (SR-IOV) A specification that, when implemented by a physical PCIe device,
enables it to appear as multiple separate PCIe devices. This enables multiple virtualized guests to share
direct access to the physical device, offering improved performance over an equivalent virtual device.
Currently supported in OpenStack Havana and later releases.
SmokeStack Runs automated tests against the core OpenStack API; written in Rails.
snapshot A point-in-time copy of an OpenStack storage volume or image. Use storage volume snapshots to
back up volumes. Use image snapshots to back up data, or as “gold” images for additional servers.
soft reboot A controlled reboot where a VM instance is properly restarted through operating system commands.
Software Development Lifecycle Automation service (solum) OpenStack project that aims to make cloud
services easier to consume and integrate with application development process by automating the sourceto-image
process, and simplifying app-centric deployment.
Software-defined networking (SDN) Provides an approach for network administrators to manage computer
network services through abstraction of lower-level functionality.
SolidFire Volume Driver The Block Storage driver for the SolidFire iSCSI storage appliance.
solum Code name for the Software Development Lifecycle Automation service.
Appendix 203
Operations Guide (Release Version: 15.0.0)
spread-first scheduler The Compute VM scheduling algorithm that attempts to start a new VM on the host
with the least amount of load.
SQLAlchemy An open source SQL toolkit for Python, used in OpenStack.
SQLite A lightweight SQL database, used as the default persistent storage method in many OpenStack services.
stack A set of OpenStack resources created and managed by the Orchestration service according to a given
template (either an AWS CloudFormation template or a Heat Orchestration Template (HOT)).
StackTach Community project that captures Compute AMQP communications; useful for debugging.
static IP address Alternative term for a fixed IP address.
StaticWeb WSGI middleware component of Object Storage that serves container data as a static web page.
storage back end The method that a service uses for persistent storage, such as iSCSI, NFS, or local disk.
storage manager A XenAPI component that provides a pluggable interface to support a wide variety of persistent
storage back ends.
storage manager back end A persistent storage method supported by XenAPI, such as iSCSI or NFS.
storage node An Object Storage node that provides container services, account services, and object services;
controls the account databases, container databases, and object storage.
storage services Collective name for the Object Storage object services, container services, and account services.
strategy Specifies the authentication source used by Image service or Identity. In the Database service, it
refers to the extensions implemented for a data store.
subdomain A domain within a parent domain. Subdomains cannot be registered. Subdomains enable you to
delegate domains. Subdomains can themselves have subdomains, so third-level, fourth-level, fifth-level,
and deeper levels of nesting are possible.
subnet Logical subdivision of an IP network.
SUSE Linux Enterprise Server (SLES) A Linux distribution that is compatible with OpenStack.
suspend The VM instance is paused and its state is saved to disk of the host.
swap Disk-based virtual memory used by operating systems to provide more memory than is actually available
on the system.
swauth An authentication and authorization service for Object Storage, implemented through WSGI middleware;
uses Object Storage itself as the persistent backing store.
swift Codename for OpenStack Object Storage service.
swift All in One (SAIO) Creates a full Object Storage development environment within a single VM.
swift middleware Collective term for Object Storage components that provide additional functionality.
swift proxy server Acts as the gatekeeper to Object Storage and is responsible for authenticating the user.
swift storage node A node that runs Object Storage account, container, and object services.
sync point Point in time since the last container and accounts database sync among nodes within Object Storage.
sysadmin One of the default roles in the Compute RBAC system. Enables a user to add other users to a project,
interact with VM images that are associated with the project, and start and stop VM instances.
204 Appendix
Operations Guide (Release Version: 15.0.0)
system usage A Compute component that, along with the notification system, collects meters and usage information.
This information can be used for billing.
T
tacker Code name for the NFV Orchestration service
Telemetry service (telemetry) The OpenStack project which collects measurements of the utilization of the
physical and virtual resources comprising deployed clouds, persists this data for subsequent retrieval and
analysis, and triggers actions when defined criteria are met.
TempAuth An authentication facility within Object Storage that enables Object Storage itself to perform
authentication and authorization. Frequently used in testing and development.
Tempest Automated software test suite designed to run against the trunk of the OpenStack core project.
TempURL An Object Storage middleware component that enables creation of URLs for temporary object
access.
tenant A group of users; used to isolate access to Compute resources. An alternative term for a project.
Tenant API An API that is accessible to projects.
tenant endpoint An Identity service API endpoint that is associated with one or more projects.
tenant ID An alternative term for project ID.
token An alpha-numeric string of text used to access OpenStack APIs and resources.
token services An Identity service component that manages and validates tokens after a user or project has
been authenticated.
tombstone Used to mark Object Storage objects that have been deleted; ensures that the object is not updated
on another node after it has been deleted.
topic publisher A process that is created when a RPC call is executed; used to push the message to the topic
exchange.
Torpedo Community project used to run automated tests against the OpenStack API.
transaction ID Unique ID assigned to each Object Storage request; used for debugging and tracing.
transient Alternative term for non-durable.
transient exchange Alternative term for a non-durable exchange.
transient message A message that is stored in memory and is lost after the server is restarted.
transient queue Alternative term for a non-durable queue.
TripleO OpenStack-on-OpenStack program. The code name for the OpenStack Deployment program.
trove Codename for OpenStack Database service.
trusted platform module (TPM) Specialized microprocessor for incorporating cryptographic keys into devices
for authenticating and securing a hardware platform.
U
Ubuntu A Debian-based Linux distribution.
Appendix 205
Operations Guide (Release Version: 15.0.0)
unscoped token Alternative term for an Identity service default token.
updater Collective term for a group of Object Storage components that processes queued and failed updates
for containers and objects.
user In OpenStack Identity, entities represent individual API consumers and are owned by a specific domain.
In OpenStack Compute, a user can be associated with roles, projects, or both.
user data A blob of data that the user can specify when they launch an instance. The instance can access this
data through the metadata service or config drive. Commonly used to pass a shell script that the instance
runs on boot.
User Mode Linux (UML) An OpenStack-supported hypervisor.
V
VIF UUID Unique ID assigned to each Networking VIF.
Virtual Central Processing Unit (vCPU) Subdivides physical CPUs. Instances can then use those divisions.
Virtual Disk Image (VDI) One of the VM image disk formats supported by Image service.
Virtual Extensible LAN (VXLAN) A network virtualization technology that attempts to reduce the scalability
problems associated with large cloud computing deployments. It uses a VLAN-like encapsulation
technique to encapsulate Ethernet frames within UDP packets.
Virtual Hard Disk (VHD) One of the VM image disk formats supported by Image service.
virtual IP address (VIP) An Internet Protocol (IP) address configured on the load balancer for use by clients
connecting to a service that is load balanced. Incoming connections are distributed to back-end nodes
based on the configuration of the load balancer.
virtual machine (VM) An operating system instance that runs on top of a hypervisor. Multiple VMs can run
at the same time on the same physical host.
virtual network An L2 network segment within Networking.
Virtual Network Computing (VNC) Open source GUI and CLI tools used for remote console access to VMs.
Supported by Compute.
Virtual Network InterFace (VIF) An interface that is plugged into a port in a Networking network. Typically
a virtual network interface belonging to a VM.
virtual networking A generic term for virtualization of network functions such as switching, routing, load
balancing, and security using a combination of VMs and overlays on physical network infrastructure.
virtual port Attachment point where a virtual interface connects to a virtual network.
virtual private network (VPN) Provided by Compute in the form of cloudpipes, specialized instances that
are used to create VPNs on a per-project basis.
virtual server Alternative term for a VM or guest.
virtual switch (vSwitch) Software that runs on a host or node and provides the features and functions of a
hardware-based network switch.
virtual VLAN Alternative term for a virtual network.
VirtualBox An OpenStack-supported hypervisor.
Vitrage Code name for the Root Cause Analysis service.
206 Appendix
Operations Guide (Release Version: 15.0.0)
VLAN manager A Compute component that provides dnsmasq and radvd and sets up forwarding to and from
cloudpipe instances.
VLAN network The Network Controller provides virtual networks to enable compute servers to interact with
each other and with the public network. All machines must have a public and private network interface.
A VLAN network is a private network interface, which is controlled by the vlan_interface option
with VLAN managers.
VM disk (VMDK) One of the VM image disk formats supported by Image service.
VM image Alternative term for an image.
VM Remote Control (VMRC) Method to access VM instance consoles using a web browser. Supported by
Compute.
VMware API Supports interaction with VMware products in Compute.
VMware NSX Neutron plug-in Provides support for VMware NSX in Neutron.
VNC proxy A Compute component that provides users access to the consoles of their VM instances through
VNC or VMRC.
volume Disk-based data storage generally represented as an iSCSI target with a file system that supports
extended attributes; can be persistent or ephemeral.
Volume API Alternative name for the Block Storage API.
volume controller A Block Storage component that oversees and coordinates storage volume actions.
volume driver Alternative term for a volume plug-in.
volume ID Unique ID applied to each storage volume under the Block Storage control.
volume manager A Block Storage component that creates, attaches, and detaches persistent storage volumes.
volume node A Block Storage node that runs the cinder-volume daemon.
volume plug-in Provides support for new and specialized types of back-end storage for the Block Storage
volume manager.
volume worker A cinder component that interacts with back-end storage to manage the creation and deletion
of volumes and the creation of compute volumes, provided by the cinder-volume daemon.
vSphere An OpenStack-supported hypervisor.
W
Watcher Code name for the Infrastructure Optimization service.
weight Used by Object Storage devices to determine which storage devices are suitable for the job. Devices
are weighted by size.
weighted cost The sum of each cost used when deciding where to start a new VM instance in Compute.
weighting A Compute process that determines the suitability of the VM instances for a job for a particular
host. For example, not enough RAM on the host, too many CPUs on the host, and so on.
worker A daemon that listens to a queue and carries out tasks in response to messages. For example, the
cinder-volume worker manages volume creation and deletion on storage arrays.
Appendix 207
Operations Guide (Release Version: 15.0.0)
Workflow service (mistral) The OpenStack service that provides a simple YAML-based language to write
workflows (tasks and transition rules) and a service that allows to upload them, modify, run them at scale
and in a highly available manner, manage and monitor workflow execution state and state of individual
tasks.
X
X.509 X.509 is the most widely used standard for defining digital certificates. It is a data structure that contains
the subject (entity) identifiable information such as its name along with its public key. The certificate can
contain a few other attributes as well depending upon the version. The most recent and standard version
of X.509 is v3.
Xen Xen is a hypervisor using a microkernel design, providing services that allow multiple computer operating
systems to execute on the same computer hardware concurrently.
Xen API The Xen administrative API, which is supported by Compute.
Xen Cloud Platform (XCP) An OpenStack-supported hypervisor.
Xen Storage Manager Volume Driver A Block Storage volume plug-in that enables communication with
the Xen Storage Manager API.
XenServer An OpenStack-supported hypervisor.
XFS High-performance 64-bit file system created by Silicon Graphics. Excels in parallel I/O operations and
data consistency.
Z
zaqar Codename for the Message service.
ZeroMQ Message queue software supported by OpenStack. An alternative to RabbitMQ. Also spelled 0MQ.
Zuul Tool used in OpenStack development to ensure correctly ordered testing of changes in parallel.
208 Appendix
INDEX
Symbols
6to4, 174
A
absolute limit, 174
access control list (ACL), 174
access key, 174
account, 174
account auditor, 174
account database, 174
account reaper, 174
account server, 174
account service, 174
accounting, 174
Active Directory, 174
active/active configuration, 175
active/passive configuration, 175
address pool, 175
Address Resolution Protocol (ARP), 175
admin API, 175
admin server, 175
administrator, 175
Advanced Message Queuing Protocol (AMQP), 175
Advanced RISC Machine (ARM), 175
alert, 175
allocate, 175
Amazon Kernel Image (AKI), 175
Amazon Machine Image (AMI), 175
Amazon Ramdisk Image (ARI), 175
Anvil, 175
aodh, 175
Apache, 175
Apache License 2.0, 175
Apache Web Server, 175
API endpoint, 175
API extension, 175
API extension plug-in, 175
API key, 175
API server, 175
API token, 176
API version, 176
applet, 176
Application Catalog service (murano), 176
Application Programming Interface (API), 176
application server, 176
Application Service Provider (ASP), 176
arptables, 176
associate, 176
Asynchronous JavaScript and XML (AJAX), 176
ATA over Ethernet (AoE), 176
attach, 176
attachment (network), 176
auditing, 176
auditor, 176
Austin, 176
auth node, 176
authentication, 176
authentication token, 176
AuthN, 176
authorization, 176
authorization node, 176
AuthZ, 176
Auto ACK, 176
auto declare, 177
availability zone, 177
AWS CloudFormation template, 177
B
back end, 177
back-end catalog, 177
back-end store, 177
Backup, Restore, and Disaster Recovery service
(freezer), 177
bandwidth, 177
barbican, 177
bare, 177
Bare Metal service (ironic), 177
base image, 177
Bell-LaPadula model, 177
Benchmark service (rally), 177
Bexar, 177
binary, 177
209
Operations Guide (Release Version: 15.0.0)
bit, 177
bits per second (BPS), 177
block device, 178
block migration, 178
Block Storage API, 178
Block Storage service (cinder), 178
BMC (Baseboard Management Controller), 178
bootable disk image, 178
Bootstrap Protocol (BOOTP), 178
Border Gateway Protocol (BGP), 178
browser, 178
builder file, 178
bursting, 178
button class, 178
byte, 178
C
cache pruner, 178
Cactus, 178
CALL, 178
capability, 178
capacity cache, 178
capacity updater, 179
CAST, 179
catalog, 179
catalog service, 179
ceilometer, 179
cell, 179
cell forwarding, 179
cell manager, 179
CentOS, 179
Ceph, 179
CephFS, 179
certificate authority (CA), 179
Challenge-Handshake Authentication Protocol
(CHAP), 179
chance scheduler, 179
changes since, 179
Chef, 179
child cell, 179
cinder, 179
CirrOS, 179
Cisco neutron plug-in, 179
cloud architect, 179
Cloud Auditing Data Federation (CADF), 179
cloud computing, 180
cloud controller, 180
cloud controller node, 180
Cloud Data Management Interface (CDMI), 180
Cloud Infrastructure Management Interface (CIMI),
180
cloud-init, 180
cloudadmin, 180
Cloudbase-Init, 180
cloudpipe, 180
cloudpipe image, 180
Clustering service (senlin), 180
command filter, 180
Common Internet File System (CIFS), 180
Common Libraries (oslo), 180
community project, 180
compression, 180
Compute API (Nova API), 180
compute controller, 180
compute host, 180
compute node, 180
Compute service (nova), 181
compute worker, 181
concatenated object, 181
conductor, 181
congress, 181
consistency window, 181
console log, 181
container, 181
container auditor, 181
container database, 181
container format, 181
Container Infrastructure Management service (magnum),
181
container server, 181
container service, 181
content delivery network (CDN), 181
controller node, 181
core API, 181
core service, 181
cost, 181
credentials, 181
CRL, 181
Cross-Origin Resource Sharing (CORS), 182
Crowbar, 182
current workload, 182
customer, 182
customization module, 182
D
daemon, 182
Dashboard (horizon), 182
data encryption, 182
Data loss prevention (DLP) software, 182
Data Processing service (sahara), 182
data store, 182
database ID, 182
210 Index
Operations Guide (Release Version: 15.0.0)
database replicator, 182
Database service (trove), 182
deallocate, 182
Debian, 182
deduplication, 182
default panel, 182
default project, 182
default token, 182
delayed delete, 183
delivery mode, 183
denial of service (DoS), 183
deprecated auth, 183
designate, 183
Desktop-as-a-Service, 183
developer, 183
device ID, 183
device weight, 183
DevStack, 183
DHCP agent, 183
Diablo, 183
direct consumer, 183
direct exchange, 183
direct publisher, 183
disassociate, 183
Discretionary Access Control (DAC), 183
disk encryption, 183
disk format, 183
dispersion, 183
distributed virtual router (DVR), 183
Django, 184
DNS record, 184
DNS service (designate), 184
dnsmasq, 184
domain, 184
Domain Name System (DNS), 184
download, 184
durable exchange, 184
durable queue, 184
Dynamic Host Configuration Protocol (DHCP), 184
Dynamic HyperText Markup Language (DHTML),
184
E
east-west traffic, 184
EBS boot volume, 184
ebtables, 184
EC2, 184
EC2 access key, 184
EC2 API, 184
EC2 Compatibility API, 184
EC2 secret key, 185
Elastic Block Storage (EBS), 185
encapsulation, 185
encryption, 185
endpoint, 185
endpoint registry, 185
endpoint template, 185
entity, 185
ephemeral image, 185
ephemeral volume, 185
Essex, 185
ESXi, 185
ETag, 185
euca2ools, 185
Eucalyptus Kernel Image (EKI), 185
Eucalyptus Machine Image (EMI), 185
Eucalyptus Ramdisk Image (ERI), 185
evacuate, 185
exchange, 185
exchange type, 185
exclusive queue, 185
extended attributes (xattr), 185
extension, 185
external network, 185
extra specs, 186
F
FakeLDAP, 186
fan-out exchange, 186
federated identity, 186
Fedora, 186
Fibre Channel, 186
Fibre Channel over Ethernet (FCoE), 186
fill-first scheduler, 186
filter, 186
firewall, 186
FireWall-as-a-Service (FWaaS), 186
fixed IP address, 186
Flat Manager, 186
flat mode injection, 186
flat network, 186
FlatDHCP Manager, 186
flavor, 186
flavor ID, 186
floating IP address, 186
Folsom, 186
FormPost, 187
freezer, 187
front end, 187
G
gateway, 187
generic receive offload (GRO), 187
Index 211
Operations Guide (Release Version: 15.0.0)
generic routing encapsulation (GRE), 187
glance, 187
glance API server, 187
glance registry, 187
global endpoint template, 187
GlusterFS, 187
gnocchi, 187
golden image, 187
Governance service (congress), 187
Graphic Interchange Format (GIF), 187
Graphics Processing Unit (GPU), 187
Green Threads, 187
Grizzly, 187
Group, 187
guest OS, 187
H
Hadoop, 187
Hadoop Distributed File System (HDFS), 187
handover, 188
HAProxy, 188
hard reboot, 188
Havana, 188
health monitor, 188
heat, 188
Heat Orchestration Template (HOT), 188
high availability (HA), 188
horizon, 188
horizon plug-in, 188
host, 188
host aggregate, 188
Host Bus Adapter (HBA), 188
hybrid cloud, 188
Hyper-V, 188
hyperlink, 188
Hypertext Transfer Protocol (HTTP), 188
Hypertext Transfer Protocol Secure (HTTPS), 188
hypervisor, 188
hypervisor pool, 188
I
Icehouse, 189
ID number, 189
Identity API, 189
Identity back end, 189
identity provider, 189
Identity service (keystone), 189
Identity service API, 189
IETF, 189
image, 189
Image API, 189
image cache, 189
image ID, 189
image membership, 189
image owner, 189
image registry, 189
Image service (glance), 189
image status, 189
image store, 189
image UUID, 189
incubated project, 189
Infrastructure Optimization service (watcher), 190
Infrastructure-as-a-Service (IaaS), 190
ingress filtering, 190
INI format, 190
injection, 190
Input/Output Operations Per Second (IOPS), 190
instance, 190
instance ID, 190
instance state, 190
instance tunnels network, 190
instance type, 190
instance type ID, 190
instance UUID, 190
Intelligent Platform Management Interface (IPMI),
190
interface, 190
interface ID, 190
Internet Control Message Protocol (ICMP), 190
Internet protocol (IP), 190
Internet Service Provider (ISP), 190
Internet Small Computer System Interface (iSCSI),
190
IP address, 190
IP Address Management (IPAM), 190
ip6tables, 191
ipset, 191
iptables, 191
ironic, 191
iSCSI Qualified Name (IQN), 191
ISO9660, 191
itsec, 191
J
Java, 191
JavaScript, 191
JavaScript Object Notation (JSON), 191
jumbo frame, 191
Juno, 191
K
Kerberos, 191
kernel-based VM (KVM), 191
Key Manager service (barbican), 191
212 Index
Operations Guide (Release Version: 15.0.0)
keystone, 191
Kickstart, 192
Kilo, 192
L
large object, 192
Launchpad, 192
Layer-2 (L2) agent, 192
Layer-2 network, 192
Layer-3 (L3) agent, 192
Layer-3 network, 192
Liberty, 192
libvirt, 192
Lightweight Directory Access Protocol (LDAP), 192
Linux, 192
Linux bridge, 192
Linux Bridge neutron plug-in, 192
Linux containers (LXC), 192
live migration, 192
load balancer, 192
load balancing, 192
Load-Balancer-as-a-Service (LBaaS), 192
Load-balancing service (octavia), 192
Logical Volume Manager (LVM), 193
M
magnum, 193
management API, 193
management network, 193
manager, 193
manifest, 193
manifest object, 193
manila, 193
manila-share, 193
maximum transmission unit (MTU), 193
mechanism driver, 193
melange, 193
membership, 193
membership list, 193
memcached, 193
memory overcommit, 193
message broker, 193
message bus, 193
message queue, 193
Message service (zaqar), 193
Meta-Data Server (MDS), 193
Metadata agent, 193
migration, 193
mistral, 193
Mitaka, 194
Modular Layer 2 (ML2) neutron plug-in, 194
monasca, 194
Monitor (LBaaS), 194
Monitor (Mon), 194
Monitoring (monasca), 194
multi-factor authentication, 194
multi-host, 194
multinic, 194
murano, 194
N
Nebula, 194
netadmin, 194
NetApp volume driver, 194
network, 194
Network Address Translation (NAT), 194
network controller, 194
Network File System (NFS), 194
network ID, 194
network manager, 194
network namespace, 195
network node, 195
network segment, 195
Network Service Header (NSH), 195
Network Time Protocol (NTP), 195
network UUID, 195
network worker, 195
Networking API (Neutron API), 195
Networking service (neutron), 195
neutron, 195
neutron API, 195
neutron manager, 195
neutron plug-in, 195
Newton, 195
Nexenta volume driver, 195
NFV Orchestration Service (tacker), 195
Nginx, 195
No ACK, 195
node, 195
non-durable exchange, 195
non-durable queue, 195
non-persistent volume, 195
north-south traffic, 195
nova, 196
Nova API, 196
nova-network, 196
O
object, 196
object auditor, 196
object expiration, 196
object hash, 196
object path hash, 196
object replicator, 196
Index 213
Operations Guide (Release Version: 15.0.0)
object server, 196
Object Storage API, 196
Object Storage Device (OSD), 196
Object Storage service (swift), 196
object versioning, 196
Ocata, 196
Octavia, 196
Oldie, 196
Open Cloud Computing Interface (OCCI), 196
Open Virtualization Format (OVF), 196
Open vSwitch, 196
Open vSwitch (OVS) agent, 196
Open vSwitch neutron plug-in, 196
OpenLDAP, 196
OpenStack, 197
OpenStack code name, 197
openSUSE, 197
operator, 197
optional service, 197
Orchestration service (heat), 197
orphan, 197
Oslo, 197
P
panko, 197
parent cell, 197
partition, 197
partition index, 197
partition shift value, 197
path MTU discovery (PMTUD), 197
pause, 197
PCI passthrough, 197
persistent message, 197
persistent volume, 197
personality file, 197
Pike, 198
Platform-as-a-Service (PaaS), 198
plug-in, 198
policy service, 198
policy-based routing (PBR), 198
pool, 198
pool member, 198
port, 198
port UUID, 198
preseed, 198
private image, 198
private IP address, 198
private network, 198
project, 198
project ID, 198
project VPN, 198
promiscuous mode, 198
protected property, 198
provider, 198
proxy node, 198
proxy server, 198
public API, 198
public image, 199
public IP address, 199
public key authentication, 199
public network, 199
Puppet, 199
Python, 199
Q
QEMU Copy On Write 2 (QCOW2), 199
Qpid, 199
Quality of Service (QoS), 199
quarantine, 199
Queens, 199
Quick EMUlator (QEMU), 199
quota, 199
R
RabbitMQ, 199
Rackspace Cloud Files, 199
RADOS Block Device (RBD), 199
radvd, 199
rally, 199
RAM filter, 199
RAM overcommit, 199
rate limit, 200
raw, 200
rebalance, 200
reboot, 200
rebuild, 200
Recon, 200
record, 200
record ID, 200
Red Hat Enterprise Linux (RHEL), 200
reference architecture, 200
region, 200
registry, 200
registry server, 200
Reliable, Autonomic Distributed Object Store, 200
Remote Procedure Call (RPC), 200
replica, 200
replica count, 200
replication, 200
replicator, 200
request ID, 200
rescue image, 200
resize, 200
214 Index
Operations Guide (Release Version: 15.0.0)
RESTful, 201
ring, 201
ring builder, 201
Rocky, 201
role, 201
Role Based Access Control (RBAC), 201
role ID, 201
Root Cause Analysis (RCA) service (Vitrage), 201
rootwrap, 201
round-robin scheduler, 201
router, 201
routing key, 201
RPC driver, 201
rsync, 201
RXTX cap, 201
RXTX quota, 201
S
sahara, 201
SAML assertion, 201
scheduler manager, 201
scoped token, 201
scrubber, 201
secret key, 201
secure boot, 202
secure shell (SSH), 202
security group, 202
segmented object, 202
self-service, 202
SELinux, 202
senlin, 202
server, 202
server image, 202
server UUID, 202
service, 202
service catalog, 202
Service Function Chain (SFC), 202
service ID, 202
Service Level Agreement (SLA), 202
service project, 202
service provider, 202
service registration, 202
service token, 202
session back end, 202
session persistence, 202
session storage, 202
share, 202
share network, 203
Shared File Systems API, 203
Shared File Systems service (manila), 203
shared IP address, 203
shared IP group, 203
shared storage, 203
Sheepdog, 203
Simple Cloud Identity Management (SCIM), 203
Simple Protocol for Independent Computing Environments
(SPICE), 203
Single-root I/O Virtualization (SR-IOV), 203
SmokeStack, 203
snapshot, 203
soft reboot, 203
Software Development Lifecycle Automation service
(solum), 203
Software-defined networking (SDN), 203
SolidFire Volume Driver, 203
solum, 203
spread-first scheduler, 204
SQLAlchemy, 204
SQLite, 204
stack, 204
StackTach, 204
static IP address, 204
StaticWeb, 204
storage back end, 204
storage manager, 204
storage manager back end, 204
storage node, 204
storage services, 204
strategy, 204
subdomain, 204
subnet, 204
SUSE Linux Enterprise Server (SLES), 204
suspend, 204
swap, 204
swauth, 204
swift, 204
swift All in One (SAIO), 204
swift middleware, 204
swift proxy server, 204
swift storage node, 204
sync point, 204
sysadmin, 204
system usage, 205
T
tacker, 205
Telemetry service (telemetry), 205
TempAuth, 205
Tempest, 205
TempURL, 205
tenant, 205
Tenant API, 205
Index 215
Operations Guide (Release Version: 15.0.0)
tenant endpoint, 205
tenant ID, 205
token, 205
token services, 205
tombstone, 205
topic publisher, 205
Torpedo, 205
transaction ID, 205
transient, 205
transient exchange, 205
transient message, 205
transient queue, 205
TripleO, 205
trove, 205
trusted platform module (TPM), 205
U
Ubuntu, 205
unscoped token, 206
updater, 206
user, 206
user data, 206
User Mode Linux (UML), 206
V
VIF UUID, 206
Virtual Central Processing Unit (vCPU), 206
Virtual Disk Image (VDI), 206
Virtual Extensible LAN (VXLAN), 206
Virtual Hard Disk (VHD), 206
virtual IP address (VIP), 206
virtual machine (VM), 206
virtual network, 206
Virtual Network Computing (VNC), 206
Virtual Network InterFace (VIF), 206
virtual networking, 206
virtual port, 206
virtual private network (VPN), 206
virtual server, 206
virtual switch (vSwitch), 206
virtual VLAN, 206
VirtualBox, 206
Vitrage, 206
VLAN manager, 207
VLAN network, 207
VM disk (VMDK), 207
VM image, 207
VM Remote Control (VMRC), 207
VMware API, 207
VMware NSX Neutron plug-in, 207
VNC proxy, 207
volume, 207
Volume API, 207
volume controller, 207
volume driver, 207
volume ID, 207
volume manager, 207
volume node, 207
volume plug-in, 207
volume worker, 207
vSphere, 207
W
Watcher, 207
weight, 207
weighted cost, 207
weighting, 207
worker, 207
Workflow service (mistral), 208
X
X.509, 208
Xen, 208
Xen API, 208
Xen Cloud Platform (XCP), 208
Xen Storage Manager Volume Driver, 208
XenServer, 208
XFS, 208
Z
zaqar, 208
ZeroMQ, 208
Zuul, 208
216 Index
Virtual Machine Image Guide
Release Version: 15.0.0
OpenStack contributors
May 12, 2017
CONTENTS
Abstract 1
Contents 2
Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Get images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
Image requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Modify images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Create images manually . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Tool support for image creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
Converting between image formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Image sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
Index 90
i
ABSTRACT
This guide describes how to obtain, create, and modify virtual machine images that are compatible with OpenStack.
1
CONTENTS
Conventions
The OpenStack documentation uses several typesetting conventions.
Notices
Notices take these forms:
Note: A comment with additional information that explains a part of the text.
Important: Something you must be aware of before proceeding.
Tip: An extra but helpful piece of practical advice.
Caution: Helpful information that prevents the user from making mistakes.
Warning: Critical information about the risk of data loss or security issues.
Command prompts
$ command
Any user, including the root user, can run commands that are prefixed with the $ prompt.
# command
The root user must run commands that are prefixed with the # prompt. You can also prefix these commands
with the sudo command, if available, to run them.
Introduction
Disk and container formats for images
When you add an image to the Image service, you can specify its disk and container formats.
2
Virtual Machine Image Guide (Release Version: 15.0.0)
Disk formats
The disk format of a virtual machine image is the format of the underlying disk image. Virtual appliance vendors
have different formats for laying out the information contained in a virtual machine disk image.
Set the disk format for your image to one of the following values:
aki An Amazon kernel image.
ami An Amazon machine image.
ari An Amazon ramdisk image.
iso An archive format for the data contents of an optical disc, such as CD-ROM.
qcow2 Supported by the QEMU emulator that can expand dynamically and supports Copy on Write.
raw An unstructured disk image format; if you have a file without an extension it is possibly a raw format.
vdi Supported by VirtualBox virtual machine monitor and the QEMU emulator.
vhd The VHD disk format, a common disk format used by virtual machine monitors from VMware, Xen,
Microsoft, VirtualBox, and others.
vhdx The VHDX disk format, an enhanced version of the VHD format, which supports larger disk sizes among
other features.
vmdk Common disk format supported by many common virtual machine monitors.
Container formats
The container format indicates whether the virtual machine image is in a file format that also contains metadata
about the actual virtual machine.
Note: The Image service and other OpenStack projects do not currently support the container format. It is safe
to specify bare as the container format if you are unsure.
You can set the container format for your image to one of the following values:
aki An Amazon kernel image.
ami An Amazon machine image.
ari An Amazon ramdisk image.
bare The image does not have a container or metadata envelope.
docker A docker container format.
ova An OVF package in a tarfile.
ovf The OVF container format.
Image metadata
Image metadata can help end users determine the nature of an image, and is used by associated OpenStack
components and drivers which interface with the Image service.
Introduction 3
Virtual Machine Image Guide (Release Version: 15.0.0)
Metadata can also determine the scheduling of hosts. If the property option is set on an image, and Compute is
configured so that the ImagePropertiesFilter scheduler filter is enabled (default), then the scheduler only
considers compute hosts that satisfy that property.
Note: Compute’s ImagePropertiesFilter value is specified in the scheduler_default_filter value
in the /etc/nova/nova.conf file.
You can add metadata to Image service images by using the --property key=value parameter with the
openstack image create or openstack image set command. More than one property can be specified.
For example:
$ openstack image set --property architecture=arm \
--property hypervisor_type=qemu image_name_or_id
Common image properties are also specified in the /etc/glance/schema-image.json file. For a complete
list of valid property keys and values, refer to the OpenStack Command-Line Reference.
All associated properties for an image can be displayed using the openstack image show command. For
example:
$ openstack image show cirros
+------------------+------------------------------------------------------+
| Field | Value |
+------------------+------------------------------------------------------+
| checksum | ee1eca47dc88f4879d8a229cc70a07c6 |
| container_format | bare |
| created_at | 2016-04-15T13:57:38Z |
| disk_format | qcow2 |
| file | /v2/images/55f0907f-70a5-4376-a346-432e4ec509ed/file |
| id | 55f0907f-70a5-4376-a346-432e4ec509ed |
| min_disk | 0 |
| min_ram | 0 |
| name | cirros |
| owner | f9574e69042645d6b5539035cb8c00bf |
| properties | architecture='arm', hypervisor_type='qemu' |
| protected | False |
| schema | /v2/schemas/image |
| size | 13287936 |
| status | active |
| tags | |
| updated_at | 2016-04-15T13:57:57Z |
| virtual_size | None |
| visibility | public |
+------------------+------------------------------------------------------+
Note: Volume-from-Image properties
When creating Block Storage volumes from images, also consider your configured image properties. If
you alter the core image properties, you should also update your Block Storage configuration. Amend
glance_core_properties in the /etc/cinder/cinder.conf file on all controller nodes to match the core
properties you have set in the Image service.
4 Introduction
Virtual Machine Image Guide (Release Version: 15.0.0)
Metadata definition service
With this service you can define:
Namespace
• Contains metadata definitions.
• Specifies the access controls for everything defined in the namespace. These access controls determine
who can define and use the definitions in the namespace.
• Associates the definitions with different types of resources.
Property A single property and its primitive constraints. Each property can only be a primitive type. For
example, string, integer, number, boolean, or array.
Object Describes a group of one to many properties and their primitive constraints. Each property in the group
can only be a primitive type. For example, string, integer, number, boolean, or array.
The object may optionally define required properties under the semantic understanding that if you use
the object, you should provide all required properties.
Resource type association Specifies the relationship between resource types and the namespaces that are applicable
to them. This information can be used to drive UI and CLI views. For example, the same
namespace of objects, properties, and tags may be used for images, snapshots, volumes, and flavors. Or
a namespace may only apply to images.
The Image service has predefined namespaces for the metadata definitions catalog. To load files from this
directory into the database:
$ glance-manage db_load_metadefs
To unload the files from the database:
$ glance-manage db_unload_metadefs
To export the definitions in JSON format:
$ glance-manage db_export_metadefs
Note: By default, files are loaded from and exported to the Image service’s /etc/glance/metadefs directory.
An OpenStack Compute cloud is not very useful unless you have virtual machine images (which some people
call “virtual appliances”). This guide describes how to obtain, create, and modify virtual machine images that
are compatible with OpenStack.
To keep things brief, we will sometimes use the term image instead of virtual machine image.
What is a virtual machine image?
A virtual machine image is a single file which contains a virtual disk that has a bootable operating system
installed on it.
Virtual machine images come in different formats, some of which are described below.
AKI/AMI/ARI The AKI/AMI/ARI format was the initial image format supported by Amazon EC2. The image
consists of three files:
Introduction 5
Virtual Machine Image Guide (Release Version: 15.0.0)
AKI (Amazon Kernel Image) A kernel file that the hypervisor will load initially to boot the image. For
a Linux machine, this would be a vmlinuz file.
AMI (Amazon Machine Image) This is a virtual machine image in raw format, as described above.
ARI (Amazon Ramdisk Image) An optional ramdisk file mounted at boot time. For a Linux machine,
this would be an initrd file.
ISO The ISO format is a disk image formatted with the read-only ISO 9660 (also known as ECMA-119)
filesystem commonly used for CDs and DVDs. While we do not normally think of ISO as a virtual
machine image format, since ISOs contain bootable filesystems with an installed operating system, you
can treat them the same as you treat other virtual machine image files.
OVF OVF (Open Virtualization Format) is a packaging format for virtual machines, defined by the Distributed
Management Task Force (DMTF) standards group. An OVF package contains one or more image files,
a .ovf XML metadata file that contains information about the virtual machine, and possibly other files
as well.
An OVF package can be distributed in different ways. For example, it could be distributed as a set of
discrete files, or as a tar archive file with an .ova (open virtual appliance/application) extension.
OpenStack Compute does not currently have support for OVF packages, so you will need to extract the
image file(s) from an OVF package if you wish to use it with OpenStack.
QCOW2 The QCOW2 (QEMU copy-on-write version 2) format is commonly used with the KVM hypervisor.
It has some additional features over the raw format, such as:
• Using sparse representation, so the image size is smaller.
• Support for snapshots.
Because qcow2 is sparse, qcow2 images are typically smaller than raw images. Smaller images mean
faster uploads, so it is often faster to convert a raw image to qcow2 for uploading instead of uploading
the raw file directly.
Note: Because raw images do not support snapshots, OpenStack Compute will automatically convert
raw image files to qcow2 as needed.
Raw The raw image format is the simplest one, and is natively supported by both KVM and Xen hypervisors.
You can think of a raw image as being the bit-equivalent of a block device file, created as if somebody
had copied, say, /dev/sda to a file using the dd command.
Note: We do not recommend creating raw images by dd’ing block device files, we discuss how to create
raw images later.
UEC tarball A UEC (Ubuntu Enterprise Cloud) tarball is a gzipped tarfile that contains an AMI file, AKI file,
and ARI file.
Note: Ubuntu Enterprise Cloud refers to a discontinued Eucalyptus-based Ubuntu cloud solution that
has been replaced by the OpenStack-based Ubuntu Cloud Infrastructure.
VDI VirtualBox uses the VDI (Virtual Disk Image) format for image files. None of the OpenStack Compute
hypervisors support VDI directly, so you will need to convert these files to a different format to use them
6 Introduction
Virtual Machine Image Guide (Release Version: 15.0.0)
with OpenStack.
VHD Microsoft Hyper-V uses the VHD (Virtual Hard Disk) format for images.
VHDX The version of Hyper-V that ships with Microsoft Server 2012 uses the newer VHDX format, which
has some additional features over VHD such as support for larger disk sizes and protection against data
corruption during power failures.
VMDK VMware ESXi hypervisor uses the VMDK (Virtual Machine Disk) format for images.
Get images
The simplest way to obtain a virtual machine image that works with OpenStack is to download one that someone
else has already created. Most of the images contain the cloud-init package to support the SSH key pair and
user data injection. Because many of the images disable SSH password authentication by default, boot the
image with an injected key pair. You can SSH into the instance with the private key and default login account.
See the OpenStack End User Guide for more information on how to create and inject key pairs with OpenStack.
CentOS
The CentOS project maintains official images for direct download.
• CentOS 6 images
• CentOS 7 images
Note: In a CentOS cloud image, the login account is centos.
CirrOS (test)
CirrOS is a minimal Linux distribution that was designed for use as a test image on clouds such as OpenStack
Compute. You can download a CirrOS image in various formats from the CirrOS download page.
If your deployment uses QEMU or KVM, we recommend using the images in qcow2 format. The most recent
64-bit qcow2 image as of this writing is cirros-0.3.5-x86_64-disk.img.
Note: In a CirrOS image, the login account is cirros. The password is cubswin:).
Debian
Debian provides images for direct download. They are made at the same time as the CD and DVD images of
Debian. Therefore, images are available on each point release of Debian. Also, weekly images of the testing
distribution are available.
Note: In a Debian image, the login account is debian.
Get images 7
Virtual Machine Image Guide (Release Version: 15.0.0)
Fedora
The Fedora project maintains a list of official cloud images at Fedora download page.
Note: In a Fedora cloud image, the login account is fedora.
Microsoft Windows
Cloudbase Solutions hosts Windows Cloud Images that runs on Hyper-V, KVM, and XenServer/XCP.
Ubuntu
Canonical maintains an official set of Ubuntu-based images.
Images are arranged by Ubuntu release, and by image release date, with current being the most recent. For
example, the page that contains the most recently built image for Ubuntu 16.04 Xenial Xerus is Ubuntu 16.04
LTS (Xenial Xerus) Daily Build. Scroll to the bottom of the page for links to the images that can be downloaded
directly.
If your deployment uses QEMU or KVM, we recommend using the images in qcow2 format. The most recent
version of the 64-bit QCOW2 image for Ubuntu 16.04 is xenial-server-cloudimg-amd64-disk1.img.
Note: In an Ubuntu cloud image, the login account is ubuntu.
openSUSE and SUSE Linux Enterprise Server
The openSUSE community provides images for openSUSE.
SUSE maintains official SUSE Linux Enterprise Server cloud images. A valid SUSE Linux Enterprise Server
subscription is required to download these images.
• SUSE Linux Enterprise Server 12 SP1 JeOS
For openSUSE and SUSE Linux Enterprise Server (SLES), custom images can also be built with a web-based
tool called SUSE Studio.
Red Hat Enterprise Linux
Red Hat maintains official Red Hat Enterprise Linux cloud images. A valid Red Hat Enterprise Linux subscription
is required to download these images.
• Red Hat Enterprise Linux 7 KVM Guest Image
• Red Hat Enterprise Linux 6 KVM Guest Image
Note: In a RHEL cloud image, the login account is cloud-user.
8 Get images
Virtual Machine Image Guide (Release Version: 15.0.0)
Image requirements
Linux
For a Linux-based image to have full functionality in an OpenStack Compute cloud, there are a few requirements.
For some of these, you can fulfill the requirements by installing the cloud-init package. Read this section
before you create your own image to be sure that the image supports the OpenStack features that you plan to
use.
• Disk partitions and resize root partition on boot (cloud-init)
• No hard-coded MAC address information
• SSH server running
• Disable firewall
• Access instance using ssh public key (cloud-init)
• Process user data and other metadata (cloud-init)
• Paravirtualized Xen support in Linux kernel (Xen hypervisor only with Linux kernel version < 3.0)
Disk partitions and resize root partition on boot (cloud-init)
When you create a Linux image, you must decide how to partition the disks. The choice of partition method
can affect the resizing functionality, as described in the following sections.
The size of the disk in a virtual machine image is determined when you initially create the image. However,
OpenStack lets you launch instances with different size drives by specifying different flavors. For example, if
your image was created with a 5 GB disk, and you launch an instance with a flavor of m1.small. The resulting
virtual machine instance has, by default, a primary disk size of 20 GB. When the disk for an instance is resized
up, zeros are just added to the end.
Your image must be able to resize its partitions on boot to match the size requested by the user. Otherwise, after
the instance boots, you must manually resize the partitions to access the additional storage to which you have
access when the disk size associated with the flavor exceeds the disk size with which your image was created.
Xen: one ext3/ext4 partition (no LVM)
If you use the OpenStack XenAPI driver, the Compute service automatically adjusts the partition and file system
for your instance on boot. Automatic resize occurs if the following conditions are all true:
• auto_disk_config=True is set as a property on the image in the image registry.
• The disk on the image has only one partition.
• The file system on the one partition is ext3 or ext4.
Therefore, if you use Xen, we recommend that when you create your images, you create a single ext3 or ext4
partition (not managed by LVM). Otherwise, read on.
Non-Xen with cloud-init/cloud-tools: one ext3/ext4 partition (no LVM)
You must configure these items for your image:
Image requirements 9
Virtual Machine Image Guide (Release Version: 15.0.0)
• The partition table for the image describes the original size of the image.
• The file system for the image fills the original size of the image.
Then, during the boot process, you must:
• Modify the partition table to make it aware of the additional space:
– If you do not use LVM, you must modify the table to extend the existing root partition to encompass
this additional space.
– If you use LVM, you can add a new LVM entry to the partition table, create a new LVM physical
volume, add it to the volume group, and extend the logical partition with the root volume.
• Resize the root volume file system.
Depending on your distribution, the simplest way to support this is to install in your image:
• the cloud-init package,
• the cloud-utils package, which, on Ubuntu and Debian, also contains the growpart tool for extending
partitions,
• if you use Fedora, CentOS 7, or RHEL 7, the cloud-utils-growpart package, which contains the
growpart tool for extending partitions,
• if you use Ubuntu or Debian, the cloud-initramfs-growroot package , which supports resizing root partition
on the first boot.
With these packages installed, the image performs the root partition resize on boot. For example, in the /etc/
rc.local file.
If you cannot install cloud-initramfs-tools, Robert Plestenjak has a GitHub project called linux-rootfsresize
that contains scripts that update a ramdisk by using growpart so that the image resizes properly on
boot.
If you can install the cloud-init and cloud-utils packages, we recommend that when you create your
images, you create a single ext3 or ext4 partition (not managed by LVM).
Non-Xen without cloud-init/cloud-tools: LVM
If you cannot install cloud-init and cloud-tools inside of your guest, and you want to support resize, you
must write a script that your image runs on boot to modify the partition table. In this case, we recommend using
LVM to manage your partitions. Due to a limitation in the Linux kernel (as of this writing), you cannot modify
a partition table of a raw disk that has partitions currently mounted, but you can do this for LVM.
Your script must do something like the following:
1. Detect if any additional space is available on the disk. For example, parse the output of parted /dev/
sda --script "print free".
2. Create a new LVM partition with the additional space. For example, parted /dev/sda --script
"mkpart lvm ...".
3. Create a new physical volume. For example, pvcreate /dev/sda6.
4. Extend the volume group with this physical partition. For example, vgextend vg00 /dev/sda6.
5. Extend the logical volume contained the root partition by the amount of space. For example, lvextend
/dev/mapper/node-root /dev/sda6.
10 Image requirements
Virtual Machine Image Guide (Release Version: 15.0.0)
6. Resize the root file system. For example, resize2fs /dev/mapper/node-root.
You do not need a /boot partition unless your image is an older Linux distribution that requires that /boot is
not managed by LVM.
No hard-coded MAC address information
You must remove the network persistence rules in the image because they cause the network interface in the
instance to come up as an interface other than eth0. This is because your image has a record of the MAC address
of the network interface card when it was first installed, and this MAC address is different each time the instance
boots. You should alter the following files:
• Replace /etc/udev/rules.d/70-persistent-net.rules with an empty file (contains network persistence
rules, including MAC address).
• Replace /lib/udev/rules.d/75-persistent-net-generator.rules with an empty file (this generates
the file above).
• Remove the HWADDR line from /etc/sysconfig/network-scripts/ifcfg-eth0 on Fedora-based
images.
Note: If you delete the network persistent rules files, you may get a udev kernel warning at boot time, which
is why we recommend replacing them with empty files instead.
Ensure ssh server runs
You must install an ssh server into the image and ensure that it starts up on boot, or you cannot connect to your
instance by using ssh when it boots inside of OpenStack. This package is typically called openssh-server.
Disable firewall
In general, we recommend that you disable any firewalls inside of your image and use OpenStack security
groups to restrict access to instances. The reason is that having a firewall installed on your instance can make
it more difficult to troubleshoot networking issues if you cannot connect to your instance.
Access instance by using ssh public key (cloud-init)
The typical way that users access virtual machines running on OpenStack is to ssh using public key authentication.
For this to work, your virtual machine image must be configured to download the ssh public key from
the OpenStack metadata service or config drive, at boot time.
If both the XenAPI agent and cloud-init are present in an image, cloud-init handles ssh-key injection.
The system assumes cloud-init is present when the image has the cloud_init_installed property.
Use cloud-init to fetch the public key
The cloud-init package automatically fetches the public key from the metadata server and places the key in an
account. The account varies by distribution. On Ubuntu-based virtual machines, the account is called ubuntu,
Image requirements 11
Virtual Machine Image Guide (Release Version: 15.0.0)
on Fedora-based virtual machines, the account is called fedora, and on CentOS-based virtual machines, the
account is called centos.
You can change the name of the account used by cloud-init by editing the /etc/cloud/cloud.cfg file and
adding a line with a different user. For example, to configure cloud-init to put the key in an account named
admin, use the following syntax in the configuration file:
users:
- name: admin
(...)
Write a custom script to fetch the public key
If you are unable or unwilling to install cloud-init inside the guest, you can write a custom script to fetch the
public key and add it to a user account.
To fetch the ssh public key and add it to the root account, edit the /etc/rc.local file and add the following
lines before the line touch /var/lock/subsys/local. This code fragment is taken from the rackerjoe ozimage-build
CentOS 6 template.
if [ ! -d /root/.ssh ]; then
mkdir -p /root/.ssh
chmod 700 /root/.ssh
fi
# Fetch public key using HTTP
ATTEMPTS=30
FAILED=0
while [ ! -f /root/.ssh/authorized_keys ]; do
curl -f http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key > /tmp/metadata-
,→key 2>/dev/null
if [ $? -eq 0 ]; then
cat /tmp/metadata-key >> /root/.ssh/authorized_keys
chmod 0600 /root/.ssh/authorized_keys
restorecon /root/.ssh/authorized_keys
rm -f /tmp/metadata-key
echo "Successfully retrieved public key from instance metadata"
echo "*****************"
echo "AUTHORIZED KEYS"
echo "*****************"
cat /root/.ssh/authorized_keys
echo "*****************"
else
FAILED=`expr $FAILED + 1`
if [ $FAILED -ge $ATTEMPTS ]; then
echo "Failed to retrieve public key from instance metadata after $FAILED attempts,
,→quitting"
break
fi
echo "Could not retrieve public key from instance metadata (attempt #$FAILED/$ATTEMPTS),
,→ retrying in 5 seconds..."
sleep 5
fi
done
12 Image requirements
Virtual Machine Image Guide (Release Version: 15.0.0)
Note: Some VNC clients replace : (colon) with ; (semicolon) and _ (underscore) with - (hyphen). If editing a
file over a VNC session, make sure it is http: not http; and authorized_keys not authorized-keys.
Process user data and other metadata (cloud-init)
In addition to the ssh public key, an image might need additional information from OpenStack, such as Provide
user data to instances, that the user submitted when requesting the image. For example, you might want to set
the host name of the instance when it is booted. Or, you might wish to configure your image so that it executes
user data content as a script on boot.
You can access this information through the metadata service or referring to Store metadata on the configuration
drive. As the OpenStack metadata service is compatible with version 2009-04-04 of the Amazon EC2 metadata
service, consult the Amazon EC2 documentation on Using Instance Metadata for details on how to retrieve the
user data.
The easiest way to support this type of functionality is to install the cloud-init package into your image,
which is configured by default to treat user data as an executable script, and sets the host name.
Ensure image writes boot log to console
You must configure the image so that the kernel writes the boot log to the ttyS0 device. In particular, the
console=tty0 console=ttyS0,115200n8 arguments must be passed to the kernel on boot.
If your image uses grub2 as the boot loader, there should be a line in the grub configuration file. For example,
/boot/grub/grub.cfg, which looks something like this:
linux /boot/vmlinuz-3.2.0-49-virtual root=UUID=6d2231e4-0975-4f35-a94f-56738c1a8150 ro
,→console=tty0 console=ttyS0,115200n8
If console=tty0 console=ttyS0,115200n8 does not appear, you must modify your grub configuration. In
general, you should not update the grub.cfg directly, since it is automatically generated. Instead, you should
edit the /etc/default/grub file and modify the value of the GRUB_CMDLINE_LINUX_DEFAULT variable:
GRUB_CMDLINE_LINUX_DEFAULT="console=tty0 console=ttyS0,115200n8"
Next, update the grub configuration. On Debian-based operating systems such as Ubuntu, run this command:
# update-grub
On Fedora-based systems, such as RHEL and CentOS, and on openSUSE, run this command:
# grub2-mkconfig -o /boot/grub2/grub.cfg
Paravirtualized Xen support in the kernel (Xen hypervisor only)
Prior to Linux kernel version 3.0, the mainline branch of the Linux kernel did not have support for paravirtualized
Xen virtual machine instances (what Xen calls DomU guests). If you are running the Xen hypervisor with
paravirtualization, and you want to create an image for an older Linux distribution that has a pre 3.0 kernel, you
must ensure that the image boots a kernel that has been compiled with Xen support.
Image requirements 13
Virtual Machine Image Guide (Release Version: 15.0.0)
Manage the image cache
Use options in the nova.conf file to control whether, and for how long, unused base images are stored in the
/var/lib/nova/instances/_base/. If you have configured live migration of instances, all your compute
nodes share one common /var/lib/nova/instances/ directory.
For information about the libvirt images in OpenStack, see The life of an OpenStack libvirt image from Pádraig
Brady.
Table 1: Image cache management configuration options
Configuration option=Default value (Type) Description
preallocate_images=none (StrOpt) VM image preallocation mode:
none No storage provisioning occurs up
front.
space Storage is fully allocated at instance
start. The $instance_dir/ images
are fallocated to immediately determine
if enough space is available,
and to possibly improve VM I/O performance
due to ongoing allocation
avoidance, and better locality of block
allocations.
remove_unused_base_images=True (BoolOpt) Should unused base images be removed?
When set to True, the interval at
which base images are removed are set with
the following two settings. If set to False
base images are never removed by Compute.
remove_unused_original_minimum_age_seconds=86400 (IntOpt) Unused unresized base images
younger than this are not removed. Default
is 86400 seconds, or 24 hours.
remove_unused_resized_minimum_age_seconds=3600 (IntOpt) Unused resized base images
younger than this are not removed. Default
is 3600 seconds, or one hour.
To see how the settings affect the deletion of a running instance, check the directory where the images are
stored:
# ls -lash /var/lib/nova/instances/_base/
In the /var/log/compute/compute.log file, look for the identifier:
2012-02-18 04:24:17 41389 WARNING nova.virt.libvirt.imagecache [-] Unknown base file: /var/
,→lib/nova/instances/_base/06a057b9c7b0b27e3b496f53d1e88810a0d1d5d3_20
2012-02-18 04:24:17 41389 INFO nova.virt.libvirt.imagecache [-] Removable base files: /var/
,→lib/nova/instances/_base/06a057b9c7b0b27e3b496f53d1e88810a0d1d5d3 /var/lib/nova/instances/
,→_base/06a057b9c7b0b27e3b496f53d1e88810a0d1d5d3_20
2012-02-18 04:24:17 41389 INFO nova.virt.libvirt.imagecache [-] Removing base file: /var/
,→lib/nova/instances/_base/06a057b9c7b0b27e3b496f53d1e88810a0d1d5d3
Because 86400 seconds (24 hours) is the default time for remove_unused_original_minimum_age_seconds,
you can either wait for that time interval to see the base image removed, or set the value to a shorter time
period in the nova.conf file. Restart all nova services after changing a setting in the nova.conf file.
14 Image requirements
Virtual Machine Image Guide (Release Version: 15.0.0)
Modify images
Once you have obtained a virtual machine image, you may want to make some changes to it before uploading
it to the Image service. Here we describe several tools available that allow you to modify images.
Warning: Do not attempt to use these tools to modify an image that is attached to a running virtual
machine. These tools are designed only to modify the images that are not currently running.
guestfish
The guestfish program is a tool from the libguestfs project that allows you to modify the files inside of a
virtual machine image.
Note: guestfish does not mount the image directly into the local file system. Instead, it provides you with a
shell interface that enables you to view, edit, and delete files. Many of guestfish commands, such as touch,
chmod, and rm, resemble traditional bash commands.
Example guestfish session
Sometimes you must modify a virtual machine image to remove any traces of the MAC address that was assigned
to the virtual network interface card when the image was first created. This is because the MAC address is
different when the virtual machine images boots. This example shows how to use the guestfish to remove
references to the old MAC address by deleting the /etc/udev/rules.d/70-persistent-net.rules file
and removing the HWADDR line from the /etc/sysconfig/network-scripts/ifcfg-eth0 file.
Assume that you have a CentOS qcow2 image called centos63_desktop.img. Mount the image in read-write
mode as root, as follows:
# guestfish --rw -a centos63_desktop.img
Welcome to guestfish, the libguestfs filesystem interactive shell for
editing virtual machine filesystems.
Type: 'help' for help on commands
'man' to read the manual
'quit' to quit the shell
><fs>
This starts a guestfish session.
Note: the guestfish prompt looks like a fish: ><fs>.
We must first use the run command at the guestfish prompt before we can do anything else. This will launch a
virtual machine, which will be used to perform all of the file manipulations.
><fs> run
Modify images 15
Virtual Machine Image Guide (Release Version: 15.0.0)
1. We can now view the file systems in the image using the list-filesystems command:
><fs> list-filesystems
/dev/vda1: ext4
/dev/vg_centosbase/lv_root: ext4
/dev/vg_centosbase/lv_swap: swap
2. We need to mount the logical volume that contains the root partition:
><fs> mount /dev/vg_centosbase/lv_root /
3. Next, we want to delete a file. We can use the rm guestfish command, which works the same way it does
in a traditional shell.
><fs> rm /etc/udev/rules.d/70-persistent-net.rules
4. We want to edit the ifcfg-eth0 file to remove the HWADDR line. The edit command will copy the file
to the host, invoke your editor, and then copy the file back.
><fs> edit /etc/sysconfig/network-scripts/ifcfg-eth0
5. If you want to modify this image to load the 8021q kernel at boot time, you must create an executable
script in the /etc/sysconfig/modules/ directory. You can use the touch guestfish command to create
an empty file, the edit command to edit it, and the chmod command to make it executable.
><fs> touch /etc/sysconfig/modules/8021q.modules
><fs> edit /etc/sysconfig/modules/8021q.modules
6. We add the following line to the file and save it:
modprobe 8021q
7. Then we set to executable:
><fs> chmod 0755 /etc/sysconfig/modules/8021q.modules
8. We are done, so we can exit using the exit command:
><fs> exit
Go further with guestfish
There is an enormous amount of functionality in guestfish and a full treatment is beyond the scope of this
document. Instead, we recommend that you read the guestfs-recipes documentation page for a sense of what is
possible with these tools.
guestmount
For some types of changes, you may find it easier to mount the image’s file system directly in the guest. The
guestmount program, also from the libguestfs project, allows you to do so.
1. For example, to mount the root partition from our centos63_desktop.qcow2 image to /mnt, we can
do:
16 Modify images
Virtual Machine Image Guide (Release Version: 15.0.0)
# guestmount -a centos63_desktop.qcow2 -m /dev/vg_centosbase/lv_root --rw /mnt
2. If we did not know in advance what the mount point is in the guest, we could use the -i (inspect) flag to
tell guestmount to automatically determine what mount point to use:
# guestmount -a centos63_desktop.qcow2 -i --rw /mnt
3. Once mounted, we could do things like list the installed packages using rpm:
# rpm -qa --dbpath /mnt/var/lib/rpm
4. Once done, we unmount:
# umount /mnt
virt-* tools
The libguestfs project has a number of other useful tools, including:
• virt-edit for editing a file inside of an image.
• virt-df for displaying free space inside of an image.
• virt-resize for resizing an image.
• virt-sysprep for preparing an image for distribution (for example, delete SSH host keys, remove MAC
address info, or remove user accounts).
• virt-sparsify for making an image sparse.
• virt-p2v for converting a physical machine to an image that runs on KVM.
• virt-v2v for converting Xen and VMware images to KVM images.
Modify a single file inside of an image
This example shows how to use virt-edit to modify a file. The command can take either a filename as an
argument with the -a flag, or a domain name as an argument with the -d flag. The following examples shows
how to use this to modify the /etc/shadow file in instance with libvirt domain name instance-000000e1
that is currently running:
# virsh shutdown instance-000000e1
# virt-edit -d instance-000000e1 /etc/shadow
# virsh start instance-000000e1
Resize an image
Here is an example of how to use virt-resize to resize an image. Assume we have a 16 GB Windows image
in qcow2 format that we want to resize to 50 GB.
1. First, we use virt-filesystems to identify the partitions:
Modify images 17
Virtual Machine Image Guide (Release Version: 15.0.0)
# virt-filesystems --long --parts --blkdevs -h -a /data/images/win2012.qcow2
Name Type MBR Size Parent
/dev/sda1 partition 07 350M /dev/sda
/dev/sda2 partition 07 16G /dev/sda
/dev/sda device - 16G -
2. In this case, it is the /dev/sda2 partition that we want to resize. We create a new qcow2 image and use
the virt-resize command to write a resized copy of the original into the new image:
# qemu-img create -f qcow2 /data/images/win2012-50gb.qcow2 50G
# virt-resize --expand /dev/sda2 /data/images/win2012.qcow2 \
/data/images/win2012-50gb.qcow2
Examining /data/images/win2012.qcow2 ...
**********
Summary of changes:
/dev/sda1: This partition will be left alone.
/dev/sda2: This partition will be resized from 15.7G to 49.7G. The
filesystem ntfs on /dev/sda2 will be expanded using the
'ntfsresize' method.
**********
Setting up initial partition table on /data/images/win2012-50gb.qcow2 ...
Copying /dev/sda1 ...
100% [ ] 00:00
Copying /dev/sda2 ...
100% [ ] 00:00
Expanding /dev/sda2 using the 'ntfsresize' method ...
Resize operation completed with no errors. Before deleting the old
disk, carefully check that the resized disk boots and works correctly.
Loop devices, kpartx, network block devices
If you do not have access to the libguestfs, you can mount image file systems directly in the host using loop
devices, kpartx, and network block devices.
Warning: Mounting untrusted guest images using the tools described in this section is a security risk,
always use libguestfs tools such as guestfish and guestmount if you have access to them. See A reminder
why you should never mount guest disk images on the host OS by Daniel Berrangé for more details.
Mount a raw image (without LVM)
If you have a raw virtual machine image that is not using LVM to manage its partitions, use the losetup
command to find an unused loop device.
# losetup -f
/dev/loop0
In this example, /dev/loop0 is free. Associate a loop device with the raw image:
18 Modify images
Virtual Machine Image Guide (Release Version: 15.0.0)
# losetup /dev/loop0 fedora17.img
If the image only has a single partition, you can mount the loop device directly:
# mount /dev/loop0 /mnt
If the image has multiple partitions, use kpartx to expose the partitions as separate devices (for example,
/dev/mapper/loop0p1), then mount the partition that corresponds to the root file system:
# kpartx -av /dev/loop0
If the image has, say three partitions (/boot, /, swap), there should be one new device created per partition:
$ ls -l /dev/mapper/loop0p*
brw-rw---- 1 root disk 43, 49 2012-03-05 15:32 /dev/mapper/loop0p1
brw-rw---- 1 root disk 43, 50 2012-03-05 15:32 /dev/mapper/loop0p2
brw-rw---- 1 root disk 43, 51 2012-03-05 15:32 /dev/mapper/loop0p3
To mount the second partition, as root:
# mkdir /mnt/image
# mount /dev/mapper/loop0p2 /mnt/image
Once you are done, to clean up:
# umount /mnt/image
# rmdir /mnt/image
# kpartx -d /dev/loop0
# losetup -d /dev/loop0
Mount a raw image (with LVM)
If your partitions are managed with LVM, use losetup and kpartx commands as in the previous example to
expose the partitions to the host.
# losetup -f
/dev/loop0
# losetup /dev/loop0 rhel62.img
# kpartx -av /dev/loop0
Next, you need to use the vgscan command to identify the LVM volume groups and then the vgchange command
to expose the volumes as devices:
# vgscan
Reading all physical volumes. This may take a while...
Found volume group "vg_rhel62x8664" using metadata type lvm2
# vgchange -ay
2 logical volume(s) in volume group "vg_rhel62x8664" now active
# mount /dev/vg_rhel62x8664/lv_root /mnt
Clean up when you are done:
# umount /mnt
# vgchange -an vg_rhel62x8664
Modify images 19
Virtual Machine Image Guide (Release Version: 15.0.0)
# kpartx -d /dev/loop0
# losetup -d /dev/loop0
Mount a qcow2 image (without LVM)
You need the nbd (network block device) kernel module loaded to mount qcow2 images. This will load it with
support for 16 block devices, which is fine for our purposes. As root:
# modprobe nbd max_part=16
Assuming the first block device (/dev/nbd0) is not currently in use, we can expose the disk partitions using
the qemu-nbd and partprobe commands. As root:
# qemu-nbd -c /dev/nbd0 image.qcow2
# partprobe /dev/nbd0
If the image has, say three partitions (/boot, /, swap), there should be one new device created for each partition:
$ ls -l /dev/nbd3*
brw-rw---- 1 root disk 43, 48 2012-03-05 15:32 /dev/nbd0
brw-rw---- 1 root disk 43, 49 2012-03-05 15:32 /dev/nbd0p1
brw-rw---- 1 root disk 43, 50 2012-03-05 15:32 /dev/nbd0p2
brw-rw---- 1 root disk 43, 51 2012-03-05 15:32 /dev/nbd0p3
Note: If the network block device you selected was already in use, the initial qemu-nbd command will fail
silently, and the /dev/nbd3p{1,2,3} device files will not be created.
If the image partitions are not managed with LVM, they can be mounted directly:
# mkdir /mnt/image
# mount /dev/nbd3p2 /mnt/image
When you are done, clean up:
# umount /mnt/image
# rmdir /mnt/image
# qemu-nbd -d /dev/nbd0
Mount a qcow2 image (with LVM)
If the image partitions are managed with LVM, after you use qemu-nbd and partprobe, you must use vgscan
and vgchange -ay in order to expose the LVM partitions as devices that can be mounted:
# modprobe nbd max_part=16
# qemu-nbd -c /dev/nbd0 image.qcow2
# partprobe /dev/nbd0
# vgscan
Reading all physical volumes. This may take a while...
Found volume group "vg_rhel62x8664" using metadata type lvm2
# vgchange -ay
20 Modify images
Virtual Machine Image Guide (Release Version: 15.0.0)
2 logical volume(s) in volume group "vg_rhel62x8664" now active
# mount /dev/vg_rhel62x8664/lv_root /mnt
When you are done, clean up:
# umount /mnt
# vgchange -an vg_rhel62x8664
# qemu-nbd -d /dev/nbd0
Create images manually
Verify the libvirt default network is running
Before starting a virtual machine with libvirt, verify that the libvirt default network has started. This network
must be active for your virtual machine to be able to connect out to the network. Starting this network will
create a Linux bridge (usually called virbr0), iptables rules, and a dnsmasq process that will serve as a DHCP
server.
To verify that the libvirt default network is enabled, use the virsh net-list command and verify that the
default network is active:
# virsh net-list
Name State Autostart
-----------------------------------------
default active yes
If the network is not active, start it by doing:
# virsh net-start default
Use the virt-manager X11 GUI
If you plan to create a virtual machine image on a machine that can run X11 applications, the simplest way to do
so is to use the virt-manager GUI, which is installable as the virt-manager package on both Fedora-based
and Debian-based systems. This GUI has an embedded VNC client that will let you view and interact with the
guest’s graphical console.
If you are building the image on a headless server, and you have an X server on your local machine, you can
launch virt-manager using ssh X11 forwarding to access the GUI. Since virt-manager interacts directly with
libvirt, you typically need to be root to access it. If you can ssh directly in as root (or with a user that has
permissions to interact with libvirt), do:
$ ssh -X root@server virt-manager
If the account you use to ssh into your server does not have permissions to run libvirt, but has sudo privileges,
do:
$ ssh -X user@server
$ sudo virt-manager
Create images manually 21
Virtual Machine Image Guide (Release Version: 15.0.0)
Note: The -X flag passed to ssh will enable X11 forwarding over ssh. If this does not work, try replacing it
with the -Y flag.
Click the Create a new virtual machine button at the top-left, or go to File → New Virtual Machine. Then,
follow the instructions.
You will be shown a series of dialog boxes that will allow you to specify information about the virtual machine.
Note: When using qcow2 format images, you should check the option Customize configuration before
install, go to disk properties and explicitly select the qcow2 format. This ensures the virtual machine disk
size will be correct.
Use virt-install and connect by using a local VNC client
If you do not wish to use virt-manager (for example, you do not want to install the dependencies on your
server, you do not have an X server running locally, the X11 forwarding over SSH is not working), you can use
the virt-install tool to boot the virtual machine through libvirt and connect to the graphical console from a
VNC client installed on your local machine.
Because VNC is a standard protocol, there are multiple clients available that implement the VNC spec, including
TigerVNC (multiple platforms), TightVNC (multiple platforms), RealVNC (multiple platforms), Chicken (Mac
OS X), Krde (KDE), Vinagre (GNOME).
The following example shows how to use the qemu-img command to create an empty image file, and virtinstall
command to start up a virtual machine using that image file. As root:
# qemu-img create -f qcow2 /tmp/centos.qcow2 10G
# virt-install --virt-type kvm --name centos --ram 1024 \
--disk /tmp/centos.qcow2,format=qcow2 \
--network network=default \
--graphics vnc,listen=0.0.0.0 --noautoconsole \
--os-type=linux --os-variant=centos7.0 \
--location=/data/isos/CentOS-7-x86_64-NetInstall-1611.iso
Starting install...
Creating domain... | 0 B 00:00
Domain installation still in progress. You can reconnect to
the console to complete the installation process.
The KVM hypervisor starts the virtual machine with the libvirt name, centos, with 1024 MB of RAM.
The virtual machine also has a virtual CD-ROM drive associated with the /data/isos/CentOS-7-x86_64-
22 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
NetInstall-1611.iso file and a local 10 GB hard disk in qcow2 format that is stored in the host at /tmp/
centos.qcow2. It configures networking to use libvirt default network. There is a VNC server that is listening
on all interfaces, and libvirt will not attempt to launch a VNC client automatically nor try to display the text
console (--no-autoconsole). Finally, libvirt will attempt to optimize the configuration for a Linux guest
running a CentOS 7 distribution.
Note: When using the libvirt default network, libvirt will connect the virtual machine’s interface to a
bridge called virbr0. There is a dnsmasq process managed by libvirt that will hand out an IP address on
the 192.168.122.0/24 subnet, and libvirt has iptables rules for doing NAT for IP addresses on this subnet.
Run the osinfo-query os command to see a range of allowed --os-variant options.
Use the virsh vncdisplay vm-name command to get the VNC port number.
# virsh vncdisplay centos
:1
In the example above, the guest centos uses VNC display :1, which corresponds to TCP port 5901. You
should be able to connect a VNC client running on your local machine to display :1 on the remote machine and
step through the installation process.
Example: CentOS image
This example shows you how to install a CentOS image and focuses mainly on CentOS 7. Because the CentOS
installation process might differ across versions, the installation steps might differ if you use a different version
of CentOS.
Download a CentOS install ISO
1. Navigate to the CentOS mirrors page.
2. Click one of the HTTP links in the right-hand column next to one of the mirrors.
3. Click the folder link of the CentOS version that you want to use. For example, 7/.
4. Click the isos/ folder link.
5. Click the x86_64/ folder link for 64-bit images.
6. Click the netinstall ISO image that you want to download. For example, CentOS-7-x86_64-
NetInstall-1611.iso is a good choice because it is a smaller image that downloads missing packages
from the Internet during installation.
Start the installation process
Start the installation process using either the virt-manager or the virt-install command as described
previously. If you use the virt-install command, do not forget to connect your VNC client to the virtual
machine.
Assume that:
• The name of your virtual machine image is centos; you need this name when you use virsh commands
to manipulate the state of the image.
Create images manually 23
Virtual Machine Image Guide (Release Version: 15.0.0)
• You saved the netinstall ISO image to the /data/isos directory.
If you use the virt-install command, the commands should look something like this:
# qemu-img create -f qcow2 /tmp/centos.qcow2 10G
# virt-install --virt-type kvm --name centos --ram 1024 \
--disk /tmp/centos.qcow2,format=qcow2 \
--network network=default \
--graphics vnc,listen=0.0.0.0 --noautoconsole \
--os-type=linux --os-variant=centos7.0 \
--location=/data/isos/CentOS-7-x86_64-NetInstall-1611.iso
Step through the installation
At the initial Installer boot menu, choose the Install CentOS 7 option. After the installation program starts,
choose your preferred language and click Continue to get to the installation summary. Accept the defaults.
Change the Ethernet status
The default Ethernet setting is OFF. Change the setting of the Ethernet form OFF to ON. In particular, ensure that
IPv4 Settings' Method is Automatic (DHCP), which is the default.
24 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Hostname
The installer allows you to choose a host name. The default (localhost.localdomain) is fine. You install
the cloud-init package later, which sets the host name on boot when a new instance is provisioned using this
image.
Point the installer to a CentOS web server
Depending on the version of CentOS, the net installer requires the user to specify either a URL or the web site
and a CentOS directory that corresponds to one of the CentOS mirrors. If the installer asks for a single URL, a
valid URL might be http://mirror.umd.edu/centos/7/os/x86_64.
Note: Consider using other mirrors as an alternative to mirror.umd.edu.
Create images manually 25
Virtual Machine Image Guide (Release Version: 15.0.0)
If the installer asks for web site name and CentOS directory separately, you might enter:
• Web site name: mirror.umd.edu
• CentOS directory: centos/7/os/x86_64
See CentOS mirror page to get a full list of mirrors, click on the HTTP link of a mirror to retrieve the web site
name of a mirror.
Storage devices
If prompted about which type of devices your installation uses, choose Virtio Block Device.
Partition the disks
There are different options for partitioning the disks. The default installation uses LVM partitions, and creates
three partitions (/boot, /, swap), which works fine. Alternatively, you might want to create a single ext4
partition that is mounted to /, which also works fine.
If unsure, use the default partition scheme for the installer. While no scheme is inherently better than another,
having the partition that you want to dynamically grow at the end of the list will allow it to grow without crossing
another partition’s boundary.
26 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Select installation option
Step through the installation, using the default options. The simplest thing to do is to choose the Minimal
Install install, which installs an SSH server.
Set the root password
During the installation, remember to set the root password when prompted.
Detach the CD-ROM and reboot
Wait until the installation is complete.
To eject a disk by using the virsh command, libvirt requires that you attach an empty disk at the same target
that the CD-ROM was previously attached, which may be hda. You can confirm the appropriate target using
the virsh dumpxml vm-image command.
# virsh dumpxml centos
<domain type='kvm' id='19'>
<name>centos</name>
...
<disk type='block' device='cdrom'>
<driver name='qemu' type='raw'/>
<target dev='hda' bus='ide'/>
<readonly/>
Create images manually 27
Virtual Machine Image Guide (Release Version: 15.0.0)
<address type='drive' controller='0' bus='1' target='0' unit='0'/>
</disk>
...
</domain>
Run the following commands from the host to eject the disk and reboot using virsh, as root. If you are using
virt-manager, the commands below will work, but you can also use the GUI to detach and reboot it by
manually stopping and starting.
# virsh attach-disk --type cdrom --mode readonly centos "" hda
# virsh reboot centos
Install the ACPI service
To enable the hypervisor to reboot or shutdown an instance, you must install and run the acpid service on the
guest system.
Log in as root to the CentOS guest and run the following commands to install the ACPI service and configure
it to start when the system boots:
# yum install acpid
# systemctl enable acpid
Configure to fetch metadata
An instance must interact with the metadata service to perform several tasks on start up. For example, the
instance must get the ssh public key and run the user data script. To ensure that the instance performs these
tasks, use one of these methods:
• Install a cloud-init RPM, which is a port of the Ubuntu cloud-init package. This is the recommended
approach.
• Modify the /etc/rc.local file to fetch desired information from the metadata service, as described in
the next section.
Use cloud-init to fetch the public key
The cloud-init package automatically fetches the public key from the metadata server and places the key in
an account. Install cloud-init inside the CentOS guest by running:
# yum install cloud-init
The account varies by distribution. On CentOS-based virtual machines, the account is called centos.
You can change the name of the account used by cloud-init by editing the /etc/cloud/cloud.cfg file and
adding a line with a different user. For example, to configure cloud-init to put the key in an account named
admin, use the following syntax in the configuration file:
users:
- name: admin
(...)
28 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Install cloud-utils-growpart to allow partitions to resize
In order for the root partition to properly resize, install the cloud-utils-growpart package, which contains
the proper tools to allow the disk to resize using cloud-init.
# yum install cloud-utils-growpart
Write a script to fetch the public key (if no cloud-init)
If you are not able to install the cloud-init package in your image, to fetch the ssh public key and add it to
the root account, edit the /etc/rc.d/rc.local file and add the following lines before the line touch /var/
lock/subsys/local:
if [ ! -d /root/.ssh ]; then
mkdir -p /root/.ssh
chmod 700 /root/.ssh
fi
# Fetch public key using HTTP
ATTEMPTS=30
FAILED=0
while [ ! -f /root/.ssh/authorized_keys ]; do
curl -f http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key \
> /tmp/metadata-key 2>/dev/null
if [ \$? -eq 0 ]; then
cat /tmp/metadata-key >> /root/.ssh/authorized_keys
chmod 0600 /root/.ssh/authorized_keys
restorecon /root/.ssh/authorized_keys
rm -f /tmp/metadata-key
echo "Successfully retrieved public key from instance metadata"
echo "*****************"
echo "AUTHORIZED KEYS"
echo "*****************"
cat /root/.ssh/authorized_keys
echo "*****************"
fi
done
Note: Some VNC clients replace the colon (:) with a semicolon (;) and the underscore (_) with a hyphen (-).
Make sure to specify http: and not http;. Make sure to specify authorized_keys and not authorizedkeys.
Note: The previous script only gets the ssh public key from the metadata server. It does not get user data,
which is optional data that can be passed by the user when requesting a new instance. User data is often used
to run a custom script when an instance boots.
As the OpenStack metadata service is compatible with version 2009-04-04 of the Amazon EC2 metadata service,
consult the Amazon EC2 documentation on Using Instance Metadata for details on how to get user data.
Create images manually 29
Virtual Machine Image Guide (Release Version: 15.0.0)
Disable the zeroconf route
For the instance to access the metadata service, you must disable the default zeroconf route:
# echo "NOZEROCONF=yes" >> /etc/sysconfig/network
Configure console
For the nova console-log command to work properly on CentOS 7, you might need to do the following
steps:
1. Edit the /etc/default/grub file and configure the GRUB_CMDLINE_LINUX option. Delete the rhgb
quiet and add console=tty0 console=ttyS0,115200n8 to the option.
For example:
...
GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=cl/root rd.lvm.lv=cl/swap console=tty0
,→console=ttyS0,115200n8"
2. Run the following command to save the changes:
# grub2-mkconfig -o /boot/grub2/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.10.0-229.14.1.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-229.14.1.el7.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-229.4.2.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-229.4.2.el7.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-229.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-229.el7.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-605f01abef434fb98dd1309e774b72ba
Found initrd image: /boot/initramfs-0-rescue-605f01abef434fb98dd1309e774b72ba.img
done
Shut down the instance
From inside the instance, run as root:
# poweroff
Clean up (remove MAC address details)
The operating system records the MAC address of the virtual Ethernet card in locations such as /etc/
sysconfig/network-scripts/ifcfg-eth0 during the instance process. However, each time the image
boots up, the virtual Ethernet card will have a different MAC address, so this information must be deleted from
the configuration file.
There is a utility called virt-sysprep, that performs various cleanup tasks such as removing the MAC address
references. It will clean up a virtual machine image in place:
# virt-sysprep -d centos
30 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Undefine the libvirt domain
Now that you can upload the image to the Image service, you no longer need to have this virtual machine image
managed by libvirt. Use the virsh undefine vm-image command to inform libvirt:
# virsh undefine centos
Image is complete
The underlying image file that you created with the qemu-img create command is ready to be uploaded. For
example, you can upload the /tmp/centos.qcow2 image to the Image service by using the openstack image
create command. For more information, see the Create or update an image.
Example: Ubuntu image
This example installs an Ubuntu 14.04 (Trusty Tahr) image. To create an image for a different version of
Ubuntu, follow these steps with the noted differences.
Download an Ubuntu installation ISO
Because the goal is to make the smallest possible base image, this example uses the network installation ISO.
The Ubuntu 64-bit 14.04 network installation ISO is at the Ubuntu download page.
Start the installation process
Start the installation process by using either virt-manager or virt-install as described in the previous
section. If you use virt-install, do not forget to connect your VNC client to the virtual machine.
Assume that the name of your virtual machine image is ubuntu-14.04, which you need to know when you
use virsh commands to manipulate the state of the image.
If you are using virt-manager, the commands should look something like this:
# qemu-img create -f qcow2 /tmp/trusty.qcow2 10G
# virt-install --virt-type kvm --name trusty --ram 1024 \
--cdrom=/data/isos/trusty-64-mini.iso \
--disk /tmp/trusty.qcow2,format=qcow2 \
--network network=default \
--graphics vnc,listen=0.0.0.0 --noautoconsole \
--os-type=linux --os-variant=ubuntutrusty
Step through the installation
At the initial Installer boot menu, choose the Install option. Step through the installation prompts, the defaults
should be fine.
Create images manually 31
Virtual Machine Image Guide (Release Version: 15.0.0)
Hostname
The installer may ask you to choose a host name. The default (ubuntu) is fine. We will install the cloud-init
package later, which will set the host name on boot when a new instance is provisioned using this image.
Select a mirror
The default mirror proposed by the installer should be fine.
Step through the install
Step through the install, using the default options. When prompted for a user name, the default (ubuntu) is
fine.
Partition the disks
There are different options for partitioning the disks. The default installation will use LVM partitions, and will
create three partitions (/boot, /, swap), and this will work fine. Alternatively, you may wish to create a single
ext4 partition, mounted to “/”, should also work fine.
If unsure, we recommend you use the installer’s default partition scheme, since there is no clear advantage to
one scheme or another.
32 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Automatic updates
The Ubuntu installer will ask how you want to manage upgrades on your system. This option depends on your
specific use case. If your virtual machine instances will be connected to the Internet, we recommend “Install
security updates automatically”.
Software selection: OpenSSH server
Choose OpenSSH server so that you will be able to SSH into the virtual machine when it launches inside of an
OpenStack cloud.
Install GRUB boot loader
Select Yes when asked about installing the GRUB boot loader to the master boot record.
Create images manually 33
Virtual Machine Image Guide (Release Version: 15.0.0)
For more information on configuring Grub, see the section called “Ensure image writes boot log to console”.
Detach the CD-ROM and reboot
Select the defaults for all of the remaining options. When the installation is complete, you will be prompted to
remove the CD-ROM.
34 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Note: There is a known bug in Ubuntu 14.04; when you select Continue, the virtual machine will shut down,
even though it says it will reboot.
To eject a disk using virsh, libvirt requires that you attach an empty disk at the same target that the CDROM was
previously attached, which should be hdc. You can confirm the appropriate target using the virsh dumpxml
vm-image command.
# virsh dumpxml trusty
<domain type='kvm'>
<name>trusty</name>
...
<disk type='block' device='cdrom'>
<driver name='qemu' type='raw'/>
<target dev='hdc' bus='ide'/>
<readonly/>
<address type='drive' controller='0' bus='1' target='0' unit='0'/>
</disk>
...
</domain>
Run the following commands in the host as root to start up the machine again as paused, eject the disk and
resume. If you are using virt-manager, you may use the GUI instead.
Create images manually 35
Virtual Machine Image Guide (Release Version: 15.0.0)
# virsh start trusty --paused
# virsh attach-disk --type cdrom --mode readonly trusty "" hdc
# virsh resume trusty
Note: In the previous example, you paused the instance, ejected the disk, and unpaused the instance. In
theory, you could have ejected the disk at the Installation complete screen. However, our testing indicates that
the Ubuntu installer locks the drive so that it cannot be ejected at that point.
Log in to newly created image
When you boot for the first time after install, it may ask you about authentication tools, you can just choose
Exit. Then, log in as root using the root password you specified.
Install cloud-init
The cloud-init script starts on instance boot and will search for a metadata provider to fetch a public key
from. The public key will be placed in the default user account for the image.
Install the cloud-init package:
# apt-get install cloud-init
When building Ubuntu images cloud-init must be explicitly configured for the metadata source in use. The
OpenStack metadata server emulates the EC2 metadata service used by images in Amazon EC2.
To set the metadata source to be used by the image run the dpkg-reconfigure command against the cloudinit
package. When prompted select the EC2 data source:
# dpkg-reconfigure cloud-init
The account varies by distribution. On Ubuntu-based virtual machines, the account is called ubuntu. On
Fedora-based virtual machines, the account is called ec2-user.
You can change the name of the account used by cloud-init by editing the /etc/cloud/cloud.cfg file and
adding a line with a different user. For example, to configure cloud-init to put the key in an account named
admin, use the following syntax in the configuration file:
users:
- name: admin
(...)
Shut down the instance
From inside the instance, as root:
# /sbin/shutdown -h now
36 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Clean up (remove MAC address details)
The operating system records the MAC address of the virtual Ethernet card in locations such as /etc/udev/
rules.d/70-persistent-net.rules during the installation process. However, each time the image boots
up, the virtual Ethernet card will have a different MAC address, so this information must be deleted from the
configuration file.
There is a utility called virt-sysprep, that performs various cleanup tasks such as removing the MAC address
references. It will clean up a virtual machine image in place:
# virt-sysprep -d trusty
Undefine the libvirt domain
Now that the image is ready to be uploaded to the Image service, you no longer need to have this virtual machine
image managed by libvirt. Use the virsh undefine vm-image command to inform libvirt:
# virsh undefine trusty
Image is complete
The underlying image file that you created with the qemu-img create command, such as /tmp/trusty.
qcow2, is now ready for uploading to the Image service by using the openstack image create command.
For more information, see the Create or update an image.
Example: Fedora image
This example shows you how to install a Fedora image and focuses mainly on Fedora 25. Because the Fedora
installation process might differ across versions, the installation steps might differ if you use a different version
of Fedora.
Download a Fedora install ISO
1. Visit the Fedora download site.
2. Navigate to the Download Fedora Server page for a Fedora Server ISO image.
3. Choose the ISO image you want to download.
For example, the Netinstall Image is a good choice because it is a smaller image that downloads
missing packages from the Internet during installation.
Start the installation process
Start the installation process using either the virt-manager or the virt-install command as described
previously. If you use the virt-install command, do not forget to connect your VNC client to the virtual
machine.
Assume that:
Create images manually 37
Virtual Machine Image Guide (Release Version: 15.0.0)
• The name of your virtual machine image is fedora; you need this name when you use virsh commands
to manipulate the state of the image.
• You saved the netinstall ISO image to the /tmp directory.
If you use the virt-install command, the commands should look something like this:
# qemu-img create -f qcow2 /tmp/fedora.qcow2 10G
# virt-install --virt-type kvm --name fedora --ram 1024 \
--disk /tmp/fedora.qcow2,format=qcow2 \
--network network=default \
--graphics vnc,listen=0.0.0.0 --noautoconsole \
--os-type=linux --os-variant=fedora23 \
--location=/tmp/Fedora-Server-netinst-x86_64-25-1.3.iso
Step through the installation
After the installation program starts, choose your preferred language and click Continue to get to the installation
summary. Accept the defaults.
Review the Ethernet status
Ensure that the Ethernet setting is ON. Additionally, make sure that IPv4 Settings' Method is Automatic
(DHCP), which is the default.
Hostname
The installer allows you to choose a host name. The default (localhost.localdomain) is fine. You install
the cloud-init package later, which sets the host name on boot when a new instance is provisioned using this
image.
Partition the disks
There are different options for partitioning the disks. The default installation uses LVM partitions, and creates
three partitions (/boot, /, swap), which works fine. Alternatively, you might want to create a single ext4
partition that is mounted to /, which also works fine.
If unsure, use the default partition scheme for the installer. While no scheme is inherently better than another,
having the partition that you want to dynamically grow at the end of the list will allow it to grow without crossing
another partition’s boundary.
Select software to install
Step through the installation, using the default options. The simplest thing to do is to choose the Minimal
Install install, which installs an SSH server.
38 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Set the root password
During the installation, remember to set the root password when prompted.
Detach the CD-ROM and reboot
Wait until the installation is complete.
To eject a disk by using the virsh command, libvirt requires that you attach an empty disk at the same target
that the CD-ROM was previously attached, which may be hda. You can confirm the appropriate target using
the virsh dumpxml vm-image command.
# virsh dumpxml fedora
<domain type='kvm' id='30'>
<name>fedora</name>
...
<disk type='file' device='cdrom'>
<driver name='qemu' type='raw'/>
<source file='/tmp/Fedora-Server-netinst-x86_64-25-1.3.iso'/>
<backingStore/>
<target dev='hda' bus='ide'/>
<readonly/>
<alias name='ide0-0-0'/>
<address type='drive' controller='0' bus='0' target='0' unit='0'/>
</disk>
...
</domain>
Run the following commands from the host to eject the disk and reboot using virsh, as root. If you are using
virt-manager, the commands below will work, but you can also use the GUI to detach and reboot it by
manually stopping and starting.
# virsh attach-disk --type cdrom --mode readonly fedora "" hda
# virsh reboot fedora
Install the ACPI service
To enable the hypervisor to reboot or shutdown an instance, you must install and run the acpid service on the
guest system.
Log in as root to the Fedora guest and run the following commands to install the ACPI service and configure it
to start when the system boots:
# dnf install acpid
# systemctl enable acpid
Configure cloud-init to fetch metadata
An instance must interact with the metadata service to perform several tasks on start up. For example, the
instance must get the ssh public key and run the user data script. To ensure that the instance performs these
tasks, use the cloud-init package.
Create images manually 39
Virtual Machine Image Guide (Release Version: 15.0.0)
The cloud-init package automatically fetches the public key from the metadata server and places the key in
an account. Install cloud-init inside the Fedora guest by running:
# yum install cloud-init
The account varies by distribution. On Fedora-based virtual machines, the account is called fedora.
You can change the name of the account used by cloud-init by editing the /etc/cloud/cloud.cfg file and
adding a line with a different user. For example, to configure cloud-init to put the key in an account named
admin, use the following syntax in the configuration file:
users:
- name: admin
(...)
Install cloud-utils-growpart to allow partitions to resize
In order for the root partition to properly resize, install the cloud-utils-growpart package, which contains
the proper tools to allow the disk to resize using cloud-init.
# dnf install cloud-utils-growpart
Disable the zeroconf route
For the instance to access the metadata service, you must disable the default zeroconf route:
# echo "NOZEROCONF=yes" >> /etc/sysconfig/network
Configure console
For the nova console-log command to work properly on Fedora, you might need to do the following steps:
1. Edit the /etc/default/grub file and configure the GRUB_CMDLINE_LINUX option. Delete the rhgb
quiet and add console=tty0 console=ttyS0,115200n8 to the option. For example:
...
GRUB_CMDLINE_LINUX="rd.lvm.lv=fedora/root rd.lvm.lv=fedora/swap console=tty0
,→console=ttyS0,115200n8"
2. Run the following command to save the changes:
# grub2-mkconfig -o /boot/grub2/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-4.10.10-200.fc25.x86_64
Found initrd image: /boot/initramfs-4.10.10-200.fc25.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-c613978614c7426ea3e550527f63710c
Found initrd image: /boot/initramfs-0-rescue-c613978614c7426ea3e550527f63710c.img
done
40 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Shut down the instance
From inside the instance, run as root:
# poweroff
Clean up (remove MAC address details)
The operating system records the MAC address of the virtual Ethernet card in locations such as /etc/
sysconfig/network-scripts/ifcfg-eth0 during the instance process. However, each time the image
boots up, the virtual Ethernet card will have a different MAC address, so this information must be deleted from
the configuration file.
There is a utility called virt-sysprep, that performs various cleanup tasks such as removing the MAC address
references. It will clean up a virtual machine image in place:
# virt-sysprep -d fedora
Undefine the libvirt domain
Now that you can upload the image to the Image service, you no longer need to have this virtual machine image
managed by libvirt. Use the virsh undefine vm-image command to inform libvirt:
# virsh undefine fedora
Image is complete
The underlying image file that you created with the qemu-img create command is ready to be uploaded. For
example, you can upload the /tmp/fedora.qcow2 image to the Image service by using the openstack image
create command. For more information, see the Create or update an image.
Example: Microsoft Windows image
This example creates a Windows Server 2012 qcow2 image, using the virt-install command and the KVM
hypervisor.
1. Follow these steps to prepare the installation:
(a) Download a Windows Server 2012 installation ISO. Evaluation images are available on the Microsoft
website (registration required).
(b) Download the signed VirtIO drivers ISO from the Fedora website.
(c) Create a 15 GB qcow2 image:
$ qemu-img create -f qcow2 ws2012.qcow2 15G
2. Start the Windows Server 2012 installation with the virt-install command:
Create images manually 41
Virtual Machine Image Guide (Release Version: 15.0.0)
# virt-install --connect qemu:///system \
--name ws2012 --ram 2048 --vcpus 2 \
--network network=default,model=virtio \
--disk path=ws2012.qcow2,format=qcow2,device=disk,bus=virtio \
--cdrom /path/to/en_windows_server_2012_x64_dvd.iso \
--disk path=/path/to/virtio-win-0.1-XX.iso,device=cdrom \
--vnc --os-type windows --os-variant win2k12 \
--os-distro windows --os-version 2012
Use virt-manager or virt-viewer to connect to the VM and start the Windows installation.
3. Enable the VirtIO drivers.
The disk is not detected by default by the Windows installer. When requested to choose an installation
target, click Load driver and browse the file system to select the E:\virtio-win-0.
1XX\viostor\w7\amd64 folder. The Windows installer displays a list of drivers to install. Select the
VirtIO SCSI and network drivers and continue the installation.
Once the installation is completed, the VM restarts. Define a password for the administrator when
prompted.
4. Log in as administrator and start a command window.
5. Complete the VirtIO drivers installation by running the following command:
C:\pnputil -i -a E:\virtio-win-0.1XX\viostor\w7\amd64\*.INF
6. To allow the Cloudbase-Init to run scripts during an instance boot, set the PowerShell execution policy
to be unrestricted:
C:\powershell
C:\Set-ExecutionPolicy Unrestricted
7. Download and install the Cloudbase-Init:
C:\Invoke-WebRequest -UseBasicParsing https://cloudbase.it/downloads/
,→CloudbaseInitSetup_Stable_x64.msi -OutFile cloudbaseinit.msi
C:\.\cloudbaseinit.msi
In the configuration options window, change the following settings:
• Username: Administrator
• Network adapter to configure: Red Hat VirtIO Ethernet Adapter
• Serial port for logging: COM1
When the installation is done, in the Complete the Cloudbase-Init Setup Wizard window, select the Run
Sysprep and Shutdown check boxes and click Finish.
Wait for the machine shutdown.
Your image is ready to upload to the Image service:
$ openstack image create --disk-format qcow2 --file ws2012.qcow2 WS2012
42 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Example: FreeBSD image
This example creates a minimal FreeBSD image that is compatible with OpenStack and bsd-cloudinit. The
bsd-cloudinit program is independently maintained and in active development. The best source of information
on the current state of the project is at bsd-cloudinit.
KVM with virtio drivers is used as the virtualization platform because that is the most widely used among
OpenStack operators. If you use a different platform for your cloud virtualization, use that same platform in
the image creation step.
This example shows how to create a FreeBSD 10 image. To create a FreeBSD 9.2 image, follow these steps
with the noted differences.
To create a FreeBSD image
1. Make a virtual drive:
$ qemu-img create -f qcow2 freebsd.qcow2 1G
The minimum supported disk size for FreeBSD is 1 GB. Because the goal is to make the smallest possible
base image, the example uses that minimum size. This size is sufficient to include the optional doc,
games, and lib32 collections. To include the ports collection, add another 1 GB. To include src, add
512 MB.
2. Get the installer ISO:
$ curl ftp://ftp.freebsd.org/pub/FreeBSD/releases/amd64/amd64/ISO-IMAGES/10.1/FreeBSD-
,→10.1-RELEASE-amd64-bootonly.iso \
> FreeBSD-10.1-RELEASE-amd64-bootonly.iso
3. Launch a VM on your local workstation. Use the same hypervisor, virtual disk, and virtual network
drivers as you use in your production environment.
The following command uses the minimum amount of RAM, which is 256 MB:
$ kvm -smp 1 -m 256 -cdrom FreeBSD-10.1-RELEASE-amd64-bootonly.iso \
-drive if=virtio,file=freebsd.qcow2 \
-net nic,model=virtio -net user
You can specify up to 1 GB additional RAM to make the installation process run faster.
This VM must also have Internet access to download packages.
Note: By using the same hypervisor, you can ensure that you emulate the same devices that exist in
production. However, if you use full hardware virtualization instead of paravirtualization, you do not
need to use the same hypervisor; you must use the same type of virtualized hardware because FreeBSD
device names are related to their drivers. If the name of your root block device or primary network
interface in production differs than the names used during image creation, errors can occur.
You now have a VM that boots from the downloaded install ISO and is connected to the blank virtual
disk that you created previously.
4. To install the operating system, complete the following steps inside the VM:
(a) When prompted, choose to run the ISO in Install mode.
(b) Accept the default keymap or select an appropriate mapping for your needs.
Create images manually 43
Virtual Machine Image Guide (Release Version: 15.0.0)
(c) Provide a host name for your image. If you use bsd-cloudinit, it overrides this value with the
name provided by OpenStack when an instance boots from this image.
(d) When prompted about the optional doc, games, lib32, ports, and src system components, select
only those that you need. It is possible to have a fully functional installation without selecting
additional components selected. As noted previously, a minimal system with a 1 GB virtual disk
supports doc, games, and lib32 inclusive. The ports collection requires at least 1 GB additional
space and possibly more if you plan to install many ports. The src collection requires an additional
512 MB.
(e) Configure the primary network interface to use DHCP. In this example, which uses a virtio network
device, this interface is named vtnet0.
(f) Accept the default network mirror.
(g) Set up disk partitioning.
Disk partitioning is a critical element of the image creation process and the auto-generated default
partitioning scheme does not work with bsd-cloudinit at this time.
Because the default does not work, you must select manual partitioning. The partition editor should
list only one block device. If you use virtio for the disk device driver, it is named vtbd0. Select
this device and run the create command three times:
i. Select Create to create a partition table. This action is the default when no partition table
exists. Then, select GPT GUID Partition Table from the list. This choice is the default.
ii. Create two partitions:
• First partition: A 64 kB freebsd-boot partition with no mount point.
• Second partition: A freebsd-ufs partition with a mount point of / with all remaining
free space.
The following figure shows a completed partition table with a 1 GB virtual disk:
Select Finish and then Commit to commit your changes.
44 Create images manually
Virtual Machine Image Guide (Release Version: 15.0.0)
Note: If you modify this example, the root partition, which is mounted on /, must be the last
partition on the drive so that it can expand at run time to the disk size that your instance type
provides. Also note that bsd-cloudinit currently has a hard-coded assumption that this is the
second partition.
5. Select a root password.
6. Select the CMOS time zone.
The virtualized CMOS almost always stores its time in UTC, so unless you know otherwise, select UTC.
7. Select the time zone appropriate to your environment.
8. From the list of services to start on boot, you must select ssh. Optionally, select other services.
9. Optionally, add users.
You do not need to add users at this time. The bsd-cloudinit program adds a freebsd user account
if one does not exist. The ssh keys for this user are associated with OpenStack. To customize this user
account, you can create it now. For example, you might want to customize the shell for the user.
10. Final config
This menu enables you to update previous settings. Check that the settings are correct, and click exit.
11. After you exit, you can open a shell to complete manual configuration steps. Select Yes to make a few
OpenStack-specific changes:
(a) Set up the console:
# echo 'console="comconsole,vidconsole"' >> /boot/loader.conf
This sets console output to go to the serial console, which is displayed by nova consolelog, and
the video console for sites with VNC or Spice configured.
(b) Minimize boot delay:
# echo 'autoboot_delay="1"' >> /boot/loader.conf
(c) Download the latest bsd-cloudinit-installer. The download commands differ between
FreeBSD 10.1 and 9.2 because of differences in how the fetch command handles HTTPS URLs.
In FreeBSD 10.1 the fetch command verifies SSL peers by default, so you need to install the
ca_root_nss package that contains certificate authority root certificates and tell fetch where to
find them. For FreeBSD 10.1 run these commands:
# pkg install ca_root_nss
# fetch --ca-cert=/usr/local/share/certs/ca-root-nss.crt \
https://raw.github.com/pellaeon/bsd-cloudinit-installer/master/installer.sh
FreeBSD 9.2 fetch does not support peer-verification for https. For FreeBSD 9.2, run this command:
# fetch https://raw.github.com/pellaeon/bsd-cloudinit-installer/master/installer.
,→sh
(d) Run the installer:
Create images manually 45
Virtual Machine Image Guide (Release Version: 15.0.0)
# sh ./installer.sh
Issue this command to download and install the latest bsd-cloudinit package, and install the
necessary prerequisites.
(e) Install sudo and configure the freebsd user to have passwordless access:
# pkg install sudo
# echo 'freebsd ALL=(ALL) NOPASSWD: ALL' > /usr/local/etc/sudoers.d/10-cloudinit
12. Power off the system:
# shutdown -h now
Creating a new image is a step done outside of your OpenStack installation. You create the new image manually
on your own system and then upload the image to your cloud.
To create a new image, you will need the installation CD or DVD ISO file for the guest operating system.
You will also need access to a virtualization tool. You can use KVM for this. Or, if you have a GUI desktop
virtualization tool (such as, VMware Fusion or VirtualBox), you can use that instead. Convert the file to raw
once you are done.
When you create a new virtual machine image, you will need to connect to the graphical console of the hypervisor,
which acts as the virtual machine’s display and allows you to interact with the guest operating system’s
installer using your keyboard and mouse. KVM can expose the graphical console using the VNC (Virtual Network
Computing) protocol or the newer SPICE protocol. We will use the VNC protocol here, since you are
more likely to find a VNC client that works on your local desktop.
To create an image for the Database service, see Building Guest Images for OpenStack Trove.
Tool support for image creation
There are several tools that are designed to automate image creation.
Diskimage-builder
Diskimage-builder is an automated disk image creation tool that supports a variety of distributions and architectures.
Diskimage-builder (DIB) can build images for Fedora, Red Hat Enterprise Linux, Ubuntu, Debian,
CentOS, and openSUSE. DIB is organized in a series of elements that build on top of each other to create
specific images.
To build an image, call the following script:
# disk-image-create ubuntu vm
This example creates a generic, bootable Ubuntu image of the latest release.
Further customization could be accomplished by setting environment variables or adding elements to the
command-line:
# disk-image-create -a armhf ubuntu vm
This example creates the image as before, but for arm architecture. More elements are available in the git source
directory and documented in the diskimage-builder elements documentation.
46 Tool support for image creation
Virtual Machine Image Guide (Release Version: 15.0.0)
Oz
Oz is a command-line tool that automates the process of creating a virtual machine image file. Oz is a Python
app that interacts with KVM to step through the process of installing a virtual machine.
It uses a predefined set of kickstart (Red Hat-based systems) and preseed files (Debian-based systems) for
operating systems that it supports, and it can also be used to create Microsoft Windows images.
A full treatment of Oz is beyond the scope of this document, but we will provide an example. You can find
additional examples of Oz template files on GitHub at rcbops/oz-image-build/tree/master/templates. Here’s
how you would create a CentOS 6.4 image with Oz.
Create a template file called centos64.tdl with the following contents. The only entry you will need to change
is the <rootpw> contents.
<template>
<name>centos64</name>
<os>
<name>CentOS-6</name>
<version>4</version>
<arch>x86_64</arch>
<install type='iso'>
<iso>http://mirror.rackspace.com/CentOS/6/isos/x86_64/CentOS-6.4-x86_64-bin-DVD1.iso</
,→iso>
</install>
<rootpw>CHANGE THIS TO YOUR ROOT PASSWORD</rootpw>
</os>
<description>CentOS 6.4 x86_64</description>
<repositories>
<repository name='epel-6'>
<url>http://download.fedoraproject.org/pub/epel/6/$basearch</url>
<signed>no</signed>
</repository>
</repositories>
<packages>
<package name='epel-release'/>
<package name='cloud-utils'/>
<package name='cloud-init'/>
</packages>
<commands>
<command name='update'>
yum -y update
yum clean all
sed -i '/^HWADDR/d' /etc/sysconfig/network-scripts/ifcfg-eth0
echo -n > /etc/udev/rules.d/70-persistent-net.rules
echo -n > /lib/udev/rules.d/75-persistent-net-generator.rules
</command>
</commands>
</template>
This Oz template specifies where to download the Centos 6.4 install ISO. Oz will use the version information
to identify which kickstart file to use. In this case, it will be RHEL6.auto. It adds EPEL as a repository and
install the epel-release, cloud-utils, and cloud-init packages, as specified in the packages section of
the file.
After Oz completes the initial OS install using the kickstart file, it customizes the image with an update. It also
removes any reference to the eth0 device that libvirt creates while Oz does the customizing, as specified in the
Tool support for image creation 47
Virtual Machine Image Guide (Release Version: 15.0.0)
command section of the XML file.
To run this:
# oz-install -d3 -u centos64.tdl -x centos64-libvirt.xml
• The -d3 flag tells Oz to show status information as it runs.
• The -u tells Oz to do the customization (install extra packages, run the commands) once it does the initial
install.
• The -x flag tells Oz what filename to use to write out a libvirt XML file (otherwise it will default to
something like centos64Apr_03_2013-12:39:42).
If you leave out the -u flag, or you want to edit the file to do additional customizations, you can use the ozcustomize
command, using the libvirt XML file that oz-install creates. For example:
# oz-customize -d3 centos64.tdl centos64-libvirt.xml
Oz will invoke libvirt to boot the image inside of KVM, then Oz will ssh into the instance and perform the
customizations.
VeeWee
VeeWee is often used to build Vagrant boxes, but it can also be used to build the KVM images.
Packer
Packer is a tool for creating machine images for multiple platforms from a single source configuration.
image-bootstrap
image-bootstrap is a command line tool that generates bootable virtual machine images with support of Arch,
Debian, Gentoo, Ubuntu, and is prepared for use with OpenStack.
imagefactory
imagefactory is a newer tool designed to automate the building, converting, and uploading images to different
cloud providers. It uses Oz as its back-end and includes support for OpenStack-based clouds.
KIWI
The KIWI OS image builder provides an operating system image builder for various Linux supported hardware
platforms as well as for virtualization and cloud systems. It allows building of images based on openSUSE,
SUSE Linux Enterprise, and Red Hat Enterprise Linux. The openSUSE Documentation explains how to use
KIWI.
48 Tool support for image creation
Virtual Machine Image Guide (Release Version: 15.0.0)
SUSE Studio
SUSE Studio is a web application for building and testing software applications in a web browser. It supports the
creation of physical, virtual or cloud-based applications and includes support for building images for OpenStack
based clouds using SUSE Linux Enterprise and openSUSE as distributions.
virt-builder
Virt-builder is a tool for quickly building new virtual machines. You can build a variety of VMs for local or
cloud use, usually within a few minutes or less. Virt-builder also has many ways to customize these VMs.
Everything is run from the command line and nothing requires root privileges, so automation and scripting is
simple.
To build an image, call the following script:
# virt-builder fedora-23 -o image.qcow2 --format qcow2 \
--update --selinux-relabel --size 20G
To list the operating systems available to install:
$ virt-builder --list
To import it into libvirt with virsh:
# virt-install --name fedora --ram 2048 \
--disk path=image.qcow2,format=qcow2 --import
Converting between image formats
Converting images from one format to another is generally straightforward.
qemu-img convert: raw, qcow2, qed, vdi, vmdk, vhd
The qemu-img convert command can do conversion between multiple formats, including qcow2, qed, raw,
vdi, vhd, and vmdk.
Table 2: qemu-img format strings
Image format Argument to qemu-img
QCOW2 (KVM, Xen) qcow2
QED (KVM) qed
raw raw
VDI (VirtualBox) vdi
VHD (Hyper-V) vpc
VMDK (VMware) vmdk
This example will convert a raw image file named image.img to a qcow2 image file.
$ qemu-img convert -f raw -O qcow2 image.img image.qcow2
Converting between image formats 49
Virtual Machine Image Guide (Release Version: 15.0.0)
Run the following command to convert a vmdk image file to a raw image file.
$ qemu-img convert -f vmdk -O raw image.vmdk image.img
Run the following command to convert a vmdk image file to a qcow2 image file.
$ qemu-img convert -f vmdk -O qcow2 image.vmdk image.qcow2
Note: The -f format flag is optional. If omitted, qemu-img will try to infer the image format.
When converting an image file with Windows, ensure the virtio driver is installed. Otherwise, you will get a blue
screen when launching the image due to lack of the virtio driver. Another option is to set the image properties
as below when you update the image in the Image service to avoid this issue, but it will reduce virtual machine
performance significantly.
$ openstack image set --property hw_disk_bus='ide' image_name_or_id
VBoxManage: VDI (VirtualBox) to raw
If you’ve created a VDI image using VirtualBox, you can convert it to raw format using the VBoxManage
command-line tool that ships with VirtualBox. On Mac OS X, and Linux, VirtualBox stores images by default
in the ~/VirtualBox VMs/ directory. The following example creates a raw image in the current directory from
a VirtualBox VDI image.
$ VBoxManage clonehd ~/VirtualBox\ VMs/image.vdi image.img --format raw
Image sharing
Image producers and consumers are both OpenStack users, or projects. Image producers create and share
images with image consumers, allowing the consumers to use the shared image when booting a server. The
producer shares an image with the consumer by making the consumer a member of that image. The consumer
then accepts or rejects the image by changing the image member status. After it is accepted, the image appears
in the consumer’s image list. As long as the consumer is a member of the image, the consumer can use the
image, regardless of the image member status, if the consumer knows the image ID.
Note: In the OpenStack Image API, the image member status serves three purposes:
• The member status controls whether image appears in the consumer’s image list. If the image member
status is accepted, the image appears in the consumer’s image list. Otherwise, the image does not
appear in the image list. The image may still be used as long as the consumer knows the image ID.
• The member status can be used to filter the consumer’s image list.
• The member status lets the producer know whether the consumer has seen and acted on the shared image.
If the status is accepted or rejected, the consumer has definitely seen the shared image. If the status
is pending, the consumer may not be aware that an image was shared.
Image producers and consumers have different abilities and responsibilities regarding image sharing, which the
following list shows.
50 Image sharing
Virtual Machine Image Guide (Release Version: 15.0.0)
• Image producers add members to images, or remove members from images, but they may not modify
the member status of an image member.
• Image producers and consumers view the status of image members. When listing image members, the
producers see all the image members, and the consumers see only themselves.
• Image consumers change their own member status, but they may not add or remove themselves as an
image member.
• Image consumers can boot from any image shared by the image producer, regardless of the member
status, as long as the consumer knows the image ID.
Sharing an image
The following procedure is a workflow for image sharing after image creation.
Communications between the image producer and the consumer, such as those described in this example, must
be arranged independently of the OpenStack Image API. The consumer and producer can send notifications by
using email, phone, Twitter, or other channels.
1. The producer posts the availability of specific images for consumers to review.
2. A potential consumer provides the producer with the consumer’s project ID. Optionally, the producer
might request the consumer’s email address for notification purposes, but this is outside the scope of the
API.
3. The producer shares the image with the consumer, by using the Create image member API operation.
4. Optionally, the producer notifies the consumer that the image has been shared and provides the image’s
ID (UUID).
5. If the consumer wants the image to appear in the image list, the consumer uses the OpenStack Image API
to change the image member status to accepted, by using the Update image member API operation.
6. If the consumer subsequently wants to hide the image, the consumer uses the OpenStack Image API to
change the image member status to rejected. If the consumer wants to hide the image, but is open
to the possibility of being reminded by the producer that the image is available, the consumer uses the
OpenStack Image API to change the image member status back to pending, by using the Update image
member API operation.
Appendix
Community support
The following resources are available to help you run and use OpenStack. The OpenStack community constantly
improves and adds to the main features of OpenStack, but if you have any questions, do not hesitate to ask. Use
the following resources to get OpenStack support and troubleshoot your installations.
Documentation
For the available OpenStack documentation, see docs.openstack.org.
Appendix 51
Virtual Machine Image Guide (Release Version: 15.0.0)
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at
OpenStack Documentation Mailing List, join our IRC channel #openstack-doc on the freenode IRC network,
or report a bug.
The following books explain how to install an OpenStack cloud and its associated components:
• Installation Tutorial for openSUSE Leap 42.2 and SUSE Linux Enterprise Server 12 SP2
• Installation Tutorial for Red Hat Enterprise Linux 7 and CentOS 7
• Installation Tutorial for Ubuntu 16.04 (LTS)
The following books explain how to configure and run an OpenStack cloud:
• Architecture Design Guide
• Administrator Guide
• Configuration Reference
• Operations Guide
• Networking Guide
• High Availability Guide
• Security Guide
• Virtual Machine Image Guide
The following books explain how to use the OpenStack Dashboard and command-line clients:
• End User Guide
• Command-Line Interface Reference
The following documentation provides reference and guidance information for the OpenStack APIs:
• API Guide
The following guide provides how to contribute to OpenStack documentation:
• Documentation Contributor Guide
ask.openstack.org
During the set up or testing of OpenStack, you might have questions about how a specific task is completed or
be in a situation where a feature does not work correctly. Use the ask.openstack.org site to ask questions and
get answers. When you visit the Ask OpenStack site, scan the recently asked questions to see whether your
question has already been answered. If not, ask a new question. Be sure to give a clear, concise summary in the
title and provide as much detail as possible in the description. Paste in your command output or stack traces,
links to screen shots, and any other information which might be useful.
OpenStack mailing lists
A great way to get answers and insights is to post your question or problematic scenario to the OpenStack
mailing list. You can learn from and help others who might have similar issues. To subscribe or view the
archives, go to the general OpenStack mailing list. If you are interested in the other mailing lists for specific
projects or development, refer to Mailing Lists.
52 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
The OpenStack wiki
The OpenStack wiki contains a broad range of topics but some of the information can be difficult to find or is a
few pages deep. Fortunately, the wiki search feature enables you to search by title or content. If you search for
specific information, such as about networking or OpenStack Compute, you can find a large amount of relevant
material. More is being added all the time, so be sure to check back often. You can find the search box in the
upper-right corner of any OpenStack wiki page.
The Launchpad Bugs area
The OpenStack community values your set up and testing efforts and wants your feedback. To log a bug, you
must sign up for a Launchpad account. You can view existing bugs and report bugs in the Launchpad Bugs
area. Use the search feature to determine whether the bug has already been reported or already been fixed. If it
still seems like your bug is unreported, fill out a bug report.
Some tips:
• Give a clear, concise summary.
• Provide as much detail as possible in the description. Paste in your command output or stack traces, links
to screen shots, and any other information which might be useful.
• Be sure to include the software and package versions that you are using, especially
if you are using a development branch, such as, "Kilo release" vs git commit
bc79c3ecc55929bac585d04a03475b72e06a3208.
• Any deployment-specific information is helpful, such as whether you are using Ubuntu 14.04 or are
performing a multi-node installation.
The following Launchpad Bugs areas are available:
• Bugs: OpenStack Block Storage (cinder)
• Bugs: OpenStack Compute (nova)
• Bugs: OpenStack Dashboard (horizon)
• Bugs: OpenStack Identity (keystone)
• Bugs: OpenStack Image service (glance)
• Bugs: OpenStack Networking (neutron)
• Bugs: OpenStack Object Storage (swift)
• Bugs: Application catalog (murano)
• Bugs: Bare metal service (ironic)
• Bugs: Clustering service (senlin)
• Bugs: Container Infrastructure Management service (magnum)
• Bugs: Data processing service (sahara)
• Bugs: Database service (trove)
• Bugs: Deployment service (fuel)
• Bugs: DNS service (designate)
• Bugs: Key Manager Service (barbican)
Appendix 53
Virtual Machine Image Guide (Release Version: 15.0.0)
• Bugs: Monitoring (monasca)
• Bugs: Orchestration (heat)
• Bugs: Rating (cloudkitty)
• Bugs: Shared file systems (manila)
• Bugs: Telemetry (ceilometer)
• Bugs: Telemetry v3 (gnocchi)
• Bugs: Workflow service (mistral)
• Bugs: Messaging service (zaqar)
• Bugs: OpenStack API Documentation (developer.openstack.org)
• Bugs: OpenStack Documentation (docs.openstack.org)
The OpenStack IRC channel
The OpenStack community lives in the #openstack IRC channel on the Freenode network. You can hang out, ask
questions, or get immediate feedback for urgent and pressing issues. To install an IRC client or use a browserbased
client, go to https://webchat.freenode.net/. You can also use Colloquy (Mac OS X), mIRC (Windows),
or XChat (Linux). When you are in the IRC channel and want to share code or command output, the generally
accepted method is to use a Paste Bin. The OpenStack project has one at Paste. Just paste your longer amounts
of text or logs in the web form and you get a URL that you can paste into the channel. The OpenStack IRC
channel is #openstack on irc.freenode.net. You can find a list of all OpenStack IRC channels on the IRC
page on the wiki.
Documentation feedback
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at
OpenStack Documentation Mailing List, or report a bug.
OpenStack distribution packages
The following Linux distributions provide community-supported packages for OpenStack:
• Debian: https://wiki.debian.org/OpenStack
• CentOS, Fedora, and Red Hat Enterprise Linux: https://www.rdoproject.org/
• openSUSE and SUSE Linux Enterprise Server: https://en.opensuse.org/Portal:OpenStack
• Ubuntu: https://wiki.ubuntu.com/ServerTeam/CloudArchive
Glossary
This glossary offers a list of terms and definitions to define a vocabulary for OpenStack-related concepts.
To add to OpenStack glossary, clone the openstack/openstack-manuals repository and update the source file
doc/common/glossary.rst through the OpenStack contribution process.
54 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
0-9
6to4 A mechanism that allows IPv6 packets to be transmitted over an IPv4 network, providing a strategy for
migrating to IPv6.
A
absolute limit Impassable limits for guest VMs. Settings include total RAM size, maximum number of vCPUs,
and maximum disk size.
access control list (ACL) A list of permissions attached to an object. An ACL specifies which users or system
processes have access to objects. It also defines which operations can be performed on specified objects.
Each entry in a typical ACL specifies a subject and an operation. For instance, the ACL entry (Alice,
delete) for a file gives Alice permission to delete the file.
access key Alternative term for an Amazon EC2 access key. See EC2 access key.
account The Object Storage context of an account. Do not confuse with a user account from an authentication
service, such as Active Directory, /etc/passwd, OpenLDAP, OpenStack Identity, and so on.
account auditor Checks for missing replicas and incorrect or corrupted objects in a specified Object Storage
account by running queries against the back-end SQLite database.
account database A SQLite database that contains Object Storage accounts and related metadata and that the
accounts server accesses.
account reaper An Object Storage worker that scans for and deletes account databases and that the account
server has marked for deletion.
account server Lists containers in Object Storage and stores container information in the account database.
account service An Object Storage component that provides account services such as list, create, modify, and
audit. Do not confuse with OpenStack Identity service, OpenLDAP, or similar user-account services.
accounting The Compute service provides accounting information through the event notification and system
usage data facilities.
Active Directory Authentication and identity service by Microsoft, based on LDAP. Supported in OpenStack.
active/active configuration In a high-availability setup with an active/active configuration, several systems
share the load together and if one fails, the load is distributed to the remaining systems.
active/passive configuration In a high-availability setup with an active/passive configuration, systems are set
up to bring additional resources online to replace those that have failed.
address pool A group of fixed and/or floating IP addresses that are assigned to a project and can be used by
or assigned to the VM instances in a project.
Address Resolution Protocol (ARP) The protocol by which layer-3 IP addresses are resolved into layer-2
link local addresses.
admin API A subset of API calls that are accessible to authorized administrators and are generally not accessible
to end users or the public Internet. They can exist as a separate service (keystone) or can be a
subset of another API (nova).
admin server In the context of the Identity service, the worker process that provides access to the admin API.
administrator The person responsible for installing, configuring, and managing an OpenStack cloud.
Appendix 55
Virtual Machine Image Guide (Release Version: 15.0.0)
Advanced Message Queuing Protocol (AMQP) The open standard messaging protocol used by OpenStack
components for intra-service communications, provided by RabbitMQ, Qpid, or ZeroMQ.
Advanced RISC Machine (ARM) Lower power consumption CPU often found in mobile and embedded
devices. Supported by OpenStack.
alert The Compute service can send alerts through its notification system, which includes a facility to create
custom notification drivers. Alerts can be sent to and displayed on the dashboard.
allocate The process of taking a floating IP address from the address pool so it can be associated with a fixed
IP on a guest VM instance.
Amazon Kernel Image (AKI) Both a VM container format and disk format. Supported by Image service.
Amazon Machine Image (AMI) Both a VM container format and disk format. Supported by Image service.
Amazon Ramdisk Image (ARI) Both a VM container format and disk format. Supported by Image service.
Anvil A project that ports the shell script-based project named DevStack to Python.
aodh Part of the OpenStack Telemetry service; provides alarming functionality.
Apache The Apache Software Foundation supports the Apache community of open-source software projects.
These projects provide software products for the public good.
Apache License 2.0 All OpenStack core projects are provided under the terms of the Apache License 2.0
license.
Apache Web Server The most common web server software currently used on the Internet.
API endpoint The daemon, worker, or service that a client communicates with to access an API. API endpoints
can provide any number of services, such as authentication, sales data, performance meters, Compute
VM commands, census data, and so on.
API extension Custom modules that extend some OpenStack core APIs.
API extension plug-in Alternative term for a Networking plug-in or Networking API extension.
API key Alternative term for an API token.
API server Any node running a daemon or worker that provides an API endpoint.
API token Passed to API requests and used by OpenStack to verify that the client is authorized to run the
requested operation.
API version In OpenStack, the API version for a project is part of the URL. For example, example.com/
nova/v1/foobar.
applet A Java program that can be embedded into a web page.
Application Catalog service (murano) The project that provides an application catalog service so that users
can compose and deploy composite environments on an application abstraction level while managing
the application lifecycle.
Application Programming Interface (API) A collection of specifications used to access a service, application,
or program. Includes service calls, required parameters for each call, and the expected return
values.
application server A piece of software that makes available another piece of software over a network.
Application Service Provider (ASP) Companies that rent specialized applications that help businesses and
organizations provide additional services with lower cost.
56 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
arptables Tool used for maintaining Address Resolution Protocol packet filter rules in the Linux kernel firewall
modules. Used along with iptables, ebtables, and ip6tables in Compute to provide firewall services
for VMs.
associate The process associating a Compute floating IP address with a fixed IP address.
Asynchronous JavaScript and XML (AJAX) A group of interrelated web development techniques used on
the client-side to create asynchronous web applications. Used extensively in horizon.
ATA over Ethernet (AoE) A disk storage protocol tunneled within Ethernet.
attach The process of connecting a VIF or vNIC to a L2 network in Networking. In the context of Compute,
this process connects a storage volume to an instance.
attachment (network) Association of an interface ID to a logical port. Plugs an interface into a port.
auditing Provided in Compute through the system usage data facility.
auditor A worker process that verifies the integrity of Object Storage objects, containers, and accounts. Auditors
is the collective term for the Object Storage account auditor, container auditor, and object auditor.
Austin The code name for the initial release of OpenStack. The first design summit took place in Austin,
Texas, US.
auth node Alternative term for an Object Storage authorization node.
authentication The process that confirms that the user, process, or client is really who they say they are
through private key, secret token, password, fingerprint, or similar method.
authentication token A string of text provided to the client after authentication. Must be provided by the user
or process in subsequent requests to the API endpoint.
AuthN The Identity service component that provides authentication services.
authorization The act of verifying that a user, process, or client is authorized to perform an action.
authorization node An Object Storage node that provides authorization services.
AuthZ The Identity component that provides high-level authorization services.
Auto ACK Configuration setting within RabbitMQ that enables or disables message acknowledgment. Enabled
by default.
auto declare A Compute RabbitMQ setting that determines whether a message exchange is automatically
created when the program starts.
availability zone An Amazon EC2 concept of an isolated area that is used for fault tolerance. Do not confuse
with an OpenStack Compute zone or cell.
AWS CloudFormation template AWS CloudFormation allows Amazon Web Services (AWS) users to create
and manage a collection of related resources. The Orchestration service supports a CloudFormationcompatible
format (CFN).
B
back end Interactions and processes that are obfuscated from the user, such as Compute volume mount, data
transmission to an iSCSI target by a daemon, or Object Storage object integrity checks.
back-end catalog The storage method used by the Identity service catalog service to store and retrieve information
about API endpoints that are available to the client. Examples include an SQL database, LDAP
database, or KVS back end.
Appendix 57
Virtual Machine Image Guide (Release Version: 15.0.0)
back-end store The persistent data store used to save and retrieve information for a service, such as lists of
Object Storage objects, current state of guest VMs, lists of user names, and so on. Also, the method that
the Image service uses to get and store VM images. Options include Object Storage, locally mounted
file system, RADOS block devices, VMware datastore, and HTTP.
Backup, Restore, and Disaster Recovery service (freezer) The project that provides integrated tooling for
backing up, restoring, and recovering file systems, instances, or database backups.
bandwidth The amount of available data used by communication resources, such as the Internet. Represents
the amount of data that is used to download things or the amount of data available to download.
barbican Code name of the Key Manager service.
bare An Image service container format that indicates that no container exists for the VM image.
Bare Metal service (ironic) The OpenStack service that provides a service and associated libraries capable
of managing and provisioning physical machines in a security-aware and fault-tolerant manner.
base image An OpenStack-provided image.
Bell-LaPadula model A security model that focuses on data confidentiality and controlled access to classified
information. This model divides the entities into subjects and objects. The clearance of a subject is
compared to the classification of the object to determine if the subject is authorized for the specific
access mode. The clearance or classification scheme is expressed in terms of a lattice.
Benchmark service (rally) OpenStack project that provides a framework for performance analysis and benchmarking
of individual OpenStack components as well as full production OpenStack cloud deployments.
Bexar A grouped release of projects related to OpenStack that came out in February of 2011. It included only
Compute (nova) and Object Storage (swift). Bexar is the code name for the second release of OpenStack.
The design summit took place in San Antonio, Texas, US, which is the county seat for Bexar county.
binary Information that consists solely of ones and zeroes, which is the language of computers.
bit A bit is a single digit number that is in base of 2 (either a zero or one). Bandwidth usage is measured in
bits per second.
bits per second (BPS) The universal measurement of how quickly data is transferred from place to place.
block device A device that moves data in the form of blocks. These device nodes interface the devices, such
as hard disks, CD-ROM drives, flash drives, and other addressable regions of memory.
block migration A method of VM live migration used by KVM to evacuate instances from one host to another
with very little downtime during a user-initiated switchover. Does not require shared storage. Supported
by Compute.
Block Storage API An API on a separate endpoint for attaching, detaching, and creating block storage for
compute VMs.
Block Storage service (cinder) The OpenStack service that implement services and libraries to provide ondemand,
self-service access to Block Storage resources via abstraction and automation on top of other
block storage devices.
BMC (Baseboard Management Controller) The intelligence in the IPMI architecture, which is a specialized
micro-controller that is embedded on the motherboard of a computer and acts as a server. Manages the
interface between system management software and platform hardware.
bootable disk image A type of VM image that exists as a single, bootable file.
58 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
Bootstrap Protocol (BOOTP) A network protocol used by a network client to obtain an IP address from a
configuration server. Provided in Compute through the dnsmasq daemon when using either the FlatDHCP
manager or VLAN manager network manager.
Border Gateway Protocol (BGP) The Border Gateway Protocol is a dynamic routing protocol that connects
autonomous systems. Considered the backbone of the Internet, this protocol connects disparate networks
to form a larger network.
browser Any client software that enables a computer or device to access the Internet.
builder file Contains configuration information that Object Storage uses to reconfigure a ring or to re-create
it from scratch after a serious failure.
bursting The practice of utilizing a secondary environment to elastically build instances on-demand when the
primary environment is resource constrained.
button class A group of related button types within horizon. Buttons to start, stop, and suspend VMs are in
one class. Buttons to associate and disassociate floating IP addresses are in another class, and so on.
byte Set of bits that make up a single character; there are usually 8 bits to a byte.
C
cache pruner A program that keeps the Image service VM image cache at or below its configured maximum
size.
Cactus An OpenStack grouped release of projects that came out in the spring of 2011. It included Compute
(nova), Object Storage (swift), and the Image service (glance). Cactus is a city in Texas, US and is the
code name for the third release of OpenStack. When OpenStack releases went from three to six months
long, the code name of the release changed to match a geography nearest the previous summit.
CALL One of the RPC primitives used by the OpenStack message queue software. Sends a message and
waits for a response.
capability Defines resources for a cell, including CPU, storage, and networking. Can apply to the specific
services within a cell or a whole cell.
capacity cache A Compute back-end database table that contains the current workload, amount of free RAM,
and number of VMs running on each host. Used to determine on which host a VM starts.
capacity updater A notification driver that monitors VM instances and updates the capacity cache as needed.
CAST One of the RPC primitives used by the OpenStack message queue software. Sends a message and does
not wait for a response.
catalog A list of API endpoints that are available to a user after authentication with the Identity service.
catalog service An Identity service that lists API endpoints that are available to a user after authentication
with the Identity service.
ceilometer Part of the OpenStack Telemetry service; gathers and stores metrics from other OpenStack services.
cell Provides logical partitioning of Compute resources in a child and parent relationship. Requests are passed
from parent cells to child cells if the parent cannot provide the requested resource.
cell forwarding A Compute option that enables parent cells to pass resource requests to child cells if the parent
cannot provide the requested resource.
cell manager The Compute component that contains a list of the current capabilities of each host within the
cell and routes requests as appropriate.
Appendix 59
Virtual Machine Image Guide (Release Version: 15.0.0)
CentOS A Linux distribution that is compatible with OpenStack.
Ceph Massively scalable distributed storage system that consists of an object store, block store, and POSIXcompatible
distributed file system. Compatible with OpenStack.
CephFS The POSIX-compliant file system provided by Ceph.
certificate authority (CA) In cryptography, an entity that issues digital certificates. The digital certificate
certifies the ownership of a public key by the named subject of the certificate. This enables others
(relying parties) to rely upon signatures or assertions made by the private key that corresponds to the
certified public key. In this model of trust relationships, a CA is a trusted third party for both the subject
(owner) of the certificate and the party relying upon the certificate. CAs are characteristic of many public
key infrastructure (PKI) schemes. In OpenStack, a simple certificate authority is provided by Compute
for cloudpipe VPNs and VM image decryption.
Challenge-Handshake Authentication Protocol (CHAP) An iSCSI authentication method supported by
Compute.
chance scheduler A scheduling method used by Compute that randomly chooses an available host from the
pool.
changes since A Compute API parameter that downloads changes to the requested item since your last request,
instead of downloading a new, fresh set of data and comparing it against the old data.
Chef An operating system configuration management tool supporting OpenStack deployments.
child cell If a requested resource such as CPU time, disk storage, or memory is not available in the parent
cell, the request is forwarded to its associated child cells. If the child cell can fulfill the request, it does.
Otherwise, it attempts to pass the request to any of its children.
cinder Codename for Block Storage service.
CirrOS A minimal Linux distribution designed for use as a test image on clouds such as OpenStack.
Cisco neutron plug-in A Networking plug-in for Cisco devices and technologies, including UCS and Nexus.
cloud architect A person who plans, designs, and oversees the creation of clouds.
Cloud Auditing Data Federation (CADF) Cloud Auditing Data Federation (CADF) is a specification for
audit event data. CADF is supported by OpenStack Identity.
cloud computing A model that enables access to a shared pool of configurable computing resources, such as
networks, servers, storage, applications, and services, that can be rapidly provisioned and released with
minimal management effort or service provider interaction.
cloud controller Collection of Compute components that represent the global state of the cloud; talks to services,
such as Identity authentication, Object Storage, and node/storage workers through a queue.
cloud controller node A node that runs network, volume, API, scheduler, and image services. Each service
may be broken out into separate nodes for scalability or availability.
Cloud Data Management Interface (CDMI) SINA standard that defines a RESTful API for managing objects
in the cloud, currently unsupported in OpenStack.
Cloud Infrastructure Management Interface (CIMI) An in-progress specification for cloud management.
Currently unsupported in OpenStack.
cloud-init A package commonly installed in VM images that performs initialization of an instance after boot
using information that it retrieves from the metadata service, such as the SSH public key and user data.
cloudadmin One of the default roles in the Compute RBAC system. Grants complete system access.
60 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
Cloudbase-Init A Windows project providing guest initialization features, similar to cloud-init.
cloudpipe A compute service that creates VPNs on a per-project basis.
cloudpipe image A pre-made VM image that serves as a cloudpipe server. Essentially, OpenVPN running on
Linux.
Clustering service (senlin) The project that implements clustering services and libraries for the management
of groups of homogeneous objects exposed by other OpenStack services.
command filter Lists allowed commands within the Compute rootwrap facility.
Common Internet File System (CIFS) A file sharing protocol. It is a public or open variation of the original
Server Message Block (SMB) protocol developed and used by Microsoft. Like the SMB protocol, CIFS
runs at a higher level and uses the TCP/IP protocol.
Common Libraries (oslo) The project that produces a set of python libraries containing code shared by OpenStack
projects. The APIs provided by these libraries should be high quality, stable, consistent, documented
and generally applicable.
community project A project that is not officially endorsed by the OpenStack Foundation. If the project is
successful enough, it might be elevated to an incubated project and then to a core project, or it might be
merged with the main code trunk.
compression Reducing the size of files by special encoding, the file can be decompressed again to its original
content. OpenStack supports compression at the Linux file system level but does not support compression
for things such as Object Storage objects or Image service VM images.
Compute API (Nova API) The nova-api daemon provides access to nova services. Can communicate with
other APIs, such as the Amazon EC2 API.
compute controller The Compute component that chooses suitable hosts on which to start VM instances.
compute host Physical host dedicated to running compute nodes.
compute node A node that runs the nova-compute daemon that manages VM instances that provide a wide
range of services, such as web applications and analytics.
Compute service (nova) The OpenStack core project that implements services and associated libraries to
provide massively-scalable, on-demand, self-service access to compute resources, including bare metal,
virtual machines, and containers.
compute worker The Compute component that runs on each compute node and manages the VM instance
lifecycle, including run, reboot, terminate, attach/detach volumes, and so on. Provided by the novacompute
daemon.
concatenated object A set of segment objects that Object Storage combines and sends to the client.
conductor In Compute, conductor is the process that proxies database requests from the compute process.
Using conductor improves security because compute nodes do not need direct access to the database.
congress Code name for the Governance service.
consistency window The amount of time it takes for a new Object Storage object to become accessible to all
clients.
console log Contains the output from a Linux VM console in Compute.
container Organizes and stores objects in Object Storage. Similar to the concept of a Linux directory but
cannot be nested. Alternative term for an Image service container format.
Appendix 61
Virtual Machine Image Guide (Release Version: 15.0.0)
container auditor Checks for missing replicas or incorrect objects in specified Object Storage containers
through queries to the SQLite back-end database.
container database A SQLite database that stores Object Storage containers and container metadata. The
container server accesses this database.
container format A wrapper used by the Image service that contains a VM image and its associated metadata,
such as machine state, OS disk size, and so on.
Container Infrastructure Management service (magnum) The project which provides a set of services for
provisioning, scaling, and managing container orchestration engines.
container server An Object Storage server that manages containers.
container service The Object Storage component that provides container services, such as create, delete, list,
and so on.
content delivery network (CDN) A content delivery network is a specialized network that is used to distribute
content to clients, typically located close to the client for increased performance.
controller node Alternative term for a cloud controller node.
core API Depending on context, the core API is either the OpenStack API or the main API of a specific core
project, such as Compute, Networking, Image service, and so on.
core service An official OpenStack service defined as core by DefCore Committee. Currently, consists of
Block Storage service (cinder), Compute service (nova), Identity service (keystone), Image service
(glance), Networking service (neutron), and Object Storage service (swift).
cost Under the Compute distributed scheduler, this is calculated by looking at the capabilities of each host
relative to the flavor of the VM instance being requested.
credentials Data that is only known to or accessible by a user and used to verify that the user is who he says
he is. Credentials are presented to the server during authentication. Examples include a password, secret
key, digital certificate, and fingerprint.
CRL A Certificate Revocation List (CRL) in a PKI model is a list of certificates that have been revoked. End
entities presenting these certificates should not be trusted.
Cross-Origin Resource Sharing (CORS) A mechanism that allows many resources (for example, fonts,
JavaScript) on a web page to be requested from another domain outside the domain from which the
resource originated. In particular, JavaScript’s AJAX calls can use the XMLHttpRequest mechanism.
Crowbar An open source community project by SUSE that aims to provide all necessary services to quickly
deploy and manage clouds.
current workload An element of the Compute capacity cache that is calculated based on the number of build,
snapshot, migrate, and resize operations currently in progress on a given host.
customer Alternative term for project.
customization module A user-created Python module that is loaded by horizon to change the look and feel
of the dashboard.
D
daemon A process that runs in the background and waits for requests. May or may not listen on a TCP or
UDP port. Do not confuse with a worker.
62 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
Dashboard (horizon) OpenStack project which provides an extensible, unified, web-based user interface for
all OpenStack services.
data encryption Both Image service and Compute support encrypted virtual machine (VM) images (but not
instances). In-transit data encryption is supported in OpenStack using technologies such as HTTPS,
SSL, TLS, and SSH. Object Storage does not support object encryption at the application level but may
support storage that uses disk encryption.
Data loss prevention (DLP) software Software programs used to protect sensitive information and prevent
it from leaking outside a network boundary through the detection and denying of the data transportation.
Data Processing service (sahara) OpenStack project that provides a scalable data-processing stack and associated
management interfaces.
data store A database engine supported by the Database service.
database ID A unique ID given to each replica of an Object Storage database.
database replicator An Object Storage component that copies changes in the account, container, and object
databases to other nodes.
Database service (trove) An integrated project that provides scalable and reliable Cloud Database-as-aService
functionality for both relational and non-relational database engines.
deallocate The process of removing the association between a floating IP address and a fixed IP address. Once
this association is removed, the floating IP returns to the address pool.
Debian A Linux distribution that is compatible with OpenStack.
deduplication The process of finding duplicate data at the disk block, file, and/or object level to minimize
storage use—currently unsupported within OpenStack.
default panel The default panel that is displayed when a user accesses the dashboard.
default project New users are assigned to this project if no project is specified when a user is created.
default token An Identity service token that is not associated with a specific project and is exchanged for a
scoped token.
delayed delete An option within Image service so that an image is deleted after a predefined number of seconds
instead of immediately.
delivery mode Setting for the Compute RabbitMQ message delivery mode; can be set to either transient or
persistent.
denial of service (DoS) Denial of service (DoS) is a short form for denial-of-service attack. This is a malicious
attempt to prevent legitimate users from using a service.
deprecated auth An option within Compute that enables administrators to create and manage users through
the nova-manage command as opposed to using the Identity service.
designate Code name for the DNS service.
Desktop-as-a-Service A platform that provides a suite of desktop environments that users access to receive
a desktop experience from any location. This may provide general use, development, or even homogeneous
testing environments.
developer One of the default roles in the Compute RBAC system and the default role assigned to a new user.
device ID Maps Object Storage partitions to physical storage devices.
Appendix 63
Virtual Machine Image Guide (Release Version: 15.0.0)
device weight Distributes partitions proportionately across Object Storage devices based on the storage capacity
of each device.
DevStack Community project that uses shell scripts to quickly build complete OpenStack development environments.
DHCP agent OpenStack Networking agent that provides DHCP services for virtual networks.
Diablo A grouped release of projects related to OpenStack that came out in the fall of 2011, the fourth release
of OpenStack. It included Compute (nova 2011.3), Object Storage (swift 1.4.3), and the Image service
(glance). Diablo is the code name for the fourth release of OpenStack. The design summit took place in
the Bay Area near Santa Clara, California, US and Diablo is a nearby city.
direct consumer An element of the Compute RabbitMQ that comes to life when a RPC call is executed. It
connects to a direct exchange through a unique exclusive queue, sends the message, and terminates.
direct exchange A routing table that is created within the Compute RabbitMQ during RPC calls; one is created
for each RPC call that is invoked.
direct publisher Element of RabbitMQ that provides a response to an incoming MQ message.
disassociate The process of removing the association between a floating IP address and fixed IP and thus
returning the floating IP address to the address pool.
Discretionary Access Control (DAC) Governs the ability of subjects to access objects, while enabling users
to make policy decisions and assign security attributes. The traditional UNIX system of users, groups,
and read-write-execute permissions is an example of DAC.
disk encryption The ability to encrypt data at the file system, disk partition, or whole-disk level. Supported
within Compute VMs.
disk format The underlying format that a disk image for a VM is stored as within the Image service back-end
store. For example, AMI, ISO, QCOW2, VMDK, and so on.
dispersion In Object Storage, tools to test and ensure dispersion of objects and containers to ensure fault
tolerance.
distributed virtual router (DVR) Mechanism for highly available multi-host routing when using OpenStack
Networking (neutron).
Django A web framework used extensively in horizon.
DNS record A record that specifies information about a particular domain and belongs to the domain.
DNS service (designate) OpenStack project that provides scalable, on demand, self service access to authoritative
DNS services, in a technology-agnostic manner.
dnsmasq Daemon that provides DNS, DHCP, BOOTP, and TFTP services for virtual networks.
domain An Identity API v3 entity. Represents a collection of projects, groups and users that defines administrative
boundaries for managing OpenStack Identity entities. On the Internet, separates a website from
other sites. Often, the domain name has two or more parts that are separated by dots. For example,
yahoo.com, usa.gov, harvard.edu, or mail.yahoo.com. Also, a domain is an entity or container of all
DNS-related information containing one or more records.
Domain Name System (DNS) A system by which Internet domain name-to-address and address-to-name resolutions
are determined. DNS helps navigate the Internet by translating the IP address into an address
that is easier to remember. For example, translating 111.111.111.1 into www.yahoo.com. All domains
and their components, such as mail servers, utilize DNS to resolve to the appropriate locations. DNS
servers are usually set up in a master-slave relationship such that failure of the master invokes the slave.
64 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
DNS servers might also be clustered or replicated such that changes made to one DNS server are automatically
propagated to other active servers. In Compute, the support that enables associating DNS
entries with floating IP addresses, nodes, or cells so that hostnames are consistent across reboots.
download The transfer of data, usually in the form of files, from one computer to another.
durable exchange The Compute RabbitMQ message exchange that remains active when the server restarts.
durable queue A Compute RabbitMQ message queue that remains active when the server restarts.
Dynamic Host Configuration Protocol (DHCP) A network protocol that configures devices that are connected
to a network so that they can communicate on that network by using the Internet Protocol (IP).
The protocol is implemented in a client-server model where DHCP clients request configuration data,
such as an IP address, a default route, and one or more DNS server addresses from a DHCP server. A
method to automatically configure networking for a host at boot time. Provided by both Networking and
Compute.
Dynamic HyperText Markup Language (DHTML) Pages that use HTML, JavaScript, and Cascading Style
Sheets to enable users to interact with a web page or show simple animation.
E
east-west traffic Network traffic between servers in the same cloud or data center. See also north-south traffic.
EBS boot volume An Amazon EBS storage volume that contains a bootable VM image, currently unsupported
in OpenStack.
ebtables Filtering tool for a Linux bridging firewall, enabling filtering of network traffic passing through
a Linux bridge. Used in Compute along with arptables, iptables, and ip6tables to ensure isolation of
network communications.
EC2 The Amazon commercial compute product, similar to Compute.
EC2 access key Used along with an EC2 secret key to access the Compute EC2 API.
EC2 API OpenStack supports accessing the Amazon EC2 API through Compute.
EC2 Compatibility API A Compute component that enables OpenStack to communicate with Amazon EC2.
EC2 secret key Used along with an EC2 access key when communicating with the Compute EC2 API; used
to digitally sign each request.
Elastic Block Storage (EBS) The Amazon commercial block storage product.
encapsulation The practice of placing one packet type within another for the purposes of abstracting or securing
data. Examples include GRE, MPLS, or IPsec.
encryption OpenStack supports encryption technologies such as HTTPS, SSH, SSL, TLS, digital certificates,
and data encryption.
endpoint See API endpoint.
endpoint registry Alternative term for an Identity service catalog.
endpoint template A list of URL and port number endpoints that indicate where a service, such as Object
Storage, Compute, Identity, and so on, can be accessed.
entity Any piece of hardware or software that wants to connect to the network services provided by Networking,
the network connectivity service. An entity can make use of Networking by implementing a
VIF.
Appendix 65
Virtual Machine Image Guide (Release Version: 15.0.0)
ephemeral image A VM image that does not save changes made to its volumes and reverts them to their
original state after the instance is terminated.
ephemeral volume Volume that does not save the changes made to it and reverts to its original state when the
current user relinquishes control.
Essex A grouped release of projects related to OpenStack that came out in April 2012, the fifth release of
OpenStack. It included Compute (nova 2012.1), Object Storage (swift 1.4.8), Image (glance), Identity
(keystone), and Dashboard (horizon). Essex is the code name for the fifth release of OpenStack. The
design summit took place in Boston, Massachusetts, US and Essex is a nearby city.
ESXi An OpenStack-supported hypervisor.
ETag MD5 hash of an object within Object Storage, used to ensure data integrity.
euca2ools A collection of command-line tools for administering VMs; most are compatible with OpenStack.
Eucalyptus Kernel Image (EKI) Used along with an ERI to create an EMI.
Eucalyptus Machine Image (EMI) VM image container format supported by Image service.
Eucalyptus Ramdisk Image (ERI) Used along with an EKI to create an EMI.
evacuate The process of migrating one or all virtual machine (VM) instances from one host to another, compatible
with both shared storage live migration and block migration.
exchange Alternative term for a RabbitMQ message exchange.
exchange type A routing algorithm in the Compute RabbitMQ.
exclusive queue Connected to by a direct consumer in RabbitMQ—Compute, the message can be consumed
only by the current connection.
extended attributes (xattr) File system option that enables storage of additional information beyond owner,
group, permissions, modification time, and so on. The underlying Object Storage file system must support
extended attributes.
extension Alternative term for an API extension or plug-in. In the context of Identity service, this is a call that
is specific to the implementation, such as adding support for OpenID.
external network A network segment typically used for instance Internet access.
extra specs Specifies additional requirements when Compute determines where to start a new instance. Examples
include a minimum amount of network bandwidth or a GPU.
F
FakeLDAP An easy method to create a local LDAP directory for testing Identity and Compute. Requires
Redis.
fan-out exchange Within RabbitMQ and Compute, it is the messaging interface that is used by the scheduler
service to receive capability messages from the compute, volume, and network nodes.
federated identity A method to establish trusts between identity providers and the OpenStack cloud.
Fedora A Linux distribution compatible with OpenStack.
Fibre Channel Storage protocol similar in concept to TCP/IP; encapsulates SCSI commands and data.
Fibre Channel over Ethernet (FCoE) The fibre channel protocol tunneled within Ethernet.
66 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
fill-first scheduler The Compute scheduling method that attempts to fill a host with VMs rather than starting
new VMs on a variety of hosts.
filter The step in the Compute scheduling process when hosts that cannot run VMs are eliminated and not
chosen.
firewall Used to restrict communications between hosts and/or nodes, implemented in Compute using iptables,
arptables, ip6tables, and ebtables.
FireWall-as-a-Service (FWaaS) A Networking extension that provides perimeter firewall functionality.
fixed IP address An IP address that is associated with the same instance each time that instance boots, is
generally not accessible to end users or the public Internet, and is used for management of the instance.
Flat Manager The Compute component that gives IP addresses to authorized nodes and assumes DHCP, DNS,
and routing configuration and services are provided by something else.
flat mode injection A Compute networking method where the OS network configuration information is injected
into the VM image before the instance starts.
flat network Virtual network type that uses neither VLANs nor tunnels to segregate project traffic. Each
flat network typically requires a separate underlying physical interface defined by bridge mappings.
However, a flat network can contain multiple subnets.
FlatDHCP Manager The Compute component that provides dnsmasq (DHCP, DNS, BOOTP, TFTP) and
radvd (routing) services.
flavor Alternative term for a VM instance type.
flavor ID UUID for each Compute or Image service VM flavor or instance type.
floating IP address An IP address that a project can associate with a VM so that the instance has the same
public IP address each time that it boots. You create a pool of floating IP addresses and assign them to
instances as they are launched to maintain a consistent IP address for maintaining DNS assignment.
Folsom A grouped release of projects related to OpenStack that came out in the fall of 2012, the sixth release
of OpenStack. It includes Compute (nova), Object Storage (swift), Identity (keystone), Networking
(neutron), Image service (glance), and Volumes or Block Storage (cinder). Folsom is the code name
for the sixth release of OpenStack. The design summit took place in San Francisco, California, US and
Folsom is a nearby city.
FormPost Object Storage middleware that uploads (posts) an image through a form on a web page.
freezer Code name for the Backup, Restore, and Disaster Recovery service.
front end The point where a user interacts with a service; can be an API endpoint, the dashboard, or a
command-line tool.
G
gateway An IP address, typically assigned to a router, that passes network traffic between different networks.
generic receive offload (GRO) Feature of certain network interface drivers that combines many smaller received
packets into a large packet before delivery to the kernel IP stack.
generic routing encapsulation (GRE) Protocol that encapsulates a wide variety of network layer protocols
inside virtual point-to-point links.
glance Codename for the Image service.
Appendix 67
Virtual Machine Image Guide (Release Version: 15.0.0)
glance API server Alternative name for the Image API.
glance registry Alternative term for the Image service image registry.
global endpoint template The Identity service endpoint template that contains services available to all
projects.
GlusterFS A file system designed to aggregate NAS hosts, compatible with OpenStack.
gnocchi Part of the OpenStack Telemetry service; provides an indexer and time-series database.
golden image A method of operating system installation where a finalized disk image is created and then used
by all nodes without modification.
Governance service (congress) The project that provides Governance-as-a-Service across any collection of
cloud services in order to monitor, enforce, and audit policy over dynamic infrastructure.
Graphic Interchange Format (GIF) A type of image file that is commonly used for animated images on web
pages.
Graphics Processing Unit (GPU) Choosing a host based on the existence of a GPU is currently unsupported
in OpenStack.
Green Threads The cooperative threading model used by Python; reduces race conditions and only context
switches when specific library calls are made. Each OpenStack service is its own thread.
Grizzly The code name for the seventh release of OpenStack. The design summit took place in San Diego,
California, US and Grizzly is an element of the state flag of California.
Group An Identity v3 API entity. Represents a collection of users that is owned by a specific domain.
guest OS An operating system instance running under the control of a hypervisor.
H
Hadoop Apache Hadoop is an open source software framework that supports data-intensive distributed applications.
Hadoop Distributed File System (HDFS) A distributed, highly fault-tolerant file system designed to run on
low-cost commodity hardware.
handover An object state in Object Storage where a new replica of the object is automatically created due to
a drive failure.
HAProxy Provides a load balancer for TCP and HTTP-based applications that spreads requests across multiple
servers.
hard reboot A type of reboot where a physical or virtual power button is pressed as opposed to a graceful,
proper shutdown of the operating system.
Havana The code name for the eighth release of OpenStack. The design summit took place in Portland,
Oregon, US and Havana is an unincorporated community in Oregon.
health monitor Determines whether back-end members of a VIP pool can process a request. A pool can
have several health monitors associated with it. When a pool has several monitors associated with it, all
monitors check each member of the pool. All monitors must declare a member to be healthy for it to stay
active.
heat Codename for the Orchestration service.
Heat Orchestration Template (HOT) Heat input in the format native to OpenStack.
68 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
high availability (HA) A high availability system design approach and associated service implementation
ensures that a prearranged level of operational performance will be met during a contractual measurement
period. High availability systems seek to minimize system downtime and data loss.
horizon Codename for the Dashboard.
horizon plug-in A plug-in for the OpenStack Dashboard (horizon).
host A physical computer, not a VM instance (node).
host aggregate A method to further subdivide availability zones into hypervisor pools, a collection of common
hosts.
Host Bus Adapter (HBA) Device plugged into a PCI slot, such as a fibre channel or network card.
hybrid cloud A hybrid cloud is a composition of two or more clouds (private, community or public) that
remain distinct entities but are bound together, offering the benefits of multiple deployment models.
Hybrid cloud can also mean the ability to connect colocation, managed and/or dedicated services with
cloud resources.
Hyper-V One of the hypervisors supported by OpenStack.
hyperlink Any kind of text that contains a link to some other site, commonly found in documents where
clicking on a word or words opens up a different website.
Hypertext Transfer Protocol (HTTP) An application protocol for distributed, collaborative, hypermedia information
systems. It is the foundation of data communication for the World Wide Web. Hypertext is
structured text that uses logical links (hyperlinks) between nodes containing text. HTTP is the protocol
to exchange or transfer hypertext.
Hypertext Transfer Protocol Secure (HTTPS) An encrypted communications protocol for secure communication
over a computer network, with especially wide deployment on the Internet. Technically, it is not a
protocol in and of itself; rather, it is the result of simply layering the Hypertext Transfer Protocol (HTTP)
on top of the TLS or SSL protocol, thus adding the security capabilities of TLS or SSL to standard HTTP
communications. Most OpenStack API endpoints and many inter-component communications support
HTTPS communication.
hypervisor Software that arbitrates and controls VM access to the actual underlying hardware.
hypervisor pool A collection of hypervisors grouped together through host aggregates.
I
Icehouse The code name for the ninth release of OpenStack. The design summit took place in Hong Kong
and Ice House is a street in that city.
ID number Unique numeric ID associated with each user in Identity, conceptually similar to a Linux or LDAP
UID.
Identity API Alternative term for the Identity service API.
Identity back end The source used by Identity service to retrieve user information; an OpenLDAP server, for
example.
identity provider A directory service, which allows users to login with a user name and password. It is a
typical source of authentication tokens.
Identity service (keystone) The project that facilitates API client authentication, service discovery, distributed
multi-project authorization, and auditing. It provides a central directory of users mapped to
Appendix 69
Virtual Machine Image Guide (Release Version: 15.0.0)
the OpenStack services they can access. It also registers endpoints for OpenStack services and acts as a
common authentication system.
Identity service API The API used to access the OpenStack Identity service provided through keystone.
IETF Internet Engineering Task Force (IETF) is an open standards organization that develops Internet standards,
particularly the standards pertaining to TCP/IP.
image A collection of files for a specific operating system (OS) that you use to create or rebuild a server.
OpenStack provides pre-built images. You can also create custom images, or snapshots, from servers
that you have launched. Custom images can be used for data backups or as “gold” images for additional
servers.
Image API The Image service API endpoint for management of VM images. Processes client requests for
VMs, updates Image service metadata on the registry server, and communicates with the store adapter to
upload VM images from the back-end store.
image cache Used by Image service to obtain images on the local host rather than re-downloading them from
the image server each time one is requested.
image ID Combination of a URI and UUID used to access Image service VM images through the image API.
image membership A list of projects that can access a given VM image within Image service.
image owner The project who owns an Image service virtual machine image.
image registry A list of VM images that are available through Image service.
Image service (glance) The OpenStack service that provide services and associated libraries to store, browse,
share, distribute and manage bootable disk images, other data closely associated with initializing compute
resources, and metadata definitions.
image status The current status of a VM image in Image service, not to be confused with the status of a running
instance.
image store The back-end store used by Image service to store VM images, options include Object Storage,
locally mounted file system, RADOS block devices, VMware datastore, or HTTP.
image UUID UUID used by Image service to uniquely identify each VM image.
incubated project A community project may be elevated to this status and is then promoted to a core project.
Infrastructure Optimization service (watcher) OpenStack project that aims to provide a flexible and scalable
resource optimization service for multi-project OpenStack-based clouds.
Infrastructure-as-a-Service (IaaS) IaaS is a provisioning model in which an organization outsources physical
components of a data center, such as storage, hardware, servers, and networking components. A service
provider owns the equipment and is responsible for housing, operating and maintaining it. The client
typically pays on a per-use basis. IaaS is a model for providing cloud services.
ingress filtering The process of filtering incoming network traffic. Supported by Compute.
INI format The OpenStack configuration files use an INI format to describe options and their values. It
consists of sections and key value pairs.
injection The process of putting a file into a virtual machine image before the instance is started.
Input/Output Operations Per Second (IOPS) IOPS are a common performance measurement used to
benchmark computer storage devices like hard disk drives, solid state drives, and storage area networks.
instance A running VM, or a VM in a known state such as suspended, that can be used like a hardware server.
70 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
instance ID Alternative term for instance UUID.
instance state The current state of a guest VM image.
instance tunnels network A network segment used for instance traffic tunnels between compute nodes and
the network node.
instance type Describes the parameters of the various virtual machine images that are available to users;
includes parameters such as CPU, storage, and memory. Alternative term for flavor.
instance type ID Alternative term for a flavor ID.
instance UUID Unique ID assigned to each guest VM instance.
Intelligent Platform Management Interface (IPMI) IPMI is a standardized computer system interface used
by system administrators for out-of-band management of computer systems and monitoring of their operation.
In layman’s terms, it is a way to manage a computer using a direct network connection, whether
it is turned on or not; connecting to the hardware rather than an operating system or login shell.
interface A physical or virtual device that provides connectivity to another device or medium.
interface ID Unique ID for a Networking VIF or vNIC in the form of a UUID.
Internet Control Message Protocol (ICMP) A network protocol used by network devices for control messages.
For example, ping uses ICMP to test connectivity.
Internet protocol (IP) Principal communications protocol in the internet protocol suite for relaying datagrams
across network boundaries.
Internet Service Provider (ISP) Any business that provides Internet access to individuals or businesses.
Internet Small Computer System Interface (iSCSI) Storage protocol that encapsulates SCSI frames for
transport over IP networks. Supported by Compute, Object Storage, and Image service.
IP address Number that is unique to every computer system on the Internet. Two versions of the Internet
Protocol (IP) are in use for addresses: IPv4 and IPv6.
IP Address Management (IPAM) The process of automating IP address allocation, deallocation, and management.
Currently provided by Compute, melange, and Networking.
ip6tables Tool used to set up, maintain, and inspect the tables of IPv6 packet filter rules in the Linux kernel.
In OpenStack Compute, ip6tables is used along with arptables, ebtables, and iptables to create firewalls
for both nodes and VMs.
ipset Extension to iptables that allows creation of firewall rules that match entire “sets” of IP addresses simultaneously.
These sets reside in indexed data structures to increase efficiency, particularly on systems
with a large quantity of rules.
iptables Used along with arptables and ebtables, iptables create firewalls in Compute. iptables are the tables
provided by the Linux kernel firewall (implemented as different Netfilter modules) and the chains and
rules it stores. Different kernel modules and programs are currently used for different protocols: iptables
applies to IPv4, ip6tables to IPv6, arptables to ARP, and ebtables to Ethernet frames. Requires root
privilege to manipulate.
ironic Codename for the Bare Metal service.
iSCSI Qualified Name (IQN) IQN is the format most commonly used for iSCSI names, which uniquely identify
nodes in an iSCSI network. All IQNs follow the pattern iqn.yyyy-mm.domain:identifier, where
‘yyyy-mm’ is the year and month in which the domain was registered, ‘domain’ is the reversed domain
name of the issuing organization, and ‘identifier’ is an optional string which makes each IQN under the
same domain unique. For example, ‘iqn.2015-10.org.openstack.408ae959bce1’.
Appendix 71
Virtual Machine Image Guide (Release Version: 15.0.0)
ISO9660 One of the VM image disk formats supported by Image service.
itsec A default role in the Compute RBAC system that can quarantine an instance in any project.
J
Java A programming language that is used to create systems that involve more than one computer by way of
a network.
JavaScript A scripting language that is used to build web pages.
JavaScript Object Notation (JSON) One of the supported response formats in OpenStack.
jumbo frame Feature in modern Ethernet networks that supports frames up to approximately 9000 bytes.
Juno The code name for the tenth release of OpenStack. The design summit took place in Atlanta, Georgia,
US and Juno is an unincorporated community in Georgia.
K
Kerberos A network authentication protocol which works on the basis of tickets. Kerberos allows nodes
communication over a non-secure network, and allows nodes to prove their identity to one another in a
secure manner.
kernel-based VM (KVM) An OpenStack-supported hypervisor. KVM is a full virtualization solution for
Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V), ARM, IBM Power,
and IBM zSeries. It consists of a loadable kernel module, that provides the core virtualization infrastructure
and a processor specific module.
Key Manager service (barbican) The project that produces a secret storage and generation system capable
of providing key management for services wishing to enable encryption features.
keystone Codename of the Identity service.
Kickstart A tool to automate system configuration and installation on Red Hat, Fedora, and CentOS-based
Linux distributions.
Kilo The code name for the eleventh release of OpenStack. The design summit took place in Paris, France.
Due to delays in the name selection, the release was known only as K. Because k is the unit symbol
for kilo and the kilogram reference artifact is stored near Paris in the Pavillon de Breteuil in Sèvres, the
community chose Kilo as the release name.
L
large object An object within Object Storage that is larger than 5 GB.
Launchpad The collaboration site for OpenStack.
Layer-2 (L2) agent OpenStack Networking agent that provides layer-2 connectivity for virtual networks.
Layer-2 network Term used in the OSI network architecture for the data link layer. The data link layer is
responsible for media access control, flow control and detecting and possibly correcting errors that may
occur in the physical layer.
Layer-3 (L3) agent OpenStack Networking agent that provides layer-3 (routing) services for virtual networks.
72 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
Layer-3 network Term used in the OSI network architecture for the network layer. The network layer is
responsible for packet forwarding including routing from one node to another.
Liberty The code name for the twelfth release of OpenStack. The design summit took place in Vancouver,
Canada and Liberty is the name of a village in the Canadian province of Saskatchewan.
libvirt Virtualization API library used by OpenStack to interact with many of its supported hypervisors.
Lightweight Directory Access Protocol (LDAP) An application protocol for accessing and maintaining distributed
directory information services over an IP network.
Linux Unix-like computer operating system assembled under the model of free and open-source software
development and distribution.
Linux bridge Software that enables multiple VMs to share a single physical NIC within Compute.
Linux Bridge neutron plug-in Enables a Linux bridge to understand a Networking port, interface attachment,
and other abstractions.
Linux containers (LXC) An OpenStack-supported hypervisor.
live migration The ability within Compute to move running virtual machine instances from one host to another
with only a small service interruption during switchover.
load balancer A load balancer is a logical device that belongs to a cloud account. It is used to distribute
workloads between multiple back-end systems or services, based on the criteria defined as part of its
configuration.
load balancing The process of spreading client requests between two or more nodes to improve performance
and availability.
Load-Balancer-as-a-Service (LBaaS) Enables Networking to distribute incoming requests evenly between
designated instances.
Load-balancing service (octavia) The project that aims to provide scalable, on demand, self service access
to load-balancer services, in technology-agnostic manner.
Logical Volume Manager (LVM) Provides a method of allocating space on mass-storage devices that is more
flexible than conventional partitioning schemes.
M
magnum Code name for the Containers Infrastructure Management service.
management API Alternative term for an admin API.
management network A network segment used for administration, not accessible to the public Internet.
manager Logical groupings of related code, such as the Block Storage volume manager or network manager.
manifest Used to track segments of a large object within Object Storage.
manifest object A special Object Storage object that contains the manifest for a large object.
manila Codename for OpenStack Shared File Systems service.
manila-share Responsible for managing Shared File System Service devices, specifically the back-end devices.
maximum transmission unit (MTU) Maximum frame or packet size for a particular network medium. Typically
1500 bytes for Ethernet networks.
Appendix 73
Virtual Machine Image Guide (Release Version: 15.0.0)
mechanism driver A driver for the Modular Layer 2 (ML2) neutron plug-in that provides layer-2 connectivity
for virtual instances. A single OpenStack installation can use multiple mechanism drivers.
melange Project name for OpenStack Network Information Service. To be merged with Networking.
membership The association between an Image service VM image and a project. Enables images to be shared
with specified projects.
membership list A list of projects that can access a given VM image within Image service.
memcached A distributed memory object caching system that is used by Object Storage for caching.
memory overcommit The ability to start new VM instances based on the actual memory usage of a host, as
opposed to basing the decision on the amount of RAM each running instance thinks it has available. Also
known as RAM overcommit.
message broker The software package used to provide AMQP messaging capabilities within Compute. Default
package is RabbitMQ.
message bus The main virtual communication line used by all AMQP messages for inter-cloud communications
within Compute.
message queue Passes requests from clients to the appropriate workers and returns the output to the client
after the job completes.
Message service (zaqar) The project that provides a messaging service that affords a variety of distributed
application patterns in an efficient, scalable and highly available manner, and to create and maintain
associated Python libraries and documentation.
Meta-Data Server (MDS) Stores CephFS metadata.
Metadata agent OpenStack Networking agent that provides metadata services for instances.
migration The process of moving a VM instance from one host to another.
mistral Code name for Workflow service.
Mitaka The code name for the thirteenth release of OpenStack. The design summit took place in Tokyo,
Japan. Mitaka is a city in Tokyo.
Modular Layer 2 (ML2) neutron plug-in Can concurrently use multiple layer-2 networking technologies,
such as 802.1Q and VXLAN, in Networking.
monasca Codename for OpenStack Monitoring.
Monitor (LBaaS) LBaaS feature that provides availability monitoring using the ping command, TCP, and
HTTP/HTTPS GET.
Monitor (Mon) A Ceph component that communicates with external clients, checks data state and consistency,
and performs quorum functions.
Monitoring (monasca) The OpenStack service that provides a multi-project, highly scalable, performant,
fault-tolerant monitoring-as-a-service solution for metrics, complex event processing and logging. To
build an extensible platform for advanced monitoring services that can be used by both operators and
projects to gain operational insight and visibility, ensuring availability and stability.
multi-factor authentication Authentication method that uses two or more credentials, such as a password
and a private key. Currently not supported in Identity.
multi-host High-availability mode for legacy (nova) networking. Each compute node handles NAT and DHCP
and acts as a gateway for all of the VMs on it. A networking failure on one compute node doesn’t affect
VMs on other compute nodes.
74 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
multinic Facility in Compute that allows each virtual machine instance to have more than one VIF connected
to it.
murano Codename for the Application Catalog service.
N
Nebula Released as open source by NASA in 2010 and is the basis for Compute.
netadmin One of the default roles in the Compute RBAC system. Enables the user to allocate publicly accessible
IP addresses to instances and change firewall rules.
NetApp volume driver Enables Compute to communicate with NetApp storage devices through the NetApp
OnCommand Provisioning Manager.
network A virtual network that provides connectivity between entities. For example, a collection of virtual
ports that share network connectivity. In Networking terminology, a network is always a layer-2 network.
Network Address Translation (NAT) Process of modifying IP address information while in transit. Supported
by Compute and Networking.
network controller A Compute daemon that orchestrates the network configuration of nodes, including IP
addresses, VLANs, and bridging. Also manages routing for both public and private networks.
Network File System (NFS) A method for making file systems available over the network. Supported by
OpenStack.
network ID Unique ID assigned to each network segment within Networking. Same as network UUID.
network manager The Compute component that manages various network components, such as firewall rules,
IP address allocation, and so on.
network namespace Linux kernel feature that provides independent virtual networking instances on a single
host with separate routing tables and interfaces. Similar to virtual routing and forwarding (VRF) services
on physical network equipment.
network node Any compute node that runs the network worker daemon.
network segment Represents a virtual, isolated OSI layer-2 subnet in Networking.
Network Service Header (NSH) Provides a mechanism for metadata exchange along the instantiated service
path.
Network Time Protocol (NTP) Method of keeping a clock for a host or node correct via communication with
a trusted, accurate time source.
network UUID Unique ID for a Networking network segment.
network worker The nova-network worker daemon; provides services such as giving an IP address to a
booting nova instance.
Networking API (Neutron API) API used to access OpenStack Networking. Provides an extensible architecture
to enable custom plug-in creation.
Networking service (neutron) The OpenStack project which implements services and associated libraries to
provide on-demand, scalable, and technology-agnostic network abstraction.
neutron Codename for OpenStack Networking service.
neutron API An alternative name for Networking API.
Appendix 75
Virtual Machine Image Guide (Release Version: 15.0.0)
neutron manager Enables Compute and Networking integration, which enables Networking to perform network
management for guest VMs.
neutron plug-in Interface within Networking that enables organizations to create custom plug-ins for advanced
features, such as QoS, ACLs, or IDS.
Newton The code name for the fourteenth release of OpenStack. The design summit took place in Austin,
Texas, US. The release is named after “Newton House” which is located at 1013 E. Ninth St., Austin,
TX. which is listed on the National Register of Historic Places.
Nexenta volume driver Provides support for NexentaStor devices in Compute.
NFV Orchestration Service (tacker) OpenStack service that aims to implement Network Function Virtualization
(NFV) orchestration services and libraries for end-to-end life-cycle management of network
services and Virtual Network Functions (VNFs).
Nginx An HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server.
No ACK Disables server-side message acknowledgment in the Compute RabbitMQ. Increases performance
but decreases reliability.
node A VM instance that runs on a host.
non-durable exchange Message exchange that is cleared when the service restarts. Its data is not written to
persistent storage.
non-durable queue Message queue that is cleared when the service restarts. Its data is not written to persistent
storage.
non-persistent volume Alternative term for an ephemeral volume.
north-south traffic Network traffic between a user or client (north) and a server (south), or traffic into the
cloud (south) and out of the cloud (north). See also east-west traffic.
nova Codename for OpenStack Compute service.
Nova API Alternative term for the Compute API.
nova-network A Compute component that manages IP address allocation, firewalls, and other network-related
tasks. This is the legacy networking option and an alternative to Networking.
O
object A BLOB of data held by Object Storage; can be in any format.
object auditor Opens all objects for an object server and verifies the MD5 hash, size, and metadata for each
object.
object expiration A configurable option within Object Storage to automatically delete objects after a specified
amount of time has passed or a certain date is reached.
object hash Unique ID for an Object Storage object.
object path hash Used by Object Storage to determine the location of an object in the ring. Maps objects to
partitions.
object replicator An Object Storage component that copies an object to remote partitions for fault tolerance.
object server An Object Storage component that is responsible for managing objects.
Object Storage API API used to access OpenStack Object Storage.
76 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
Object Storage Device (OSD) The Ceph storage daemon.
Object Storage service (swift) The OpenStack core project that provides eventually consistent and redundant
storage and retrieval of fixed digital content.
object versioning Allows a user to set a flag on an Object Storage container so that all objects within the
container are versioned.
Ocata The code name for the fifteenth release of OpenStack. The design summit will take place in Barcelona,
Spain. Ocata is a beach north of Barcelona.
Octavia Code name for the Load-balancing service.
Oldie Term for an Object Storage process that runs for a long time. Can indicate a hung process.
Open Cloud Computing Interface (OCCI) A standardized interface for managing compute, data, and network
resources, currently unsupported in OpenStack.
Open Virtualization Format (OVF) Standard for packaging VM images. Supported in OpenStack.
Open vSwitch Open vSwitch is a production quality, multilayer virtual switch licensed under the open source
Apache 2.0 license. It is designed to enable massive network automation through programmatic extension,
while still supporting standard management interfaces and protocols (for example NetFlow, sFlow,
SPAN, RSPAN, CLI, LACP, 802.1ag).
Open vSwitch (OVS) agent Provides an interface to the underlying Open vSwitch service for the Networking
plug-in.
Open vSwitch neutron plug-in Provides support for Open vSwitch in Networking.
OpenLDAP An open source LDAP server. Supported by both Compute and Identity.
OpenStack OpenStack is a cloud operating system that controls large pools of compute, storage, and networking
resources throughout a data center, all managed through a dashboard that gives administrators
control while empowering their users to provision resources through a web interface. OpenStack is an
open source project licensed under the Apache License 2.0.
OpenStack code name Each OpenStack release has a code name. Code names ascend in alphabetical order:
Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, Icehouse, Juno, Kilo, Liberty, Mitaka,
Newton, Ocata, Pike, Queens, and Rocky. Code names are cities or counties near where the corresponding
OpenStack design summit took place. An exception, called the Waldon exception, is granted to
elements of the state flag that sound especially cool. Code names are chosen by popular vote.
openSUSE A Linux distribution that is compatible with OpenStack.
operator The person responsible for planning and maintaining an OpenStack installation.
optional service An official OpenStack service defined as optional by DefCore Committee. Currently, consists
of Dashboard (horizon), Telemetry service (Telemetry), Orchestration service (heat), Database service
(trove), Bare Metal service (ironic), and so on.
Orchestration service (heat) The OpenStack service which orchestrates composite cloud applications using
a declarative template format through an OpenStack-native REST API.
orphan In the context of Object Storage, this is a process that is not terminated after an upgrade, restart, or
reload of the service.
Oslo Codename for the Common Libraries project.
Appendix 77
Virtual Machine Image Guide (Release Version: 15.0.0)
P
panko Part of the OpenStack Telemetry service; provides event storage.
parent cell If a requested resource, such as CPU time, disk storage, or memory, is not available in the parent
cell, the request is forwarded to associated child cells.
partition A unit of storage within Object Storage used to store objects. It exists on top of devices and is
replicated for fault tolerance.
partition index Contains the locations of all Object Storage partitions within the ring.
partition shift value Used by Object Storage to determine which partition data should reside on.
path MTU discovery (PMTUD) Mechanism in IP networks to detect end-to-end MTU and adjust packet size
accordingly.
pause A VM state where no changes occur (no changes in memory, network communications stop, etc); the
VM is frozen but not shut down.
PCI passthrough Gives guest VMs exclusive access to a PCI device. Currently supported in OpenStack
Havana and later releases.
persistent message A message that is stored both in memory and on disk. The message is not lost after a
failure or restart.
persistent volume Changes to these types of disk volumes are saved.
personality file A file used to customize a Compute instance. It can be used to inject SSH keys or a specific
network configuration.
Pike The code name for the sixteenth release of OpenStack. The design summit will take place in Boston,
Massachusetts, US. The release is named after the Massachusetts Turnpike, abbreviated commonly as
the Mass Pike, which is the easternmost stretch of Interstate 90.
Platform-as-a-Service (PaaS) Provides to the consumer the ability to deploy applications through a programming
language or tools supported by the cloud platform provider. An example of Platform-as-a-Service
is an Eclipse/Java programming platform provided with no downloads required.
plug-in Software component providing the actual implementation for Networking APIs, or for Compute APIs,
depending on the context.
policy service Component of Identity that provides a rule-management interface and a rule-based authorization
engine.
policy-based routing (PBR) Provides a mechanism to implement packet forwarding and routing according
to the policies defined by the network administrator.
pool A logical set of devices, such as web servers, that you group together to receive and process traffic.
The load balancing function chooses which member of the pool handles the new requests or connections
received on the VIP address. Each VIP has one pool.
pool member An application that runs on the back-end server in a load-balancing system.
port A virtual network port within Networking; VIFs / vNICs are connected to a port.
port UUID Unique ID for a Networking port.
preseed A tool to automate system configuration and installation on Debian-based Linux distributions.
private image An Image service VM image that is only available to specified projects.
78 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
private IP address An IP address used for management and administration, not available to the public Internet.
private network The Network Controller provides virtual networks to enable compute servers to interact
with each other and with the public network. All machines must have a public and private network
interface. A private network interface can be a flat or VLAN network interface. A flat network interface
is controlled by the flat_interface with flat managers. A VLAN network interface is controlled by the
vlan_interface option with VLAN managers.
project Projects represent the base unit of “ownership” in OpenStack, in that all resources in OpenStack should
be owned by a specific project. In OpenStack Identity, a project must be owned by a specific domain.
project ID Unique ID assigned to each project by the Identity service.
project VPN Alternative term for a cloudpipe.
promiscuous mode Causes the network interface to pass all traffic it receives to the host rather than passing
only the frames addressed to it.
protected property Generally, extra properties on an Image service image to which only cloud administrators
have access. Limits which user roles can perform CRUD operations on that property. The cloud
administrator can configure any image property as protected.
provider An administrator who has access to all hosts and instances.
proxy node A node that provides the Object Storage proxy service.
proxy server Users of Object Storage interact with the service through the proxy server, which in turn looks
up the location of the requested data within the ring and returns the results to the user.
public API An API endpoint used for both service-to-service communication and end-user interactions.
public image An Image service VM image that is available to all projects.
public IP address An IP address that is accessible to end-users.
public key authentication Authentication method that uses keys rather than passwords.
public network The Network Controller provides virtual networks to enable compute servers to interact with
each other and with the public network. All machines must have a public and private network interface.
The public network interface is controlled by the public_interface option.
Puppet An operating system configuration-management tool supported by OpenStack.
Python Programming language used extensively in OpenStack.
Q
QEMU Copy On Write 2 (QCOW2) One of the VM image disk formats supported by Image service.
Qpid Message queue software supported by OpenStack; an alternative to RabbitMQ.
Quality of Service (QoS) The ability to guarantee certain network or storage requirements to satisfy a Service
Level Agreement (SLA) between an application provider and end users. Typically includes performance
requirements like networking bandwidth, latency, jitter correction, and reliability as well as storage performance
in Input/Output Operations Per Second (IOPS), throttling agreements, and performance expectations
at peak load.
quarantine If Object Storage finds objects, containers, or accounts that are corrupt, they are placed in this
state, are not replicated, cannot be read by clients, and a correct copy is re-replicated.
Appendix 79
Virtual Machine Image Guide (Release Version: 15.0.0)
Queens The code name for the seventeenth release of OpenStack. The design summit will take place in
Sydney, Australia. The release is named after the Queens Pound river in the South Coast region of New
South Wales.
Quick EMUlator (QEMU) QEMU is a generic and open source machine emulator and virtualizer. One of
the hypervisors supported by OpenStack, generally used for development purposes.
quota In Compute and Block Storage, the ability to set resource limits on a per-project basis.
R
RabbitMQ The default message queue software used by OpenStack.
Rackspace Cloud Files Released as open source by Rackspace in 2010; the basis for Object Storage.
RADOS Block Device (RBD) Ceph component that enables a Linux block device to be striped over multiple
distributed data stores.
radvd The router advertisement daemon, used by the Compute VLAN manager and FlatDHCP manager to
provide routing services for VM instances.
rally Codename for the Benchmark service.
RAM filter The Compute setting that enables or disables RAM overcommitment.
RAM overcommit The ability to start new VM instances based on the actual memory usage of a host, as
opposed to basing the decision on the amount of RAM each running instance thinks it has available.
Also known as memory overcommit.
rate limit Configurable option within Object Storage to limit database writes on a per-account and/or percontainer
basis.
raw One of the VM image disk formats supported by Image service; an unstructured disk image.
rebalance The process of distributing Object Storage partitions across all drives in the ring; used during initial
ring creation and after ring reconfiguration.
reboot Either a soft or hard reboot of a server. With a soft reboot, the operating system is signaled to restart,
which enables a graceful shutdown of all processes. A hard reboot is the equivalent of power cycling
the server. The virtualization platform should ensure that the reboot action has completed successfully,
even in cases in which the underlying domain/VM is paused or halted/stopped.
rebuild Removes all data on the server and replaces it with the specified image. Server ID and IP addresses
remain the same.
Recon An Object Storage component that collects meters.
record Belongs to a particular domain and is used to specify information about the domain. There are several
types of DNS records. Each record type contains particular information used to describe the purpose of
that record. Examples include mail exchange (MX) records, which specify the mail server for a particular
domain; and name server (NS) records, which specify the authoritative name servers for a domain.
record ID A number within a database that is incremented each time a change is made. Used by Object
Storage when replicating.
Red Hat Enterprise Linux (RHEL) A Linux distribution that is compatible with OpenStack.
reference architecture A recommended architecture for an OpenStack cloud.
80 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
region A discrete OpenStack environment with dedicated API endpoints that typically shares only the Identity
(keystone) with other regions.
registry Alternative term for the Image service registry.
registry server An Image service that provides VM image metadata information to clients.
Reliable, Autonomic Distributed Object Store (RADOS)
A collection of components that provides object storage within Ceph. Similar to OpenStack Object
Storage.
Remote Procedure Call (RPC) The method used by the Compute RabbitMQ for intra-service communications.
replica Provides data redundancy and fault tolerance by creating copies of Object Storage objects, accounts,
and containers so that they are not lost when the underlying storage fails.
replica count The number of replicas of the data in an Object Storage ring.
replication The process of copying data to a separate physical device for fault tolerance and performance.
replicator The Object Storage back-end process that creates and manages object replicas.
request ID Unique ID assigned to each request sent to Compute.
rescue image A special type of VM image that is booted when an instance is placed into rescue mode. Allows
an administrator to mount the file systems for an instance to correct the problem.
resize Converts an existing server to a different flavor, which scales the server up or down. The original server
is saved to enable rollback if a problem occurs. All resizes must be tested and explicitly confirmed, at
which time the original server is removed.
RESTful A kind of web service API that uses REST, or Representational State Transfer. REST is the style of
architecture for hypermedia systems that is used for the World Wide Web.
ring An entity that maps Object Storage data to partitions. A separate ring exists for each service, such as
account, object, and container.
ring builder Builds and manages rings within Object Storage, assigns partitions to devices, and pushes the
configuration to other storage nodes.
Rocky The code name for the eightteenth release of OpenStack. The design summit will take place in Vancouver,
Kanada. The release is named after the Rocky Mountains.
role A personality that a user assumes to perform a specific set of operations. A role includes a set of rights
and privileges. A user assuming that role inherits those rights and privileges.
Role Based Access Control (RBAC) Provides a predefined list of actions that the user can perform, such
as start or stop VMs, reset passwords, and so on. Supported in both Identity and Compute and can be
configured using the dashboard.
role ID Alphanumeric ID assigned to each Identity service role.
Root Cause Analysis (RCA) service (Vitrage) OpenStack project that aims to organize, analyze and visualize
OpenStack alarms and events, yield insights regarding the root cause of problems and deduce their
existence before they are directly detected.
rootwrap A feature of Compute that allows the unprivileged “nova” user to run a specified list of commands
as the Linux root user.
round-robin scheduler Type of Compute scheduler that evenly distributes instances among available hosts.
Appendix 81
Virtual Machine Image Guide (Release Version: 15.0.0)
router A physical or virtual network device that passes network traffic between different networks.
routing key The Compute direct exchanges, fanout exchanges, and topic exchanges use this key to determine
how to process a message; processing varies depending on exchange type.
RPC driver Modular system that allows the underlying message queue software of Compute to be changed.
For example, from RabbitMQ to ZeroMQ or Qpid.
rsync Used by Object Storage to push object replicas.
RXTX cap Absolute limit on the amount of network traffic a Compute VM instance can send and receive.
RXTX quota Soft limit on the amount of network traffic a Compute VM instance can send and receive.
S
sahara Codename for the Data Processing service.
SAML assertion Contains information about a user as provided by the identity provider. It is an indication
that a user has been authenticated.
scheduler manager A Compute component that determines where VM instances should start. Uses modular
design to support a variety of scheduler types.
scoped token An Identity service API access token that is associated with a specific project.
scrubber Checks for and deletes unused VMs; the component of Image service that implements delayed
delete.
secret key String of text known only by the user; used along with an access key to make requests to the
Compute API.
secure boot Process whereby the system firmware validates the authenticity of the code involved in the boot
process.
secure shell (SSH) Open source tool used to access remote hosts through an encrypted communications channel,
SSH key injection is supported by Compute.
security group A set of network traffic filtering rules that are applied to a Compute instance.
segmented object An Object Storage large object that has been broken up into pieces. The re-assembled
object is called a concatenated object.
self-service For IaaS, ability for a regular (non-privileged) account to manage a virtual infrastructure component
such as networks without involving an administrator.
SELinux Linux kernel security module that provides the mechanism for supporting access control policies.
senlin Code name for the Clustering service.
server Computer that provides explicit services to the client software running on that system, often managing
a variety of computer operations. A server is a VM instance in the Compute system. Flavor and image
are requisite elements when creating a server.
server image Alternative term for a VM image.
server UUID Unique ID assigned to each guest VM instance.
service An OpenStack service, such as Compute, Object Storage, or Image service. Provides one or more
endpoints through which users can access resources and perform operations.
service catalog Alternative term for the Identity service catalog.
82 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
Service Function Chain (SFC) For a given service, SFC is the abstracted view of the required service functions
and the order in which they are to be applied.
service ID Unique ID assigned to each service that is available in the Identity service catalog.
Service Level Agreement (SLA) Contractual obligations that ensure the availability of a service.
service project Special project that contains all services that are listed in the catalog.
service provider A system that provides services to other system entities. In case of federated identity, OpenStack
Identity is the service provider.
service registration An Identity service feature that enables services, such as Compute, to automatically register
with the catalog.
service token An administrator-defined token used by Compute to communicate securely with the Identity
service.
session back end The method of storage used by horizon to track client sessions, such as local memory, cookies,
a database, or memcached.
session persistence A feature of the load-balancing service. It attempts to force subsequent connections to a
service to be redirected to the same node as long as it is online.
session storage A horizon component that stores and tracks client session information. Implemented through
the Django sessions framework.
share A remote, mountable file system in the context of the Shared File Systems service. You can mount a
share to, and access a share from, several hosts by several users at a time.
share network An entity in the context of the Shared File Systems service that encapsulates interaction with
the Networking service. If the driver you selected runs in the mode requiring such kind of interaction,
you need to specify the share network to create a share.
Shared File Systems API A Shared File Systems service that provides a stable RESTful API. The service authenticates
and routes requests throughout the Shared File Systems service. There is python-manilaclient
to interact with the API.
Shared File Systems service (manila) The service that provides a set of services for management of shared
file systems in a multi-project cloud environment, similar to how OpenStack provides block-based storage
management through the OpenStack Block Storage service project. With the Shared File Systems
service, you can create a remote file system and mount the file system on your instances. You can also
read and write data from your instances to and from your file system.
shared IP address An IP address that can be assigned to a VM instance within the shared IP group. Public IP
addresses can be shared across multiple servers for use in various high-availability scenarios. When an
IP address is shared to another server, the cloud network restrictions are modified to enable each server
to listen to and respond on that IP address. You can optionally specify that the target server network
configuration be modified. Shared IP addresses can be used with many standard heartbeat facilities,
such as keepalive, that monitor for failure and manage IP failover.
shared IP group A collection of servers that can share IPs with other members of the group. Any server in a
group can share one or more public IPs with any other server in the group. With the exception of the first
server in a shared IP group, servers must be launched into shared IP groups. A server may be a member
of only one shared IP group.
shared storage Block storage that is simultaneously accessible by multiple clients, for example, NFS.
Sheepdog Distributed block storage system for QEMU, supported by OpenStack.
Appendix 83
Virtual Machine Image Guide (Release Version: 15.0.0)
Simple Cloud Identity Management (SCIM) Specification for managing identity in the cloud, currently unsupported
by OpenStack.
Simple Protocol for Independent Computing Environments (SPICE) SPICE provides remote desktop access
to guest virtual machines. It is an alternative to VNC. SPICE is supported by OpenStack.
Single-root I/O Virtualization (SR-IOV) A specification that, when implemented by a physical PCIe device,
enables it to appear as multiple separate PCIe devices. This enables multiple virtualized guests to share
direct access to the physical device, offering improved performance over an equivalent virtual device.
Currently supported in OpenStack Havana and later releases.
SmokeStack Runs automated tests against the core OpenStack API; written in Rails.
snapshot A point-in-time copy of an OpenStack storage volume or image. Use storage volume snapshots to
back up volumes. Use image snapshots to back up data, or as “gold” images for additional servers.
soft reboot A controlled reboot where a VM instance is properly restarted through operating system commands.
Software Development Lifecycle Automation service (solum) OpenStack project that aims to make cloud
services easier to consume and integrate with application development process by automating the sourceto-image
process, and simplifying app-centric deployment.
Software-defined networking (SDN) Provides an approach for network administrators to manage computer
network services through abstraction of lower-level functionality.
SolidFire Volume Driver The Block Storage driver for the SolidFire iSCSI storage appliance.
solum Code name for the Software Development Lifecycle Automation service.
spread-first scheduler The Compute VM scheduling algorithm that attempts to start a new VM on the host
with the least amount of load.
SQLAlchemy An open source SQL toolkit for Python, used in OpenStack.
SQLite A lightweight SQL database, used as the default persistent storage method in many OpenStack services.
stack A set of OpenStack resources created and managed by the Orchestration service according to a given
template (either an AWS CloudFormation template or a Heat Orchestration Template (HOT)).
StackTach Community project that captures Compute AMQP communications; useful for debugging.
static IP address Alternative term for a fixed IP address.
StaticWeb WSGI middleware component of Object Storage that serves container data as a static web page.
storage back end The method that a service uses for persistent storage, such as iSCSI, NFS, or local disk.
storage manager A XenAPI component that provides a pluggable interface to support a wide variety of persistent
storage back ends.
storage manager back end A persistent storage method supported by XenAPI, such as iSCSI or NFS.
storage node An Object Storage node that provides container services, account services, and object services;
controls the account databases, container databases, and object storage.
storage services Collective name for the Object Storage object services, container services, and account services.
strategy Specifies the authentication source used by Image service or Identity. In the Database service, it
refers to the extensions implemented for a data store.
84 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
subdomain A domain within a parent domain. Subdomains cannot be registered. Subdomains enable you to
delegate domains. Subdomains can themselves have subdomains, so third-level, fourth-level, fifth-level,
and deeper levels of nesting are possible.
subnet Logical subdivision of an IP network.
SUSE Linux Enterprise Server (SLES) A Linux distribution that is compatible with OpenStack.
suspend The VM instance is paused and its state is saved to disk of the host.
swap Disk-based virtual memory used by operating systems to provide more memory than is actually available
on the system.
swauth An authentication and authorization service for Object Storage, implemented through WSGI middleware;
uses Object Storage itself as the persistent backing store.
swift Codename for OpenStack Object Storage service.
swift All in One (SAIO) Creates a full Object Storage development environment within a single VM.
swift middleware Collective term for Object Storage components that provide additional functionality.
swift proxy server Acts as the gatekeeper to Object Storage and is responsible for authenticating the user.
swift storage node A node that runs Object Storage account, container, and object services.
sync point Point in time since the last container and accounts database sync among nodes within Object Storage.
sysadmin One of the default roles in the Compute RBAC system. Enables a user to add other users to a project,
interact with VM images that are associated with the project, and start and stop VM instances.
system usage A Compute component that, along with the notification system, collects meters and usage information.
This information can be used for billing.
T
tacker Code name for the NFV Orchestration service
Telemetry service (telemetry) The OpenStack project which collects measurements of the utilization of the
physical and virtual resources comprising deployed clouds, persists this data for subsequent retrieval and
analysis, and triggers actions when defined criteria are met.
TempAuth An authentication facility within Object Storage that enables Object Storage itself to perform
authentication and authorization. Frequently used in testing and development.
Tempest Automated software test suite designed to run against the trunk of the OpenStack core project.
TempURL An Object Storage middleware component that enables creation of URLs for temporary object
access.
tenant A group of users; used to isolate access to Compute resources. An alternative term for a project.
Tenant API An API that is accessible to projects.
tenant endpoint An Identity service API endpoint that is associated with one or more projects.
tenant ID An alternative term for project ID.
token An alpha-numeric string of text used to access OpenStack APIs and resources.
Appendix 85
Virtual Machine Image Guide (Release Version: 15.0.0)
token services An Identity service component that manages and validates tokens after a user or project has
been authenticated.
tombstone Used to mark Object Storage objects that have been deleted; ensures that the object is not updated
on another node after it has been deleted.
topic publisher A process that is created when a RPC call is executed; used to push the message to the topic
exchange.
Torpedo Community project used to run automated tests against the OpenStack API.
transaction ID Unique ID assigned to each Object Storage request; used for debugging and tracing.
transient Alternative term for non-durable.
transient exchange Alternative term for a non-durable exchange.
transient message A message that is stored in memory and is lost after the server is restarted.
transient queue Alternative term for a non-durable queue.
TripleO OpenStack-on-OpenStack program. The code name for the OpenStack Deployment program.
trove Codename for OpenStack Database service.
trusted platform module (TPM) Specialized microprocessor for incorporating cryptographic keys into devices
for authenticating and securing a hardware platform.
U
Ubuntu A Debian-based Linux distribution.
unscoped token Alternative term for an Identity service default token.
updater Collective term for a group of Object Storage components that processes queued and failed updates
for containers and objects.
user In OpenStack Identity, entities represent individual API consumers and are owned by a specific domain.
In OpenStack Compute, a user can be associated with roles, projects, or both.
user data A blob of data that the user can specify when they launch an instance. The instance can access this
data through the metadata service or config drive. Commonly used to pass a shell script that the instance
runs on boot.
User Mode Linux (UML) An OpenStack-supported hypervisor.
V
VIF UUID Unique ID assigned to each Networking VIF.
Virtual Central Processing Unit (vCPU) Subdivides physical CPUs. Instances can then use those divisions.
Virtual Disk Image (VDI) One of the VM image disk formats supported by Image service.
Virtual Extensible LAN (VXLAN) A network virtualization technology that attempts to reduce the scalability
problems associated with large cloud computing deployments. It uses a VLAN-like encapsulation
technique to encapsulate Ethernet frames within UDP packets.
Virtual Hard Disk (VHD) One of the VM image disk formats supported by Image service.
86 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
virtual IP address (VIP) An Internet Protocol (IP) address configured on the load balancer for use by clients
connecting to a service that is load balanced. Incoming connections are distributed to back-end nodes
based on the configuration of the load balancer.
virtual machine (VM) An operating system instance that runs on top of a hypervisor. Multiple VMs can run
at the same time on the same physical host.
virtual network An L2 network segment within Networking.
Virtual Network Computing (VNC) Open source GUI and CLI tools used for remote console access to VMs.
Supported by Compute.
Virtual Network InterFace (VIF) An interface that is plugged into a port in a Networking network. Typically
a virtual network interface belonging to a VM.
virtual networking A generic term for virtualization of network functions such as switching, routing, load
balancing, and security using a combination of VMs and overlays on physical network infrastructure.
virtual port Attachment point where a virtual interface connects to a virtual network.
virtual private network (VPN) Provided by Compute in the form of cloudpipes, specialized instances that
are used to create VPNs on a per-project basis.
virtual server Alternative term for a VM or guest.
virtual switch (vSwitch) Software that runs on a host or node and provides the features and functions of a
hardware-based network switch.
virtual VLAN Alternative term for a virtual network.
VirtualBox An OpenStack-supported hypervisor.
Vitrage Code name for the Root Cause Analysis service.
VLAN manager A Compute component that provides dnsmasq and radvd and sets up forwarding to and from
cloudpipe instances.
VLAN network The Network Controller provides virtual networks to enable compute servers to interact with
each other and with the public network. All machines must have a public and private network interface.
A VLAN network is a private network interface, which is controlled by the vlan_interface option
with VLAN managers.
VM disk (VMDK) One of the VM image disk formats supported by Image service.
VM image Alternative term for an image.
VM Remote Control (VMRC) Method to access VM instance consoles using a web browser. Supported by
Compute.
VMware API Supports interaction with VMware products in Compute.
VMware NSX Neutron plug-in Provides support for VMware NSX in Neutron.
VNC proxy A Compute component that provides users access to the consoles of their VM instances through
VNC or VMRC.
volume Disk-based data storage generally represented as an iSCSI target with a file system that supports
extended attributes; can be persistent or ephemeral.
Volume API Alternative name for the Block Storage API.
volume controller A Block Storage component that oversees and coordinates storage volume actions.
Appendix 87
Virtual Machine Image Guide (Release Version: 15.0.0)
volume driver Alternative term for a volume plug-in.
volume ID Unique ID applied to each storage volume under the Block Storage control.
volume manager A Block Storage component that creates, attaches, and detaches persistent storage volumes.
volume node A Block Storage node that runs the cinder-volume daemon.
volume plug-in Provides support for new and specialized types of back-end storage for the Block Storage
volume manager.
volume worker A cinder component that interacts with back-end storage to manage the creation and deletion
of volumes and the creation of compute volumes, provided by the cinder-volume daemon.
vSphere An OpenStack-supported hypervisor.
W
Watcher Code name for the Infrastructure Optimization service.
weight Used by Object Storage devices to determine which storage devices are suitable for the job. Devices
are weighted by size.
weighted cost The sum of each cost used when deciding where to start a new VM instance in Compute.
weighting A Compute process that determines the suitability of the VM instances for a job for a particular
host. For example, not enough RAM on the host, too many CPUs on the host, and so on.
worker A daemon that listens to a queue and carries out tasks in response to messages. For example, the
cinder-volume worker manages volume creation and deletion on storage arrays.
Workflow service (mistral) The OpenStack service that provides a simple YAML-based language to write
workflows (tasks and transition rules) and a service that allows to upload them, modify, run them at scale
and in a highly available manner, manage and monitor workflow execution state and state of individual
tasks.
X
X.509 X.509 is the most widely used standard for defining digital certificates. It is a data structure that contains
the subject (entity) identifiable information such as its name along with its public key. The certificate can
contain a few other attributes as well depending upon the version. The most recent and standard version
of X.509 is v3.
Xen Xen is a hypervisor using a microkernel design, providing services that allow multiple computer operating
systems to execute on the same computer hardware concurrently.
Xen API The Xen administrative API, which is supported by Compute.
Xen Cloud Platform (XCP) An OpenStack-supported hypervisor.
Xen Storage Manager Volume Driver A Block Storage volume plug-in that enables communication with
the Xen Storage Manager API.
XenServer An OpenStack-supported hypervisor.
XFS High-performance 64-bit file system created by Silicon Graphics. Excels in parallel I/O operations and
data consistency.
88 Appendix
Virtual Machine Image Guide (Release Version: 15.0.0)
Z
zaqar Codename for the Message service.
ZeroMQ Message queue software supported by OpenStack. An alternative to RabbitMQ. Also spelled 0MQ.
Zuul Tool used in OpenStack development to ensure correctly ordered testing of changes in parallel.
Appendix 89
INDEX
Symbols
6to4, 55
A
absolute limit, 55
access control list (ACL), 55
access key, 55
account, 55
account auditor, 55
account database, 55
account reaper, 55
account server, 55
account service, 55
accounting, 55
Active Directory, 55
active/active configuration, 55
active/passive configuration, 55
address pool, 55
Address Resolution Protocol (ARP), 55
admin API, 55
admin server, 55
administrator, 55
Advanced Message Queuing Protocol (AMQP), 56
Advanced RISC Machine (ARM), 56
alert, 56
allocate, 56
Amazon Kernel Image (AKI), 56
Amazon Machine Image (AMI), 56
Amazon Ramdisk Image (ARI), 56
Anvil, 56
aodh, 56
Apache, 56
Apache License 2.0, 56
Apache Web Server, 56
API endpoint, 56
API extension, 56
API extension plug-in, 56
API key, 56
API server, 56
API token, 56
API version, 56
applet, 56
Application Catalog service (murano), 56
Application Programming Interface (API), 56
application server, 56
Application Service Provider (ASP), 56
arptables, 57
associate, 57
Asynchronous JavaScript and XML (AJAX), 57
ATA over Ethernet (AoE), 57
attach, 57
attachment (network), 57
auditing, 57
auditor, 57
Austin, 57
auth node, 57
authentication, 57
authentication token, 57
AuthN, 57
authorization, 57
authorization node, 57
AuthZ, 57
Auto ACK, 57
auto declare, 57
availability zone, 57
AWS CloudFormation template, 57
B
back end, 57
back-end catalog, 57
back-end store, 58
Backup, Restore, and Disaster Recovery service
(freezer), 58
bandwidth, 58
barbican, 58
bare, 58
Bare Metal service (ironic), 58
base image, 58
Bell-LaPadula model, 58
Benchmark service (rally), 58
Bexar, 58
binary, 58
90
Virtual Machine Image Guide (Release Version: 15.0.0)
bit, 58
bits per second (BPS), 58
block device, 58
block migration, 58
Block Storage API, 58
Block Storage service (cinder), 58
BMC (Baseboard Management Controller), 58
bootable disk image, 58
Bootstrap Protocol (BOOTP), 59
Border Gateway Protocol (BGP), 59
browser, 59
builder file, 59
bursting, 59
button class, 59
byte, 59
C
cache pruner, 59
Cactus, 59
CALL, 59
capability, 59
capacity cache, 59
capacity updater, 59
CAST, 59
catalog, 59
catalog service, 59
ceilometer, 59
cell, 59
cell forwarding, 59
cell manager, 59
CentOS, 60
Ceph, 60
CephFS, 60
certificate authority (CA), 60
Challenge-Handshake Authentication Protocol
(CHAP), 60
chance scheduler, 60
changes since, 60
Chef, 60
child cell, 60
cinder, 60
CirrOS, 60
Cisco neutron plug-in, 60
cloud architect, 60
Cloud Auditing Data Federation (CADF), 60
cloud computing, 60
cloud controller, 60
cloud controller node, 60
Cloud Data Management Interface (CDMI), 60
Cloud Infrastructure Management Interface (CIMI),
60
cloud-init, 60
cloudadmin, 60
Cloudbase-Init, 61
cloudpipe, 61
cloudpipe image, 61
Clustering service (senlin), 61
command filter, 61
Common Internet File System (CIFS), 61
Common Libraries (oslo), 61
community project, 61
compression, 61
Compute API (Nova API), 61
compute controller, 61
compute host, 61
compute node, 61
Compute service (nova), 61
compute worker, 61
concatenated object, 61
conductor, 61
congress, 61
consistency window, 61
console log, 61
container, 61
container auditor, 62
container database, 62
container format, 62
Container Infrastructure Management service (magnum),
62
container server, 62
container service, 62
content delivery network (CDN), 62
controller node, 62
core API, 62
core service, 62
cost, 62
credentials, 62
CRL, 62
Cross-Origin Resource Sharing (CORS), 62
Crowbar, 62
current workload, 62
customer, 62
customization module, 62
D
daemon, 62
Dashboard (horizon), 63
data encryption, 63
Data loss prevention (DLP) software, 63
Data Processing service (sahara), 63
data store, 63
database ID, 63
Index 91
Virtual Machine Image Guide (Release Version: 15.0.0)
database replicator, 63
Database service (trove), 63
deallocate, 63
Debian, 63
deduplication, 63
default panel, 63
default project, 63
default token, 63
delayed delete, 63
delivery mode, 63
denial of service (DoS), 63
deprecated auth, 63
designate, 63
Desktop-as-a-Service, 63
developer, 63
device ID, 63
device weight, 64
DevStack, 64
DHCP agent, 64
Diablo, 64
direct consumer, 64
direct exchange, 64
direct publisher, 64
disassociate, 64
Discretionary Access Control (DAC), 64
disk encryption, 64
disk format, 64
dispersion, 64
distributed virtual router (DVR), 64
Django, 64
DNS record, 64
DNS service (designate), 64
dnsmasq, 64
domain, 64
Domain Name System (DNS), 64
download, 65
durable exchange, 65
durable queue, 65
Dynamic Host Configuration Protocol (DHCP), 65
Dynamic HyperText Markup Language (DHTML), 65
E
east-west traffic, 65
EBS boot volume, 65
ebtables, 65
EC2, 65
EC2 access key, 65
EC2 API, 65
EC2 Compatibility API, 65
EC2 secret key, 65
Elastic Block Storage (EBS), 65
encapsulation, 65
encryption, 65
endpoint, 65
endpoint registry, 65
endpoint template, 65
entity, 65
ephemeral image, 66
ephemeral volume, 66
Essex, 66
ESXi, 66
ETag, 66
euca2ools, 66
Eucalyptus Kernel Image (EKI), 66
Eucalyptus Machine Image (EMI), 66
Eucalyptus Ramdisk Image (ERI), 66
evacuate, 66
exchange, 66
exchange type, 66
exclusive queue, 66
extended attributes (xattr), 66
extension, 66
external network, 66
extra specs, 66
F
FakeLDAP, 66
fan-out exchange, 66
federated identity, 66
Fedora, 66
Fibre Channel, 66
Fibre Channel over Ethernet (FCoE), 66
fill-first scheduler, 67
filter, 67
firewall, 67
FireWall-as-a-Service (FWaaS), 67
fixed IP address, 67
Flat Manager, 67
flat mode injection, 67
flat network, 67
FlatDHCP Manager, 67
flavor, 67
flavor ID, 67
floating IP address, 67
Folsom, 67
FormPost, 67
freezer, 67
front end, 67
G
gateway, 67
generic receive offload (GRO), 67
generic routing encapsulation (GRE), 67
92 Index
Virtual Machine Image Guide (Release Version: 15.0.0)
glance, 67
glance API server, 68
glance registry, 68
global endpoint template, 68
GlusterFS, 68
gnocchi, 68
golden image, 68
Governance service (congress), 68
Graphic Interchange Format (GIF), 68
Graphics Processing Unit (GPU), 68
Green Threads, 68
Grizzly, 68
Group, 68
guest OS, 68
H
Hadoop, 68
Hadoop Distributed File System (HDFS), 68
handover, 68
HAProxy, 68
hard reboot, 68
Havana, 68
health monitor, 68
heat, 68
Heat Orchestration Template (HOT), 68
high availability (HA), 69
horizon, 69
horizon plug-in, 69
host, 69
host aggregate, 69
Host Bus Adapter (HBA), 69
hybrid cloud, 69
Hyper-V, 69
hyperlink, 69
Hypertext Transfer Protocol (HTTP), 69
Hypertext Transfer Protocol Secure (HTTPS), 69
hypervisor, 69
hypervisor pool, 69
I
Icehouse, 69
ID number, 69
Identity API, 69
Identity back end, 69
identity provider, 69
Identity service (keystone), 69
Identity service API, 70
IETF, 70
image, 70
Image API, 70
image cache, 70
image ID, 70
image membership, 70
image owner, 70
image registry, 70
Image service (glance), 70
image status, 70
image store, 70
image UUID, 70
incubated project, 70
Infrastructure Optimization service (watcher), 70
Infrastructure-as-a-Service (IaaS), 70
ingress filtering, 70
INI format, 70
injection, 70
Input/Output Operations Per Second (IOPS), 70
instance, 70
instance ID, 71
instance state, 71
instance tunnels network, 71
instance type, 71
instance type ID, 71
instance UUID, 71
Intelligent Platform Management Interface (IPMI), 71
interface, 71
interface ID, 71
Internet Control Message Protocol (ICMP), 71
Internet protocol (IP), 71
Internet Service Provider (ISP), 71
Internet Small Computer System Interface (iSCSI), 71
IP address, 71
IP Address Management (IPAM), 71
ip6tables, 71
ipset, 71
iptables, 71
ironic, 71
iSCSI Qualified Name (IQN), 71
ISO9660, 72
itsec, 72
J
Java, 72
JavaScript, 72
JavaScript Object Notation (JSON), 72
jumbo frame, 72
Juno, 72
K
Kerberos, 72
kernel-based VM (KVM), 72
Key Manager service (barbican), 72
keystone, 72
Kickstart, 72
Kilo, 72
Index 93
Virtual Machine Image Guide (Release Version: 15.0.0)
L
large object, 72
Launchpad, 72
Layer-2 (L2) agent, 72
Layer-2 network, 72
Layer-3 (L3) agent, 72
Layer-3 network, 73
Liberty, 73
libvirt, 73
Lightweight Directory Access Protocol (LDAP), 73
Linux, 73
Linux bridge, 73
Linux Bridge neutron plug-in, 73
Linux containers (LXC), 73
live migration, 73
load balancer, 73
load balancing, 73
Load-Balancer-as-a-Service (LBaaS), 73
Load-balancing service (octavia), 73
Logical Volume Manager (LVM), 73
M
magnum, 73
management API, 73
management network, 73
manager, 73
manifest, 73
manifest object, 73
manila, 73
manila-share, 73
maximum transmission unit (MTU), 73
mechanism driver, 74
melange, 74
membership, 74
membership list, 74
memcached, 74
memory overcommit, 74
message broker, 74
message bus, 74
message queue, 74
Message service (zaqar), 74
Meta-Data Server (MDS), 74
Metadata agent, 74
migration, 74
mistral, 74
Mitaka, 74
Modular Layer 2 (ML2) neutron plug-in, 74
monasca, 74
Monitor (LBaaS), 74
Monitor (Mon), 74
Monitoring (monasca), 74
multi-factor authentication, 74
multi-host, 74
multinic, 75
murano, 75
N
Nebula, 75
netadmin, 75
NetApp volume driver, 75
network, 75
Network Address Translation (NAT), 75
network controller, 75
Network File System (NFS), 75
network ID, 75
network manager, 75
network namespace, 75
network node, 75
network segment, 75
Network Service Header (NSH), 75
Network Time Protocol (NTP), 75
network UUID, 75
network worker, 75
Networking API (Neutron API), 75
Networking service (neutron), 75
neutron, 75
neutron API, 75
neutron manager, 76
neutron plug-in, 76
Newton, 76
Nexenta volume driver, 76
NFV Orchestration Service (tacker), 76
Nginx, 76
No ACK, 76
node, 76
non-durable exchange, 76
non-durable queue, 76
non-persistent volume, 76
north-south traffic, 76
nova, 76
Nova API, 76
nova-network, 76
O
object, 76
object auditor, 76
object expiration, 76
object hash, 76
object path hash, 76
object replicator, 76
object server, 76
Object Storage API, 76
Object Storage Device (OSD), 77
94 Index
Virtual Machine Image Guide (Release Version: 15.0.0)
Object Storage service (swift), 77
object versioning, 77
Ocata, 77
Octavia, 77
Oldie, 77
Open Cloud Computing Interface (OCCI), 77
Open Virtualization Format (OVF), 77
Open vSwitch, 77
Open vSwitch (OVS) agent, 77
Open vSwitch neutron plug-in, 77
OpenLDAP, 77
OpenStack, 77
OpenStack code name, 77
openSUSE, 77
operator, 77
optional service, 77
Orchestration service (heat), 77
orphan, 77
Oslo, 77
P
panko, 78
parent cell, 78
partition, 78
partition index, 78
partition shift value, 78
path MTU discovery (PMTUD), 78
pause, 78
PCI passthrough, 78
persistent message, 78
persistent volume, 78
personality file, 78
Pike, 78
Platform-as-a-Service (PaaS), 78
plug-in, 78
policy service, 78
policy-based routing (PBR), 78
pool, 78
pool member, 78
port, 78
port UUID, 78
preseed, 78
private image, 78
private IP address, 79
private network, 79
project, 79
project ID, 79
project VPN, 79
promiscuous mode, 79
protected property, 79
provider, 79
proxy node, 79
proxy server, 79
public API, 79
public image, 79
public IP address, 79
public key authentication, 79
public network, 79
Puppet, 79
Python, 79
Q
QEMU Copy On Write 2 (QCOW2), 79
Qpid, 79
Quality of Service (QoS), 79
quarantine, 79
Queens, 80
Quick EMUlator (QEMU), 80
quota, 80
R
RabbitMQ, 80
Rackspace Cloud Files, 80
RADOS Block Device (RBD), 80
radvd, 80
rally, 80
RAM filter, 80
RAM overcommit, 80
rate limit, 80
raw, 80
rebalance, 80
reboot, 80
rebuild, 80
Recon, 80
record, 80
record ID, 80
Red Hat Enterprise Linux (RHEL), 80
reference architecture, 80
region, 81
registry, 81
registry server, 81
Reliable, Autonomic Distributed Object Store, 81
Remote Procedure Call (RPC), 81
replica, 81
replica count, 81
replication, 81
replicator, 81
request ID, 81
rescue image, 81
resize, 81
RESTful, 81
ring, 81
ring builder, 81
Index 95
Virtual Machine Image Guide (Release Version: 15.0.0)
Rocky, 81
role, 81
Role Based Access Control (RBAC), 81
role ID, 81
Root Cause Analysis (RCA) service (Vitrage), 81
rootwrap, 81
round-robin scheduler, 81
router, 82
routing key, 82
RPC driver, 82
rsync, 82
RXTX cap, 82
RXTX quota, 82
S
sahara, 82
SAML assertion, 82
scheduler manager, 82
scoped token, 82
scrubber, 82
secret key, 82
secure boot, 82
secure shell (SSH), 82
security group, 82
segmented object, 82
self-service, 82
SELinux, 82
senlin, 82
server, 82
server image, 82
server UUID, 82
service, 82
service catalog, 82
Service Function Chain (SFC), 83
service ID, 83
Service Level Agreement (SLA), 83
service project, 83
service provider, 83
service registration, 83
service token, 83
session back end, 83
session persistence, 83
session storage, 83
share, 83
share network, 83
Shared File Systems API, 83
Shared File Systems service (manila), 83
shared IP address, 83
shared IP group, 83
shared storage, 83
Sheepdog, 83
Simple Cloud Identity Management (SCIM), 84
Simple Protocol for Independent Computing Environments
(SPICE), 84
Single-root I/O Virtualization (SR-IOV), 84
SmokeStack, 84
snapshot, 84
soft reboot, 84
Software Development Lifecycle Automation service
(solum), 84
Software-defined networking (SDN), 84
SolidFire Volume Driver, 84
solum, 84
spread-first scheduler, 84
SQLAlchemy, 84
SQLite, 84
stack, 84
StackTach, 84
static IP address, 84
StaticWeb, 84
storage back end, 84
storage manager, 84
storage manager back end, 84
storage node, 84
storage services, 84
strategy, 84
subdomain, 85
subnet, 85
SUSE Linux Enterprise Server (SLES), 85
suspend, 85
swap, 85
swauth, 85
swift, 85
swift All in One (SAIO), 85
swift middleware, 85
swift proxy server, 85
swift storage node, 85
sync point, 85
sysadmin, 85
system usage, 85
T
tacker, 85
Telemetry service (telemetry), 85
TempAuth, 85
Tempest, 85
TempURL, 85
tenant, 85
Tenant API, 85
tenant endpoint, 85
tenant ID, 85
token, 85
96 Index
Virtual Machine Image Guide (Release Version: 15.0.0)
token services, 86
tombstone, 86
topic publisher, 86
Torpedo, 86
transaction ID, 86
transient, 86
transient exchange, 86
transient message, 86
transient queue, 86
TripleO, 86
trove, 86
trusted platform module (TPM), 86
U
Ubuntu, 86
unscoped token, 86
updater, 86
user, 86
user data, 86
User Mode Linux (UML), 86
V
VIF UUID, 86
Virtual Central Processing Unit (vCPU), 86
Virtual Disk Image (VDI), 86
Virtual Extensible LAN (VXLAN), 86
Virtual Hard Disk (VHD), 86
virtual IP address (VIP), 87
virtual machine (VM), 87
virtual network, 87
Virtual Network Computing (VNC), 87
Virtual Network InterFace (VIF), 87
virtual networking, 87
virtual port, 87
virtual private network (VPN), 87
virtual server, 87
virtual switch (vSwitch), 87
virtual VLAN, 87
VirtualBox, 87
Vitrage, 87
VLAN manager, 87
VLAN network, 87
VM disk (VMDK), 87
VM image, 87
VM Remote Control (VMRC), 87
VMware API, 87
VMware NSX Neutron plug-in, 87
VNC proxy, 87
volume, 87
Volume API, 87
volume controller, 87
volume driver, 88
volume ID, 88
volume manager, 88
volume node, 88
volume plug-in, 88
volume worker, 88
vSphere, 88
W
Watcher, 88
weight, 88
weighted cost, 88
weighting, 88
worker, 88
Workflow service (mistral), 88
X
X.509, 88
Xen, 88
Xen API, 88
Xen Cloud Platform (XCP), 88
Xen Storage Manager Volume Driver, 88
XenServer, 88
XFS, 88
Z
zaqar, 89
ZeroMQ, 89
Zuul, 89
Index 97
Architecture Design Guide
Release Version: 15.0.0
OpenStack contributors
May 12, 2017
CONTENTS
Abstract 2
Contents 3
Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Architecture requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Use cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
Index 105
i
Architecture Design Guide (Release Version: 15.0.0)
Note: This guide is a work in progress. Contributions are welcome.
CONTENTS 1
ABSTRACT
The Architecture Design Guide provides information on planning and designing an OpenStack cloud. It explains
core concepts, cloud architecture design requirements, and the design criteria of key components and services
in an OpenStack cloud. The guide also describes five common cloud use cases.
Before reading this book, we recommend:
• Prior knowledge of cloud architecture and principles.
• Linux and virtualization experience.
• A basic understanding of networking principles and protocols.
For information about deploying and operating OpenStack, see the Installation Tutorials and Guides, Deployment
Guides, and the OpenStack Operations Guide.
2
CONTENTS
Conventions
The OpenStack documentation uses several typesetting conventions.
Notices
Notices take these forms:
Note: A comment with additional information that explains a part of the text.
Important: Something you must be aware of before proceeding.
Tip: An extra but helpful piece of practical advice.
Caution: Helpful information that prevents the user from making mistakes.
Warning: Critical information about the risk of data loss or security issues.
Command prompts
$ command
Any user, including the root user, can run commands that are prefixed with the $ prompt.
# command
The root user must run commands that are prefixed with the # prompt. You can also prefix these commands
with the sudo command, if available, to run them.
Architecture requirements
This chapter describes the enterprise and operational factors that impacts the design of an OpenStack cloud.
3
Architecture Design Guide (Release Version: 15.0.0)
Enterprise requirements
The following sections describe business, usage, and performance considerations for customers which will
impact cloud architecture design.
Cost
Financial factors are a primary concern for any organization. Cost considerations may influence the type of
cloud that you build. For example, a general purpose cloud is unlikely to be the most cost-effective environment
for specialized applications. Unless business needs dictate that cost is a critical factor, cost should not be the
sole consideration when choosing or designing a cloud.
As a general guideline, increasing the complexity of a cloud architecture increases the cost of building and
maintaining it. For example, a hybrid or multi-site cloud architecture involving multiple vendors and technical
architectures may require higher setup and operational costs because of the need for more sophisticated orchestration
and brokerage tools than in other architectures. However, overall operational costs might be lower by
virtue of using a cloud brokerage tool to deploy the workloads to the most cost effective platform.
Consider the following costs categories when designing a cloud:
• Compute resources
• Networking resources
• Replication
• Storage
• Management
• Operational costs
It is also important to consider how costs will increase as your cloud scales. Choices that have a negligible
impact in small systems may considerably increase costs in large systems. In these cases, it is important to minimize
capital expenditure (CapEx) at all layers of the stack. Operators of massively scalable OpenStack clouds
require the use of dependable commodity hardware and freely available open source software components to
reduce deployment costs and operational expenses. Initiatives like Open Compute (more information available
in the Open Compute Project) provide additional information.
Time-to-market
The ability to deliver services or products within a flexible time frame is a common business factor when
building a cloud. Allowing users to self-provision and gain access to compute, network, and storage resources
on-demand may decrease time-to-market for new products and applications.
You must balance the time required to build a new cloud platform against the time saved by migrating users
away from legacy platforms. In some cases, existing infrastructure may influence your architecture choices.
For example, using multiple cloud platforms may be a good option when there is an existing investment in
several applications, as it could be faster to tie the investments together rather than migrating the components
and refactoring them to a single platform.
4 Architecture requirements
Architecture Design Guide (Release Version: 15.0.0)
Revenue opportunity
Revenue opportunities vary based on the intent and use case of the cloud. The requirements of a commercial,
customer-facing product are often very different from an internal, private cloud. You must consider what
features make your design most attractive to your users.
Capacity planning and scalability
Capacity and the placement of workloads are key design considerations for clouds. A long-term capacity plan
for these designs must incorporate growth over time to prevent permanent consumption of more expensive
external clouds. To avoid this scenario, account for future applications’ capacity requirements and plan growth
appropriately.
It is difficult to predict the amount of load a particular application might incur if the number of users fluctuates,
or the application experiences an unexpected increase in use. It is possible to define application requirements
in terms of vCPU, RAM, bandwidth, or other resources and plan appropriately. However, other clouds might
not use the same meter or even the same oversubscription rates.
Oversubscription is a method to emulate more capacity than may physically be present. For example, a physical
hypervisor node with 32 GB RAM may host 24 instances, each provisioned with 2 GB RAM. As long as all
24 instances do not concurrently use 2 full gigabytes, this arrangement works well. However, some hosts take
oversubscription to extremes and, as a result, performance can be inconsistent. If at all possible, determine
what the oversubscription rates of each host are and plan capacity accordingly.
Performance
Performance is a critical consideration when designing any cloud, and becomes increasingly important as size
and complexity grow. While single-site, private clouds can be closely controlled, multi-site and hybrid deployments
require more careful planning to reduce problems such as network latency between sites.
For example, you should consider the time required to run a workload in different clouds and methods for
reducing this time. This may require moving data closer to applications or applications closer to the data they
process, and grouping functionality so that connections that require low latency take place over a single cloud
rather than spanning clouds.
This may also require a CMP that can determine which cloud can most efficiently run which types of workloads.
Using native OpenStack tools can help improve performance. For example, you can use Telemetry to measure
performance and the Orchestration service (heat) to react to changes in demand.
Note: Orchestration requires special client configurations to integrate with Amazon Web Services. For other
types of clouds, use CMP features.
Cloud resource deployment The cloud user expects repeatable, dependable, and deterministic processes for
launching and deploying cloud resources. You could deliver this through a web-based interface or publicly
available API endpoints. All appropriate options for requesting cloud resources must be available
through some type of user interface, a command-line interface (CLI), or API endpoints.
Consumption model Cloud users expect a fully self-service and on-demand consumption model. When an
OpenStack cloud reaches the massively scalable size, expect consumption as a service in each and every
way.
Architecture requirements 5
Architecture Design Guide (Release Version: 15.0.0)
• Everything must be capable of automation. For example, everything from compute hardware,
storage hardware, networking hardware, to the installation and configuration of the supporting
software. Manual processes are impractical in a massively scalable OpenStack design architecture.
• Massively scalable OpenStack clouds require extensive metering and monitoring functionality to
maximize the operational efficiency by keeping the operator informed about the status and state
of the infrastructure. This includes full scale metering of the hardware and software status. A
corresponding framework of logging and alerting is also required to store and enable operations
to act on the meters provided by the metering and monitoring solutions. The cloud operator also
needs a solution that uses the data provided by the metering and monitoring solution to provide
capacity planning and capacity trending analysis.
Location For many use cases the proximity of the user to their workloads has a direct influence on the performance
of the application and therefore should be taken into consideration in the design. Certain applications
require zero to minimal latency that can only be achieved by deploying the cloud in multiple
locations. These locations could be in different data centers, cities, countries or geographical regions,
depending on the user requirement and location of the users.
Input-Output requirements Input-Output performance requirements require researching and modeling before
deciding on a final storage framework. Running benchmarks for Input-Output performance provides
a baseline for expected performance levels. If these tests include details, then the resulting data
can help model behavior and results during different workloads. Running scripted smaller benchmarks
during the lifecycle of the architecture helps record the system health at different points in time. The
data from these scripted benchmarks assist in future scoping and gaining a deeper understanding of an
organization’s needs.
Scale Scaling storage solutions in a storage-focused OpenStack architecture design is driven by initial requirements,
including IOPS, capacity, bandwidth, and future needs. Planning capacity based on projected
needs over the course of a budget cycle is important for a design. The architecture should balance cost
and capacity, while also allowing flexibility to implement new technologies and methods as they become
available.
Network
It is important to consider the functionality, security, scalability, availability, and testability of the network when
choosing a CMP and cloud provider.
• Decide on a network framework and design minimum functionality tests. This ensures testing and functionality
persists during and after upgrades.
• Scalability across multiple cloud providers may dictate which underlying network framework you choose
in different cloud providers. It is important to present the network API functions and to verify that
functionality persists across all cloud endpoints chosen.
• High availability implementations vary in functionality and design. Examples of some common methods
are active-hot-standby, active-passive, and active-active. Development of high availability and test
frameworks is necessary to insure understanding of functionality and limitations.
• Consider the security of data between the client and the endpoint, and of traffic that traverses the multiple
clouds.
For example, degraded video streams and low quality VoIP sessions negatively impact user experience and may
lead to productivity and economic loss.
6 Architecture requirements
Architecture Design Guide (Release Version: 15.0.0)
Network misconfigurations Configuring incorrect IP addresses, VLANs, and routers can cause outages to
areas of the network or, in the worst-case scenario, the entire cloud infrastructure. Automate network
configurations to minimize the opportunity for operator error as it can cause disruptive problems.
Capacity planning Cloud networks require management for capacity and growth over time. Capacity planning
includes the purchase of network circuits and hardware that can potentially have lead times measured in
months or years.
Network tuning Configure cloud networks to minimize link loss, packet loss, packet storms, broadcast storms,
and loops.
Single Point Of Failure (SPOF) Consider high availability at the physical and environmental layers. If there
is a single point of failure due to only one upstream link, or only one power supply, an outage can become
unavoidable.
Complexity An overly complex network design can be difficult to maintain and troubleshoot. While devicelevel
configuration can ease maintenance concerns and automated tools can handle overlay networks,
avoid or document non-traditional interconnects between functions and specialized hardware to prevent
outages.
Non-standard features There are additional risks that arise from configuring the cloud network to take advantage
of vendor specific features. One example is multi-link aggregation (MLAG) used to provide
redundancy at the aggregator switch level of the network. MLAG is not a standard and, as a result,
each vendor has their own proprietary implementation of the feature. MLAG architectures are not interoperable
across switch vendors, which leads to vendor lock-in, and can cause delays or inability when
upgrading components.
Dynamic resource expansion or bursting An application that requires additional resources may suit a multiple
cloud architecture. For example, a retailer needs additional resources during the holiday season, but
does not want to add private cloud resources to meet the peak demand. The user can accommodate the
increased load by bursting to a public cloud for these peak load periods. These bursts could be for long
or short cycles ranging from hourly to yearly.
Compliance and geo-location
An organization may have certain legal obligations and regulatory compliance measures which could require
certain workloads or data to not be located in certain regions.
Compliance considerations are particularly important for multi-site clouds. Considerations include:
• federal legal requirements
• local jurisdictional legal and compliance requirements
• image consistency and availability
• storage replication and availability (both block and file/object storage)
• authentication, authorization, and auditing (AAA)
Geographical considerations may also impact the cost of building or leasing data centers. Considerations include:
• floor space
• floor weight
• rack height and type
Architecture requirements 7
Architecture Design Guide (Release Version: 15.0.0)
• environmental considerations
• power usage and power usage efficiency (PUE)
• physical security
Auditing
A well-considered auditing plan is essential for quickly finding issues. Keeping track of changes made to
security groups and tenant changes can be useful in rolling back the changes if they affect production. For
example, if all security group rules for a tenant disappeared, the ability to quickly track down the issue would
be important for operational and legal reasons. For more details on auditing, see the Compliance chapter in the
OpenStack Security Guide.
Security
The importance of security varies based on the type of organization using a cloud. For example, government and
financial institutions often have very high security requirements. Security should be implemented according to
asset, threat, and vulnerability risk assessment matrices. See security-requirements.
Service level agreements
Service level agreements (SLA) must be developed in conjunction with business, technical, and legal input.
Small, private clouds may operate under an informal SLA, but hybrid or public clouds generally require more
formal agreements with their users.
For a user of a massively scalable OpenStack public cloud, there are no expectations for control over security,
performance, or availability. Users expect only SLAs related to uptime of API services, and very basic SLAs
for services offered. It is the user’s responsibility to address these issues on their own. The exception to
this expectation is the rare case of a massively scalable cloud infrastructure built for a private or government
organization that has specific requirements.
High performance systems have SLA requirements for a minimum quality of service with regard to guaranteed
uptime, latency, and bandwidth. The level of the SLA can have a significant impact on the network architecture
and requirements for redundancy in the systems.
Hybrid cloud designs must accommodate differences in SLAs between providers, and consider their enforceability.
Application readiness
Some applications are tolerant of a lack of synchronized object storage, while others may need those objects
to be replicated and available across regions. Understanding how the cloud implementation impacts new and
existing applications is important for risk mitigation, and the overall success of a cloud project. Applications
may have to be written or rewritten for an infrastructure with little to no redundancy, or with the cloud in mind.
Application momentum Businesses with existing applications may find that it is more cost effective to integrate
applications on multiple cloud platforms than migrating them to a single platform.
No predefined usage model The lack of a pre-defined usage model enables the user to run a wide variety of
applications without having to know the application requirements in advance. This provides a degree of
independence and flexibility that no other cloud scenarios are able to provide.
8 Architecture requirements
Architecture Design Guide (Release Version: 15.0.0)
On-demand and self-service application By definition, a cloud provides end users with the ability to selfprovision
computing power, storage, networks, and software in a simple and flexible way. The user
must be able to scale their resources up to a substantial level without disrupting the underlying host
operations. One of the benefits of using a general purpose cloud architecture is the ability to start with
limited resources and increase them over time as the user demand grows.
Authentication
It is recommended to have a single authentication domain rather than a separate implementation for each and every
site. This requires an authentication mechanism that is highly available and distributed to ensure continuous
operation. Authentication server locality might be required and should be planned for.
Migration, availability, site loss and recovery
Outages can cause partial or full loss of site functionality. Strategies should be implemented to understand and
plan for recovery scenarios.
• The deployed applications need to continue to function and, more importantly, you must consider the
impact on the performance and reliability of the application when a site is unavailable.
• It is important to understand what happens to the replication of objects and data between the sites when
a site goes down. If this causes queues to start building up, consider how long these queues can safely
exist until an error occurs.
• After an outage, ensure the method for resuming proper operations of a site is implemented when it
comes back online. We recommend you architect the recovery to avoid race conditions.
Disaster recovery and business continuity Cheaper storage makes the public cloud suitable for maintaining
backup applications.
Migration scenarios Hybrid cloud architecture enables the migration of applications between different clouds.
Provider availability or implementation details Business changes can affect provider availability. Likewise,
changes in a provider’s service can disrupt a hybrid cloud environment or increase costs.
Provider API changes Consumers of external clouds rarely have control over provider changes to APIs, and
changes can break compatibility. Using only the most common and basic APIs can minimize potential
conflicts.
Image portability As of the Kilo release, there is no common image format that is usable by all clouds. Conversion
or recreation of images is necessary if migrating between clouds. To simplify deployment, use
the smallest and simplest images feasible, install only what is necessary, and use a deployment manager
such as Chef or Puppet. Do not use golden images to speed up the process unless you repeatedly deploy
the same images on the same cloud.
API differences Avoid using a hybrid cloud deployment with more than just OpenStack (or with different
versions of OpenStack) as API changes can cause compatibility issues.
Business or technical diversity Organizations leveraging cloud-based services can embrace business diversity
and utilize a hybrid cloud design to spread their workloads across multiple cloud providers. This
ensures that no single cloud provider is the sole host for an application.
Architecture requirements 9
Architecture Design Guide (Release Version: 15.0.0)
Operational requirements
This section describes operational factors affecting the design of an OpenStack cloud.
Network design
The network design for an OpenStack cluster includes decisions regarding the interconnect needs within the
cluster, the need to allow clients to access their resources, and the access requirements for operators to administrate
the cluster. You should consider the bandwidth, latency, and reliability of these networks.
Consider additional design decisions about monitoring and alarming. If you are using an external provider,
service level agreements (SLAs) are typically defined in your contract. Operational considerations such as
bandwidth, latency, and jitter can be part of the SLA.
As demand for network resources increase, make sure your network design accommodates expansion and upgrades.
Operators add additional IP address blocks and add additional bandwidth capacity. In addition, consider
managing hardware and software lifecycle events, for example upgrades, decommissioning, and outages, while
avoiding service interruptions for tenants.
Factor maintainability into the overall network design. This includes the ability to manage and maintain IP
addresses as well as the use of overlay identifiers including VLAN tag IDs, GRE tunnel IDs, and MPLS tags.
As an example, if you may need to change all of the IP addresses on a network, a process known as renumbering,
then the design must support this function.
Address network-focused applications when considering certain operational realities. For example, consider
the impending exhaustion of IPv4 addresses, the migration to IPv6, and the use of private networks to segregate
different types of traffic that an application receives or generates. In the case of IPv4 to IPv6 migrations,
applications should follow best practices for storing IP addresses. We recommend you avoid relying on IPv4
features that did not carry over to the IPv6 protocol or have differences in implementation.
To segregate traffic, allow applications to create a private tenant network for database and storage network
traffic. Use a public network for services that require direct client access from the Internet. Upon segregating
the traffic, consider quality of service (QoS) and security to ensure each network has the required level of service.
Also consider the routing of network traffic. For some applications, develop a complex policy framework for
routing. To create a routing policy that satisfies business requirements, consider the economic cost of transmitting
traffic over expensive links versus cheaper links, in addition to bandwidth, latency, and jitter requirements.
Finally, consider how to respond to network events. How load transfers from one link to another during a failure
scenario could be a factor in the design. If you do not plan network capacity correctly, failover traffic could
overwhelm other ports or network links and create a cascading failure scenario. In this case, traffic that fails
over to one link overwhelms that link and then moves to the subsequent links until all network traffic stops.
SLA considerations
Service-level agreements (SLAs) define the levels of availability that will impact the design of an OpenStack
cloud to provide redundancy and high availability.
SLA terms that affect the design include:
• API availability guarantees implying multiple infrastructure services and highly available load balancers.
• Network uptime guarantees affecting switch design, which might require redundant switching and power.
• Networking security policy requirements.
10 Architecture requirements
Architecture Design Guide (Release Version: 15.0.0)
In any environment larger than just a few hosts, there are two areas that might be subject to a SLA:
• Data Plane - services that provide virtualization, networking, and storage. Customers usually require
these services to be continuously available.
• Control Plane - ancillary services such as API endpoints, and services that control CRUD operations.
The services in this category are usually subject to a different SLA expectation and may be better suited
on separate hardware or containers from the Data Plane services.
To effectively run cloud installations, initial downtime planning includes creating processes and architectures
that support planned maintenance and unplanned system faults.
It is important to determine as part of the SLA negotiation which party is responsible for monitoring and starting
up the Compute service instances if an outage occurs.
Upgrading, patching, and changing configuration items may require downtime for some services. Stopping
services that form the Control Plane may not impact the Data Plane. Live-migration of Compute instances may
be required to perform any actions that require downtime to Data Plane components.
There are many services outside the realms of pure OpenStack code which affects the ability of a cloud design
to meet SLAs, including:
• Database services, such as MySQL or PostgreSQL.
• Services providing RPC, such as RabbitMQ.
• External network attachments.
• Physical constraints such as power, rack space, network cabling, etc.
• Shared storage including SAN based arrays, storage clusters such as Ceph, and/or NFS services.
Depending on the design, some network service functions may fall into both the Control and Data Plane categories.
For example, the neutron L3 Agent service may be considered a Control Plane component, but the
routers themselves would be a Data Plane component.
In a design with multiple regions, the SLA would also need to take into consideration the use of shared services
such as the Identity service and Dashboard.
Any SLA negotiation must also take into account the reliance on third parties for critical aspects of the design.
For example, if there is an existing SLA on a component such as a storage system, the SLA must take into
account this limitation. If the required SLA for the cloud exceeds the agreed uptime levels of the cloud components,
additional redundancy would be required. This consideration is critical in a hybrid cloud design, where
there are multiple third parties involved.
Support and maintenance
An operations staff supports, manages, and maintains an OpenStack environment. Their skills may be specialized
or varied depending on the size and purpose of the installation.
The maintenance function of an operator should be taken into consideration:
Maintenance tasks Operating system patching, hardware/firmware upgrades, and datacenter related changes,
as well as minor and release upgrades to OpenStack components are all ongoing operational tasks. The
six monthly release cycle of the OpenStack projects needs to be considered as part of the cost of ongoing
maintenance. The solution should take into account storage and network maintenance and the impact on
underlying workloads.
Architecture requirements 11
Architecture Design Guide (Release Version: 15.0.0)
Reliability and availability Reliability and availability depend on the many supporting components’ availability
and on the level of precautions taken by the service provider. This includes network, storage
systems, datacenter, and operating systems.
For more information on managing and maintaining your OpenStack environment, see the OpenStack Operations
Guide.
Logging and monitoring
OpenStack clouds require appropriate monitoring platforms to identify and manage errors.
Note: We recommend leveraging existing monitoring systems to see if they are able to effectively monitor an
OpenStack environment.
Specific meters that are critically important to capture include:
• Image disk utilization
• Response time to the Compute API
Logging and monitoring does not significantly differ for a multi-site OpenStack cloud. The tools described
in the Logging and monitoring in the Operations Guide remain applicable. Logging and monitoring can be
provided on a per-site basis, and in a common centralized location.
When attempting to deploy logging and monitoring facilities to a centralized location, care must be taken with
the load placed on the inter-site networking links
Management software
Management software providing clustering, logging, monitoring, and alerting details for a cloud environment
is often used. This impacts and affects the overall OpenStack cloud design, and must account for the additional
resource consumption such as CPU, RAM, storage, and network bandwidth.
The inclusion of clustering software, such as Corosync or Pacemaker, is primarily determined by the availability
of the cloud infrastructure and the complexity of supporting the configuration after it is deployed. The
OpenStack High Availability Guide provides more details on the installation and configuration of Corosync and
Pacemaker, should these packages need to be included in the design.
Some other potential design impacts include:
• OS-hypervisor combination Ensure that the selected logging, monitoring, or alerting tools support the
proposed OS-hypervisor combination.
• Network hardware The network hardware selection needs to be supported by the logging, monitoring,
and alerting software.
Database software
Most OpenStack components require access to back-end database services to store state and configuration information.
Choose an appropriate back-end database which satisfies the availability and fault tolerance requirements
of the OpenStack services.
12 Architecture requirements
Architecture Design Guide (Release Version: 15.0.0)
MySQL is the default database for OpenStack, but other compatible databases are available.
Note: Telemetry uses MongoDB.
The chosen high availability database solution changes according to the selected database. MySQL, for example,
provides several options. Use a replication technology such as Galera for active-active clustering. For
active-passive use some form of shared storage. Each of these potential solutions has an impact on the design:
• Solutions that employ Galera/MariaDB require at least three MySQL nodes.
• MongoDB has its own design considerations for high availability.
• OpenStack design, generally, does not include shared storage. However, for some high availability designs,
certain components might require it depending on the specific implementation.
Operator access to systems
There is a trend for cloud operations systems being hosted within the cloud environment. Operators require
access to these systems to resolve a major incident.
Ensure that the network structure connects all clouds to form an integrated system. Also consider the state of
handoffs which must be reliable and have minimal latency for optimal performance of the system.
If a significant portion of the cloud is on externally managed systems, prepare for situations where it may not
be possible to make changes. Additionally, cloud providers may differ on how infrastructure must be managed
and exposed. This can lead to delays in root cause analysis where a provider insists the blame lies with the other
provider.
High availability
Data plane and control plane
When designing an OpenStack cloud, it is important to consider the needs dictated by the Service Level Agreement
(SLA). This includes the core services required to maintain availability of running Compute service instances,
networks, storage, and additional services running on top of those resources. These services are often
referred to as the Data Plane services, and are generally expected to be available all the time.
The remaining services, responsible for create, read, update and delete (CRUD) operations, metering, monitoring,
and so on, are often referred to as the Control Plane. The SLA is likely to dictate a lower uptime requirement
for these services.
The services comprising an OpenStack cloud have a number of requirements that you need to understand in
order to be able to meet SLA terms. For example, in order to provide the Compute service a minimum of
storage, message queueing and database services are necessary as well as the networking between them.
Ongoing maintenance operations are made much simpler if there is logical and physical separation of Data
Plane and Control Plane systems. It then becomes possible to, for example, reboot a controller without affecting
customers. If one service failure affects the operation of an entire server (noisy neighbor), the separation
between Control and Data Planes enables rapid maintenance with a limited effect on customer operations.
Architecture requirements 13
Architecture Design Guide (Release Version: 15.0.0)
Eliminating single points of failure within each site
OpenStack lends itself to deployment in a highly available manner where it is expected that at least 2 servers be
utilized. These can run all the services involved from the message queuing service, for example RabbitMQ or
QPID, and an appropriately deployed database service such as MySQL or MariaDB. As services in the cloud are
scaled out, back-end services will need to scale too. Monitoring and reporting on server utilization and response
times, as well as load testing your systems, will help determine scale out decisions.
The OpenStack services themselves should be deployed across multiple servers that do not represent a single
point of failure. Ensuring availability can be achieved by placing these services behind highly available load
balancers that have multiple OpenStack servers as members.
There are a small number of OpenStack services which are intended to only run in one place at a time (for
example, the ceilometer-agent-central service) . In order to prevent these services from becoming a
single point of failure, they can be controlled by clustering software such as Pacemaker.
In OpenStack, the infrastructure is integral to providing services and should always be available, especially
when operating with SLAs. Ensuring network availability is accomplished by designing the network architecture
so that no single point of failure exists. A consideration of the number of switches, routes and redundancies
of power should be factored into core infrastructure, as well as the associated bonding of networks to provide
diverse routes to your highly available switch infrastructure.
Care must be taken when deciding network functionality. Currently, OpenStack supports both the legacy
networking (nova-network) system and the newer, extensible OpenStack Networking (neutron). OpenStack
Networking and legacy networking both have their advantages and disadvantages. They are both valid and
supported options that fit different network deployment models described in the OpenStack Operations Guide.
When using the Networking service, the OpenStack controller servers or separate Networking hosts handle routing
unless the dynamic virtual routers pattern for routing is selected. Running routing directly on the controller
servers mixes the Data and Control Planes and can cause complex issues with performance and troubleshooting.
It is possible to use third party software and external appliances that help maintain highly available layer
three routes. Doing so allows for common application endpoints to control network hardware, or to provide
complex multi-tier web applications in a secure manner. It is also possible to completely remove routing from
Networking, and instead rely on hardware routing capabilities. In this case, the switching infrastructure must
support layer three routing.
Application design must also be factored into the capabilities of the underlying cloud infrastructure. If the
compute hosts do not provide a seamless live migration capability, then it must be expected that if a compute host
fails, that instance and any data local to that instance will be deleted. However, when providing an expectation
to users that instances have a high-level of uptime guaranteed, the infrastructure must be deployed in a way
that eliminates any single point of failure if a compute host disappears. This may include utilizing shared file
systems on enterprise storage or OpenStack Block storage to provide a level of guarantee to match service
features.
If using a storage design that includes shared access to centralized storage, ensure that this is also designed
without single points of failure and the SLA for the solution matches or exceeds the expected SLA for the Data
Plane.
Eliminating single points of failure in a multi-region design
Some services are commonly shared between multiple regions, including the Identity service and the Dashboard.
In this case, it is necessary to ensure that the databases backing the services are replicated, and that access to
multiple workers across each site can be maintained in the event of losing a single region.
14 Architecture requirements
Architecture Design Guide (Release Version: 15.0.0)
Multiple network links should be deployed between sites to provide redundancy for all components. This
includes storage replication, which should be isolated to a dedicated network or VLAN with the ability to
assign QoS to control the replication traffic or provide priority for this traffic.
Note: If the data store is highly changeable, the network requirements could have a significant effect on the
operational cost of maintaining the sites.
If the design incorporates more than one site, the ability to maintain object availability in both sites has significant
implications on the Object Storage design and implementation. It also has a significant impact on the
WAN network design between the sites.
If applications running in a cloud are not cloud-aware, there should be clear measures and expectations to
define what the infrastructure can and cannot support. An example would be shared storage between sites. It
is possible, however such a solution is not native to OpenStack and requires a third-party hardware vendor to
fulfill such a requirement. Another example can be seen in applications that are able to consume resources in
object storage directly.
Connecting more than two sites increases the challenges and adds more complexity to the design considerations.
Multi-site implementations require planning to address the additional topology used for internal and external
connectivity. Some options include full mesh topology, hub spoke, spine leaf, and 3D Torus.
For more information on high availability in OpenStack, see the OpenStack High Availability Guide.
Site loss and recovery
Outages can cause partial or full loss of site functionality. Strategies should be implemented to understand and
plan for recovery scenarios.
• The deployed applications need to continue to function and, more importantly, you must consider the
impact on the performance and reliability of the application if a site is unavailable.
• It is important to understand what happens to the replication of objects and data between the sites when
a site goes down. If this causes queues to start building up, consider how long these queues can safely
exist until an error occurs.
• After an outage, ensure that operations of a site are resumed when it comes back online. We recommend
that you architect the recovery to avoid race conditions.
Replicating inter-site data
Traditionally, replication has been the best method of protecting object store implementations. A variety of
replication methods exist in storage architectures, for example synchronous and asynchronous mirroring. Most
object stores and back-end storage systems implement methods for replication at the storage subsystem layer.
Object stores also tailor replication techniques to fit a cloud’s requirements.
Organizations must find the right balance between data integrity and data availability. Replication strategy may
also influence disaster recovery methods.
Replication across different racks, data centers, and geographical regions increases focus on determining and
ensuring data locality. The ability to guarantee data is accessed from the nearest or fastest storage can be
necessary for applications to perform well.
Architecture requirements 15
Architecture Design Guide (Release Version: 15.0.0)
Note: When running embedded object store methods, ensure that you do not instigate extra data replication as
this may cause performance issues.
Design
Designing an OpenStack cloud requires a understanding of the cloud user’s requirements and needs to determine
the best possible configuration. This chapter provides guidance on the decisions you need to make during the
design process.
To design, deploy, and configure OpenStack, administrators must understand the logical architecture. OpenStack
modules are one of the following types:
Daemon Runs as a background process. On Linux platforms, a daemon is usually installed as a service.
Script Installs a virtual environment and runs tests.
Command-line interface (CLI) Enables users to submit API calls to OpenStack services through commands.
OpenStack Logical Architecture shows one example of the most common integrated services within OpenStack
and how they interact with each other. End users can interact through the dashboard, CLIs, and APIs. All services
authenticate through a common Identity service, and individual services interact with each other through
public APIs, except where privileged administrator commands are necessary.
Fig. 1: OpenStack Logical Architecture
16 Design
Architecture Design Guide (Release Version: 15.0.0)
Compute node design
Compute server architecture overview
When designing compute resource pools, consider the number of processors, amount of memory, network
requirements, the quantity of storage required for each hypervisor, and any requirements for bare metal hosts
provisioned through ironic.
When architecting an OpenStack cloud, as part of the planning process, you must not only determine what
hardware to utilize but whether compute resources will be provided in a single pool or in multiple pools or
availability zones. You should consider if the cloud will provide distinctly different profiles for compute.
For example, CPU, memory or local storage based compute nodes. For NFV or HPC based clouds, there
may even be specific network configurations that should be reserved for those specific workloads on specific
compute nodes. This method of designing specific resources into groups or zones of compute can be referred
to as bin packing.
Note: In a bin packing design, each independent resource pool provides service for specific flavors. Since
instances are scheduled onto compute hypervisors, each independent node’s resources will be allocated to efficiently
use the available hardware. While bin packing can separate workload specific resources onto individual
servers, bin packing also requires a common hardware design, with all hardware nodes within a compute resource
pool sharing a common processor, memory, and storage layout. This makes it easier to deploy, support,
and maintain nodes throughout their lifecycle.
Increasing the size of the supporting compute environment increases the network traffic and messages, adding
load to the controllers and administrative services used to support the OpenStack cloud or networking nodes.
When considering hardware for controller nodes, whether using the monolithic controller design, where all of
the controller services live on one or more physical hardware nodes, or in any of the newer shared nothing
control plane models, adequate resources must be allocated and scaled to meet scale requirements. Effective
monitoring of the environment will help with capacity decisions on scaling. Proper planning will help avoid
bottlenecks and network oversubscription as the cloud scales.
Compute nodes automatically attach to OpenStack clouds, resulting in a horizontally scaling process when
adding extra compute capacity to an OpenStack cloud. To further group compute nodes and place nodes into
appropriate availability zones and host aggregates, additional work is required. It is necessary to plan rack
capacity and network switches as scaling out compute hosts directly affects data center infrastructure resources
as would any other infrastructure expansion.
While not as common in large enterprises, compute host components can also be upgraded to account for
increases in demand, known as vertical scaling. Upgrading CPUs with more cores, or increasing the overall
server memory, can add extra needed capacity depending on whether the running applications are more CPU
intensive or memory intensive. Since OpenStack schedules workload placement based on capacity and technical
requirements, removing compute nodes from availability and upgrading them using a rolling upgrade design.
When selecting a processor, compare features and performance characteristics. Some processors include features
specific to virtualized compute hosts, such as hardware-assisted virtualization, and technology related to
memory paging (also known as EPT shadowing). These types of features can have a significant impact on the
performance of your virtual machine.
The number of processor cores and threads impacts the number of worker threads which can be run on a resource
node. Design decisions must relate directly to the service being run on it, as well as provide a balanced
infrastructure for all services.
Design 17
Architecture Design Guide (Release Version: 15.0.0)
Another option is to assess the average workloads and increase the number of instances that can run within
the compute environment by adjusting the overcommit ratio. This ratio is configurable for CPU and memory.
The default CPU overcommit ratio is 16:1, and the default memory overcommit ratio is 1.5:1. Determining the
tuning of the overcommit ratios during the design phase is important as it has a direct impact on the hardware
layout of your compute nodes.
Note: Changing the CPU overcommit ratio can have a detrimental effect and cause a potential increase in a
noisy neighbor.
Insufficient disk capacity could also have a negative effect on overall performance including CPU and memory
usage. Depending on the back end architecture of the OpenStack Block Storage layer, capacity includes adding
disk shelves to enterprise storage systems or installing additional Block Storage nodes. Upgrading directly
attached storage installed in Compute hosts, and adding capacity to the shared storage for additional ephemeral
storage to instances, may be necessary.
Consider the Compute requirements of non-hypervisor nodes (also referred to as resource nodes). This includes
controller, Object Storage nodes, Block Storage nodes, and networking services.
The ability to create pools or availability zones for unpredictable workloads should be considered. In some
cases, the demand for certain instance types or flavors may not justify individual hardware design. Allocate
hardware designs that are capable of servicing the most common instance requests. Adding hardware to the
overall architecture can be done later.
Choosing a CPU
The type of CPU in your compute node is a very important decision. You must ensure that the CPU supports
virtualization by way of VT-x for Intel chips and AMD-v for AMD chips.
Tip: Consult the vendor documentation to check for virtualization support. For Intel CPUs, see Does my
processor support Intel® Virtualization Technology?. For AMD CPUs, see AMD Virtualization. Your CPU
may support virtualization but it may be disabled. Consult your BIOS documentation for how to enable CPU
features.
The number of cores that the CPU has also affects your decision. It is common for current CPUs to have up
to 24 cores. Additionally, if an Intel CPU supports hyper-threading, those 24 cores are doubled to 48 cores. If
you purchase a server that supports multiple CPUs, the number of cores is further multiplied.
As of the Kilo release, key enhancements have been added to the OpenStack code to improve guest performance.
These improvements allow the Compute service to take advantage of greater insight into a compute host’s
physical layout and therefore make smarter decisions regarding workload placement. Administrators can use
this functionality to enable smarter planning choices for use cases like NFV (Network Function Virtualization)
and HPC (High Performance Computing).
Considering non-uniform memory access (NUMA) is important when selecting CPU sizes and types, as there
are use cases that use NUMA pinning to reserve host cores for operating system processes. These reduce the
available CPU for workloads and protects the operating system.
Tip: When CPU pinning is requested for for a guest, it is assumed there is no overcommit (or, an overcommit
ratio of 1.0). When dedicated resourcing is not requested for a workload, the normal overcommit ratios are
applied.
18 Design
Architecture Design Guide (Release Version: 15.0.0)
Therefore, we recommend that host aggregates are used to separate not only bare metal hosts, but hosts that will
provide resources for workloads that require dedicated resources. This said, when workloads are provisioned
to NUMA host aggregates, NUMA nodes are chosen at random and vCPUs can float across NUMA nodes
on a host. If workloads require SR-IOV or DPDK, they should be assigned to a NUMA node aggregate with
hosts that supply the functionality. More importantly, the workload or vCPUs that are executing processes for
a workload should be on the same NUMA node due to the limited amount of cross-node memory bandwidth.
In all cases, the NUMATopologyFilter must be enabled for nova-scheduler.
Additionally, CPU selection may not be one-size-fits-all across enterprises, but more of a list of SKUs that are
tuned for the enterprise workloads.
For more information about NUMA, see CPU topologies in the Administrator Guide.
In order to take advantage of these new enhancements in the Compute service, compute hosts must be using
NUMA capable CPUs.
Tip: Multithread Considerations
Hyper-Threading is Intel’s proprietary simultaneous multithreading implementation used to improve parallelization
on their CPUs. You might consider enabling Hyper-Threading to improve the performance of multithreaded
applications.
Whether you should enable Hyper-Threading on your CPUs depends upon your use case. For example, disabling
Hyper-Threading can be beneficial in intense computing environments. We recommend performance
testing with your local workload with both Hyper-Threading on and off to determine what is more appropriate
in your case.
In most cases, hyper-threading CPUs can provide a 1.3x to 2.0x performance benefit over non-hyper-threaded
CPUs depending on types of workload.
Choosing a hypervisor
A hypervisor provides software to manage virtual machine access to the underlying hardware. The hypervisor
creates, manages, and monitors virtual machines. OpenStack Compute (nova) supports many hypervisors to
various degrees, including:
• KVM
• LXC
• QEMU
• VMware ESX/ESXi
• Xen
• Hyper-V
• Docker
An important factor in your choice of hypervisor is your current organization’s hypervisor usage or experience.
Also important is the hypervisor’s feature parity, documentation, and the level of community experience.
As per the recent OpenStack user survey, KVM is the most widely adopted hypervisor in the OpenStack community.
Besides KVM, there are many deployments that run other hypervisors such as LXC, VMware, Xen, and
Design 19
Architecture Design Guide (Release Version: 15.0.0)
Hyper-V. However, these hypervisors are either less used, are niche hypervisors, or have limited functionality
compared to more commonly used hypervisors.
Note: It is also possible to run multiple hypervisors in a single deployment using host aggregates or cells.
However, an individual compute node can run only a single hypervisor at a time.
For more information about feature support for hypervisors as well as ironic and Virtuozzo (formerly Parallels),
see Hypervisor Support Matrix and Hypervisors in the Configuration Reference.
Choosing server hardware
Consider the following factors when selecting compute server hardware:
• Server density A measure of how many servers can fit into a given measure of physical space, such as
a rack unit [U].
• Resource capacity The number of CPU cores, how much RAM, or how much storage a given server
delivers.
• Expandability The number of additional resources you can add to a server before it reaches capacity.
• Cost The relative cost of the hardware weighed against the total amount of capacity available on the
hardware based on predetermined requirements.
Weigh these considerations against each other to determine the best design for the desired purpose. For example,
increasing server density means sacrificing resource capacity or expandability. It also can decrease availability
and increase the chance of noisy neighbor issues. Increasing resource capacity and expandability can increase
cost but decrease server density. Decreasing cost often means decreasing supportability, availability, server
density, resource capacity, and expandability.
Determine the requirements for the cloud prior to constructing the cloud, and plan for hardware lifecycles, and
expansion and new features that may require different hardware.
If the cloud is initially built with near end of life, but cost effective hardware, then the performance and capacity
demand of new workloads will drive the purchase of more modern hardware. With individual hardware
components changing over time, you may prefer to manage configurations as stock keeping units (SKU)s. This
method provides an enterprise with a standard configuration unit of compute (server) that can be placed in any
IT service manager or vendor supplied ordering system that can be triggered manually or through advanced
operational automations. This simplifies ordering, provisioning, and activating additional compute resources.
For example, there are plug-ins for several commercial service management tools that enable integration with
hardware APIs. These configure and activate new compute resources from standby hardware based on a standard
configurations. Using this methodology, spare hardware can be ordered for a datacenter and provisioned
based on capacity data derived from OpenStack Telemetry.
Compute capacity (CPU cores and RAM capacity) is a secondary consideration for selecting server hardware.
The required server hardware must supply adequate CPU sockets, additional CPU cores, and adequate RA. For
more information, see Choosing a CPU.
In compute server architecture design, you must also consider network and storage requirements. For more
information on network considerations, see Networking.
20 Design
Architecture Design Guide (Release Version: 15.0.0)
Considerations when choosing hardware
Here are some other factors to consider when selecting hardware for your compute servers.
Instance density
More hosts are required to support the anticipated scale if the design architecture uses dual-socket hardware
designs.
For a general purpose OpenStack cloud, sizing is an important consideration. The expected or anticipated
number of instances that each hypervisor can host is a common meter used in sizing the deployment. The
selected server hardware needs to support the expected or anticipated instance density.
Host density
Another option to address the higher host count is to use a quad-socket platform. Taking this approach decreases
host density which also increases rack count. This configuration affects the number of power connections and
also impacts network and cooling requirements.
Physical data centers have limited physical space, power, and cooling. The number of hosts (or hypervisors)
that can be fitted into a given metric (rack, rack unit, or floor tile) is another important method of sizing. Floor
weight is an often overlooked consideration.
The data center floor must be able to support the weight of the proposed number of hosts within a rack or set
of racks. These factors need to be applied as part of the host density calculation and server hardware selection.
Power and cooling density
The power and cooling density requirements might be lower than with blade, sled, or 1U server designs due to
lower host density (by using 2U, 3U or even 4U server designs). For data centers with older infrastructure, this
might be a desirable feature.
Data centers have a specified amount of power fed to a given rack or set of racks. Older data centers may have
power densities as low as 20A per rack, and current data centers can be designed to support power densities as
high as 120A per rack. The selected server hardware must take power density into account.
Selecting hardware form factor
Consider the following in selecting server hardware form factor suited for your OpenStack design architecture:
• Most blade servers can support dual-socket multi-core CPUs. To avoid this CPU limit, select full
width or full height blades. Be aware, however, that this also decreases server density. For example,
high density blade servers such as HP BladeSystem or Dell PowerEdge M1000e support up to 16 servers
in only ten rack units. Using half-height blades is twice as dense as using full-height blades, which results
in only eight servers per ten rack units.
• 1U rack-mounted servers have the ability to offer greater server density than a blade server solution, but
are often limited to dual-socket, multi-core CPU configurations. It is possible to place forty 1U servers
in a rack, providing space for the top of rack (ToR) switches, compared to 32 full width blade servers.
Design 21
Architecture Design Guide (Release Version: 15.0.0)
To obtain greater than dual-socket support in a 1U rack-mount form factor, customers need to buy their
systems from Original Design Manufacturers (ODMs) or second-tier manufacturers.
Warning: This may cause issues for organizations that have preferred vendor policies or concerns
with support and hardware warranties of non-tier 1 vendors.
• 2U rack-mounted servers provide quad-socket, multi-core CPU support, but with a corresponding decrease
in server density (half the density that 1U rack-mounted servers offer).
• Larger rack-mounted servers, such as 4U servers, often provide even greater CPU capacity, commonly
supporting four or even eight CPU sockets. These servers have greater expandability, but such servers
have much lower server density and are often more expensive.
• Sled servers are rack-mounted servers that support multiple independent servers in a single 2U or
3U enclosure. These deliver higher density as compared to typical 1U or 2U rack-mounted servers. For
example, many sled servers offer four independent dual-socket nodes in 2U for a total of eight CPU
sockets in 2U.
Scaling your cloud
When designing a OpenStack cloud compute server architecture, you must decide whether you intend to scale up
or scale out. Selecting a smaller number of larger hosts, or a larger number of smaller hosts, depends on a combination
of factors: cost, power, cooling, physical rack and floor space, support-warranty, and manageability.
Typically, the scale out model has been popular for OpenStack because it reduces the number of possible failure
domains by spreading workloads across more infrastructure. However, the downside is the cost of additional
servers and the datacenter resources needed to power, network, and cool the servers.
Overcommitting CPU and RAM
OpenStack allows you to overcommit CPU and RAM on compute nodes. This allows you to increase the
number of instances running on your cloud at the cost of reducing the performance of the instances. The
Compute service uses the following ratios by default:
• CPU allocation ratio: 16:1
• RAM allocation ratio: 1.5:1
The default CPU allocation ratio of 16:1 means that the scheduler allocates up to 16 virtual cores per physical
core. For example, if a physical node has 12 cores, the scheduler sees 192 available virtual cores. With typical
flavor definitions of 4 virtual cores per instance, this ratio would provide 48 instances on a physical node.
The formula for the number of virtual instances on a compute node is (OR*PC)/VC, where:
OR CPU overcommit ratio (virtual cores per physical core)
PC Number of physical cores
VC Number of virtual cores per instance
Similarly, the default RAM allocation ratio of 1.5:1 means that the scheduler allocates instances to a physical
node as long as the total amount of RAM associated with the instances is less than 1.5 times the amount of
RAM available on the physical node.
22 Design
Architecture Design Guide (Release Version: 15.0.0)
For example, if a physical node has 48 GB of RAM, the scheduler allocates instances to that node until the
sum of the RAM associated with the instances reaches 72 GB (such as nine instances, in the case where each
instance has 8 GB of RAM).
Note: Regardless of the overcommit ratio, an instance can not be placed on any physical node with fewer raw
(pre-overcommit) resources than the instance flavor requires.
You must select the appropriate CPU and RAM allocation ratio for your particular use case.
Instance storage solutions
As part of the architecture design for a compute cluster, you must specify storage for the disk on which the
instantiated instance runs. There are three main approaches to providing temporary storage:
• Off compute node storage—shared file system
• On compute node storage—shared file system
• On compute node storage—nonshared file system
In general, the questions you should ask when selecting storage are as follows:
• What are my workloads?
• Do my workloads have IOPS requirements?
• Are there read, write, or random access performance requirements?
• What is my forecast for the scaling of storage for compute?
• What storage is my enterprise currently using? Can it be re-purposed?
• How do I manage the storage operationally?
Many operators use separate compute and storage hosts instead of a hyperconverged solution. Compute services
and storage services have different requirements, and compute hosts typically require more CPU and RAM than
storage hosts. Therefore, for a fixed budget, it makes sense to have different configurations for your compute
nodes and your storage nodes. Compute nodes will be invested in CPU and RAM, and storage nodes will be
invested in block storage.
However, if you are more restricted in the number of physical hosts you have available for creating your cloud
and you want to be able to dedicate as many of your hosts as possible to running instances, it makes sense to
run compute and storage on the same machines or use an existing storage array that is available.
The three main approaches to instance storage are provided in the next few sections.
Non-compute node based shared file system
In this option, the disks storing the running instances are hosted in servers outside of the compute nodes.
If you use separate compute and storage hosts, you can treat your compute hosts as “stateless”. As long as you
do not have any instances currently running on a compute host, you can take it offline or wipe it completely
without having any effect on the rest of your cloud. This simplifies maintenance for the compute hosts.
There are several advantages to this approach:
• If a compute node fails, instances are usually easily recoverable.
Design 23
Architecture Design Guide (Release Version: 15.0.0)
• Running a dedicated storage system can be operationally simpler.
• You can scale to any number of spindles.
• It may be possible to share the external storage for other purposes.
The main disadvantages to this approach are:
• Depending on design, heavy I/O usage from some instances can affect unrelated instances.
• Use of the network can decrease performance.
• Scalability can be affected by network architecture.
On compute node storage—shared file system
In this option, each compute node is specified with a significant amount of disk space, but a distributed file
system ties the disks from each compute node into a single mount.
The main advantage of this option is that it scales to external storage when you require additional storage.
However, this option has several disadvantages:
• Running a distributed file system can make you lose your data locality compared with nonshared storage.
• Recovery of instances is complicated by depending on multiple hosts.
• The chassis size of the compute node can limit the number of spindles able to be used in a compute node.
• Use of the network can decrease performance.
• Loss of compute nodes decreases storage availability for all hosts.
On compute node storage—nonshared file system
In this option, each compute node is specified with enough disks to store the instances it hosts.
There are two main advantages:
• Heavy I/O usage on one compute node does not affect instances on other compute nodes. Direct I/O
access can increase performance.
• Each host can have different storage profiles for hosts aggregation and availability zones.
There are several disadvantages:
• If a compute node fails, the data associated with the instances running on that node is lost.
• The chassis size of the compute node can limit the number of spindles able to be used in a compute node.
• Migrations of instances from one node to another are more complicated and rely on features that may
not continue to be developed.
• If additional storage is required, this option does not scale.
Running a shared file system on a storage system apart from the compute nodes is ideal for clouds where
reliability and scalability are the most important factors. Running a shared file system on the compute nodes
themselves may be best in a scenario where you have to deploy to pre-existing servers for which you have little
to no control over their specifications or have specific storage performance needs but do not have a need for
persistent storage.
24 Design
Architecture Design Guide (Release Version: 15.0.0)
Issues with live migration
Live migration is an integral part of the operations of the cloud. This feature provides the ability to seamlessly
move instances from one physical host to another, a necessity for performing upgrades that require reboots of
the compute hosts, but only works well with shared storage.
Live migration can also be done with non-shared storage, using a feature known as KVM live block migration.
While an earlier implementation of block-based migration in KVM and QEMU was considered unreliable, there
is a newer, more reliable implementation of block-based live migration as of the Mitaka release.
Live migration and block migration still have some issues:
• Error reporting has received some attention in Mitaka and Newton but there are improvements needed.
• Live migration resource tracking issues.
• Live migration of rescued images.
Choice of file system
If you want to support shared-storage live migration, you need to configure a distributed file system.
Possible options include:
• NFS (default for Linux)
• GlusterFS
• MooseFS
• Lustre
We recommend that you choose the option operators are most familiar with. NFS is the easiest to set up and
there is extensive community knowledge about it.
Network connectivity
The selected server hardware must have the appropriate number of network connections, as well as the right
type of network connections, in order to support the proposed architecture. Ensure that, at a minimum, there
are at least two diverse network connections coming into each rack.
The selection of form factors or architectures affects the selection of server hardware. Ensure that the selected
server hardware is configured to support enough storage capacity (or storage expandability) to match
the requirements of selected scale-out storage solution. Similarly, the network architecture impacts the server
hardware selection and vice versa.
While each enterprise install is different, the following networks with their proposed bandwidth is highly recommended
for a basic production OpenStack install.
Install or OOB network - Typically used by most distributions and provisioning tools as the network for
deploying base software to the OpenStack compute nodes. This network should be connected at a minimum of
1Gb and no routing is usually needed.
Internal or Management network - Used as the internal communication network between OpenStack compute
and control nodes. Can also be used as a network for iSCSI communication between the compute and iSCSI
storage nodes. Again, this should be a minimum of a 1Gb NIC and should be a non-routed network. This
interface should be redundant for high availability (HA).
Design 25
Architecture Design Guide (Release Version: 15.0.0)
Tenant network - A private network that enables communication between each tenant’s instances. If using flat
networking and provider networks, this network is optional. This network should also be isolated from all other
networks for security compliance. A 1Gb interface should be sufficient and redundant for HA.
Storage network - A private network which could be connected to the Ceph frontend or other shared storage.
For HA purposes this should be a redundant configuration with suggested 10Gb NICs. This network isolates
the storage for the instances away from other networks. Under load, this storage traffic could overwhelm other
networks and cause outages on other OpenStack services.
(Optional) External or Public network - This network is used to communicate externally from the VMs to
the public network space. These addresses are typically handled by the neutron agent on the controller nodes
and can also be handled by a SDN other than neutron. However, when using neutron DVR with OVS, this
network must be present on the compute node since north and south traffic will not be handled by the controller
nodes, but by the compute node itself. For more information on DVR with OVS and compute nodes, see Open
vSwitch: High availability using DVR
Compute server logging
The logs on the compute nodes, or any server running nova-compute (for example in a hyperconverged architecture),
are the primary points for troubleshooting issues with the hypervisor and compute services. Additionally,
operating system logs can also provide useful information.
As the cloud environment grows, the amount of log data increases exponentially. Enabling debugging on either
the OpenStack services or the operating system further compounds the data issues.
Logging is described in more detail in the Operations Guide. However, it is an important design consideration
to take into account before commencing operations of your cloud.
OpenStack produces a great deal of useful logging information, but for the information to be useful for operations
purposes, you should consider having a central logging server to send logs to, and a log parsing/analysis
system such as Elastic Stack [formerly known as ELK].
Elastic Stack consists of mainly three components: Elasticsearch (log search and analysis), Logstash (log intake,
processing and output) and Kibana (log dashboard service).
Due to the amount of logs being sent from servers in the OpenStack environment, an optional in-memory data
structure store can be used. Common examples are Redis and Memcached. In newer versions of Elastic Stack,
a file buffer called Filebeat is used for a similar purpose but adds a “backpressure-sensitive” protocol when
sending data to Logstash or Elasticsearch.
Log analysis often requires disparate logs of differing formats. Elastic Stack (namely Logstash) was created to
take many different log inputs and transform them into a consistent format that Elasticsearch can catalog and
26 Design
Architecture Design Guide (Release Version: 15.0.0)
analyze. As seen in the image above, the process of ingestion starts on the servers by Logstash, is forwarded to
the Elasticsearch server for storage and searching, and then displayed through Kibana for visual analysis and
interaction.
For instructions on installing Logstash, Elasticsearch and Kibana, see the Elasticsearch reference.
There are some specific configuration parameters that are needed to configure Logstash for OpenStack. For
example, in order to get Logstash to collect, parse, and send the correct portions of log files to the Elasticsearch
server, you need to format the configuration file properly. There are input, output and filter configurations.
Input configurations tell Logstash where to recieve data from (log files/forwarders/filebeats/StdIn/Eventlog),
output configurations specify where to put the data, and filter configurations define the input contents to forward
to the output.
The Logstash filter performs intermediary processing on each event. Conditional filters are applied based on
the characteristics of the input and the event. Some examples of filtering are:
• grok
• date
• csv
• json
There are also output filters available that send event data to many different destinations. Some examples are:
• csv
• redis
• elasticsearch
• file
• jira
• nagios
• pagerduty
• stdout
Additionally there are several codecs that can be used to change the data representation of events such as:
• collectd
• graphite
• json
• plan
• rubydebug
These input, output and filter configurations are typically stored in /etc/logstash/conf.d but may vary by
linux distribution. Separate configuration files should be created for different logging systems such as syslog,
Apache, and OpenStack.
General examples and configuration guides can be found on the Elastic Logstash Configuration page.
OpenStack input, output and filter examples can be found at https://github.com/sorantis/elkstack/tree/master/
elk/logstash.
Design 27
Architecture Design Guide (Release Version: 15.0.0)
Once a configuration is complete, Kibana can be used as a visualization tool for OpenStack and system logging.
This will allow operators to configure custom dashboards for performance, monitoring and security.
This section describes some of the choices you need to consider when designing and building your compute
nodes. Compute nodes form the resource core of the OpenStack Compute cloud, providing the processing,
memory, network and storage resources to run instances.
Storage design
Storage is found in many parts of the OpenStack cloud environment. This chapter describes storage type, design
considerations and options when selecting persistent storage options for your cloud environment.
Storage concepts
Storage is found in many parts of the OpenStack cloud environment. It is important to understand the distinction
between ephemeral storage and persistent storage:
• Ephemeral storage - If you only deploy OpenStack Compute service (nova), by default your users do not
have access to any form of persistent storage. The disks associated with VMs are ephemeral, meaning
that from the user’s point of view they disappear when a virtual machine is terminated.
• Persistent storage - Persistent storage means that the storage resource outlives any other resource and is
always available, regardless of the state of a running instance.
OpenStack clouds explicitly support three types of persistent storage: Object Storage, Block Storage, and Filebased
storage.
Object storage
Object storage is implemented in OpenStack by the Object Storage service (swift). Users access binary objects
through a REST API. If your intended users need to archive or manage large datasets, you should provide them
with Object Storage service. Additional benefits include:
• OpenStack can store your virtual machine (VM) images inside of an Object Storage system, as an alternative
to storing the images on a file system.
• Integration with OpenStack Identity, and works with the OpenStack Dashboard.
• Better support for distributed deployments across multiple datacenters through support for asynchronous
eventual consistency replication.
You should consider using the OpenStack Object Storage service if you eventually plan on distributing your
storage cluster across multiple data centers, if you need unified accounts for your users for both compute and object
storage, or if you want to control your object storage with the OpenStack Dashboard. For more information,
see the Swift project page.
Block storage
The Block Storage service (cinder) in OpenStacs. Because these volumes are persistent, they can be detached
from one instance and re-attached to another instance and the data remains intact.
The Block Storage service supports multiple back ends in the form of drivers. Your choice of a storage back
end must be supported by a block storage driver.
28 Design
Architecture Design Guide (Release Version: 15.0.0)
Most block storage drivers allow the instance to have direct access to the underlying storage hardware’s block
device. This helps increase the overall read/write IO. However, support for utilizing files as volumes is also
well established, with full support for NFS, GlusterFS and others.
These drivers work a little differently than a traditional block storage driver. On an NFS or GlusterFS file
system, a single file is created and then mapped as a virtual volume into the instance. This mapping and
translation is similar to how OpenStack utilizes QEMU’s file-based virtual machines stored in /var/lib/
nova/instances.
File-based storage
In multi-tenant OpenStack cloud environment, the Shared File Systems service (manila) provides a set of services
for management of shared file systems. The Shared File Systems service supports multiple back-ends in
the form of drivers, and can be configured to provision shares from one or more back-ends. Share servers are
virtual machines that export file shares using different file system protocols such as NFS, CIFS, GlusterFS, or
HDFS.
The Shared File Systems service is persistent storage and can be mounted to any number of client machines. It
can also be detached from one instance and attached to another instance without data loss. During this process
the data are safe unless the Shared File Systems service itself is changed or removed.
Users interact with the Shared File Systems service by mounting remote file systems on their instances with
the following usage of those systems for file storing and exchange. The Shared File Systems service provides
shares which is a remote, mountable file system. You can mount a share and access a share from several hosts
by several users at a time. With shares, you can also:
• Create a share specifying its size, shared file system protocol, visibility level.
• Create a share on either a share server or standalone, depending on the selected back-end mode, with or
without using a share network.
• Specify access rules and security services for existing shares.
• Combine several shares in groups to keep data consistency inside the groups for the following safe group
operations.
• Create a snapshot of a selected share or a share group for storing the existing shares consistently or
creating new shares from that snapshot in a consistent way.
• Create a share from a snapshot.
• Set rate limits and quotas for specific shares and snapshots.
• View usage of share resources.
• Remove shares.
Differences between storage types
Table. OpenStack storage explains the differences between Openstack storage types.
Design 29
Architecture Design Guide (Release Version: 15.0.0)
Table 1: Table. OpenStack storage
Ephemeral storage Block storage Object storage Shared File System
storage
Application Run operating
system and scratch
space
Add additional
persistent storage
to a virtual
machine (VM)
Store data, including
VM images
Add additional
persistent storage
to a virtual
machine
Accessed
through…
A file system A block device
that can be partitioned,
formatted,
and mounted (such
as, /dev/vdc)
The REST API A Shared File Systems
service share
(either manila
managed or an
external one registered
in manila)
that can be partitioned,
formatted
and mounted (such
as /dev/vdc)
Accessible from… Within a VM Within a VM Anywhere Within a VM
Managed by… OpenStack Compute
(nova)
OpenStack Block
Storage (cinder)
OpenStack Object
Storage (swift)
OpenStack Shared
File System Storage
(manila)
Persists until… VM is terminated Deleted by user Deleted by user Deleted by user
Sizing determined
by…
Administrator configuration
of size
settings, known as
flavors
User specification
in initial request
Amount of available
physical storage
• User specification
in initial
request
• Requests
for extension
• Available
user-level
quotes
• Limitations
applied by
Administrator
Encryption configuration
Parameter in
nova.conf
Admin establishing
encrypted
volume type, then
user selecting
encrypted volume
Not yet available Shared File Systems
service does
not apply any additional
encryption
above what the
share’s back-end
storage provides
Example of typical
usage…
10 GB first disk,
30 GB second disk
1 TB disk 10s of TBs of
dataset storage
Depends completely
on the size
of back-end storage
specified when
a share was being
created. In case of
thin provisioning
it can be partial
space reservation
(for more details
see Capabilities
and Extra-Specs
specification)
30 Design
Architecture Design Guide (Release Version: 15.0.0)
Note: File-level storage for live migration
With file-level storage, users access stored data using the operating system’s file system interface. Most users
who have used a network storage solution before have encountered this form of networked storage. The most
common file system protocol for Unix is NFS, and for Windows, CIFS (previously, SMB).
OpenStack clouds do not present file-level storage to end users. However, it is important to consider file-level
storage for storing instances under /var/lib/nova/instances when designing your cloud, since you must
have a shared file system if you want to support live migration.
Commodity storage technologies
There are various commodity storage back end technologies available. Depending on your cloud user’s needs,
you can implement one or many of these technologies in different combinations.
Ceph
Ceph is a scalable storage solution that replicates data across commodity storage nodes.
Ceph utilises and object storage mechanism for data storage and exposes the data via different types of storage
interfaces to the end user it supports interfaces for: - Object storage - Block storage - File-system interfaces
Ceph provides support for the same Object Storage API as swift and can be used as a back end for the Block
Storage service (cinder) as well as back-end storage for glance images.
Ceph supports thin provisioning implemented using copy-on-write. This can be useful when booting from volume
because a new volume can be provisioned very quickly. Ceph also supports keystone-based authentication
(as of version 0.56), so it can be a seamless swap in for the default OpenStack swift implementation.
Ceph’s advantages include:
• The administrator has more fine-grained control over data distribution and replication strategies.
• Consolidation of object storage and block storage.
• Fast provisioning of boot-from-volume instances using thin provisioning.
• Support for the distributed file-system interface CephFS.
You should consider Ceph if you want to manage your object and block storage within a single system, or if
you want to support fast boot-from-volume.
Gluster
A distributed shared file system. As of Gluster version 3.3, you can use Gluster to consolidate your object
storage and file storage into one unified file and object storage solution, which is called Gluster For OpenStack
(GFO). GFO uses a customized version of swift that enables Gluster to be used as the back-end storage.
The main reason to use GFO rather than swift is if you also want to support a distributed file system, either to
support shared storage live migration or to provide it as a separate service to your end users. If you want to
manage your object and file storage within a single system, you should consider GFO.
Design 31
Architecture Design Guide (Release Version: 15.0.0)
LVM
The Logical Volume Manager (LVM) is a Linux-based system that provides an abstraction layer on top of
physical disks to expose logical volumes to the operating system. The LVM back-end implements block storage
as LVM logical partitions.
On each host that will house block storage, an administrator must initially create a volume group dedicated to
Block Storage volumes. Blocks are created from LVM logical volumes.
Note: LVM does not provide any replication. Typically, administrators configure RAID on nodes that use
LVM as block storage to protect against failures of individual hard drives. However, RAID does not protect
against a failure of the entire host.
iSCSI
Internet Small Computer Systems Interface (iSCSI) is a network protocol that operates on top of the Transport
Control Protocol (TCP) for linking data storage devices. It transports data between an iSCSI initiator on a server
and iSCSI target on a storage device.
iSCSI is suitable for cloud environments with Block Storage service to support applications or for file sharing
systems. Network connectivity can be achieved at a lower cost compared to other storage back end technologies
since iSCSI does not require host bus adaptors (HBA) or storage-specific network devices.
NFS
Network File System (NFS) is a file system protocol that allows a user or administrator to mount a file system
on a server. File clients can access mounted file systems through Remote Procedure Calls (RPC).
The benefits of NFS is low implementation cost due to shared NICs and traditional network components, and
a simpler configuration and setup process.
For more information on configuring Block Storage to use NFS storage, see Configure an NFS storage back
end in the OpenStack Administrator Guide.
Sheepdog
Sheepdog is a userspace distributed storage system. Sheepdog scales to several hundred nodes, and has powerful
virtual disk management features like snapshot, cloning, rollback and thin provisioning.
It is essentially an object storage system that manages disks and aggregates the space and performance of disks
linearly in hyper scale on commodity hardware in a smart way. On top of its object store, Sheepdog provides
elastic volume service and http service. Sheepdog does require a specific kernel version and can work nicely
with xattr-supported file systems.
ZFS
The Solaris iSCSI driver for OpenStack Block Storage implements blocks as ZFS entities. ZFS is a file system
that also has the functionality of a volume manager. This is unlike on a Linux system, where there is a separation
32 Design
Architecture Design Guide (Release Version: 15.0.0)
of volume manager (LVM) and file system (such as, ext3, ext4, xfs, and btrfs). ZFS has a number of advantages
over ext4, including improved data-integrity checking.
The ZFS back end for OpenStack Block Storage supports only Solaris-based systems, such as Illumos. While
there is a Linux port of ZFS, it is not included in any of the standard Linux distributions, and it has not been
tested with OpenStack Block Storage. As with LVM, ZFS does not provide replication across hosts on its own,
you need to add a replication solution on top of ZFS if your cloud needs to be able to handle storage-node
failures.
Storage architecture
There are many different storage architectures available when designing an OpenStack cloud. The convergence
of orchestration and automation within the OpenStack platform enables rapid storage provisioning without the
hassle of the traditional manual processes like volume creation and attachment.
However, before choosing a storage architecture, a few generic questions should be answered:
• Will the storage architecture scale linearly as the cloud grows and what are its limits?
• What is the desired attachment method: NFS, iSCSI, FC, or other?
• Is the storage proven with the OpenStack platform?
• What is the level of support provided by the vendor within the community?
• What OpenStack features and enhancements does the cinder driver enable?
• Does it include tools to help troubleshoot and resolve performance issues?
• Is it interoperable with all of the projects you are planning on using in your cloud?
Choosing storage back ends
Users will indicate different needs for their cloud architecture. Some may need fast access to many objects that
do not change often, or want to set a time-to-live (TTL) value on a file. Others may access only storage that is
mounted with the file system itself, but want it to be replicated instantly when starting a new instance. For other
systems, ephemeral storage is the preferred choice. When you select storage back ends, consider the following
questions from user’s perspective:
First and foremost:
• Do I need block storage?
• Do I need object storage?
• Do I need file-based storage?
Next answer the following:
• Do I need to support live migration?
• Should my persistent storage drives be contained in my compute nodes, or should I use external storage?
• What type of performance do I need in regards to IOPS? Total IOPS and IOPS per instance? Do I have
applications with IOPS SLAs?
• Are my storage needs mostly read, or write, or mixed?
• Which storage choices result in the best cost-performance scenario I am aiming for?
Design 33
Architecture Design Guide (Release Version: 15.0.0)
• How do I manage the storage operationally?
• How redundant and distributed is the storage? What happens if a storage node fails? To what extent can
it mitigate my data-loss disaster scenarios?
• What is my company currently using and can I use it with OpenStack?
• Do I need more than one storage choice? Do I need tiered performance storage?
While this is not a definitive list of all the questions possible, the list above will hopefully help narrow the list
of possible storage choices down.
A wide variety of use case requirements dictate the nature of the storage back end. Examples of such requirements
are as follows:
• Public, private, or a hybrid cloud (performance profiles, shared storage, replication options)
• Storage-intensive use cases like HPC and Big Data clouds
• Web-scale or development clouds where storage is typically ephemeral in nature
Data security recommendations:
• We recommend that data be encrypted both in transit and at-rest. To this end, carefully select disks,
appliances, and software. Do not assume these features are included with all storage solutions.
• Determine the security policy of your organization and understand the data sovereignty of your cloud
geography and plan accordingly.
If you plan to use live migration, we highly recommend a shared storage configuration. This allows the operating
system and application volumes for instances to reside outside of the compute nodes and adds significant
performance increases when live migrating.
To deploy your storage by using only commodity hardware, you can use a number of open-source packages, as
described in Persistent file-based storage support.
34 Design
Architecture Design Guide (Release Version: 15.0.0)
Table 2: Persistent file-based storage support
Object Block File-level
Swift
LVM
Ceph Experimental
Gluster
NFS
ZFS
Sheepdog
This list of open source file-level shared storage solutions is not exhaustive. Your organization may already
have deployed a file-level shared storage solution that you can use.
Note: Storage driver support
In addition to the open source technologies, there are a number of proprietary solutions that are officially supported
by OpenStack Block Storage. You can find a matrix of the functionality provided by all of the supported
Block Storage drivers on the CinderSupportMatrix wiki.
Also, you need to decide whether you want to support object storage in your cloud. The two common use cases
for providing object storage in a compute cloud are to provide:
• Users with a persistent storage mechanism for objects like images and video.
• A scalable, reliable data store for OpenStack virtual machine images.
• An API driven S3 compatible object store for application use.
Selecting storage hardware
Storage hardware architecture is determined by selecting specific storage architecture. Determine the selection
of storage architecture by evaluating possible solutions against the critical factors, the user requirements,
technical considerations, and operational considerations. Consider the following factors when selecting storage
hardware:
Cost Storage can be a significant portion of the overall system cost. For an organization that is concerned with
vendor support, a commercial storage solution is advisable, although it comes with a higher price tag.
If initial capital expenditure requires minimization, designing a system based on commodity hardware
Design 35
Architecture Design Guide (Release Version: 15.0.0)
would apply. The trade-off is potentially higher support costs and a greater risk of incompatibility and
interoperability issues.
Performance The latency of storage I/O requests indicates performance. Performance requirements affect
which solution you choose.
Scalability Scalability, along with expandability, is a major consideration in a general purpose OpenStack
cloud. It might be difficult to predict the final intended size of the implementation as there are no established
usage patterns for a general purpose cloud. It might become necessary to expand the initial
deployment in order to accommodate growth and user demand.
Expandability Expandability is a major architecture factor for storage solutions with general purpose OpenStack
cloud. A storage solution that expands to 50 PB is considered more expandable than a solution that
only scales to 10 PB. This meter is related to scalability, which is the measure of a solution’s performance
as it expands.
Implementing Block Storage
Configure Block Storage resource nodes with advanced RAID controllers and high-performance disks to provide
fault tolerance at the hardware level.
Deploy high performing storage solutions such as SSD drives or flash storage systems for applications requiring
additional performance out of Block Storage devices.
In environments that place substantial demands on Block Storage, we recommend using multiple storage pools.
In this case, each pool of devices should have a similar hardware design and disk configuration across all hardware
nodes in that pool. This allows for a design that provides applications with access to a wide variety of
Block Storage pools, each with their own redundancy, availability, and performance characteristics. When deploying
multiple pools of storage, it is also important to consider the impact on the Block Storage scheduler
which is responsible for provisioning storage across resource nodes. Ideally, ensure that applications can schedule
volumes in multiple regions, each with their own network, power, and cooling infrastructure. This will give
tenants the option of building fault-tolerant applications that are distributed across multiple availability zones.
In addition to the Block Storage resource nodes, it is important to design for high availability and redundancy
of the APIs, and related services that are responsible for provisioning and providing access to storage. We
recommend designing a layer of hardware or software load balancers in order to achieve high availability of
the appropriate REST API services to provide uninterrupted service. In some cases, it may also be necessary
to deploy an additional layer of load balancing to provide access to back-end database services responsible for
servicing and storing the state of Block Storage volumes. It is imperative that a highly available database cluster
is used to store the Block Storage metadata.
In a cloud with significant demands on Block Storage, the network architecture should take into account the
amount of East-West bandwidth required for instances to make use of the available storage resources. The selected
network devices should support jumbo frames for transferring large blocks of data, and utilize a dedicated
network for providing connectivity between instances and Block Storage.
Implementing Object Storage
While consistency and partition tolerance are both inherent features of the Object Storage service, it is important
to design the overall storage architecture to ensure that the implemented system meets those goals. The OpenStack
Object Storage service places a specific number of data replicas as objects on resource nodes. Replicas
are distributed throughout the cluster, based on a consistent hash ring also stored on each node in the cluster.
36 Design
Architecture Design Guide (Release Version: 15.0.0)
When designing your cluster, you must consider durability and availability which is dependent on the spread
and placement of your data, rather than the reliability of the hardware.
Consider the default value of the number of replicas, which is three. This means that before an object is marked
as having been written, at least two copies exist in case a single server fails to write, the third copy may or may
not yet exist when the write operation initially returns. Altering this number increases the robustness of your
data, but reduces the amount of storage you have available. Look at the placement of your servers. Consider
spreading them widely throughout your data center’s network and power-failure zones. Is a zone a rack, a
server, or a disk?
Consider these main traffic flows for an Object Storage network:
• Among object, container, and account servers
• Between servers and the proxies
• Between the proxies and your users
Object Storage frequent communicates among servers hosting data. Even a small cluster generates megabytes
per second of traffic.
Consider the scenario where an entire server fails and 24 TB of data needs to be transferred “immediately” to
remain at three copies — this can put significant load on the network.
Another consideration is when a new file is being uploaded, the proxy server must write out as many streams
as there are replicas, multiplying network traffic. For a three-replica cluster, 10 Gbps in means 30 Gbps out.
Combining this with the previous high bandwidth bandwidth private versus public network recommendations
demands of replication is what results in the recommendation that your private network be of significantly
higher bandwidth than your public network requires. OpenStack Object Storage communicates internally with
unencrypted, unauthenticated rsync for performance, so the private network is required.
The remaining point on bandwidth is the public-facing portion. The swift-proxy service is stateless, which
means that you can easily add more and use HTTP load-balancing methods to share bandwidth and availability
between them. More proxies means more bandwidth.
You should consider designing the Object Storage system with a sufficient number of zones to provide quorum
for the number of replicas defined. For example, with three replicas configured in the swift cluster, the recommended
number of zones to configure within the Object Storage cluster in order to achieve quorum is five.
While it is possible to deploy a solution with fewer zones, the implied risk of doing so is that some data may
not be available and API requests to certain objects stored in the cluster might fail. For this reason, ensure you
properly account for the number of zones in the Object Storage cluster.
Each Object Storage zone should be self-contained within its own availability zone. Each availability zone
should have independent access to network, power, and cooling infrastructure to ensure uninterrupted access
to data. In addition, a pool of Object Storage proxy servers providing access to data stored on the object
nodes should service each availability zone. Object proxies in each region should leverage local read and write
affinity so that local storage resources facilitate access to objects wherever possible. We recommend deploying
upstream load balancing to ensure that proxy services are distributed across the multiple zones and, in some
cases, it may be necessary to make use of third-party solutions to aid with geographical distribution of services.
A zone within an Object Storage cluster is a logical division. Any of the following may represent a zone:
• A disk within a single node
• One zone per node
• Zone per collection of nodes
• Multiple racks
Design 37
Architecture Design Guide (Release Version: 15.0.0)
• Multiple data centers
Selecting the proper zone design is crucial for allowing the Object Storage cluster to scale while providing an
available and redundant storage system. It may be necessary to configure storage policies that have different
requirements with regards to replicas, retention, and other factors that could heavily affect the design of storage
in a specific zone.
Planning and scaling storage capacity
An important consideration in running a cloud over time is projecting growth and utilization trends in order
to plan capital expenditures for the short and long term. Gather utilization meters for compute, network, and
storage, along with historical records of these meters. While securing major anchor tenants can lead to rapid
jumps in the utilization of resources, the average rate of adoption of cloud services through normal usage also
needs to be carefully monitored.
Scaling Block Storage
You can upgrade Block Storage pools to add storage capacity without interrupting the overall Block Storage
service. Add nodes to the pool by installing and configuring the appropriate hardware and software and then
allowing that node to report in to the proper storage pool through the message bus. Block Storage nodes generally
report into the scheduler service advertising their availability. As a result, after the node is online and
available, tenants can make use of those storage resources instantly.
In some cases, the demand on Block Storage may exhaust the available network bandwidth. As a result, design
network infrastructure that services Block Storage resources in such a way that you can add capacity and
bandwidth easily. This often involves the use of dynamic routing protocols or advanced networking solutions
to add capacity to downstream devices easily. Both the front-end and back-end storage network designs should
encompass the ability to quickly and easily add capacity and bandwidth.
Note: Sufficient monitoring and data collection should be in-place from the start, such that timely decisions
regarding capacity, input/output metrics (IOPS) or storage-associated bandwidth can be made.
Scaling Object Storage
Adding back-end storage capacity to an Object Storage cluster requires careful planning and forethought. In the
design phase, it is important to determine the maximum partition power required by the Object Storage service,
which determines the maximum number of partitions which can exist. Object Storage distributes data among
all available storage, but a partition cannot span more than one disk, so the maximum number of partitions can
only be as high as the number of disks.
For example, a system that starts with a single disk and a partition power of 3 can have 8 (2^3) partitions.
Adding a second disk means that each has 4 partitions. The one-disk-per-partition limit means that this system
can never have more than 8 disks, limiting its scalability. However, a system that starts with a single disk and
a partition power of 10 can have up to 1024 (2^10) disks.
As you add back-end storage capacity to the system, the partition maps redistribute data amongst the storage
nodes. In some cases, this involves replication of extremely large data sets. In these cases, we recommend
using back-end replication links that do not contend with tenants’ access to data.
38 Design
Architecture Design Guide (Release Version: 15.0.0)
As more tenants begin to access data within the cluster and their data sets grow, it is necessary to add front-end
bandwidth to service data access requests. Adding front-end bandwidth to an Object Storage cluster requires
careful planning and design of the Object Storage proxies that tenants use to gain access to the data, along with
the high availability solutions that enable easy scaling of the proxy layer. We recommend designing a front-end
load balancing layer that tenants and consumers use to gain access to data stored within the cluster. This load
balancing layer may be distributed across zones, regions or even across geographic boundaries, which may also
require that the design encompass geo-location solutions.
In some cases, you must add bandwidth and capacity to the network resources servicing requests between proxy
servers and storage nodes. For this reason, the network architecture used for access to storage nodes and proxy
servers should make use of a design which is scalable.
Redundancy
Replication
Networking
Networking concepts
A cloud environment fundamentally changes the ways that networking is provided and consumed. Understanding
the following concepts and decisions is imperative when making architectural decisions. For detailed
information on networking concepts, see the OpenStack Networking Guide.
Network zones
The cloud networks are divided into a number of logical zones that support the network traffic flow requirements.
We recommend defining at the least four distinct network zones.
Underlay
The underlay zone is defined as the physical network switching infrastructure that connects the storage, compute
and control platforms. There are a large number of potential underlay options available.
Overlay
The overlay zone is defined as any L3 connectivity between the cloud components and could take the form of
SDN solutions such as the neutron overlay solution or 3rd Party SDN solutions.
Edge
The edge zone is where network traffic transitions from the cloud overlay or SDN networks into the traditional
network environments.
Design 39
Architecture Design Guide (Release Version: 15.0.0)
External
The external network is defined as the configuration and components that are required to provide access to
cloud resources and workloads, the external network is defined as all the components outside of the cloud edge
gateways.
Traffic flow
There are two primary types of traffic flow within a cloud infrastructure, the choice of networking technologies
is influenced by the expected loads.
East/West - The internal traffic flow between workload within the cloud as well as the traffic flow between the
compute nodes and storage nodes falls into the East/West category. Generally this is the heaviest traffic flow
and due to the need to cater for storage access needs to cater for a minimum of hops and low latency.
North/South - The flow of traffic between the workload and all external networks, including clients and remote
services. This traffic flow is highly dependant on the workload within the cloud and the type of network services
being offered.
Layer networking choices
There are several factors to take into consideration when deciding on whether to use Layer 2 networking architecture
or a layer 3 networking architecture. For more information about OpenStack networking concepts, see
the OpenStack Networking section in the OpenStack Networking Guide.
Benefits using a Layer-2 network
There are several reasons a network designed on layer-2 protocols is selected over a network designed on layer-
3 protocols. In spite of the difficulties of using a bridge to perform the network role of a router, many vendors,
customers, and service providers choose to use Ethernet in as many parts of their networks as possible. The
benefits of selecting a layer-2 design are:
• Ethernet frames contain all the essentials for networking. These include, but are not limited to, globally
unique source addresses, globally unique destination addresses, and error control.
• Ethernet frames contain all the essentials for networking. These include, but are not limited to, globally
unique source addresses, globally unique destination addresses, and error control.
• Ethernet frames can carry any kind of packet. Networking at layer-2 is independent of the layer-3 protocol.
• Adding more layers to the Ethernet frame only slows the networking process down. This is known as
nodal processing delay.
• You can add adjunct networking features, for example class of service (CoS) or multicasting, to Ethernet
as readily as IP networks.
• VLANs are an easy mechanism for isolating networks.
Most information starts and ends inside Ethernet frames. Today this applies to data, voice, and video. The
concept is that the network will benefit more from the advantages of Ethernet if the transfer of information
from a source to a destination is in the form of Ethernet frames.
40 Design
Architecture Design Guide (Release Version: 15.0.0)
Although it is not a substitute for IP networking, networking at layer-2 can be a powerful adjunct to IP networking.
Layer-2 Ethernet usage has additional benefits over layer-3 IP network usage:
• Speed
• Reduced overhead of the IP hierarchy.
• No need to keep track of address configuration as systems move around.
Whereas the simplicity of layer-2 protocols might work well in a data center with hundreds of physical machines,
cloud data centers have the additional burden of needing to keep track of all virtual machine addresses and
networks. In these data centers, it is not uncommon for one physical node to support 30-40 instances.
Important: Networking at the frame level says nothing about the presence or absence of IP addresses at the
packet level. Almost all ports, links, and devices on a network of LAN switches still have IP addresses, as do
all the source and destination hosts. There are many reasons for the continued need for IP addressing. The
largest one is the need to manage the network. A device or link without an IP address is usually invisible to
most management applications. Utilities including remote access for diagnostics, file transfer of configurations
and software, and similar applications cannot run without IP addresses as well as MAC addresses.
Layer-2 architecture limitations
Layer-2 network architectures have some limitations that become noticeable when used outside of traditional
data centers.
• Number of VLANs is limited to 4096.
• The number of MACs stored in switch tables is limited.
• You must accommodate the need to maintain a set of layer-4 devices to handle traffic control.
• MLAG, often used for switch redundancy, is a proprietary solution that does not scale beyond two devices
and forces vendor lock-in.
• It can be difficult to troubleshoot a network without IP addresses and ICMP.
• Configuring ARP can be complicated on a large layer-2 networks.
• All network devices need to be aware of all MACs, even instance MACs, so there is constant churn in
MAC tables and network state changes as instances start and stop.
• Migrating MACs (instance migration) to different physical locations are a potential problem if you do
not set ARP table timeouts properly.
It is important to know that layer-2 has a very limited set of network management tools. It is difficult to control
traffic as it does not have mechanisms to manage the network or shape the traffic. Network troubleshooting is
also troublesome, in part because network devices have no IP addresses. As a result, there is no reasonable way
to check network delay.
In a layer-2 network all devices are aware of all MACs, even those that belong to instances. The network state
information in the backbone changes whenever an instance starts or stops. Because of this, there is far too much
churn in the MAC tables on the backbone switches.
Furthermore, on large layer-2 networks, configuring ARP learning can be complicated. The setting for the
MAC address timer on switches is critical and, if set incorrectly, can cause significant performance problems.
Design 41
Architecture Design Guide (Release Version: 15.0.0)
So when migrating MACs to different physical locations to support instance migration, problems may arise.
As an example, the Cisco default MAC address timer is extremely long. As such, the network information
maintained in the switches could be out of sync with the new location of the instance.
Benefits using a Layer-3 network
In layer-3 networking, routing takes instance MAC and IP addresses out of the network core, reducing state
churn. The only time there would be a routing state change is in the case of a Top of Rack (ToR) switch failure
or a link failure in the backbone itself. Other advantages of using a layer-3 architecture include:
• Layer-3 networks provide the same level of resiliency and scalability as the Internet.
• Controlling traffic with routing metrics is straightforward.
• You can configure layer-3 to use Border Gateway Protocol (BGP) confederation for scalability. This way
core routers have state proportional to the number of racks, not to the number of servers or instances.
• There are a variety of well tested tools, such as Internet Control Message Protocol (ICMP) to monitor
and manage traffic.
• Layer-3 architectures enable the use of quality of service (QoS) to manage network performance.
Layer-3 architecture limitations
The main limitation of layer-3 networking is that there is no built-in isolation mechanism comparable to the
VLANs in layer-2 networks. Furthermore, the hierarchical nature of IP addresses means that an instance is on
the same subnet as its physical host, making migration out of the subnet difficult. For these reasons, network
virtualization needs to use IP encapsulation and software at the end hosts. This is for isolation and the separation
of the addressing in the virtual layer from the addressing in the physical layer. Other potential disadvantages
of layer-3 networking include the need to design an IP addressing scheme rather than relying on the switches
to keep track of the MAC addresses automatically, and to configure the interior gateway routing protocol in the
switches.
Networking service (neutron)
OpenStack Networking (neutron) is the component of OpenStack that provides the Networking service API
and a reference architecture that implements a Software Defined Network (SDN) solution.
The Networking service provides full control over creation of virtual network resources to tenants. This is often
accomplished in the form of tunneling protocols that establish encapsulated communication paths over existing
network infrastructure in order to segment tenant traffic. This method varies depending on the specific implementation,
but some of the more common methods include tunneling over GRE, encapsulating with VXLAN,
and VLAN tags.
Designing an OpenStack network
There are many reasons an OpenStack network has complex requirements. One main factor is that many components
interact at different levels of the system stack. Data flows are also complex.
Data in an OpenStack cloud moves between instances across the network (known as east-west traffic), as well
as in and out of the system (known as north-south traffic). Physical server nodes have network requirements
42 Design
Architecture Design Guide (Release Version: 15.0.0)
that are independent of instance network requirements and must be isolated to account for scalability. We
recommend separating the networks for security purposes and tuning performance through traffic shaping.
You must consider a number of important technical and business requirements when planning and designing an
OpenStack network:
• Avoid hardware or software vendor lock-in. The design should not rely on specific features of a vendor’s
network router or switch.
• Massively scale the ecosystem to support millions of end users.
• Support an indeterminate variety of platforms and applications.
• Design for cost efficient operations to take advantage of massive scale.
• Ensure that there is no single point of failure in the cloud ecosystem.
• High availability architecture to meet customer SLA requirements.
• Tolerant to rack level failure.
• Maximize flexibility to architect future production environments.
Considering these requirements, we recommend the following:
• Design a Layer-3 network architecture rather than a layer-2 network architecture.
• Design a dense multi-path network core to support multi-directional scaling and flexibility.
• Use hierarchical addressing because it is the only viable option to scale a network ecosystem.
• Use virtual networking to isolate instance service network traffic from the management and internal
network traffic.
• Isolate virtual networks using encapsulation technologies.
• Use traffic shaping for performance tuning.
• Use External Border Gateway Protocol (eBGP) to connect to the Internet up-link.
• Use Internal Border Gateway Protocol (iBGP) to flatten the internal traffic on the layer-3 mesh.
• Determine the most effective configuration for block storage network.
Additional network design considerations
There are several other considerations when designing a network-focused OpenStack cloud.
Redundant networking
You should conduct a high availability risk analysis to determine whether to use redundant switches such as
Top of Rack (ToR) switches. In most cases, it is much more economical to use single switches with a small
pool of spare switches to replace failed units than it is to outfit an entire data center with redundant switches.
Applications should tolerate rack level outages without affecting normal operations since network and compute
resources are easily provisioned and plentiful.
Research indicates the mean time between failures (MTBF) on switches is between 100,000 and 200,000 hours.
This number is dependent on the ambient temperature of the switch in the data center. When properly cooled and
Design 43
Architecture Design Guide (Release Version: 15.0.0)
maintained, this translates to between 11 and 22 years before failure. Even in the worst case of poor ventilation
and high ambient temperatures in the data center, the MTBF is still 2-3 years.
Providing IPv6 support
One of the most important networking topics today is the exhaustion of IPv4 addresses. As of late 2015,
ICANN announced that the final IPv4 address blocks have been fully assigned. Because of this, IPv6 protocol
has become the future of network focused applications. IPv6 increases the address space significantly, fixes
long standing issues in the IPv4 protocol, and will become essential for network focused applications in the
future.
OpenStack Networking, when configured for it, supports IPv6. To enable IPv6, create an IPv6 subnet in Networking
and use IPv6 prefixes when creating security groups.
Supporting asymmetric links
When designing a network architecture, the traffic patterns of an application heavily influence the allocation
of total bandwidth and the number of links that you use to send and receive traffic. Applications that provide
file storage for customers allocate bandwidth and links to favor incoming traffic; whereas video streaming
applications allocate bandwidth and links to favor outgoing traffic.
Optimizing network performance
It is important to analyze the applications tolerance for latency and jitter when designing an environment to
support network focused applications. Certain applications, for example VoIP, are less tolerant of latency and
jitter. When latency and jitter are issues, certain applications may require tuning of QoS parameters and network
device queues to ensure that they immediately queue for transmitting or guarantee minimum bandwidth. Since
OpenStack currently does not support these functions, consider carefully your selected network plug-in.
The location of a service may also impact the application or consumer experience. If an application serves
differing content to different users, it must properly direct connections to those specific locations. Where appropriate,
use a multi-site installation for these situations.
You can implement networking in two separate ways. Legacy networking (nova-network) provides a flat DHCP
network with a single broadcast domain. This implementation does not support tenant isolation networks or
advanced plug-ins, but it is currently the only way to implement a distributed layer-3 (L3) agent using the multihost
configuration. The Networking service (neutron) is the official networking implementation and provides
a pluggable architecture that supports a large variety of network methods. Some of these include a layer-2 only
provider network model, external device plug-ins, or even OpenFlow controllers.
Networking at large scales becomes a set of boundary questions. The determination of how large a layer-2
domain must be is based on the number of nodes within the domain and the amount of broadcast traffic that
passes between instances. Breaking layer-2 boundaries may require the implementation of overlay networks
and tunnels. This decision is a balancing act between the need for a smaller overhead or a need for a smaller
domain.
When selecting network devices, be aware that making a decision based on the greatest port density often comes
with a drawback. Aggregation switches and routers have not all kept pace with ToR switches and may induce
bottlenecks on north-south traffic. As a result, it may be possible for massive amounts of downstream network
utilization to impact upstream network devices, impacting service to the cloud. Since OpenStack does not
44 Design
Architecture Design Guide (Release Version: 15.0.0)
currently provide a mechanism for traffic shaping or rate limiting, it is necessary to implement these features
at the network hardware level.
Using tunable networking components
Consider configurable networking components related to an OpenStack architecture design when designing for
network intensive workloads that include MTU and QoS. Some workloads require a larger MTU than normal
due to the transfer of large blocks of data. When providing network service for applications such as video
streaming or storage replication, we recommend that you configure both OpenStack hardware nodes and the
supporting network equipment for jumbo frames where possible. This allows for better use of available bandwidth.
Configure jumbo frames across the complete path the packets traverse. If one network component is not
capable of handling jumbo frames then the entire path reverts to the default MTU.
Quality of Service (QoS) also has a great impact on network intensive workloads as it provides instant service
to packets which have a higher priority due to the impact of poor network performance. In applications such
as Voice over IP (VoIP), differentiated services code points are a near requirement for proper operation. You
can also use QoS in the opposite direction for mixed workloads to prevent low priority but high bandwidth
applications, for example backup services, video conferencing, or file sharing, from blocking bandwidth that
is needed for the proper operation of other workloads. It is possible to tag file storage traffic as a lower class,
such as best effort or scavenger, to allow the higher priority traffic through. In cases where regions within a
cloud might be geographically distributed it may also be necessary to plan accordingly to implement WAN
optimization to combat latency or packet loss.
Choosing network hardware
The network architecture determines which network hardware will be used. Networking software is determined
by the selected networking hardware.
There are more subtle design impacts that need to be considered. The selection of certain networking hardware
(and the networking software) affects the management tools that can be used. There are exceptions to this; the
rise of open networking software that supports a range of networking hardware means there are instances where
the relationship between networking hardware and networking software are not as tightly defined.
Some of the key considerations in the selection of networking hardware include:
Port count The design will require networking hardware that has the requisite port count.
Port density The network design will be affected by the physical space that is required to provide the requisite
port count. A higher port density is preferred, as it leaves more rack space for compute or storage
components. This can also lead into considerations about fault domains and power density. Higher
density switches are more expensive, therefore it is important not to over design the network.
Port speed The networking hardware must support the proposed network speed, for example: 1 GbE, 10 GbE,
or 40 GbE (or even 100 GbE).
Redundancy User requirements for high availability and cost considerations influence the level of network
hardware redundancy. Network redundancy can be achieved by adding redundant power supplies or
paired switches.
Note: Hardware must support network redundancy.
Design 45
Architecture Design Guide (Release Version: 15.0.0)
Power requirements Ensure that the physical data center provides the necessary power for the selected network
hardware.
Note: This is not an issue for top of rack (ToR) switches. This may be an issue for spine switches in a
leaf and spine fabric, or end of row (EoR) switches.
Protocol support It is possible to gain more performance out of a single storage system by using specialized
network technologies such as RDMA, SRP, iSER and SCST. The specifics of using these technologies
is beyond the scope of this book.
There is no single best practice architecture for the networking hardware supporting an OpenStack cloud. Some
of the key factors that will have a major influence on selection of networking hardware include:
Connectivity All nodes within an OpenStack cloud require network connectivity. In some cases, nodes require
access to more than one network segment. The design must encompass sufficient network capacity and
bandwidth to ensure that all communications within the cloud, both north-south and east-west traffic,
have sufficient resources available.
Scalability The network design should encompass a physical and logical network design that can be easily
expanded upon. Network hardware should offer the appropriate types of interfaces and speeds that are
required by the hardware nodes.
Availability To ensure access to nodes within the cloud is not interrupted, we recommend that the network architecture
identifies any single points of failure and provides some level of redundancy or fault tolerance.
The network infrastructure often involves use of networking protocols such as LACP, VRRP or others
to achieve a highly available network connection. It is also important to consider the networking implications
on API availability. We recommend a load balancing solution is designed within the network
architecture to ensure that the APIs and potentially other services in the cloud are highly available.
Choosing networking software
OpenStack Networking (neutron) provides a wide variety of networking services for instances. There are many
additional networking software packages that can be useful when managing OpenStack components. Some
examples include:
• Software to provide load balancing
• Network redundancy protocols
• Routing daemons.
Additional networking services
OpenStack, like any network application, has a number of standard services to consider, such as NTP and DNS.
NTP
Time synchronization is a critical element to ensure continued operation of OpenStack components. Ensuring
that all components have the correct time is necessary to avoid errors in instance scheduling, replication of
objects in the object store, and matching log timestamps for debugging.
46 Design
Architecture Design Guide (Release Version: 15.0.0)
All servers running OpenStack components should be able to access an appropriate NTP server. You may decide
to set up one locally or use the public pools available from the Network Time Protocol project.
DNS
OpenStack does not currently provide DNS services, aside from the dnsmasq daemon, which resides on novanetwork
hosts. You could consider providing a dynamic DNS service to allow instances to update a DNS
entry with new IP addresses. You can also consider making a generic forward and reverse DNS mapping for
instances’ IP addresses, such as vm-203-0-113-123.example.com.
DHCP
LBaaS
OpenStack provides a rich networking environment. This chapter details the requirements and options to consider
when designing your cloud. This includes examples of network implementations to consider, information
about some OpenStack network layouts and networking services that are essential for stable operation.
Warning: If this is the first time you are deploying a cloud infrastructure in your organization, your first
conversations should be with your networking team. Network usage in a running cloud is vastly different
from traditional network deployments and has the potential to be disruptive at both a connectivity and a
policy level.
For example, you must plan the number of IP addresses that you need for both your guest instances as well
as management infrastructure. Additionally, you must research and discuss cloud network connectivity
through proxy servers and firewalls.
Identity
Images
Control Plane
OpenStack is designed to be massively horizontally scalable, which allows all services to be distributed widely.
However, to simplify this guide, we have decided to discuss services of a more central nature, using the concept
of a cloud controller. A cloud controller is a conceptual simplification. In the real world, you design an
architecture for your cloud controller that enables high availability so that if any node fails, another can take
over the required tasks. In reality, cloud controller tasks are spread out across more than a single node.
The cloud controller provides the central management system for OpenStack deployments. Typically, the cloud
controller manages authentication and sends messaging to all the systems through a message queue.
For many deployments, the cloud controller is a single node. However, to have high availability, you have to
take a few considerations into account, which we’ll cover in this chapter.
The cloud controller manages the following services for the cloud:
Databases Tracks current information about users and instances, for example, in a database, typically one
database instance managed per service
Design 47
Architecture Design Guide (Release Version: 15.0.0)
Message queue services All Advanced Message Queuing Protocol (AMQP) messages for services are received
and sent according to the queue broker
Conductor services Proxy requests to a database
Authentication and authorization for identity management Indicates which users can do what actions on
certain cloud resources; quota management is spread out among services, howeverauthentication
Image-management services Stores and serves images with metadata on each, for launching in the cloud
Scheduling services Indicates which resources to use first; for example, spreading out where instances are
launched based on an algorithm
User dashboard Provides a web-based front end for users to consume OpenStack cloud services
API endpoints Offers each service’s REST API access, where the API endpoint catalog is managed by the
Identity service
For our example, the cloud controller has a collection of nova-* components that represent the global state of
the cloud; talks to services such as authentication; maintains information about the cloud in a database; communicates
to all compute nodes and storage workers through a queue; and provides API access. Each service
running on a designated cloud controller may be broken out into separate nodes for scalability or availability.
As another example, you could use pairs of servers for a collective cloud controller—one active, one standby—
for redundant nodes providing a given set of related services, such as:
• Front end web for API requests, the scheduler for choosing which compute node to boot an instance on,
Identity services, and the dashboard
• Database and message queue server (such as MySQL, RabbitMQ)
• Image service for the image management
Now that you see the myriad designs for controlling your cloud, read more about the further considerations to
help with your design decisions.
Hardware Considerations
A cloud controller’s hardware can be the same as a compute node, though you may want to further specify
based on the size and type of cloud that you run.
It’s also possible to use virtual machines for all or some of the services that the cloud controller manages, such
as the message queuing. In this guide, we assume that all services are running directly on the cloud controller.
Table. Cloud controller hardware sizing considerations contains common considerations to review when sizing
hardware for the cloud controller design.
48 Design
Architecture Design Guide (Release Version: 15.0.0)
Table 3: Table. Cloud controller hardware sizing considerations
Consideration Ramification
How many instances will run
at once?
Size your database server accordingly, and scale out beyond one cloud
controller if many instances will report status at the same time and
scheduling where a new instance starts up needs computing power.
How many compute nodes will
run at once?
Ensure that your messaging queue handles requests successfully and
size accordingly.
How many users will access
the API?
If many users will make multiple requests, make sure that the CPU load
for the cloud controller can handle it.
How many users will access
the dashboard versus the REST
API directly?
The dashboard makes many requests, even more than the API access, so
add even more CPU if your dashboard is the main interface for your
users.
How many nova-api services
do you run at once for your
cloud?
You need to size the controller with a core per service.
How long does a single
instance run?
Starting instances and deleting instances is demanding on the compute
node but also demanding on the controller node because of all the API
queries and scheduling needs.
Does your authentication
system also verify externally?
External systems such as LDAP or Active Directory require network
connectivity between the cloud controller and an external authentication
system. Also ensure that the cloud controller has the CPU power to keep
up with requests.
Separation of Services
While our example contains all central services in a single location, it is possible and indeed often a good idea to
separate services onto different physical servers. Table. Deployment scenarios is a list of deployment scenarios
we’ve seen and their justifications.
Table 4: Table. Deployment scenarios
Scenario Justification
Run glance-* servers
on the swift-proxy
server.
This deployment felt that the spare I/O on the Object Storage proxy server was
sufficient and that the Image Delivery portion of glance benefited from being
on physical hardware and having good connectivity to the Object Storage back
end it was using.
Run a central dedicated
database server.
This deployment used a central dedicated server to provide the databases for all
services. This approach simplified operations by isolating database server
updates and allowed for the simple creation of slave database servers for
failover.
Run one VM per
service.
This deployment ran central services on a set of servers running KVM. A
dedicated VM was created for each service (nova-scheduler, rabbitmq,
database, etc). This assisted the deployment with scaling because administrators
could tune the resources given to each virtual machine based on the load it
received (something that was not well understood during installation).
Use an external load
balancer.
This deployment had an expensive hardware load balancer in its organization.
It ran multiple nova-api and swift-proxy servers on different physical
servers and used the load balancer to switch between them.
Design 49
Architecture Design Guide (Release Version: 15.0.0)
One choice that always comes up is whether to virtualize. Some services, such as nova-compute, swiftproxy
and swift-object servers, should not be virtualized. However, control servers can often be happily
virtualized—the performance penalty can usually be offset by simply running more of the service.
Database
OpenStack Compute uses an SQL database to store and retrieve stateful information. MySQL is the popular
database choice in the OpenStack community.
Loss of the database leads to errors. As a result, we recommend that you cluster your database to make it
failure tolerant. Configuring and maintaining a database cluster is done outside OpenStack and is determined
by the database software you choose to use in your cloud environment. MySQL/Galera is a popular option for
MySQL-based databases.
Message Queue
Most OpenStack services communicate with each other using the message queue. For example, Compute
communicates to block storage services and networking services through the message queue. Also, you can
optionally enable notifications for any service. RabbitMQ, Qpid, and Zeromq are all popular choices for a
message-queue service. In general, if the message queue fails or becomes inaccessible, the cluster grinds to
a halt and ends up in a read-only state, with information stuck at the point where the last message was sent.
Accordingly, we recommend that you cluster the message queue. Be aware that clustered message queues can
be a pain point for many OpenStack deployments. While RabbitMQ has native clustering support, there have
been reports of issues when running it at a large scale. While other queuing solutions are available, such as
Zeromq and Qpid, Zeromq does not offer stateful queues. Qpid is the messaging system of choice for Red
Hat and its derivatives. Qpid does not have native clustering capabilities and requires a supplemental service,
such as Pacemaker or Corsync. For your message queue, you need to determine what level of data loss you are
comfortable with and whether to use an OpenStack project’s ability to retry multiple MQ hosts in the event of
a failure, such as using Compute’s ability to do so.
Conductor Services
In the previous version of OpenStack, all nova-compute services required direct access to the database hosted
on the cloud controller. This was problematic for two reasons: security and performance. With regard to
security, if a compute node is compromised, the attacker inherently has access to the database. With regard to
performance, nova-compute calls to the database are single-threaded and blocking. This creates a performance
bottleneck because database requests are fulfilled serially rather than in parallel.
The conductor service resolves both of these issues by acting as a proxy for the nova-compute service. Now,
instead of nova-compute directly accessing the database, it contacts the nova-conductor service, and novaconductor
accesses the database on nova-compute‘s behalf. Since nova-compute no longer has direct access
to the database, the security issue is resolved. Additionally, nova-conductor is a nonblocking service, so
requests from all compute nodes are fulfilled in parallel.
Note: If you are using nova-network and multi-host networking in your cloud environment, nova-compute
still requires direct access to the database.
The nova-conductor service is horizontally scalable. To make nova-conductor highly available and fault
50 Design
Architecture Design Guide (Release Version: 15.0.0)
tolerant, just launch more instances of the nova-conductor process, either on the same server or across multiple
servers.
Application Programming Interface (API)
All public access, whether direct, through a command-line client, or through the web-based dashboard, uses the
API service. Find the API reference at Development resources for OpenStack clouds.
You must choose whether you want to support the Amazon EC2 compatibility APIs, or just the OpenStack
APIs. One issue you might encounter when running both APIs is an inconsistent experience when referring to
images and instances.
For example, the EC2 API refers to instances using IDs that contain hexadecimal, whereas the OpenStack API
uses names and digits. Similarly, the EC2 API tends to rely on DNS aliases for contacting virtual machines, as
opposed to OpenStack, which typically lists IP addresses.
If OpenStack is not set up in the right way, it is simple to have scenarios in which users are unable to contact
their instances due to having only an incorrect DNS alias. Despite this, EC2 compatibility can assist users
migrating to your cloud.
As with databases and message queues, having more than one API server is a good thing. Traditional HTTP
load-balancing techniques can be used to achieve a highly available nova-api service.
Extensions
The API Specifications define the core actions, capabilities, and mediatypes of the OpenStack API. A client
can always depend on the availability of this core API, and implementers are always required to support it in its
entirety. Requiring strict adherence to the core API allows clients to rely upon a minimal level of functionality
when interacting with multiple implementations of the same API.
The OpenStack Compute API is extensible. An extension adds capabilities to an API beyond those defined in
the core. The introduction of new features, MIME types, actions, states, headers, parameters, and resources can
all be accomplished by means of extensions to the core API. This allows the introduction of new features in the
API without requiring a version change and allows the introduction of vendor-specific niche functionality.
Scheduling
The scheduling services are responsible for determining the compute or storage node where a virtual machine or
block storage volume should be created. The scheduling services receive creation requests for these resources
from the message queue and then begin the process of determining the appropriate node where the resource
should reside. This process is done by applying a series of user-configurable filters against the available collection
of nodes.
There are currently two schedulers: nova-scheduler for virtual machines and cinder-scheduler for block
storage volumes. Both schedulers are able to scale horizontally, so for high-availability purposes, or for very
large or high-schedule-frequency installations, you should consider running multiple instances of each scheduler.
The schedulers all listen to the shared message queue, so no special load balancing is required.
Images
The OpenStack Image service consists of two parts: glance-api and glance-registry. The former is
responsible for the delivery of images; the compute node uses it to download images from the back end. The
Design 51
Architecture Design Guide (Release Version: 15.0.0)
latter maintains the metadata information associated with virtual machine images and requires a database.
The glance-api part is an abstraction layer that allows a choice of back end. Currently, it supports:
OpenStack Object Storage Allows you to store images as objects.
File system Uses any traditional file system to store the images as files.
S3 Allows you to fetch images from Amazon S3.
HTTP Allows you to fetch images from a web server. You cannot write images by using this mode.
If you have an OpenStack Object Storage service, we recommend using this as a scalable place to store your
images. You can also use a file system with sufficient performance or Amazon S3—unless you do not need the
ability to upload new images through OpenStack.
Dashboard
The OpenStack dashboard (horizon) provides a web-based user interface to the various OpenStack components.
The dashboard includes an end-user area for users to manage their virtual infrastructure and an admin area for
cloud operators to manage the OpenStack environment as a whole.
The dashboard is implemented as a Python web application that normally runs in Apache httpd. Therefore,
you may treat it the same as any other web application, provided it can reach the API servers (including their
admin endpoints) over the network.
Authentication and Authorization
The concepts supporting OpenStack’s authentication and authorization are derived from well-understood and
widely used systems of a similar nature. Users have credentials they can use to authenticate, and they can be a
member of one or more groups (known as projects or tenants, interchangeably).
For example, a cloud administrator might be able to list all instances in the cloud, whereas a user can see only
those in his current group. Resources quotas, such as the number of cores that can be used, disk space, and so
on, are associated with a project.
OpenStack Identity provides authentication decisions and user attribute information, which is then used by the
other OpenStack services to perform authorization. The policy is set in the policy.json file. For information
on how to configure these, see Managing Projects and Users in the OpenStack Operations Guide.
OpenStack Identity supports different plug-ins for authentication decisions and identity storage. Examples of
these plug-ins include:
• In-memory key-value Store (a simplified internal storage structure)
• SQL database (such as MySQL or PostgreSQL)
• Memcached (a distributed memory object caching system)
• LDAP (such as OpenLDAP or Microsoft’s Active Directory)
Many deployments use the SQL database; however, LDAP is also a popular choice for those with existing
authentication infrastructure that needs to be integrated.
52 Design
Architecture Design Guide (Release Version: 15.0.0)
Network Considerations
Because the cloud controller handles so many different services, it must be able to handle the amount of traffic
that hits it. For example, if you choose to host the OpenStack Image service on the cloud controller, the cloud
controller should be able to support the transferring of the images at an acceptable speed.
As another example, if you choose to use single-host networking where the cloud controller is the network
gateway for all instances, then the cloud controller must support the total amount of traffic that travels between
your cloud and the public Internet.
We recommend that you use a fast NIC, such as 10 GB. You can also choose to use two 10 GB NICs and bond
them together. While you might not be able to get a full bonded 20 GB speed, different transmission streams
use different NICs. For example, if the cloud controller transfers two images, each image uses a different NIC
and gets a full 10 GB of bandwidth.
Cloud management platform tools
Complex clouds, in particular hybrid clouds, may require tools to facilitate working across multiple clouds.
Broker between clouds Brokering software evaluates relative costs between different cloud platforms. Cloud
Management Platforms (CMP) allow the designer to determine the right location for the workload based
on predetermined criteria.
Facilitate orchestration across the clouds CMPs simplify the migration of application workloads between
public, private, and hybrid cloud platforms.
We recommend using cloud orchestration tools for managing a diverse portfolio of systems and applications
across multiple cloud platforms.
Technical details
Capacity and scale
High availability
Operator requirements
Deployment considerations
Maintenance considerations
Use cases
Development cloud
Design model
Requirements
Component block diagram
Use cases 53
Architecture Design Guide (Release Version: 15.0.0)
General compute cloud
Design model
An online classified advertising company wants to run web applications consisting of Tomcat, Nginx, and
MariaDB in a private cloud. To meet the policy requirements, the cloud infrastructure will run in their own
data center. The company has predictable load requirements but requires scaling to cope with nightly increases
in demand. Their current environment does not have the flexibility to align with their goal of running an open
source API environment. The current environment consists of the following:
• Between 120 and 140 installations of Nginx and Tomcat, each with 2 vCPUs and 4 GB of RAM
• A three node MariaDB and Galera cluster, each with 4 vCPUs and 8 GB of RAM
The company runs hardware load balancers and multiple web applications serving their websites and orchestrates
environments using combinations of scripts and Puppet. The website generates large amounts of log data
daily that requires archiving.
The solution would consist of the following OpenStack components:
• A firewall, switches and load balancers on the public facing network connections.
• OpenStack Controller service running Image service, Identity service, Networking service, combined
with support services such as MariaDB and RabbitMQ, configured for high availability on at least three
controller nodes.
• OpenStack compute nodes running the KVM hypervisor.
• OpenStack Block Storage for use by compute instances, requiring persistent storage (such as databases
for dynamic sites).
• OpenStack Object Storage for serving static objects (such as images).
54 Use cases
Architecture Design Guide (Release Version: 15.0.0)
Running up to 140 web instances and the small number of MariaDB instances requires 292 vCPUs available, as
well as 584 GB of RAM. On a typical 1U server using dual-socket hex-core Intel CPUs with Hyperthreading,
and assuming 2:1 CPU overcommit ratio, this would require 8 OpenStack Compute nodes.
The web application instances run from local storage on each of the OpenStack Compute nodes. The web
application instances are stateless, meaning that any of the instances can fail and the application will continue
to function.
MariaDB server instances store their data on shared enterprise storage, such as NetApp or Solidfire devices. If
a MariaDB instance fails, storage would be expected to be re-attached to another instance and rejoined to the
Galera cluster.
Logs from the web application servers are shipped to OpenStack Object Storage for processing and archiving.
Additional capabilities can be realized by moving static web content to be served from OpenStack Object
Storage containers, and backing the OpenStack Image service with OpenStack Object Storage.
Use cases 55
Architecture Design Guide (Release Version: 15.0.0)
Note: Increasing OpenStack Object Storage means network bandwidth needs to be taken into consideration.
Running OpenStack Object Storage with network connections offering 10 GbE or better connectivity is advised.
Leveraging Orchestration and Telemetry services is also a potential issue when providing auto-scaling, orchestrated
web application environments. Defining the web applications in a Heat Orchestration Template (HOT)
negates the reliance on the current scripted Puppet solution.
OpenStack Networking can be used to control hardware load balancers through the use of plug-ins and the
Networking API. This allows users to control hardware load balance pools and instances as members in these
pools, but their use in production environments must be carefully weighed against current stability.
Requirements
Storage requirements
Using a scale-out storage solution with direct-attached storage (DAS) in the servers is well suited for a general
purpose OpenStack cloud. Cloud services requirements determine your choice of scale-out solution. You need
to determine if a single, highly expandable and highly vertical, scalable, centralized storage array is suitable for
your design. After determining an approach, select the storage hardware based on this criteria.
This list expands upon the potential impacts for including a particular storage architecture (and corresponding
storage hardware) into the design for a general purpose OpenStack cloud:
Connectivity If storage protocols other than Ethernet are part of the storage solution, ensure the appropriate
hardware has been selected. If a centralized storage array is selected, ensure that the hypervisor will be
able to connect to that storage array for image storage.
Usage How the particular storage architecture will be used is critical for determining the architecture. Some of
the configurations that will influence the architecture include whether it will be used by the hypervisors
for ephemeral instance storage, or if OpenStack Object Storage will use it for object storage.
Instance and image locations Where instances and images will be stored will influence the architecture.
Server hardware If the solution is a scale-out storage architecture that includes DAS, it will affect the server
hardware selection. This could ripple into the decisions that affect host density, instance density, power
density, OS-hypervisor, management tools and others.
A general purpose OpenStack cloud has multiple options. The key factors that will have an influence on selection
of storage hardware for a general purpose OpenStack cloud are as follows:
Capacity Hardware resources selected for the resource nodes should be capable of supporting enough storage
for the cloud services. Defining the initial requirements and ensuring the design can support adding
capacity is important. Hardware nodes selected for object storage should be capable of support a large
number of inexpensive disks with no reliance on RAID controller cards. Hardware nodes selected for
block storage should be capable of supporting high speed storage solutions and RAID controller cards
to provide performance and redundancy to storage at a hardware level. Selecting hardware RAID controllers
that automatically repair damaged arrays will assist with the replacement and repair of degraded
or deleted storage devices.
Performance Disks selected for object storage services do not need to be fast performing disks. We recommend
that object storage nodes take advantage of the best cost per terabyte available for storage.
Contrastingly, disks chosen for block storage services should take advantage of performance boosting
56 Use cases
Architecture Design Guide (Release Version: 15.0.0)
features that may entail the use of SSDs or flash storage to provide high performance block storage pools.
Storage performance of ephemeral disks used for instances should also be taken into consideration.
Fault tolerance Object storage resource nodes have no requirements for hardware fault tolerance or RAID
controllers. It is not necessary to plan for fault tolerance within the object storage hardware because
the object storage service provides replication between zones as a feature of the service. Block storage
nodes, compute nodes, and cloud controllers should all have fault tolerance built in at the hardware level
by making use of hardware RAID controllers and varying levels of RAID configuration. The level of
RAID chosen should be consistent with the performance and availability requirements of the cloud.
Network hardware requirements
For a compute-focus architecture, we recommend designing the network architecture using a scalable network
model that makes it easy to add capacity and bandwidth. A good example of such a model is the leaf-spine
model. In this type of network design, you can add additional bandwidth as well as scale out to additional racks
of gear. It is important to select network hardware that supports port count, port speed, and port density while
allowing for future growth as workload demands increase. In the network architecture, it is also important to
evaluate where to provide redundancy.
Network software requirements
For a general purpose OpenStack cloud, the OpenStack infrastructure components need to be highly available.
If the design does not include hardware load balancing, networking software packages like HAProxy will need
to be included.
Component block diagram
Web scale cloud
Design model
Requirements
Component block diagram
Storage cloud
Design model
Storage-focused architecture depends on specific use cases. This section discusses three example use cases:
• An object store with a RESTful interface
• Compute analytics with parallel file systems
• High performance database
Use cases 57
Architecture Design Guide (Release Version: 15.0.0)
An object store with a RESTful interface
The example below shows a REST interface without a high performance requirement. The following diagram
depicts the example architecture:
The example REST interface, presented as a traditional Object Store running on traditional spindles, does not
require a high performance caching tier.
This example uses the following components:
Network:
• 10 GbE horizontally scalable spine leaf back-end storage and front end network.
58 Use cases
Architecture Design Guide (Release Version: 15.0.0)
Storage hardware:
• 10 storage servers each with 12x4 TB disks equaling 480 TB total space with approximately 160 TB of
usable space after replicas.
Proxy:
• 3x proxies
• 2x10 GbE bonded front end
• 2x10 GbE back-end bonds
• Approximately 60 Gb of total bandwidth to the back-end storage cluster
Note: It may be necessary to implement a third party caching layer for some applications to achieve suitable
performance.
Compute analytics with data processing service
Analytics of large data sets are dependent on the performance of the storage system. Clouds using storage
systems such as Hadoop Distributed File System (HDFS) have inefficiencies which can cause performance
issues.
One potential solution to this problem is the implementation of storage systems designed for performance. Parallel
file systems have previously filled this need in the HPC space and are suitable for large scale performanceorientated
systems.
OpenStack has integration with Hadoop to manage the Hadoop cluster within the cloud. The following diagram
shows an OpenStack store with a high performance requirement:
Use cases 59
Architecture Design Guide (Release Version: 15.0.0)
The hardware requirements and configuration are similar to those of the High Performance Database example
below. In this case, the architecture uses Ceph’s Swift-compatible REST interface, features that allow for
connecting a caching pool to allow for acceleration of the presented pool.
High performance database with Database service
Databases are a common workload that benefit from high performance storage back ends. Although enterprise
storage is not a requirement, many environments have existing storage that OpenStack cloud can use as back
ends. You can create a storage pool to provide block devices with OpenStack Block Storage for instances as
well as object interfaces. In this example, the database I-O requirements are high and demand storage presented
from a fast SSD pool.
A storage system presents a LUN backed by a set of SSDs using a traditional storage array with OpenStack
Block Storage integration or a storage platform such as Ceph or Gluster.
This system can provide additional performance. For example, in the database example below, a portion of
the SSD pool can act as a block device to the Database server. In the high performance analytics example, the
inline SSD cache layer accelerates the REST interface.
60 Use cases
Architecture Design Guide (Release Version: 15.0.0)
In this example, Ceph presents a swift-compatible REST interface, as well as a block level storage from a
distributed storage cluster. It is highly flexible and has features that enable reduced cost of operations such as
self healing and auto balancing. Using erasure coded pools are a suitable way of maximizing the amount of
usable space.
Note: There are special considerations around erasure coded pools. For example, higher computational reUse
cases 61
Architecture Design Guide (Release Version: 15.0.0)
quirements and limitations on the operations allowed on an object; erasure coded pools do not support partial
writes.
Using Ceph as an applicable example, a potential architecture would have the following requirements:
Network:
• 10 GbE horizontally scalable spine leaf back-end storage and front-end network
Storage hardware:
• 5 storage servers for caching layer 24x1 TB SSD
• 10 storage servers each with 12x4 TB disks which equals 480 TB total space with about approximately
160 TB of usable space after 3 replicas
REST proxy:
• 3x proxies
• 2x10 GbE bonded front end
• 2x10 GbE back-end bonds
• Approximately 60 Gb of total bandwidth to the back-end storage cluster
Using an SSD cache layer, you can present block devices directly to hypervisors or instances. The REST
interface can also use the SSD cache systems as an inline cache.
Requirements
Storage requirements
Storage-focused OpenStack clouds must address I/O intensive workloads. These workloads are not CPU intensive,
nor are they consistently network intensive. The network may be heavily utilized to transfer storage, but
they are not otherwise network intensive.
The selection of storage hardware determines the overall performance and scalability of a storage-focused OpenStack
design architecture. Several factors impact the design process, including:
Latency A key consideration in a storage-focused OpenStack cloud is latency. Using solid-state disks (SSDs)
to minimize latency and, to reduce CPU delays caused by waiting for the storage, increases performance.
Use RAID controller cards in compute hosts to improve the performance of the underlying disk subsystem.
Scale-out solutions Depending on the storage architecture, you can adopt a scale-out solution, or use a highly
expandable and scalable centralized storage array. If a centralized storage array meets your requirements,
then the array vendor determines the hardware selection. It is possible to build a storage array using
commodity hardware with Open Source software, but requires people with expertise to build such a
system.
On the other hand, a scale-out storage solution that uses direct-attached storage (DAS) in the servers
may be an appropriate choice. This requires configuration of the server hardware to support the storage
solution.
Considerations affecting storage architecture (and corresponding storage hardware) of a Storage-focused OpenStack
cloud include:
62 Use cases
Architecture Design Guide (Release Version: 15.0.0)
Connectivity Ensure the connectivity matches the storage solution requirements. We recommend confirming
that the network characteristics minimize latency to boost the overall performance of the design.
Latency Determine if the use case has consistent or highly variable latency.
Throughput Ensure that the storage solution throughput is optimized for your application requirements.
Server hardware Use of DAS impacts the server hardware choice and affects host density, instance density,
power density, OS-hypervisor, and management tools.
Component block diagram
Network virtual function cloud
Design model
Requirements
Component block diagram
Network-focused cloud examples
An organization designs a large scale cloud-based web application. The application scales horizontally in a
bursting behavior and generates a high instance count. The application requires an SSL connection to secure
data and must not lose connection state to individual servers.
The figure below depicts an example design for this workload. In this example, a hardware load balancer
provides SSL offload functionality and connects to tenant networks in order to reduce address consumption.
This load balancer links to the routing architecture as it services the VIP for the application. The router and load
balancer use the GRE tunnel ID of the application’s tenant network and an IP address within the tenant subnet
but outside of the address pool. This is to ensure that the load balancer can communicate with the application’s
HTTP servers without requiring the consumption of a public IP address.
Because sessions persist until closed, the routing and switching architecture provides high availability. Switches
mesh to each hypervisor and each other, and also provide an MLAG implementation to ensure that layer-2
connectivity does not fail. Routers use VRRP and fully mesh with switches to ensure layer-3 connectivity.
Since GRE provides an overlay network, Networking is present and uses the Open vSwitch agent in GRE
tunnel mode. This ensures all devices can reach all other devices and that you can create tenant networks for
private addressing links to the load balancer.
Use cases 63
Architecture Design Guide (Release Version: 15.0.0)
A web service architecture has many options and optional components. Due to this, it can fit into a large number
of other OpenStack designs. A few key components, however, need to be in place to handle the nature of most
web-scale workloads. You require the following components:
• OpenStack Controller services (Image service, Identity service, Networking service, and supporting services
such as MariaDB and RabbitMQ)
• OpenStack Compute running KVM hypervisor
• OpenStack Object Storage
• Orchestration service
• Telemetry service
64 Use cases
Architecture Design Guide (Release Version: 15.0.0)
Beyond the normal Identity service, Compute service, Image service, and Object Storage components, we
recommend the Orchestration service component to handle the proper scaling of workloads to adjust to demand.
Due to the requirement for auto-scaling, the design includes the Telemetry service. Web services tend to be
bursty in load, have very defined peak and valley usage patterns and, as a result, benefit from automatic scaling
of instances based upon traffic. At a network level, a split network configuration works well with databases
residing on private tenant networks since these do not emit a large quantity of broadcast traffic and may need
to interconnect to some databases for content.
Load balancing
Load balancing spreads requests across multiple instances. This workload scales well horizontally across large
numbers of instances. This enables instances to run without publicly routed IP addresses and instead to rely on
the load balancer to provide a globally reachable service. Many of these services do not require direct server
return. This aids in address planning and utilization at scale since only the virtual IP (VIP) must be public.
Overlay networks
The overlay functionality design includes OpenStack Networking in Open vSwitch GRE tunnel mode. In this
case, the layer-3 external routers pair with VRRP, and switches pair with an implementation of MLAG to ensure
that you do not lose connectivity with the upstream routing infrastructure.
Performance tuning
Network level tuning for this workload is minimal. Quality of Service (QoS) applies to these workloads for a
middle ground Class Selector depending on existing policies. It is higher than a best effort queue but lower
than an Expedited Forwarding or Assured Forwarding queue. Since this type of application generates larger
packets with longer-lived connections, you can optimize bandwidth utilization for long duration TCP. Normal
bandwidth planning applies here with regards to benchmarking a session’s usage multiplied by the expected
number of concurrent sessions with overhead.
Network functions
Network functions is a broad category but encompasses workloads that support the rest of a system’s network.
These workloads tend to consist of large amounts of small packets that are very short lived, such as DNS queries
or SNMP traps. These messages need to arrive quickly and do not deal with packet loss as there can be a very
large volume of them. There are a few extra considerations to take into account for this type of workload and
this can change a configuration all the way to the hypervisor level. For an application that generates 10 TCP
sessions per user with an average bandwidth of 512 kilobytes per second per flow and expected user count of
ten thousand concurrent users, the expected bandwidth plan is approximately 4.88 gigabits per second.
The supporting network for this type of configuration needs to have a low latency and evenly distributed availability.
This workload benefits from having services local to the consumers of the service. Use a multi-site
approach as well as deploying many copies of the application to handle load as close as possible to consumers.
Since these applications function independently, they do not warrant running overlays to interconnect tenant
networks. Overlays also have the drawback of performing poorly with rapid flow setup and may incur too much
overhead with large quantities of small packets and therefore we do not recommend them.
QoS is desirable for some workloads to ensure delivery. DNS has a major impact on the load times of other
services and needs to be reliable and provide rapid responses. Configure rules in upstream devices to apply a
higher Class Selector to DNS to ensure faster delivery or a better spot in queuing algorithms.
Use cases 65
Architecture Design Guide (Release Version: 15.0.0)
Cloud storage
Another common use case for OpenStack environments is providing a cloud-based file storage and sharing service.
You might consider this a storage-focused use case, but its network-side requirements make it a networkfocused
use case.
For example, consider a cloud backup application. This workload has two specific behaviors that impact the
network. Because this workload is an externally-facing service and an internally-replicating application, it has
both north-south and east-west traffic considerations:
north-south traffic When a user uploads and stores content, that content moves into the OpenStack installation.
When users download this content, the content moves out from the OpenStack installation. Because
this service operates primarily as a backup, most of the traffic moves southbound into the environment.
In this situation, it benefits you to configure a network to be asymmetrically downstream because the
traffic that enters the OpenStack installation is greater than the traffic that leaves the installation.
east-west traffic Likely to be fully symmetric. Because replication originates from any node and might target
multiple other nodes algorithmically, it is less likely for this traffic to have a larger volume in any specific
direction. However, this traffic might interfere with north-south traffic.
66 Use cases
Architecture Design Guide (Release Version: 15.0.0)
This application prioritizes the north-south traffic over east-west traffic: the north-south traffic involves
customer-facing data.
The network design, in this case, is less dependent on availability and more dependent on being able to handle
high bandwidth. As a direct result, it is beneficial to forgo redundant links in favor of bonding those connections.
This increases available bandwidth. It is also beneficial to configure all devices in the path, including
OpenStack, to generate and pass jumbo frames.
Appendix
Community support
The following resources are available to help you run and use OpenStack. The OpenStack community constantly
improves and adds to the main features of OpenStack, but if you have any questions, do not hesitate to ask. Use
the following resources to get OpenStack support and troubleshoot your installations.
Documentation
For the available OpenStack documentation, see docs.openstack.org.
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at
OpenStack Documentation Mailing List, join our IRC channel #openstack-doc on the freenode IRC network,
or report a bug.
The following books explain how to install an OpenStack cloud and its associated components:
• Installation Tutorial for openSUSE Leap 42.2 and SUSE Linux Enterprise Server 12 SP2
• Installation Tutorial for Red Hat Enterprise Linux 7 and CentOS 7
• Installation Tutorial for Ubuntu 16.04 (LTS)
The following books explain how to configure and run an OpenStack cloud:
• Architecture Design Guide
• Administrator Guide
• Configuration Reference
• Operations Guide
• Networking Guide
• High Availability Guide
• Security Guide
• Virtual Machine Image Guide
The following books explain how to use the OpenStack Dashboard and command-line clients:
• End User Guide
• Command-Line Interface Reference
The following documentation provides reference and guidance information for the OpenStack APIs:
• API Guide
Appendix 67
Architecture Design Guide (Release Version: 15.0.0)
The following guide provides how to contribute to OpenStack documentation:
• Documentation Contributor Guide
ask.openstack.org
During the set up or testing of OpenStack, you might have questions about how a specific task is completed or
be in a situation where a feature does not work correctly. Use the ask.openstack.org site to ask questions and
get answers. When you visit the Ask OpenStack site, scan the recently asked questions to see whether your
question has already been answered. If not, ask a new question. Be sure to give a clear, concise summary in the
title and provide as much detail as possible in the description. Paste in your command output or stack traces,
links to screen shots, and any other information which might be useful.
OpenStack mailing lists
A great way to get answers and insights is to post your question or problematic scenario to the OpenStack
mailing list. You can learn from and help others who might have similar issues. To subscribe or view the
archives, go to the general OpenStack mailing list. If you are interested in the other mailing lists for specific
projects or development, refer to Mailing Lists.
The OpenStack wiki
The OpenStack wiki contains a broad range of topics but some of the information can be difficult to find or is a
few pages deep. Fortunately, the wiki search feature enables you to search by title or content. If you search for
specific information, such as about networking or OpenStack Compute, you can find a large amount of relevant
material. More is being added all the time, so be sure to check back often. You can find the search box in the
upper-right corner of any OpenStack wiki page.
The Launchpad Bugs area
The OpenStack community values your set up and testing efforts and wants your feedback. To log a bug, you
must sign up for a Launchpad account. You can view existing bugs and report bugs in the Launchpad Bugs
area. Use the search feature to determine whether the bug has already been reported or already been fixed. If it
still seems like your bug is unreported, fill out a bug report.
Some tips:
• Give a clear, concise summary.
• Provide as much detail as possible in the description. Paste in your command output or stack traces, links
to screen shots, and any other information which might be useful.
• Be sure to include the software and package versions that you are using, especially
if you are using a development branch, such as, "Kilo release" vs git commit
bc79c3ecc55929bac585d04a03475b72e06a3208.
• Any deployment-specific information is helpful, such as whether you are using Ubuntu 14.04 or are
performing a multi-node installation.
The following Launchpad Bugs areas are available:
• Bugs: OpenStack Block Storage (cinder)
68 Appendix
Architecture Design Guide (Release Version: 15.0.0)
• Bugs: OpenStack Compute (nova)
• Bugs: OpenStack Dashboard (horizon)
• Bugs: OpenStack Identity (keystone)
• Bugs: OpenStack Image service (glance)
• Bugs: OpenStack Networking (neutron)
• Bugs: OpenStack Object Storage (swift)
• Bugs: Application catalog (murano)
• Bugs: Bare metal service (ironic)
• Bugs: Clustering service (senlin)
• Bugs: Container Infrastructure Management service (magnum)
• Bugs: Data processing service (sahara)
• Bugs: Database service (trove)
• Bugs: Deployment service (fuel)
• Bugs: DNS service (designate)
• Bugs: Key Manager Service (barbican)
• Bugs: Monitoring (monasca)
• Bugs: Orchestration (heat)
• Bugs: Rating (cloudkitty)
• Bugs: Shared file systems (manila)
• Bugs: Telemetry (ceilometer)
• Bugs: Telemetry v3 (gnocchi)
• Bugs: Workflow service (mistral)
• Bugs: Messaging service (zaqar)
• Bugs: OpenStack API Documentation (developer.openstack.org)
• Bugs: OpenStack Documentation (docs.openstack.org)
The OpenStack IRC channel
The OpenStack community lives in the #openstack IRC channel on the Freenode network. You can hang out, ask
questions, or get immediate feedback for urgent and pressing issues. To install an IRC client or use a browserbased
client, go to https://webchat.freenode.net/. You can also use Colloquy (Mac OS X), mIRC (Windows),
or XChat (Linux). When you are in the IRC channel and want to share code or command output, the generally
accepted method is to use a Paste Bin. The OpenStack project has one at Paste. Just paste your longer amounts
of text or logs in the web form and you get a URL that you can paste into the channel. The OpenStack IRC
channel is #openstack on irc.freenode.net. You can find a list of all OpenStack IRC channels on the IRC
page on the wiki.
Appendix 69
Architecture Design Guide (Release Version: 15.0.0)
Documentation feedback
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at
OpenStack Documentation Mailing List, or report a bug.
OpenStack distribution packages
The following Linux distributions provide community-supported packages for OpenStack:
• Debian: https://wiki.debian.org/OpenStack
• CentOS, Fedora, and Red Hat Enterprise Linux: https://www.rdoproject.org/
• openSUSE and SUSE Linux Enterprise Server: https://en.opensuse.org/Portal:OpenStack
• Ubuntu: https://wiki.ubuntu.com/ServerTeam/CloudArchive
Glossary
This glossary offers a list of terms and definitions to define a vocabulary for OpenStack-related concepts.
To add to OpenStack glossary, clone the openstack/openstack-manuals repository and update the source file
doc/common/glossary.rst through the OpenStack contribution process.
0-9
6to4 A mechanism that allows IPv6 packets to be transmitted over an IPv4 network, providing a strategy for
migrating to IPv6.
A
absolute limit Impassable limits for guest VMs. Settings include total RAM size, maximum number of vCPUs,
and maximum disk size.
access control list (ACL) A list of permissions attached to an object. An ACL specifies which users or system
processes have access to objects. It also defines which operations can be performed on specified objects.
Each entry in a typical ACL specifies a subject and an operation. For instance, the ACL entry (Alice,
delete) for a file gives Alice permission to delete the file.
access key Alternative term for an Amazon EC2 access key. See EC2 access key.
account The Object Storage context of an account. Do not confuse with a user account from an authentication
service, such as Active Directory, /etc/passwd, OpenLDAP, OpenStack Identity, and so on.
account auditor Checks for missing replicas and incorrect or corrupted objects in a specified Object Storage
account by running queries against the back-end SQLite database.
account database A SQLite database that contains Object Storage accounts and related metadata and that the
accounts server accesses.
account reaper An Object Storage worker that scans for and deletes account databases and that the account
server has marked for deletion.
account server Lists containers in Object Storage and stores container information in the account database.
70 Appendix
Architecture Design Guide (Release Version: 15.0.0)
account service An Object Storage component that provides account services such as list, create, modify, and
audit. Do not confuse with OpenStack Identity service, OpenLDAP, or similar user-account services.
accounting The Compute service provides accounting information through the event notification and system
usage data facilities.
Active Directory Authentication and identity service by Microsoft, based on LDAP. Supported in OpenStack.
active/active configuration In a high-availability setup with an active/active configuration, several systems
share the load together and if one fails, the load is distributed to the remaining systems.
active/passive configuration In a high-availability setup with an active/passive configuration, systems are set
up to bring additional resources online to replace those that have failed.
address pool A group of fixed and/or floating IP addresses that are assigned to a project and can be used by
or assigned to the VM instances in a project.
Address Resolution Protocol (ARP) The protocol by which layer-3 IP addresses are resolved into layer-2
link local addresses.
admin API A subset of API calls that are accessible to authorized administrators and are generally not accessible
to end users or the public Internet. They can exist as a separate service (keystone) or can be a
subset of another API (nova).
admin server In the context of the Identity service, the worker process that provides access to the admin API.
administrator The person responsible for installing, configuring, and managing an OpenStack cloud.
Advanced Message Queuing Protocol (AMQP) The open standard messaging protocol used by OpenStack
components for intra-service communications, provided by RabbitMQ, Qpid, or ZeroMQ.
Advanced RISC Machine (ARM) Lower power consumption CPU often found in mobile and embedded
devices. Supported by OpenStack.
alert The Compute service can send alerts through its notification system, which includes a facility to create
custom notification drivers. Alerts can be sent to and displayed on the dashboard.
allocate The process of taking a floating IP address from the address pool so it can be associated with a fixed
IP on a guest VM instance.
Amazon Kernel Image (AKI) Both a VM container format and disk format. Supported by Image service.
Amazon Machine Image (AMI) Both a VM container format and disk format. Supported by Image service.
Amazon Ramdisk Image (ARI) Both a VM container format and disk format. Supported by Image service.
Anvil A project that ports the shell script-based project named DevStack to Python.
aodh Part of the OpenStack Telemetry service; provides alarming functionality.
Apache The Apache Software Foundation supports the Apache community of open-source software projects.
These projects provide software products for the public good.
Apache License 2.0 All OpenStack core projects are provided under the terms of the Apache License 2.0
license.
Apache Web Server The most common web server software currently used on the Internet.
API endpoint The daemon, worker, or service that a client communicates with to access an API. API endpoints
can provide any number of services, such as authentication, sales data, performance meters, Compute
VM commands, census data, and so on.
API extension Custom modules that extend some OpenStack core APIs.
Appendix 71
Architecture Design Guide (Release Version: 15.0.0)
API extension plug-in Alternative term for a Networking plug-in or Networking API extension.
API key Alternative term for an API token.
API server Any node running a daemon or worker that provides an API endpoint.
API token Passed to API requests and used by OpenStack to verify that the client is authorized to run the
requested operation.
API version In OpenStack, the API version for a project is part of the URL. For example, example.com/
nova/v1/foobar.
applet A Java program that can be embedded into a web page.
Application Catalog service (murano) The project that provides an application catalog service so that users
can compose and deploy composite environments on an application abstraction level while managing
the application lifecycle.
Application Programming Interface (API) A collection of specifications used to access a service, application,
or program. Includes service calls, required parameters for each call, and the expected return
values.
application server A piece of software that makes available another piece of software over a network.
Application Service Provider (ASP) Companies that rent specialized applications that help businesses and
organizations provide additional services with lower cost.
arptables Tool used for maintaining Address Resolution Protocol packet filter rules in the Linux kernel firewall
modules. Used along with iptables, ebtables, and ip6tables in Compute to provide firewall services
for VMs.
associate The process associating a Compute floating IP address with a fixed IP address.
Asynchronous JavaScript and XML (AJAX) A group of interrelated web development techniques used on
the client-side to create asynchronous web applications. Used extensively in horizon.
ATA over Ethernet (AoE) A disk storage protocol tunneled within Ethernet.
attach The process of connecting a VIF or vNIC to a L2 network in Networking. In the context of Compute,
this process connects a storage volume to an instance.
attachment (network) Association of an interface ID to a logical port. Plugs an interface into a port.
auditing Provided in Compute through the system usage data facility.
auditor A worker process that verifies the integrity of Object Storage objects, containers, and accounts. Auditors
is the collective term for the Object Storage account auditor, container auditor, and object auditor.
Austin The code name for the initial release of OpenStack. The first design summit took place in Austin,
Texas, US.
auth node Alternative term for an Object Storage authorization node.
authentication The process that confirms that the user, process, or client is really who they say they are
through private key, secret token, password, fingerprint, or similar method.
authentication token A string of text provided to the client after authentication. Must be provided by the user
or process in subsequent requests to the API endpoint.
AuthN The Identity service component that provides authentication services.
authorization The act of verifying that a user, process, or client is authorized to perform an action.
72 Appendix
Architecture Design Guide (Release Version: 15.0.0)
authorization node An Object Storage node that provides authorization services.
AuthZ The Identity component that provides high-level authorization services.
Auto ACK Configuration setting within RabbitMQ that enables or disables message acknowledgment. Enabled
by default.
auto declare A Compute RabbitMQ setting that determines whether a message exchange is automatically
created when the program starts.
availability zone An Amazon EC2 concept of an isolated area that is used for fault tolerance. Do not confuse
with an OpenStack Compute zone or cell.
AWS CloudFormation template AWS CloudFormation allows Amazon Web Services (AWS) users to create
and manage a collection of related resources. The Orchestration service supports a CloudFormationcompatible
format (CFN).
B
back end Interactions and processes that are obfuscated from the user, such as Compute volume mount, data
transmission to an iSCSI target by a daemon, or Object Storage object integrity checks.
back-end catalog The storage method used by the Identity service catalog service to store and retrieve information
about API endpoints that are available to the client. Examples include an SQL database, LDAP
database, or KVS back end.
back-end store The persistent data store used to save and retrieve information for a service, such as lists of
Object Storage objects, current state of guest VMs, lists of user names, and so on. Also, the method that
the Image service uses to get and store VM images. Options include Object Storage, locally mounted
file system, RADOS block devices, VMware datastore, and HTTP.
Backup, Restore, and Disaster Recovery service (freezer) The project that provides integrated tooling for
backing up, restoring, and recovering file systems, instances, or database backups.
bandwidth The amount of available data used by communication resources, such as the Internet. Represents
the amount of data that is used to download things or the amount of data available to download.
barbican Code name of the Key Manager service.
bare An Image service container format that indicates that no container exists for the VM image.
Bare Metal service (ironic) The OpenStack service that provides a service and associated libraries capable
of managing and provisioning physical machines in a security-aware and fault-tolerant manner.
base image An OpenStack-provided image.
Bell-LaPadula model A security model that focuses on data confidentiality and controlled access to classified
information. This model divides the entities into subjects and objects. The clearance of a subject is
compared to the classification of the object to determine if the subject is authorized for the specific
access mode. The clearance or classification scheme is expressed in terms of a lattice.
Benchmark service (rally) OpenStack project that provides a framework for performance analysis and benchmarking
of individual OpenStack components as well as full production OpenStack cloud deployments.
Bexar A grouped release of projects related to OpenStack that came out in February of 2011. It included only
Compute (nova) and Object Storage (swift). Bexar is the code name for the second release of OpenStack.
The design summit took place in San Antonio, Texas, US, which is the county seat for Bexar county.
binary Information that consists solely of ones and zeroes, which is the language of computers.
Appendix 73
Architecture Design Guide (Release Version: 15.0.0)
bit A bit is a single digit number that is in base of 2 (either a zero or one). Bandwidth usage is measured in
bits per second.
bits per second (BPS) The universal measurement of how quickly data is transferred from place to place.
block device A device that moves data in the form of blocks. These device nodes interface the devices, such
as hard disks, CD-ROM drives, flash drives, and other addressable regions of memory.
block migration A method of VM live migration used by KVM to evacuate instances from one host to another
with very little downtime during a user-initiated switchover. Does not require shared storage. Supported
by Compute.
Block Storage API An API on a separate endpoint for attaching, detaching, and creating block storage for
compute VMs.
Block Storage service (cinder) The OpenStack service that implement services and libraries to provide ondemand,
self-service access to Block Storage resources via abstraction and automation on top of other
block storage devices.
BMC (Baseboard Management Controller) The intelligence in the IPMI architecture, which is a specialized
micro-controller that is embedded on the motherboard of a computer and acts as a server. Manages the
interface between system management software and platform hardware.
bootable disk image A type of VM image that exists as a single, bootable file.
Bootstrap Protocol (BOOTP) A network protocol used by a network client to obtain an IP address from a
configuration server. Provided in Compute through the dnsmasq daemon when using either the FlatDHCP
manager or VLAN manager network manager.
Border Gateway Protocol (BGP) The Border Gateway Protocol is a dynamic routing protocol that connects
autonomous systems. Considered the backbone of the Internet, this protocol connects disparate networks
to form a larger network.
browser Any client software that enables a computer or device to access the Internet.
builder file Contains configuration information that Object Storage uses to reconfigure a ring or to re-create
it from scratch after a serious failure.
bursting The practice of utilizing a secondary environment to elastically build instances on-demand when the
primary environment is resource constrained.
button class A group of related button types within horizon. Buttons to start, stop, and suspend VMs are in
one class. Buttons to associate and disassociate floating IP addresses are in another class, and so on.
byte Set of bits that make up a single character; there are usually 8 bits to a byte.
C
cache pruner A program that keeps the Image service VM image cache at or below its configured maximum
size.
Cactus An OpenStack grouped release of projects that came out in the spring of 2011. It included Compute
(nova), Object Storage (swift), and the Image service (glance). Cactus is a city in Texas, US and is the
code name for the third release of OpenStack. When OpenStack releases went from three to six months
long, the code name of the release changed to match a geography nearest the previous summit.
CALL One of the RPC primitives used by the OpenStack message queue software. Sends a message and
waits for a response.
74 Appendix
Architecture Design Guide (Release Version: 15.0.0)
capability Defines resources for a cell, including CPU, storage, and networking. Can apply to the specific
services within a cell or a whole cell.
capacity cache A Compute back-end database table that contains the current workload, amount of free RAM,
and number of VMs running on each host. Used to determine on which host a VM starts.
capacity updater A notification driver that monitors VM instances and updates the capacity cache as needed.
CAST One of the RPC primitives used by the OpenStack message queue software. Sends a message and does
not wait for a response.
catalog A list of API endpoints that are available to a user after authentication with the Identity service.
catalog service An Identity service that lists API endpoints that are available to a user after authentication
with the Identity service.
ceilometer Part of the OpenStack Telemetry service; gathers and stores metrics from other OpenStack services.
cell Provides logical partitioning of Compute resources in a child and parent relationship. Requests are passed
from parent cells to child cells if the parent cannot provide the requested resource.
cell forwarding A Compute option that enables parent cells to pass resource requests to child cells if the parent
cannot provide the requested resource.
cell manager The Compute component that contains a list of the current capabilities of each host within the
cell and routes requests as appropriate.
CentOS A Linux distribution that is compatible with OpenStack.
Ceph Massively scalable distributed storage system that consists of an object store, block store, and POSIXcompatible
distributed file system. Compatible with OpenStack.
CephFS The POSIX-compliant file system provided by Ceph.
certificate authority (CA) In cryptography, an entity that issues digital certificates. The digital certificate
certifies the ownership of a public key by the named subject of the certificate. This enables others
(relying parties) to rely upon signatures or assertions made by the private key that corresponds to the
certified public key. In this model of trust relationships, a CA is a trusted third party for both the subject
(owner) of the certificate and the party relying upon the certificate. CAs are characteristic of many public
key infrastructure (PKI) schemes. In OpenStack, a simple certificate authority is provided by Compute
for cloudpipe VPNs and VM image decryption.
Challenge-Handshake Authentication Protocol (CHAP) An iSCSI authentication method supported by
Compute.
chance scheduler A scheduling method used by Compute that randomly chooses an available host from the
pool.
changes since A Compute API parameter that downloads changes to the requested item since your last request,
instead of downloading a new, fresh set of data and comparing it against the old data.
Chef An operating system configuration management tool supporting OpenStack deployments.
child cell If a requested resource such as CPU time, disk storage, or memory is not available in the parent
cell, the request is forwarded to its associated child cells. If the child cell can fulfill the request, it does.
Otherwise, it attempts to pass the request to any of its children.
cinder Codename for Block Storage service.
CirrOS A minimal Linux distribution designed for use as a test image on clouds such as OpenStack.
Cisco neutron plug-in A Networking plug-in for Cisco devices and technologies, including UCS and Nexus.
Appendix 75
Architecture Design Guide (Release Version: 15.0.0)
cloud architect A person who plans, designs, and oversees the creation of clouds.
Cloud Auditing Data Federation (CADF) Cloud Auditing Data Federation (CADF) is a specification for
audit event data. CADF is supported by OpenStack Identity.
cloud computing A model that enables access to a shared pool of configurable computing resources, such as
networks, servers, storage, applications, and services, that can be rapidly provisioned and released with
minimal management effort or service provider interaction.
cloud controller Collection of Compute components that represent the global state of the cloud; talks to services,
such as Identity authentication, Object Storage, and node/storage workers through a queue.
cloud controller node A node that runs network, volume, API, scheduler, and image services. Each service
may be broken out into separate nodes for scalability or availability.
Cloud Data Management Interface (CDMI) SINA standard that defines a RESTful API for managing objects
in the cloud, currently unsupported in OpenStack.
Cloud Infrastructure Management Interface (CIMI) An in-progress specification for cloud management.
Currently unsupported in OpenStack.
cloud-init A package commonly installed in VM images that performs initialization of an instance after boot
using information that it retrieves from the metadata service, such as the SSH public key and user data.
cloudadmin One of the default roles in the Compute RBAC system. Grants complete system access.
Cloudbase-Init A Windows project providing guest initialization features, similar to cloud-init.
cloudpipe A compute service that creates VPNs on a per-project basis.
cloudpipe image A pre-made VM image that serves as a cloudpipe server. Essentially, OpenVPN running on
Linux.
Clustering service (senlin) The project that implements clustering services and libraries for the management
of groups of homogeneous objects exposed by other OpenStack services.
command filter Lists allowed commands within the Compute rootwrap facility.
Common Internet File System (CIFS) A file sharing protocol. It is a public or open variation of the original
Server Message Block (SMB) protocol developed and used by Microsoft. Like the SMB protocol, CIFS
runs at a higher level and uses the TCP/IP protocol.
Common Libraries (oslo) The project that produces a set of python libraries containing code shared by OpenStack
projects. The APIs provided by these libraries should be high quality, stable, consistent, documented
and generally applicable.
community project A project that is not officially endorsed by the OpenStack Foundation. If the project is
successful enough, it might be elevated to an incubated project and then to a core project, or it might be
merged with the main code trunk.
compression Reducing the size of files by special encoding, the file can be decompressed again to its original
content. OpenStack supports compression at the Linux file system level but does not support compression
for things such as Object Storage objects or Image service VM images.
Compute API (Nova API) The nova-api daemon provides access to nova services. Can communicate with
other APIs, such as the Amazon EC2 API.
compute controller The Compute component that chooses suitable hosts on which to start VM instances.
compute host Physical host dedicated to running compute nodes.
76 Appendix
Architecture Design Guide (Release Version: 15.0.0)
compute node A node that runs the nova-compute daemon that manages VM instances that provide a wide
range of services, such as web applications and analytics.
Compute service (nova) The OpenStack core project that implements services and associated libraries to
provide massively-scalable, on-demand, self-service access to compute resources, including bare metal,
virtual machines, and containers.
compute worker The Compute component that runs on each compute node and manages the VM instance
lifecycle, including run, reboot, terminate, attach/detach volumes, and so on. Provided by the novacompute
daemon.
concatenated object A set of segment objects that Object Storage combines and sends to the client.
conductor In Compute, conductor is the process that proxies database requests from the compute process.
Using conductor improves security because compute nodes do not need direct access to the database.
congress Code name for the Governance service.
consistency window The amount of time it takes for a new Object Storage object to become accessible to all
clients.
console log Contains the output from a Linux VM console in Compute.
container Organizes and stores objects in Object Storage. Similar to the concept of a Linux directory but
cannot be nested. Alternative term for an Image service container format.
container auditor Checks for missing replicas or incorrect objects in specified Object Storage containers
through queries to the SQLite back-end database.
container database A SQLite database that stores Object Storage containers and container metadata. The
container server accesses this database.
container format A wrapper used by the Image service that contains a VM image and its associated metadata,
such as machine state, OS disk size, and so on.
Container Infrastructure Management service (magnum) The project which provides a set of services for
provisioning, scaling, and managing container orchestration engines.
container server An Object Storage server that manages containers.
container service The Object Storage component that provides container services, such as create, delete, list,
and so on.
content delivery network (CDN) A content delivery network is a specialized network that is used to distribute
content to clients, typically located close to the client for increased performance.
controller node Alternative term for a cloud controller node.
core API Depending on context, the core API is either the OpenStack API or the main API of a specific core
project, such as Compute, Networking, Image service, and so on.
core service An official OpenStack service defined as core by DefCore Committee. Currently, consists of
Block Storage service (cinder), Compute service (nova), Identity service (keystone), Image service
(glance), Networking service (neutron), and Object Storage service (swift).
cost Under the Compute distributed scheduler, this is calculated by looking at the capabilities of each host
relative to the flavor of the VM instance being requested.
credentials Data that is only known to or accessible by a user and used to verify that the user is who he says
he is. Credentials are presented to the server during authentication. Examples include a password, secret
key, digital certificate, and fingerprint.
Appendix 77
Architecture Design Guide (Release Version: 15.0.0)
CRL A Certificate Revocation List (CRL) in a PKI model is a list of certificates that have been revoked. End
entities presenting these certificates should not be trusted.
Cross-Origin Resource Sharing (CORS) A mechanism that allows many resources (for example, fonts,
JavaScript) on a web page to be requested from another domain outside the domain from which the
resource originated. In particular, JavaScript’s AJAX calls can use the XMLHttpRequest mechanism.
Crowbar An open source community project by SUSE that aims to provide all necessary services to quickly
deploy and manage clouds.
current workload An element of the Compute capacity cache that is calculated based on the number of build,
snapshot, migrate, and resize operations currently in progress on a given host.
customer Alternative term for project.
customization module A user-created Python module that is loaded by horizon to change the look and feel
of the dashboard.
D
daemon A process that runs in the background and waits for requests. May or may not listen on a TCP or
UDP port. Do not confuse with a worker.
Dashboard (horizon) OpenStack project which provides an extensible, unified, web-based user interface for
all OpenStack services.
data encryption Both Image service and Compute support encrypted virtual machine (VM) images (but not
instances). In-transit data encryption is supported in OpenStack using technologies such as HTTPS,
SSL, TLS, and SSH. Object Storage does not support object encryption at the application level but may
support storage that uses disk encryption.
Data loss prevention (DLP) software Software programs used to protect sensitive information and prevent
it from leaking outside a network boundary through the detection and denying of the data transportation.
Data Processing service (sahara) OpenStack project that provides a scalable data-processing stack and associated
management interfaces.
data store A database engine supported by the Database service.
database ID A unique ID given to each replica of an Object Storage database.
database replicator An Object Storage component that copies changes in the account, container, and object
databases to other nodes.
Database service (trove) An integrated project that provides scalable and reliable Cloud Database-as-aService
functionality for both relational and non-relational database engines.
deallocate The process of removing the association between a floating IP address and a fixed IP address. Once
this association is removed, the floating IP returns to the address pool.
Debian A Linux distribution that is compatible with OpenStack.
deduplication The process of finding duplicate data at the disk block, file, and/or object level to minimize
storage use—currently unsupported within OpenStack.
default panel The default panel that is displayed when a user accesses the dashboard.
default project New users are assigned to this project if no project is specified when a user is created.
78 Appendix
Architecture Design Guide (Release Version: 15.0.0)
default token An Identity service token that is not associated with a specific project and is exchanged for a
scoped token.
delayed delete An option within Image service so that an image is deleted after a predefined number of seconds
instead of immediately.
delivery mode Setting for the Compute RabbitMQ message delivery mode; can be set to either transient or
persistent.
denial of service (DoS) Denial of service (DoS) is a short form for denial-of-service attack. This is a malicious
attempt to prevent legitimate users from using a service.
deprecated auth An option within Compute that enables administrators to create and manage users through
the nova-manage command as opposed to using the Identity service.
designate Code name for the DNS service.
Desktop-as-a-Service A platform that provides a suite of desktop environments that users access to receive
a desktop experience from any location. This may provide general use, development, or even homogeneous
testing environments.
developer One of the default roles in the Compute RBAC system and the default role assigned to a new user.
device ID Maps Object Storage partitions to physical storage devices.
device weight Distributes partitions proportionately across Object Storage devices based on the storage capacity
of each device.
DevStack Community project that uses shell scripts to quickly build complete OpenStack development environments.
DHCP agent OpenStack Networking agent that provides DHCP services for virtual networks.
Diablo A grouped release of projects related to OpenStack that came out in the fall of 2011, the fourth release
of OpenStack. It included Compute (nova 2011.3), Object Storage (swift 1.4.3), and the Image service
(glance). Diablo is the code name for the fourth release of OpenStack. The design summit took place in
the Bay Area near Santa Clara, California, US and Diablo is a nearby city.
direct consumer An element of the Compute RabbitMQ that comes to life when a RPC call is executed. It
connects to a direct exchange through a unique exclusive queue, sends the message, and terminates.
direct exchange A routing table that is created within the Compute RabbitMQ during RPC calls; one is created
for each RPC call that is invoked.
direct publisher Element of RabbitMQ that provides a response to an incoming MQ message.
disassociate The process of removing the association between a floating IP address and fixed IP and thus
returning the floating IP address to the address pool.
Discretionary Access Control (DAC) Governs the ability of subjects to access objects, while enabling users
to make policy decisions and assign security attributes. The traditional UNIX system of users, groups,
and read-write-execute permissions is an example of DAC.
disk encryption The ability to encrypt data at the file system, disk partition, or whole-disk level. Supported
within Compute VMs.
disk format The underlying format that a disk image for a VM is stored as within the Image service back-end
store. For example, AMI, ISO, QCOW2, VMDK, and so on.
dispersion In Object Storage, tools to test and ensure dispersion of objects and containers to ensure fault
tolerance.
Appendix 79
Architecture Design Guide (Release Version: 15.0.0)
distributed virtual router (DVR) Mechanism for highly available multi-host routing when using OpenStack
Networking (neutron).
Django A web framework used extensively in horizon.
DNS record A record that specifies information about a particular domain and belongs to the domain.
DNS service (designate) OpenStack project that provides scalable, on demand, self service access to authoritative
DNS services, in a technology-agnostic manner.
dnsmasq Daemon that provides DNS, DHCP, BOOTP, and TFTP services for virtual networks.
domain An Identity API v3 entity. Represents a collection of projects, groups and users that defines administrative
boundaries for managing OpenStack Identity entities. On the Internet, separates a website from
other sites. Often, the domain name has two or more parts that are separated by dots. For example,
yahoo.com, usa.gov, harvard.edu, or mail.yahoo.com. Also, a domain is an entity or container of all
DNS-related information containing one or more records.
Domain Name System (DNS) A system by which Internet domain name-to-address and address-to-name resolutions
are determined. DNS helps navigate the Internet by translating the IP address into an address
that is easier to remember. For example, translating 111.111.111.1 into www.yahoo.com. All domains
and their components, such as mail servers, utilize DNS to resolve to the appropriate locations. DNS
servers are usually set up in a master-slave relationship such that failure of the master invokes the slave.
DNS servers might also be clustered or replicated such that changes made to one DNS server are automatically
propagated to other active servers. In Compute, the support that enables associating DNS
entries with floating IP addresses, nodes, or cells so that hostnames are consistent across reboots.
download The transfer of data, usually in the form of files, from one computer to another.
durable exchange The Compute RabbitMQ message exchange that remains active when the server restarts.
durable queue A Compute RabbitMQ message queue that remains active when the server restarts.
Dynamic Host Configuration Protocol (DHCP) A network protocol that configures devices that are connected
to a network so that they can communicate on that network by using the Internet Protocol (IP).
The protocol is implemented in a client-server model where DHCP clients request configuration data,
such as an IP address, a default route, and one or more DNS server addresses from a DHCP server. A
method to automatically configure networking for a host at boot time. Provided by both Networking and
Compute.
Dynamic HyperText Markup Language (DHTML) Pages that use HTML, JavaScript, and Cascading Style
Sheets to enable users to interact with a web page or show simple animation.
E
east-west traffic Network traffic between servers in the same cloud or data center. See also north-south traffic.
EBS boot volume An Amazon EBS storage volume that contains a bootable VM image, currently unsupported
in OpenStack.
ebtables Filtering tool for a Linux bridging firewall, enabling filtering of network traffic passing through
a Linux bridge. Used in Compute along with arptables, iptables, and ip6tables to ensure isolation of
network communications.
EC2 The Amazon commercial compute product, similar to Compute.
EC2 access key Used along with an EC2 secret key to access the Compute EC2 API.
EC2 API OpenStack supports accessing the Amazon EC2 API through Compute.
80 Appendix
Architecture Design Guide (Release Version: 15.0.0)
EC2 Compatibility API A Compute component that enables OpenStack to communicate with Amazon EC2.
EC2 secret key Used along with an EC2 access key when communicating with the Compute EC2 API; used
to digitally sign each request.
Elastic Block Storage (EBS) The Amazon commercial block storage product.
encapsulation The practice of placing one packet type within another for the purposes of abstracting or securing
data. Examples include GRE, MPLS, or IPsec.
encryption OpenStack supports encryption technologies such as HTTPS, SSH, SSL, TLS, digital certificates,
and data encryption.
endpoint See API endpoint.
endpoint registry Alternative term for an Identity service catalog.
endpoint template A list of URL and port number endpoints that indicate where a service, such as Object
Storage, Compute, Identity, and so on, can be accessed.
entity Any piece of hardware or software that wants to connect to the network services provided by Networking,
the network connectivity service. An entity can make use of Networking by implementing a
VIF.
ephemeral image A VM image that does not save changes made to its volumes and reverts them to their
original state after the instance is terminated.
ephemeral volume Volume that does not save the changes made to it and reverts to its original state when the
current user relinquishes control.
Essex A grouped release of projects related to OpenStack that came out in April 2012, the fifth release of
OpenStack. It included Compute (nova 2012.1), Object Storage (swift 1.4.8), Image (glance), Identity
(keystone), and Dashboard (horizon). Essex is the code name for the fifth release of OpenStack. The
design summit took place in Boston, Massachusetts, US and Essex is a nearby city.
ESXi An OpenStack-supported hypervisor.
ETag MD5 hash of an object within Object Storage, used to ensure data integrity.
euca2ools A collection of command-line tools for administering VMs; most are compatible with OpenStack.
Eucalyptus Kernel Image (EKI) Used along with an ERI to create an EMI.
Eucalyptus Machine Image (EMI) VM image container format supported by Image service.
Eucalyptus Ramdisk Image (ERI) Used along with an EKI to create an EMI.
evacuate The process of migrating one or all virtual machine (VM) instances from one host to another, compatible
with both shared storage live migration and block migration.
exchange Alternative term for a RabbitMQ message exchange.
exchange type A routing algorithm in the Compute RabbitMQ.
exclusive queue Connected to by a direct consumer in RabbitMQ—Compute, the message can be consumed
only by the current connection.
extended attributes (xattr) File system option that enables storage of additional information beyond owner,
group, permissions, modification time, and so on. The underlying Object Storage file system must support
extended attributes.
extension Alternative term for an API extension or plug-in. In the context of Identity service, this is a call that
is specific to the implementation, such as adding support for OpenID.
Appendix 81
Architecture Design Guide (Release Version: 15.0.0)
external network A network segment typically used for instance Internet access.
extra specs Specifies additional requirements when Compute determines where to start a new instance. Examples
include a minimum amount of network bandwidth or a GPU.
F
FakeLDAP An easy method to create a local LDAP directory for testing Identity and Compute. Requires
Redis.
fan-out exchange Within RabbitMQ and Compute, it is the messaging interface that is used by the scheduler
service to receive capability messages from the compute, volume, and network nodes.
federated identity A method to establish trusts between identity providers and the OpenStack cloud.
Fedora A Linux distribution compatible with OpenStack.
Fibre Channel Storage protocol similar in concept to TCP/IP; encapsulates SCSI commands and data.
Fibre Channel over Ethernet (FCoE) The fibre channel protocol tunneled within Ethernet.
fill-first scheduler The Compute scheduling method that attempts to fill a host with VMs rather than starting
new VMs on a variety of hosts.
filter The step in the Compute scheduling process when hosts that cannot run VMs are eliminated and not
chosen.
firewall Used to restrict communications between hosts and/or nodes, implemented in Compute using iptables,
arptables, ip6tables, and ebtables.
FireWall-as-a-Service (FWaaS) A Networking extension that provides perimeter firewall functionality.
fixed IP address An IP address that is associated with the same instance each time that instance boots, is
generally not accessible to end users or the public Internet, and is used for management of the instance.
Flat Manager The Compute component that gives IP addresses to authorized nodes and assumes DHCP, DNS,
and routing configuration and services are provided by something else.
flat mode injection A Compute networking method where the OS network configuration information is injected
into the VM image before the instance starts.
flat network Virtual network type that uses neither VLANs nor tunnels to segregate project traffic. Each
flat network typically requires a separate underlying physical interface defined by bridge mappings.
However, a flat network can contain multiple subnets.
FlatDHCP Manager The Compute component that provides dnsmasq (DHCP, DNS, BOOTP, TFTP) and
radvd (routing) services.
flavor Alternative term for a VM instance type.
flavor ID UUID for each Compute or Image service VM flavor or instance type.
floating IP address An IP address that a project can associate with a VM so that the instance has the same
public IP address each time that it boots. You create a pool of floating IP addresses and assign them to
instances as they are launched to maintain a consistent IP address for maintaining DNS assignment.
Folsom A grouped release of projects related to OpenStack that came out in the fall of 2012, the sixth release
of OpenStack. It includes Compute (nova), Object Storage (swift), Identity (keystone), Networking
(neutron), Image service (glance), and Volumes or Block Storage (cinder). Folsom is the code name
82 Appendix
Architecture Design Guide (Release Version: 15.0.0)
for the sixth release of OpenStack. The design summit took place in San Francisco, California, US and
Folsom is a nearby city.
FormPost Object Storage middleware that uploads (posts) an image through a form on a web page.
freezer Code name for the Backup, Restore, and Disaster Recovery service.
front end The point where a user interacts with a service; can be an API endpoint, the dashboard, or a
command-line tool.
G
gateway An IP address, typically assigned to a router, that passes network traffic between different networks.
generic receive offload (GRO) Feature of certain network interface drivers that combines many smaller received
packets into a large packet before delivery to the kernel IP stack.
generic routing encapsulation (GRE) Protocol that encapsulates a wide variety of network layer protocols
inside virtual point-to-point links.
glance Codename for the Image service.
glance API server Alternative name for the Image API.
glance registry Alternative term for the Image service image registry.
global endpoint template The Identity service endpoint template that contains services available to all
projects.
GlusterFS A file system designed to aggregate NAS hosts, compatible with OpenStack.
gnocchi Part of the OpenStack Telemetry service; provides an indexer and time-series database.
golden image A method of operating system installation where a finalized disk image is created and then used
by all nodes without modification.
Governance service (congress) The project that provides Governance-as-a-Service across any collection of
cloud services in order to monitor, enforce, and audit policy over dynamic infrastructure.
Graphic Interchange Format (GIF) A type of image file that is commonly used for animated images on web
pages.
Graphics Processing Unit (GPU) Choosing a host based on the existence of a GPU is currently unsupported
in OpenStack.
Green Threads The cooperative threading model used by Python; reduces race conditions and only context
switches when specific library calls are made. Each OpenStack service is its own thread.
Grizzly The code name for the seventh release of OpenStack. The design summit took place in San Diego,
California, US and Grizzly is an element of the state flag of California.
Group An Identity v3 API entity. Represents a collection of users that is owned by a specific domain.
guest OS An operating system instance running under the control of a hypervisor.
H
Hadoop Apache Hadoop is an open source software framework that supports data-intensive distributed applications.
Appendix 83
Architecture Design Guide (Release Version: 15.0.0)
Hadoop Distributed File System (HDFS) A distributed, highly fault-tolerant file system designed to run on
low-cost commodity hardware.
handover An object state in Object Storage where a new replica of the object is automatically created due to
a drive failure.
HAProxy Provides a load balancer for TCP and HTTP-based applications that spreads requests across multiple
servers.
hard reboot A type of reboot where a physical or virtual power button is pressed as opposed to a graceful,
proper shutdown of the operating system.
Havana The code name for the eighth release of OpenStack. The design summit took place in Portland,
Oregon, US and Havana is an unincorporated community in Oregon.
health monitor Determines whether back-end members of a VIP pool can process a request. A pool can
have several health monitors associated with it. When a pool has several monitors associated with it, all
monitors check each member of the pool. All monitors must declare a member to be healthy for it to stay
active.
heat Codename for the Orchestration service.
Heat Orchestration Template (HOT) Heat input in the format native to OpenStack.
high availability (HA) A high availability system design approach and associated service implementation
ensures that a prearranged level of operational performance will be met during a contractual measurement
period. High availability systems seek to minimize system downtime and data loss.
horizon Codename for the Dashboard.
horizon plug-in A plug-in for the OpenStack Dashboard (horizon).
host A physical computer, not a VM instance (node).
host aggregate A method to further subdivide availability zones into hypervisor pools, a collection of common
hosts.
Host Bus Adapter (HBA) Device plugged into a PCI slot, such as a fibre channel or network card.
hybrid cloud A hybrid cloud is a composition of two or more clouds (private, community or public) that
remain distinct entities but are bound together, offering the benefits of multiple deployment models.
Hybrid cloud can also mean the ability to connect colocation, managed and/or dedicated services with
cloud resources.
Hyper-V One of the hypervisors supported by OpenStack.
hyperlink Any kind of text that contains a link to some other site, commonly found in documents where
clicking on a word or words opens up a different website.
Hypertext Transfer Protocol (HTTP) An application protocol for distributed, collaborative, hypermedia information
systems. It is the foundation of data communication for the World Wide Web. Hypertext is
structured text that uses logical links (hyperlinks) between nodes containing text. HTTP is the protocol
to exchange or transfer hypertext.
Hypertext Transfer Protocol Secure (HTTPS) An encrypted communications protocol for secure communication
over a computer network, with especially wide deployment on the Internet. Technically, it is not a
protocol in and of itself; rather, it is the result of simply layering the Hypertext Transfer Protocol (HTTP)
on top of the TLS or SSL protocol, thus adding the security capabilities of TLS or SSL to standard HTTP
communications. Most OpenStack API endpoints and many inter-component communications support
HTTPS communication.
84 Appendix
Architecture Design Guide (Release Version: 15.0.0)
hypervisor Software that arbitrates and controls VM access to the actual underlying hardware.
hypervisor pool A collection of hypervisors grouped together through host aggregates.
I
Icehouse The code name for the ninth release of OpenStack. The design summit took place in Hong Kong
and Ice House is a street in that city.
ID number Unique numeric ID associated with each user in Identity, conceptually similar to a Linux or LDAP
UID.
Identity API Alternative term for the Identity service API.
Identity back end The source used by Identity service to retrieve user information; an OpenLDAP server, for
example.
identity provider A directory service, which allows users to login with a user name and password. It is a
typical source of authentication tokens.
Identity service (keystone) The project that facilitates API client authentication, service discovery, distributed
multi-project authorization, and auditing. It provides a central directory of users mapped to
the OpenStack services they can access. It also registers endpoints for OpenStack services and acts as a
common authentication system.
Identity service API The API used to access the OpenStack Identity service provided through keystone.
IETF Internet Engineering Task Force (IETF) is an open standards organization that develops Internet standards,
particularly the standards pertaining to TCP/IP.
image A collection of files for a specific operating system (OS) that you use to create or rebuild a server.
OpenStack provides pre-built images. You can also create custom images, or snapshots, from servers
that you have launched. Custom images can be used for data backups or as “gold” images for additional
servers.
Image API The Image service API endpoint for management of VM images. Processes client requests for
VMs, updates Image service metadata on the registry server, and communicates with the store adapter to
upload VM images from the back-end store.
image cache Used by Image service to obtain images on the local host rather than re-downloading them from
the image server each time one is requested.
image ID Combination of a URI and UUID used to access Image service VM images through the image API.
image membership A list of projects that can access a given VM image within Image service.
image owner The project who owns an Image service virtual machine image.
image registry A list of VM images that are available through Image service.
Image service (glance) The OpenStack service that provide services and associated libraries to store, browse,
share, distribute and manage bootable disk images, other data closely associated with initializing compute
resources, and metadata definitions.
image status The current status of a VM image in Image service, not to be confused with the status of a running
instance.
image store The back-end store used by Image service to store VM images, options include Object Storage,
locally mounted file system, RADOS block devices, VMware datastore, or HTTP.
Appendix 85
Architecture Design Guide (Release Version: 15.0.0)
image UUID UUID used by Image service to uniquely identify each VM image.
incubated project A community project may be elevated to this status and is then promoted to a core project.
Infrastructure Optimization service (watcher) OpenStack project that aims to provide a flexible and scalable
resource optimization service for multi-project OpenStack-based clouds.
Infrastructure-as-a-Service (IaaS) IaaS is a provisioning model in which an organization outsources physical
components of a data center, such as storage, hardware, servers, and networking components. A service
provider owns the equipment and is responsible for housing, operating and maintaining it. The client
typically pays on a per-use basis. IaaS is a model for providing cloud services.
ingress filtering The process of filtering incoming network traffic. Supported by Compute.
INI format The OpenStack configuration files use an INI format to describe options and their values. It
consists of sections and key value pairs.
injection The process of putting a file into a virtual machine image before the instance is started.
Input/Output Operations Per Second (IOPS) IOPS are a common performance measurement used to
benchmark computer storage devices like hard disk drives, solid state drives, and storage area networks.
instance A running VM, or a VM in a known state such as suspended, that can be used like a hardware server.
instance ID Alternative term for instance UUID.
instance state The current state of a guest VM image.
instance tunnels network A network segment used for instance traffic tunnels between compute nodes and
the network node.
instance type Describes the parameters of the various virtual machine images that are available to users;
includes parameters such as CPU, storage, and memory. Alternative term for flavor.
instance type ID Alternative term for a flavor ID.
instance UUID Unique ID assigned to each guest VM instance.
Intelligent Platform Management Interface (IPMI) IPMI is a standardized computer system interface used
by system administrators for out-of-band management of computer systems and monitoring of their operation.
In layman’s terms, it is a way to manage a computer using a direct network connection, whether
it is turned on or not; connecting to the hardware rather than an operating system or login shell.
interface A physical or virtual device that provides connectivity to another device or medium.
interface ID Unique ID for a Networking VIF or vNIC in the form of a UUID.
Internet Control Message Protocol (ICMP) A network protocol used by network devices for control messages.
For example, ping uses ICMP to test connectivity.
Internet protocol (IP) Principal communications protocol in the internet protocol suite for relaying datagrams
across network boundaries.
Internet Service Provider (ISP) Any business that provides Internet access to individuals or businesses.
Internet Small Computer System Interface (iSCSI) Storage protocol that encapsulates SCSI frames for
transport over IP networks. Supported by Compute, Object Storage, and Image service.
IP address Number that is unique to every computer system on the Internet. Two versions of the Internet
Protocol (IP) are in use for addresses: IPv4 and IPv6.
IP Address Management (IPAM) The process of automating IP address allocation, deallocation, and management.
Currently provided by Compute, melange, and Networking.
86 Appendix
Architecture Design Guide (Release Version: 15.0.0)
ip6tables Tool used to set up, maintain, and inspect the tables of IPv6 packet filter rules in the Linux kernel.
In OpenStack Compute, ip6tables is used along with arptables, ebtables, and iptables to create firewalls
for both nodes and VMs.
ipset Extension to iptables that allows creation of firewall rules that match entire “sets” of IP addresses simultaneously.
These sets reside in indexed data structures to increase efficiency, particularly on systems
with a large quantity of rules.
iptables Used along with arptables and ebtables, iptables create firewalls in Compute. iptables are the tables
provided by the Linux kernel firewall (implemented as different Netfilter modules) and the chains and
rules it stores. Different kernel modules and programs are currently used for different protocols: iptables
applies to IPv4, ip6tables to IPv6, arptables to ARP, and ebtables to Ethernet frames. Requires root
privilege to manipulate.
ironic Codename for the Bare Metal service.
iSCSI Qualified Name (IQN) IQN is the format most commonly used for iSCSI names, which uniquely identify
nodes in an iSCSI network. All IQNs follow the pattern iqn.yyyy-mm.domain:identifier, where
‘yyyy-mm’ is the year and month in which the domain was registered, ‘domain’ is the reversed domain
name of the issuing organization, and ‘identifier’ is an optional string which makes each IQN under the
same domain unique. For example, ‘iqn.2015-10.org.openstack.408ae959bce1’.
ISO9660 One of the VM image disk formats supported by Image service.
itsec A default role in the Compute RBAC system that can quarantine an instance in any project.
J
Java A programming language that is used to create systems that involve more than one computer by way of
a network.
JavaScript A scripting language that is used to build web pages.
JavaScript Object Notation (JSON) One of the supported response formats in OpenStack.
jumbo frame Feature in modern Ethernet networks that supports frames up to approximately 9000 bytes.
Juno The code name for the tenth release of OpenStack. The design summit took place in Atlanta, Georgia,
US and Juno is an unincorporated community in Georgia.
K
Kerberos A network authentication protocol which works on the basis of tickets. Kerberos allows nodes
communication over a non-secure network, and allows nodes to prove their identity to one another in a
secure manner.
kernel-based VM (KVM) An OpenStack-supported hypervisor. KVM is a full virtualization solution for
Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V), ARM, IBM Power,
and IBM zSeries. It consists of a loadable kernel module, that provides the core virtualization infrastructure
and a processor specific module.
Key Manager service (barbican) The project that produces a secret storage and generation system capable
of providing key management for services wishing to enable encryption features.
keystone Codename of the Identity service.
Appendix 87
Architecture Design Guide (Release Version: 15.0.0)
Kickstart A tool to automate system configuration and installation on Red Hat, Fedora, and CentOS-based
Linux distributions.
Kilo The code name for the eleventh release of OpenStack. The design summit took place in Paris, France.
Due to delays in the name selection, the release was known only as K. Because k is the unit symbol
for kilo and the kilogram reference artifact is stored near Paris in the Pavillon de Breteuil in Sèvres, the
community chose Kilo as the release name.
L
large object An object within Object Storage that is larger than 5 GB.
Launchpad The collaboration site for OpenStack.
Layer-2 (L2) agent OpenStack Networking agent that provides layer-2 connectivity for virtual networks.
Layer-2 network Term used in the OSI network architecture for the data link layer. The data link layer is
responsible for media access control, flow control and detecting and possibly correcting errors that may
occur in the physical layer.
Layer-3 (L3) agent OpenStack Networking agent that provides layer-3 (routing) services for virtual networks.
Layer-3 network Term used in the OSI network architecture for the network layer. The network layer is
responsible for packet forwarding including routing from one node to another.
Liberty The code name for the twelfth release of OpenStack. The design summit took place in Vancouver,
Canada and Liberty is the name of a village in the Canadian province of Saskatchewan.
libvirt Virtualization API library used by OpenStack to interact with many of its supported hypervisors.
Lightweight Directory Access Protocol (LDAP) An application protocol for accessing and maintaining distributed
directory information services over an IP network.
Linux Unix-like computer operating system assembled under the model of free and open-source software
development and distribution.
Linux bridge Software that enables multiple VMs to share a single physical NIC within Compute.
Linux Bridge neutron plug-in Enables a Linux bridge to understand a Networking port, interface attachment,
and other abstractions.
Linux containers (LXC) An OpenStack-supported hypervisor.
live migration The ability within Compute to move running virtual machine instances from one host to another
with only a small service interruption during switchover.
load balancer A load balancer is a logical device that belongs to a cloud account. It is used to distribute
workloads between multiple back-end systems or services, based on the criteria defined as part of its
configuration.
load balancing The process of spreading client requests between two or more nodes to improve performance
and availability.
Load-Balancer-as-a-Service (LBaaS) Enables Networking to distribute incoming requests evenly between
designated instances.
Load-balancing service (octavia) The project that aims to provide scalable, on demand, self service access
to load-balancer services, in technology-agnostic manner.
88 Appendix
Architecture Design Guide (Release Version: 15.0.0)
Logical Volume Manager (LVM) Provides a method of allocating space on mass-storage devices that is more
flexible than conventional partitioning schemes.
M
magnum Code name for the Containers Infrastructure Management service.
management API Alternative term for an admin API.
management network A network segment used for administration, not accessible to the public Internet.
manager Logical groupings of related code, such as the Block Storage volume manager or network manager.
manifest Used to track segments of a large object within Object Storage.
manifest object A special Object Storage object that contains the manifest for a large object.
manila Codename for OpenStack Shared File Systems service.
manila-share Responsible for managing Shared File System Service devices, specifically the back-end devices.
maximum transmission unit (MTU) Maximum frame or packet size for a particular network medium. Typically
1500 bytes for Ethernet networks.
mechanism driver A driver for the Modular Layer 2 (ML2) neutron plug-in that provides layer-2 connectivity
for virtual instances. A single OpenStack installation can use multiple mechanism drivers.
melange Project name for OpenStack Network Information Service. To be merged with Networking.
membership The association between an Image service VM image and a project. Enables images to be shared
with specified projects.
membership list A list of projects that can access a given VM image within Image service.
memcached A distributed memory object caching system that is used by Object Storage for caching.
memory overcommit The ability to start new VM instances based on the actual memory usage of a host, as
opposed to basing the decision on the amount of RAM each running instance thinks it has available. Also
known as RAM overcommit.
message broker The software package used to provide AMQP messaging capabilities within Compute. Default
package is RabbitMQ.
message bus The main virtual communication line used by all AMQP messages for inter-cloud communications
within Compute.
message queue Passes requests from clients to the appropriate workers and returns the output to the client
after the job completes.
Message service (zaqar) The project that provides a messaging service that affords a variety of distributed
application patterns in an efficient, scalable and highly available manner, and to create and maintain
associated Python libraries and documentation.
Meta-Data Server (MDS) Stores CephFS metadata.
Metadata agent OpenStack Networking agent that provides metadata services for instances.
migration The process of moving a VM instance from one host to another.
mistral Code name for Workflow service.
Appendix 89
Architecture Design Guide (Release Version: 15.0.0)
Mitaka The code name for the thirteenth release of OpenStack. The design summit took place in Tokyo,
Japan. Mitaka is a city in Tokyo.
Modular Layer 2 (ML2) neutron plug-in Can concurrently use multiple layer-2 networking technologies,
such as 802.1Q and VXLAN, in Networking.
monasca Codename for OpenStack Monitoring.
Monitor (LBaaS) LBaaS feature that provides availability monitoring using the ping command, TCP, and
HTTP/HTTPS GET.
Monitor (Mon) A Ceph component that communicates with external clients, checks data state and consistency,
and performs quorum functions.
Monitoring (monasca) The OpenStack service that provides a multi-project, highly scalable, performant,
fault-tolerant monitoring-as-a-service solution for metrics, complex event processing and logging. To
build an extensible platform for advanced monitoring services that can be used by both operators and
projects to gain operational insight and visibility, ensuring availability and stability.
multi-factor authentication Authentication method that uses two or more credentials, such as a password
and a private key. Currently not supported in Identity.
multi-host High-availability mode for legacy (nova) networking. Each compute node handles NAT and DHCP
and acts as a gateway for all of the VMs on it. A networking failure on one compute node doesn’t affect
VMs on other compute nodes.
multinic Facility in Compute that allows each virtual machine instance to have more than one VIF connected
to it.
murano Codename for the Application Catalog service.
N
Nebula Released as open source by NASA in 2010 and is the basis for Compute.
netadmin One of the default roles in the Compute RBAC system. Enables the user to allocate publicly accessible
IP addresses to instances and change firewall rules.
NetApp volume driver Enables Compute to communicate with NetApp storage devices through the NetApp
OnCommand Provisioning Manager.
network A virtual network that provides connectivity between entities. For example, a collection of virtual
ports that share network connectivity. In Networking terminology, a network is always a layer-2 network.
Network Address Translation (NAT) Process of modifying IP address information while in transit. Supported
by Compute and Networking.
network controller A Compute daemon that orchestrates the network configuration of nodes, including IP
addresses, VLANs, and bridging. Also manages routing for both public and private networks.
Network File System (NFS) A method for making file systems available over the network. Supported by
OpenStack.
network ID Unique ID assigned to each network segment within Networking. Same as network UUID.
network manager The Compute component that manages various network components, such as firewall rules,
IP address allocation, and so on.
90 Appendix
Architecture Design Guide (Release Version: 15.0.0)
network namespace Linux kernel feature that provides independent virtual networking instances on a single
host with separate routing tables and interfaces. Similar to virtual routing and forwarding (VRF) services
on physical network equipment.
network node Any compute node that runs the network worker daemon.
network segment Represents a virtual, isolated OSI layer-2 subnet in Networking.
Network Service Header (NSH) Provides a mechanism for metadata exchange along the instantiated service
path.
Network Time Protocol (NTP) Method of keeping a clock for a host or node correct via communication with
a trusted, accurate time source.
network UUID Unique ID for a Networking network segment.
network worker The nova-network worker daemon; provides services such as giving an IP address to a
booting nova instance.
Networking API (Neutron API) API used to access OpenStack Networking. Provides an extensible architecture
to enable custom plug-in creation.
Networking service (neutron) The OpenStack project which implements services and associated libraries to
provide on-demand, scalable, and technology-agnostic network abstraction.
neutron Codename for OpenStack Networking service.
neutron API An alternative name for Networking API.
neutron manager Enables Compute and Networking integration, which enables Networking to perform network
management for guest VMs.
neutron plug-in Interface within Networking that enables organizations to create custom plug-ins for advanced
features, such as QoS, ACLs, or IDS.
Newton The code name for the fourteenth release of OpenStack. The design summit took place in Austin,
Texas, US. The release is named after “Newton House” which is located at 1013 E. Ninth St., Austin,
TX. which is listed on the National Register of Historic Places.
Nexenta volume driver Provides support for NexentaStor devices in Compute.
NFV Orchestration Service (tacker) OpenStack service that aims to implement Network Function Virtualization
(NFV) orchestration services and libraries for end-to-end life-cycle management of network
services and Virtual Network Functions (VNFs).
Nginx An HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server.
No ACK Disables server-side message acknowledgment in the Compute RabbitMQ. Increases performance
but decreases reliability.
node A VM instance that runs on a host.
non-durable exchange Message exchange that is cleared when the service restarts. Its data is not written to
persistent storage.
non-durable queue Message queue that is cleared when the service restarts. Its data is not written to persistent
storage.
non-persistent volume Alternative term for an ephemeral volume.
north-south traffic Network traffic between a user or client (north) and a server (south), or traffic into the
cloud (south) and out of the cloud (north). See also east-west traffic.
Appendix 91
Architecture Design Guide (Release Version: 15.0.0)
nova Codename for OpenStack Compute service.
Nova API Alternative term for the Compute API.
nova-network A Compute component that manages IP address allocation, firewalls, and other network-related
tasks. This is the legacy networking option and an alternative to Networking.
O
object A BLOB of data held by Object Storage; can be in any format.
object auditor Opens all objects for an object server and verifies the MD5 hash, size, and metadata for each
object.
object expiration A configurable option within Object Storage to automatically delete objects after a specified
amount of time has passed or a certain date is reached.
object hash Unique ID for an Object Storage object.
object path hash Used by Object Storage to determine the location of an object in the ring. Maps objects to
partitions.
object replicator An Object Storage component that copies an object to remote partitions for fault tolerance.
object server An Object Storage component that is responsible for managing objects.
Object Storage API API used to access OpenStack Object Storage.
Object Storage Device (OSD) The Ceph storage daemon.
Object Storage service (swift) The OpenStack core project that provides eventually consistent and redundant
storage and retrieval of fixed digital content.
object versioning Allows a user to set a flag on an Object Storage container so that all objects within the
container are versioned.
Ocata The code name for the fifteenth release of OpenStack. The design summit will take place in Barcelona,
Spain. Ocata is a beach north of Barcelona.
Octavia Code name for the Load-balancing service.
Oldie Term for an Object Storage process that runs for a long time. Can indicate a hung process.
Open Cloud Computing Interface (OCCI) A standardized interface for managing compute, data, and network
resources, currently unsupported in OpenStack.
Open Virtualization Format (OVF) Standard for packaging VM images. Supported in OpenStack.
Open vSwitch Open vSwitch is a production quality, multilayer virtual switch licensed under the open source
Apache 2.0 license. It is designed to enable massive network automation through programmatic extension,
while still supporting standard management interfaces and protocols (for example NetFlow, sFlow,
SPAN, RSPAN, CLI, LACP, 802.1ag).
Open vSwitch (OVS) agent Provides an interface to the underlying Open vSwitch service for the Networking
plug-in.
Open vSwitch neutron plug-in Provides support for Open vSwitch in Networking.
OpenLDAP An open source LDAP server. Supported by both Compute and Identity.
92 Appendix
Architecture Design Guide (Release Version: 15.0.0)
OpenStack OpenStack is a cloud operating system that controls large pools of compute, storage, and networking
resources throughout a data center, all managed through a dashboard that gives administrators
control while empowering their users to provision resources through a web interface. OpenStack is an
open source project licensed under the Apache License 2.0.
OpenStack code name Each OpenStack release has a code name. Code names ascend in alphabetical order:
Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, Icehouse, Juno, Kilo, Liberty, Mitaka,
Newton, Ocata, Pike, Queens, and Rocky. Code names are cities or counties near where the corresponding
OpenStack design summit took place. An exception, called the Waldon exception, is granted to
elements of the state flag that sound especially cool. Code names are chosen by popular vote.
openSUSE A Linux distribution that is compatible with OpenStack.
operator The person responsible for planning and maintaining an OpenStack installation.
optional service An official OpenStack service defined as optional by DefCore Committee. Currently, consists
of Dashboard (horizon), Telemetry service (Telemetry), Orchestration service (heat), Database service
(trove), Bare Metal service (ironic), and so on.
Orchestration service (heat) The OpenStack service which orchestrates composite cloud applications using
a declarative template format through an OpenStack-native REST API.
orphan In the context of Object Storage, this is a process that is not terminated after an upgrade, restart, or
reload of the service.
Oslo Codename for the Common Libraries project.
P
panko Part of the OpenStack Telemetry service; provides event storage.
parent cell If a requested resource, such as CPU time, disk storage, or memory, is not available in the parent
cell, the request is forwarded to associated child cells.
partition A unit of storage within Object Storage used to store objects. It exists on top of devices and is
replicated for fault tolerance.
partition index Contains the locations of all Object Storage partitions within the ring.
partition shift value Used by Object Storage to determine which partition data should reside on.
path MTU discovery (PMTUD) Mechanism in IP networks to detect end-to-end MTU and adjust packet size
accordingly.
pause A VM state where no changes occur (no changes in memory, network communications stop, etc); the
VM is frozen but not shut down.
PCI passthrough Gives guest VMs exclusive access to a PCI device. Currently supported in OpenStack
Havana and later releases.
persistent message A message that is stored both in memory and on disk. The message is not lost after a
failure or restart.
persistent volume Changes to these types of disk volumes are saved.
personality file A file used to customize a Compute instance. It can be used to inject SSH keys or a specific
network configuration.
Appendix 93
Architecture Design Guide (Release Version: 15.0.0)
Pike The code name for the sixteenth release of OpenStack. The design summit will take place in Boston,
Massachusetts, US. The release is named after the Massachusetts Turnpike, abbreviated commonly as
the Mass Pike, which is the easternmost stretch of Interstate 90.
Platform-as-a-Service (PaaS) Provides to the consumer the ability to deploy applications through a programming
language or tools supported by the cloud platform provider. An example of Platform-as-a-Service
is an Eclipse/Java programming platform provided with no downloads required.
plug-in Software component providing the actual implementation for Networking APIs, or for Compute APIs,
depending on the context.
policy service Component of Identity that provides a rule-management interface and a rule-based authorization
engine.
policy-based routing (PBR) Provides a mechanism to implement packet forwarding and routing according
to the policies defined by the network administrator.
pool A logical set of devices, such as web servers, that you group together to receive and process traffic.
The load balancing function chooses which member of the pool handles the new requests or connections
received on the VIP address. Each VIP has one pool.
pool member An application that runs on the back-end server in a load-balancing system.
port A virtual network port within Networking; VIFs / vNICs are connected to a port.
port UUID Unique ID for a Networking port.
preseed A tool to automate system configuration and installation on Debian-based Linux distributions.
private image An Image service VM image that is only available to specified projects.
private IP address An IP address used for management and administration, not available to the public Internet.
private network The Network Controller provides virtual networks to enable compute servers to interact
with each other and with the public network. All machines must have a public and private network
interface. A private network interface can be a flat or VLAN network interface. A flat network interface
is controlled by the flat_interface with flat managers. A VLAN network interface is controlled by the
vlan_interface option with VLAN managers.
project Projects represent the base unit of “ownership” in OpenStack, in that all resources in OpenStack should
be owned by a specific project. In OpenStack Identity, a project must be owned by a specific domain.
project ID Unique ID assigned to each project by the Identity service.
project VPN Alternative term for a cloudpipe.
promiscuous mode Causes the network interface to pass all traffic it receives to the host rather than passing
only the frames addressed to it.
protected property Generally, extra properties on an Image service image to which only cloud administrators
have access. Limits which user roles can perform CRUD operations on that property. The cloud
administrator can configure any image property as protected.
provider An administrator who has access to all hosts and instances.
proxy node A node that provides the Object Storage proxy service.
proxy server Users of Object Storage interact with the service through the proxy server, which in turn looks
up the location of the requested data within the ring and returns the results to the user.
public API An API endpoint used for both service-to-service communication and end-user interactions.
94 Appendix
Architecture Design Guide (Release Version: 15.0.0)
public image An Image service VM image that is available to all projects.
public IP address An IP address that is accessible to end-users.
public key authentication Authentication method that uses keys rather than passwords.
public network The Network Controller provides virtual networks to enable compute servers to interact with
each other and with the public network. All machines must have a public and private network interface.
The public network interface is controlled by the public_interface option.
Puppet An operating system configuration-management tool supported by OpenStack.
Python Programming language used extensively in OpenStack.
Q
QEMU Copy On Write 2 (QCOW2) One of the VM image disk formats supported by Image service.
Qpid Message queue software supported by OpenStack; an alternative to RabbitMQ.
Quality of Service (QoS) The ability to guarantee certain network or storage requirements to satisfy a Service
Level Agreement (SLA) between an application provider and end users. Typically includes performance
requirements like networking bandwidth, latency, jitter correction, and reliability as well as storage performance
in Input/Output Operations Per Second (IOPS), throttling agreements, and performance expectations
at peak load.
quarantine If Object Storage finds objects, containers, or accounts that are corrupt, they are placed in this
state, are not replicated, cannot be read by clients, and a correct copy is re-replicated.
Queens The code name for the seventeenth release of OpenStack. The design summit will take place in
Sydney, Australia. The release is named after the Queens Pound river in the South Coast region of New
South Wales.
Quick EMUlator (QEMU) QEMU is a generic and open source machine emulator and virtualizer. One of
the hypervisors supported by OpenStack, generally used for development purposes.
quota In Compute and Block Storage, the ability to set resource limits on a per-project basis.
R
RabbitMQ The default message queue software used by OpenStack.
Rackspace Cloud Files Released as open source by Rackspace in 2010; the basis for Object Storage.
RADOS Block Device (RBD) Ceph component that enables a Linux block device to be striped over multiple
distributed data stores.
radvd The router advertisement daemon, used by the Compute VLAN manager and FlatDHCP manager to
provide routing services for VM instances.
rally Codename for the Benchmark service.
RAM filter The Compute setting that enables or disables RAM overcommitment.
RAM overcommit The ability to start new VM instances based on the actual memory usage of a host, as
opposed to basing the decision on the amount of RAM each running instance thinks it has available.
Also known as memory overcommit.
Appendix 95
Architecture Design Guide (Release Version: 15.0.0)
rate limit Configurable option within Object Storage to limit database writes on a per-account and/or percontainer
basis.
raw One of the VM image disk formats supported by Image service; an unstructured disk image.
rebalance The process of distributing Object Storage partitions across all drives in the ring; used during initial
ring creation and after ring reconfiguration.
reboot Either a soft or hard reboot of a server. With a soft reboot, the operating system is signaled to restart,
which enables a graceful shutdown of all processes. A hard reboot is the equivalent of power cycling
the server. The virtualization platform should ensure that the reboot action has completed successfully,
even in cases in which the underlying domain/VM is paused or halted/stopped.
rebuild Removes all data on the server and replaces it with the specified image. Server ID and IP addresses
remain the same.
Recon An Object Storage component that collects meters.
record Belongs to a particular domain and is used to specify information about the domain. There are several
types of DNS records. Each record type contains particular information used to describe the purpose of
that record. Examples include mail exchange (MX) records, which specify the mail server for a particular
domain; and name server (NS) records, which specify the authoritative name servers for a domain.
record ID A number within a database that is incremented each time a change is made. Used by Object
Storage when replicating.
Red Hat Enterprise Linux (RHEL) A Linux distribution that is compatible with OpenStack.
reference architecture A recommended architecture for an OpenStack cloud.
region A discrete OpenStack environment with dedicated API endpoints that typically shares only the Identity
(keystone) with other regions.
registry Alternative term for the Image service registry.
registry server An Image service that provides VM image metadata information to clients.
Reliable, Autonomic Distributed Object Store (RADOS)
A collection of components that provides object storage within Ceph. Similar to OpenStack Object
Storage.
Remote Procedure Call (RPC) The method used by the Compute RabbitMQ for intra-service communications.
replica Provides data redundancy and fault tolerance by creating copies of Object Storage objects, accounts,
and containers so that they are not lost when the underlying storage fails.
replica count The number of replicas of the data in an Object Storage ring.
replication The process of copying data to a separate physical device for fault tolerance and performance.
replicator The Object Storage back-end process that creates and manages object replicas.
request ID Unique ID assigned to each request sent to Compute.
rescue image A special type of VM image that is booted when an instance is placed into rescue mode. Allows
an administrator to mount the file systems for an instance to correct the problem.
resize Converts an existing server to a different flavor, which scales the server up or down. The original server
is saved to enable rollback if a problem occurs. All resizes must be tested and explicitly confirmed, at
which time the original server is removed.
96 Appendix
Architecture Design Guide (Release Version: 15.0.0)
RESTful A kind of web service API that uses REST, or Representational State Transfer. REST is the style of
architecture for hypermedia systems that is used for the World Wide Web.
ring An entity that maps Object Storage data to partitions. A separate ring exists for each service, such as
account, object, and container.
ring builder Builds and manages rings within Object Storage, assigns partitions to devices, and pushes the
configuration to other storage nodes.
Rocky The code name for the eightteenth release of OpenStack. The design summit will take place in Vancouver,
Kanada. The release is named after the Rocky Mountains.
role A personality that a user assumes to perform a specific set of operations. A role includes a set of rights
and privileges. A user assuming that role inherits those rights and privileges.
Role Based Access Control (RBAC) Provides a predefined list of actions that the user can perform, such
as start or stop VMs, reset passwords, and so on. Supported in both Identity and Compute and can be
configured using the dashboard.
role ID Alphanumeric ID assigned to each Identity service role.
Root Cause Analysis (RCA) service (Vitrage) OpenStack project that aims to organize, analyze and visualize
OpenStack alarms and events, yield insights regarding the root cause of problems and deduce their
existence before they are directly detected.
rootwrap A feature of Compute that allows the unprivileged “nova” user to run a specified list of commands
as the Linux root user.
round-robin scheduler Type of Compute scheduler that evenly distributes instances among available hosts.
router A physical or virtual network device that passes network traffic between different networks.
routing key The Compute direct exchanges, fanout exchanges, and topic exchanges use this key to determine
how to process a message; processing varies depending on exchange type.
RPC driver Modular system that allows the underlying message queue software of Compute to be changed.
For example, from RabbitMQ to ZeroMQ or Qpid.
rsync Used by Object Storage to push object replicas.
RXTX cap Absolute limit on the amount of network traffic a Compute VM instance can send and receive.
RXTX quota Soft limit on the amount of network traffic a Compute VM instance can send and receive.
S
sahara Codename for the Data Processing service.
SAML assertion Contains information about a user as provided by the identity provider. It is an indication
that a user has been authenticated.
scheduler manager A Compute component that determines where VM instances should start. Uses modular
design to support a variety of scheduler types.
scoped token An Identity service API access token that is associated with a specific project.
scrubber Checks for and deletes unused VMs; the component of Image service that implements delayed
delete.
secret key String of text known only by the user; used along with an access key to make requests to the
Compute API.
Appendix 97
Architecture Design Guide (Release Version: 15.0.0)
secure boot Process whereby the system firmware validates the authenticity of the code involved in the boot
process.
secure shell (SSH) Open source tool used to access remote hosts through an encrypted communications channel,
SSH key injection is supported by Compute.
security group A set of network traffic filtering rules that are applied to a Compute instance.
segmented object An Object Storage large object that has been broken up into pieces. The re-assembled
object is called a concatenated object.
self-service For IaaS, ability for a regular (non-privileged) account to manage a virtual infrastructure component
such as networks without involving an administrator.
SELinux Linux kernel security module that provides the mechanism for supporting access control policies.
senlin Code name for the Clustering service.
server Computer that provides explicit services to the client software running on that system, often managing
a variety of computer operations. A server is a VM instance in the Compute system. Flavor and image
are requisite elements when creating a server.
server image Alternative term for a VM image.
server UUID Unique ID assigned to each guest VM instance.
service An OpenStack service, such as Compute, Object Storage, or Image service. Provides one or more
endpoints through which users can access resources and perform operations.
service catalog Alternative term for the Identity service catalog.
Service Function Chain (SFC) For a given service, SFC is the abstracted view of the required service functions
and the order in which they are to be applied.
service ID Unique ID assigned to each service that is available in the Identity service catalog.
Service Level Agreement (SLA) Contractual obligations that ensure the availability of a service.
service project Special project that contains all services that are listed in the catalog.
service provider A system that provides services to other system entities. In case of federated identity, OpenStack
Identity is the service provider.
service registration An Identity service feature that enables services, such as Compute, to automatically register
with the catalog.
service token An administrator-defined token used by Compute to communicate securely with the Identity
service.
session back end The method of storage used by horizon to track client sessions, such as local memory, cookies,
a database, or memcached.
session persistence A feature of the load-balancing service. It attempts to force subsequent connections to a
service to be redirected to the same node as long as it is online.
session storage A horizon component that stores and tracks client session information. Implemented through
the Django sessions framework.
share A remote, mountable file system in the context of the Shared File Systems service. You can mount a
share to, and access a share from, several hosts by several users at a time.
98 Appendix
Architecture Design Guide (Release Version: 15.0.0)
share network An entity in the context of the Shared File Systems service that encapsulates interaction with
the Networking service. If the driver you selected runs in the mode requiring such kind of interaction,
you need to specify the share network to create a share.
Shared File Systems API A Shared File Systems service that provides a stable RESTful API. The service authenticates
and routes requests throughout the Shared File Systems service. There is python-manilaclient
to interact with the API.
Shared File Systems service (manila) The service that provides a set of services for management of shared
file systems in a multi-project cloud environment, similar to how OpenStack provides block-based storage
management through the OpenStack Block Storage service project. With the Shared File Systems
service, you can create a remote file system and mount the file system on your instances. You can also
read and write data from your instances to and from your file system.
shared IP address An IP address that can be assigned to a VM instance within the shared IP group. Public IP
addresses can be shared across multiple servers for use in various high-availability scenarios. When an
IP address is shared to another server, the cloud network restrictions are modified to enable each server
to listen to and respond on that IP address. You can optionally specify that the target server network
configuration be modified. Shared IP addresses can be used with many standard heartbeat facilities,
such as keepalive, that monitor for failure and manage IP failover.
shared IP group A collection of servers that can share IPs with other members of the group. Any server in a
group can share one or more public IPs with any other server in the group. With the exception of the first
server in a shared IP group, servers must be launched into shared IP groups. A server may be a member
of only one shared IP group.
shared storage Block storage that is simultaneously accessible by multiple clients, for example, NFS.
Sheepdog Distributed block storage system for QEMU, supported by OpenStack.
Simple Cloud Identity Management (SCIM) Specification for managing identity in the cloud, currently unsupported
by OpenStack.
Simple Protocol for Independent Computing Environments (SPICE) SPICE provides remote desktop access
to guest virtual machines. It is an alternative to VNC. SPICE is supported by OpenStack.
Single-root I/O Virtualization (SR-IOV) A specification that, when implemented by a physical PCIe device,
enables it to appear as multiple separate PCIe devices. This enables multiple virtualized guests to share
direct access to the physical device, offering improved performance over an equivalent virtual device.
Currently supported in OpenStack Havana and later releases.
SmokeStack Runs automated tests against the core OpenStack API; written in Rails.
snapshot A point-in-time copy of an OpenStack storage volume or image. Use storage volume snapshots to
back up volumes. Use image snapshots to back up data, or as “gold” images for additional servers.
soft reboot A controlled reboot where a VM instance is properly restarted through operating system commands.
Software Development Lifecycle Automation service (solum) OpenStack project that aims to make cloud
services easier to consume and integrate with application development process by automating the sourceto-image
process, and simplifying app-centric deployment.
Software-defined networking (SDN) Provides an approach for network administrators to manage computer
network services through abstraction of lower-level functionality.
SolidFire Volume Driver The Block Storage driver for the SolidFire iSCSI storage appliance.
solum Code name for the Software Development Lifecycle Automation service.
Appendix 99
Architecture Design Guide (Release Version: 15.0.0)
spread-first scheduler The Compute VM scheduling algorithm that attempts to start a new VM on the host
with the least amount of load.
SQLAlchemy An open source SQL toolkit for Python, used in OpenStack.
SQLite A lightweight SQL database, used as the default persistent storage method in many OpenStack services.
stack A set of OpenStack resources created and managed by the Orchestration service according to a given
template (either an AWS CloudFormation template or a Heat Orchestration Template (HOT)).
StackTach Community project that captures Compute AMQP communications; useful for debugging.
static IP address Alternative term for a fixed IP address.
StaticWeb WSGI middleware component of Object Storage that serves container data as a static web page.
storage back end The method that a service uses for persistent storage, such as iSCSI, NFS, or local disk.
storage manager A XenAPI component that provides a pluggable interface to support a wide variety of persistent
storage back ends.
storage manager back end A persistent storage method supported by XenAPI, such as iSCSI or NFS.
storage node An Object Storage node that provides container services, account services, and object services;
controls the account databases, container databases, and object storage.
storage services Collective name for the Object Storage object services, container services, and account services.
strategy Specifies the authentication source used by Image service or Identity. In the Database service, it
refers to the extensions implemented for a data store.
subdomain A domain within a parent domain. Subdomains cannot be registered. Subdomains enable you to
delegate domains. Subdomains can themselves have subdomains, so third-level, fourth-level, fifth-level,
and deeper levels of nesting are possible.
subnet Logical subdivision of an IP network.
SUSE Linux Enterprise Server (SLES) A Linux distribution that is compatible with OpenStack.
suspend The VM instance is paused and its state is saved to disk of the host.
swap Disk-based virtual memory used by operating systems to provide more memory than is actually available
on the system.
swauth An authentication and authorization service for Object Storage, implemented through WSGI middleware;
uses Object Storage itself as the persistent backing store.
swift Codename for OpenStack Object Storage service.
swift All in One (SAIO) Creates a full Object Storage development environment within a single VM.
swift middleware Collective term for Object Storage components that provide additional functionality.
swift proxy server Acts as the gatekeeper to Object Storage and is responsible for authenticating the user.
swift storage node A node that runs Object Storage account, container, and object services.
sync point Point in time since the last container and accounts database sync among nodes within Object Storage.
sysadmin One of the default roles in the Compute RBAC system. Enables a user to add other users to a project,
interact with VM images that are associated with the project, and start and stop VM instances.
100 Appendix
Architecture Design Guide (Release Version: 15.0.0)
system usage A Compute component that, along with the notification system, collects meters and usage information.
This information can be used for billing.
T
tacker Code name for the NFV Orchestration service
Telemetry service (telemetry) The OpenStack project which collects measurements of the utilization of the
physical and virtual resources comprising deployed clouds, persists this data for subsequent retrieval and
analysis, and triggers actions when defined criteria are met.
TempAuth An authentication facility within Object Storage that enables Object Storage itself to perform
authentication and authorization. Frequently used in testing and development.
Tempest Automated software test suite designed to run against the trunk of the OpenStack core project.
TempURL An Object Storage middleware component that enables creation of URLs for temporary object
access.
tenant A group of users; used to isolate access to Compute resources. An alternative term for a project.
Tenant API An API that is accessible to projects.
tenant endpoint An Identity service API endpoint that is associated with one or more projects.
tenant ID An alternative term for project ID.
token An alpha-numeric string of text used to access OpenStack APIs and resources.
token services An Identity service component that manages and validates tokens after a user or project has
been authenticated.
tombstone Used to mark Object Storage objects that have been deleted; ensures that the object is not updated
on another node after it has been deleted.
topic publisher A process that is created when a RPC call is executed; used to push the message to the topic
exchange.
Torpedo Community project used to run automated tests against the OpenStack API.
transaction ID Unique ID assigned to each Object Storage request; used for debugging and tracing.
transient Alternative term for non-durable.
transient exchange Alternative term for a non-durable exchange.
transient message A message that is stored in memory and is lost after the server is restarted.
transient queue Alternative term for a non-durable queue.
TripleO OpenStack-on-OpenStack program. The code name for the OpenStack Deployment program.
trove Codename for OpenStack Database service.
trusted platform module (TPM) Specialized microprocessor for incorporating cryptographic keys into devices
for authenticating and securing a hardware platform.
U
Ubuntu A Debian-based Linux distribution.
Appendix 101
Architecture Design Guide (Release Version: 15.0.0)
unscoped token Alternative term for an Identity service default token.
updater Collective term for a group of Object Storage components that processes queued and failed updates
for containers and objects.
user In OpenStack Identity, entities represent individual API consumers and are owned by a specific domain.
In OpenStack Compute, a user can be associated with roles, projects, or both.
user data A blob of data that the user can specify when they launch an instance. The instance can access this
data through the metadata service or config drive. Commonly used to pass a shell script that the instance
runs on boot.
User Mode Linux (UML) An OpenStack-supported hypervisor.
V
VIF UUID Unique ID assigned to each Networking VIF.
Virtual Central Processing Unit (vCPU) Subdivides physical CPUs. Instances can then use those divisions.
Virtual Disk Image (VDI) One of the VM image disk formats supported by Image service.
Virtual Extensible LAN (VXLAN) A network virtualization technology that attempts to reduce the scalability
problems associated with large cloud computing deployments. It uses a VLAN-like encapsulation
technique to encapsulate Ethernet frames within UDP packets.
Virtual Hard Disk (VHD) One of the VM image disk formats supported by Image service.
virtual IP address (VIP) An Internet Protocol (IP) address configured on the load balancer for use by clients
connecting to a service that is load balanced. Incoming connections are distributed to back-end nodes
based on the configuration of the load balancer.
virtual machine (VM) An operating system instance that runs on top of a hypervisor. Multiple VMs can run
at the same time on the same physical host.
virtual network An L2 network segment within Networking.
Virtual Network Computing (VNC) Open source GUI and CLI tools used for remote console access to VMs.
Supported by Compute.
Virtual Network InterFace (VIF) An interface that is plugged into a port in a Networking network. Typically
a virtual network interface belonging to a VM.
virtual networking A generic term for virtualization of network functions such as switching, routing, load
balancing, and security using a combination of VMs and overlays on physical network infrastructure.
virtual port Attachment point where a virtual interface connects to a virtual network.
virtual private network (VPN) Provided by Compute in the form of cloudpipes, specialized instances that
are used to create VPNs on a per-project basis.
virtual server Alternative term for a VM or guest.
virtual switch (vSwitch) Software that runs on a host or node and provides the features and functions of a
hardware-based network switch.
virtual VLAN Alternative term for a virtual network.
VirtualBox An OpenStack-supported hypervisor.
Vitrage Code name for the Root Cause Analysis service.
102 Appendix
Architecture Design Guide (Release Version: 15.0.0)
VLAN manager A Compute component that provides dnsmasq and radvd and sets up forwarding to and from
cloudpipe instances.
VLAN network The Network Controller provides virtual networks to enable compute servers to interact with
each other and with the public network. All machines must have a public and private network interface.
A VLAN network is a private network interface, which is controlled by the vlan_interface option
with VLAN managers.
VM disk (VMDK) One of the VM image disk formats supported by Image service.
VM image Alternative term for an image.
VM Remote Control (VMRC) Method to access VM instance consoles using a web browser. Supported by
Compute.
VMware API Supports interaction with VMware products in Compute.
VMware NSX Neutron plug-in Provides support for VMware NSX in Neutron.
VNC proxy A Compute component that provides users access to the consoles of their VM instances through
VNC or VMRC.
volume Disk-based data storage generally represented as an iSCSI target with a file system that supports
extended attributes; can be persistent or ephemeral.
Volume API Alternative name for the Block Storage API.
volume controller A Block Storage component that oversees and coordinates storage volume actions.
volume driver Alternative term for a volume plug-in.
volume ID Unique ID applied to each storage volume under the Block Storage control.
volume manager A Block Storage component that creates, attaches, and detaches persistent storage volumes.
volume node A Block Storage node that runs the cinder-volume daemon.
volume plug-in Provides support for new and specialized types of back-end storage for the Block Storage
volume manager.
volume worker A cinder component that interacts with back-end storage to manage the creation and deletion
of volumes and the creation of compute volumes, provided by the cinder-volume daemon.
vSphere An OpenStack-supported hypervisor.
W
Watcher Code name for the Infrastructure Optimization service.
weight Used by Object Storage devices to determine which storage devices are suitable for the job. Devices
are weighted by size.
weighted cost The sum of each cost used when deciding where to start a new VM instance in Compute.
weighting A Compute process that determines the suitability of the VM instances for a job for a particular
host. For example, not enough RAM on the host, too many CPUs on the host, and so on.
worker A daemon that listens to a queue and carries out tasks in response to messages. For example, the
cinder-volume worker manages volume creation and deletion on storage arrays.
Appendix 103
Architecture Design Guide (Release Version: 15.0.0)
Workflow service (mistral) The OpenStack service that provides a simple YAML-based language to write
workflows (tasks and transition rules) and a service that allows to upload them, modify, run them at scale
and in a highly available manner, manage and monitor workflow execution state and state of individual
tasks.
X
X.509 X.509 is the most widely used standard for defining digital certificates. It is a data structure that contains
the subject (entity) identifiable information such as its name along with its public key. The certificate can
contain a few other attributes as well depending upon the version. The most recent and standard version
of X.509 is v3.
Xen Xen is a hypervisor using a microkernel design, providing services that allow multiple computer operating
systems to execute on the same computer hardware concurrently.
Xen API The Xen administrative API, which is supported by Compute.
Xen Cloud Platform (XCP) An OpenStack-supported hypervisor.
Xen Storage Manager Volume Driver A Block Storage volume plug-in that enables communication with
the Xen Storage Manager API.
XenServer An OpenStack-supported hypervisor.
XFS High-performance 64-bit file system created by Silicon Graphics. Excels in parallel I/O operations and
data consistency.
Z
zaqar Codename for the Message service.
ZeroMQ Message queue software supported by OpenStack. An alternative to RabbitMQ. Also spelled 0MQ.
Zuul Tool used in OpenStack development to ensure correctly ordered testing of changes in parallel.
104 Appendix
INDEX
Symbols
6to4, 70
A
absolute limit, 70
access control list (ACL), 70
access key, 70
account, 70
account auditor, 70
account database, 70
account reaper, 70
account server, 70
account service, 71
accounting, 71
Active Directory, 71
active/active configuration, 71
active/passive configuration, 71
address pool, 71
Address Resolution Protocol (ARP), 71
admin API, 71
admin server, 71
administrator, 71
Advanced Message Queuing Protocol (AMQP), 71
Advanced RISC Machine (ARM), 71
alert, 71
allocate, 71
Amazon Kernel Image (AKI), 71
Amazon Machine Image (AMI), 71
Amazon Ramdisk Image (ARI), 71
Anvil, 71
aodh, 71
Apache, 71
Apache License 2.0, 71
Apache Web Server, 71
API endpoint, 71
API extension, 71
API extension plug-in, 72
API key, 72
API server, 72
API token, 72
API version, 72
applet, 72
Application Catalog service (murano), 72
Application Programming Interface (API), 72
application server, 72
Application Service Provider (ASP), 72
arptables, 72
associate, 72
Asynchronous JavaScript and XML (AJAX), 72
ATA over Ethernet (AoE), 72
attach, 72
attachment (network), 72
auditing, 72
auditor, 72
Austin, 72
auth node, 72
authentication, 72
authentication token, 72
AuthN, 72
authorization, 72
authorization node, 73
AuthZ, 73
Auto ACK, 73
auto declare, 73
availability zone, 73
AWS CloudFormation template, 73
B
back end, 73
back-end catalog, 73
back-end store, 73
Backup, Restore, and Disaster Recovery service
(freezer), 73
bandwidth, 73
barbican, 73
bare, 73
Bare Metal service (ironic), 73
base image, 73
Bell-LaPadula model, 73
Benchmark service (rally), 73
Bexar, 73
binary, 73
105
Architecture Design Guide (Release Version: 15.0.0)
bit, 74
bits per second (BPS), 74
block device, 74
block migration, 74
Block Storage API, 74
Block Storage service (cinder), 74
BMC (Baseboard Management Controller), 74
bootable disk image, 74
Bootstrap Protocol (BOOTP), 74
Border Gateway Protocol (BGP), 74
browser, 74
builder file, 74
bursting, 74
button class, 74
byte, 74
C
cache pruner, 74
Cactus, 74
CALL, 74
capability, 75
capacity cache, 75
capacity updater, 75
CAST, 75
catalog, 75
catalog service, 75
ceilometer, 75
cell, 75
cell forwarding, 75
cell manager, 75
CentOS, 75
Ceph, 75
CephFS, 75
certificate authority (CA), 75
Challenge-Handshake Authentication Protocol
(CHAP), 75
chance scheduler, 75
changes since, 75
Chef, 75
child cell, 75
cinder, 75
CirrOS, 75
Cisco neutron plug-in, 75
cloud architect, 76
Cloud Auditing Data Federation (CADF), 76
cloud computing, 76
cloud controller, 76
cloud controller node, 76
Cloud Data Management Interface (CDMI), 76
Cloud Infrastructure Management Interface (CIMI),
76
cloud-init, 76
cloudadmin, 76
Cloudbase-Init, 76
cloudpipe, 76
cloudpipe image, 76
Clustering service (senlin), 76
command filter, 76
Common Internet File System (CIFS), 76
Common Libraries (oslo), 76
community project, 76
compression, 76
Compute API (Nova API), 76
compute controller, 76
compute host, 76
compute node, 77
Compute service (nova), 77
compute worker, 77
concatenated object, 77
conductor, 77
congress, 77
consistency window, 77
console log, 77
container, 77
container auditor, 77
container database, 77
container format, 77
Container Infrastructure Management service (magnum),
77
container server, 77
container service, 77
content delivery network (CDN), 77
controller node, 77
core API, 77
core service, 77
cost, 77
credentials, 77
CRL, 78
Cross-Origin Resource Sharing (CORS), 78
Crowbar, 78
current workload, 78
customer, 78
customization module, 78
D
daemon, 78
Dashboard (horizon), 78
data encryption, 78
Data loss prevention (DLP) software, 78
Data Processing service (sahara), 78
data store, 78
database ID, 78
106 Index
Architecture Design Guide (Release Version: 15.0.0)
database replicator, 78
Database service (trove), 78
deallocate, 78
Debian, 78
deduplication, 78
default panel, 78
default project, 78
default token, 79
delayed delete, 79
delivery mode, 79
denial of service (DoS), 79
deprecated auth, 79
designate, 79
Desktop-as-a-Service, 79
developer, 79
device ID, 79
device weight, 79
DevStack, 79
DHCP agent, 79
Diablo, 79
direct consumer, 79
direct exchange, 79
direct publisher, 79
disassociate, 79
Discretionary Access Control (DAC), 79
disk encryption, 79
disk format, 79
dispersion, 79
distributed virtual router (DVR), 80
Django, 80
DNS record, 80
DNS service (designate), 80
dnsmasq, 80
domain, 80
Domain Name System (DNS), 80
download, 80
durable exchange, 80
durable queue, 80
Dynamic Host Configuration Protocol (DHCP), 80
Dynamic HyperText Markup Language (DHTML), 80
E
east-west traffic, 80
EBS boot volume, 80
ebtables, 80
EC2, 80
EC2 access key, 80
EC2 API, 80
EC2 Compatibility API, 81
EC2 secret key, 81
Elastic Block Storage (EBS), 81
encapsulation, 81
encryption, 81
endpoint, 81
endpoint registry, 81
endpoint template, 81
entity, 81
ephemeral image, 81
ephemeral volume, 81
Essex, 81
ESXi, 81
ETag, 81
euca2ools, 81
Eucalyptus Kernel Image (EKI), 81
Eucalyptus Machine Image (EMI), 81
Eucalyptus Ramdisk Image (ERI), 81
evacuate, 81
exchange, 81
exchange type, 81
exclusive queue, 81
extended attributes (xattr), 81
extension, 81
external network, 82
extra specs, 82
F
FakeLDAP, 82
fan-out exchange, 82
federated identity, 82
Fedora, 82
Fibre Channel, 82
Fibre Channel over Ethernet (FCoE), 82
fill-first scheduler, 82
filter, 82
firewall, 82
FireWall-as-a-Service (FWaaS), 82
fixed IP address, 82
Flat Manager, 82
flat mode injection, 82
flat network, 82
FlatDHCP Manager, 82
flavor, 82
flavor ID, 82
floating IP address, 82
Folsom, 82
FormPost, 83
freezer, 83
front end, 83
G
gateway, 83
generic receive offload (GRO), 83
generic routing encapsulation (GRE), 83
Index 107
Architecture Design Guide (Release Version: 15.0.0)
glance, 83
glance API server, 83
glance registry, 83
global endpoint template, 83
GlusterFS, 83
gnocchi, 83
golden image, 83
Governance service (congress), 83
Graphic Interchange Format (GIF), 83
Graphics Processing Unit (GPU), 83
Green Threads, 83
Grizzly, 83
Group, 83
guest OS, 83
H
Hadoop, 83
Hadoop Distributed File System (HDFS), 84
handover, 84
HAProxy, 84
hard reboot, 84
Havana, 84
health monitor, 84
heat, 84
Heat Orchestration Template (HOT), 84
high availability (HA), 84
horizon, 84
horizon plug-in, 84
host, 84
host aggregate, 84
Host Bus Adapter (HBA), 84
hybrid cloud, 84
Hyper-V, 84
hyperlink, 84
Hypertext Transfer Protocol (HTTP), 84
Hypertext Transfer Protocol Secure (HTTPS), 84
hypervisor, 85
hypervisor pool, 85
I
Icehouse, 85
ID number, 85
Identity API, 85
Identity back end, 85
identity provider, 85
Identity service (keystone), 85
Identity service API, 85
IETF, 85
image, 85
Image API, 85
image cache, 85
image ID, 85
image membership, 85
image owner, 85
image registry, 85
Image service (glance), 85
image status, 85
image store, 85
image UUID, 86
incubated project, 86
Infrastructure Optimization service (watcher), 86
Infrastructure-as-a-Service (IaaS), 86
ingress filtering, 86
INI format, 86
injection, 86
Input/Output Operations Per Second (IOPS), 86
instance, 86
instance ID, 86
instance state, 86
instance tunnels network, 86
instance type, 86
instance type ID, 86
instance UUID, 86
Intelligent Platform Management Interface (IPMI), 86
interface, 86
interface ID, 86
Internet Control Message Protocol (ICMP), 86
Internet protocol (IP), 86
Internet Service Provider (ISP), 86
Internet Small Computer System Interface (iSCSI), 86
IP address, 86
IP Address Management (IPAM), 86
ip6tables, 87
ipset, 87
iptables, 87
ironic, 87
iSCSI Qualified Name (IQN), 87
ISO9660, 87
itsec, 87
J
Java, 87
JavaScript, 87
JavaScript Object Notation (JSON), 87
jumbo frame, 87
Juno, 87
K
Kerberos, 87
kernel-based VM (KVM), 87
Key Manager service (barbican), 87
keystone, 87
Kickstart, 88
Kilo, 88
108 Index
Architecture Design Guide (Release Version: 15.0.0)
L
large object, 88
Launchpad, 88
Layer-2 (L2) agent, 88
Layer-2 network, 88
Layer-3 (L3) agent, 88
Layer-3 network, 88
Liberty, 88
libvirt, 88
Lightweight Directory Access Protocol (LDAP), 88
Linux, 88
Linux bridge, 88
Linux Bridge neutron plug-in, 88
Linux containers (LXC), 88
live migration, 88
load balancer, 88
load balancing, 88
Load-Balancer-as-a-Service (LBaaS), 88
Load-balancing service (octavia), 88
Logical Volume Manager (LVM), 89
M
magnum, 89
management API, 89
management network, 89
manager, 89
manifest, 89
manifest object, 89
manila, 89
manila-share, 89
maximum transmission unit (MTU), 89
mechanism driver, 89
melange, 89
membership, 89
membership list, 89
memcached, 89
memory overcommit, 89
message broker, 89
message bus, 89
message queue, 89
Message service (zaqar), 89
Meta-Data Server (MDS), 89
Metadata agent, 89
migration, 89
mistral, 89
Mitaka, 90
Modular Layer 2 (ML2) neutron plug-in, 90
monasca, 90
Monitor (LBaaS), 90
Monitor (Mon), 90
Monitoring (monasca), 90
multi-factor authentication, 90
multi-host, 90
multinic, 90
murano, 90
N
Nebula, 90
netadmin, 90
NetApp volume driver, 90
network, 90
Network Address Translation (NAT), 90
network controller, 90
Network File System (NFS), 90
network ID, 90
network manager, 90
network namespace, 91
network node, 91
network segment, 91
Network Service Header (NSH), 91
Network Time Protocol (NTP), 91
network UUID, 91
network worker, 91
Networking API (Neutron API), 91
Networking service (neutron), 91
neutron, 91
neutron API, 91
neutron manager, 91
neutron plug-in, 91
Newton, 91
Nexenta volume driver, 91
NFV Orchestration Service (tacker), 91
Nginx, 91
No ACK, 91
node, 91
non-durable exchange, 91
non-durable queue, 91
non-persistent volume, 91
north-south traffic, 91
nova, 92
Nova API, 92
nova-network, 92
O
object, 92
object auditor, 92
object expiration, 92
object hash, 92
object path hash, 92
object replicator, 92
object server, 92
Object Storage API, 92
Object Storage Device (OSD), 92
Index 109
Architecture Design Guide (Release Version: 15.0.0)
Object Storage service (swift), 92
object versioning, 92
Ocata, 92
Octavia, 92
Oldie, 92
Open Cloud Computing Interface (OCCI), 92
Open Virtualization Format (OVF), 92
Open vSwitch, 92
Open vSwitch (OVS) agent, 92
Open vSwitch neutron plug-in, 92
OpenLDAP, 92
OpenStack, 93
OpenStack code name, 93
openSUSE, 93
operator, 93
optional service, 93
Orchestration service (heat), 93
orphan, 93
Oslo, 93
P
panko, 93
parent cell, 93
partition, 93
partition index, 93
partition shift value, 93
path MTU discovery (PMTUD), 93
pause, 93
PCI passthrough, 93
persistent message, 93
persistent volume, 93
personality file, 93
Pike, 94
Platform-as-a-Service (PaaS), 94
plug-in, 94
policy service, 94
policy-based routing (PBR), 94
pool, 94
pool member, 94
port, 94
port UUID, 94
preseed, 94
private image, 94
private IP address, 94
private network, 94
project, 94
project ID, 94
project VPN, 94
promiscuous mode, 94
protected property, 94
provider, 94
proxy node, 94
proxy server, 94
public API, 94
public image, 95
public IP address, 95
public key authentication, 95
public network, 95
Puppet, 95
Python, 95
Q
QEMU Copy On Write 2 (QCOW2), 95
Qpid, 95
Quality of Service (QoS), 95
quarantine, 95
Queens, 95
Quick EMUlator (QEMU), 95
quota, 95
R
RabbitMQ, 95
Rackspace Cloud Files, 95
RADOS Block Device (RBD), 95
radvd, 95
rally, 95
RAM filter, 95
RAM overcommit, 95
rate limit, 96
raw, 96
rebalance, 96
reboot, 96
rebuild, 96
Recon, 96
record, 96
record ID, 96
Red Hat Enterprise Linux (RHEL), 96
reference architecture, 96
region, 96
registry, 96
registry server, 96
Reliable, Autonomic Distributed Object Store, 96
Remote Procedure Call (RPC), 96
replica, 96
replica count, 96
replication, 96
replicator, 96
request ID, 96
rescue image, 96
resize, 96
RESTful, 97
ring, 97
ring builder, 97
110 Index
Architecture Design Guide (Release Version: 15.0.0)
Rocky, 97
role, 97
Role Based Access Control (RBAC), 97
role ID, 97
Root Cause Analysis (RCA) service (Vitrage), 97
rootwrap, 97
round-robin scheduler, 97
router, 97
routing key, 97
RPC driver, 97
rsync, 97
RXTX cap, 97
RXTX quota, 97
S
sahara, 97
SAML assertion, 97
scheduler manager, 97
scoped token, 97
scrubber, 97
secret key, 97
secure boot, 98
secure shell (SSH), 98
security group, 98
segmented object, 98
self-service, 98
SELinux, 98
senlin, 98
server, 98
server image, 98
server UUID, 98
service, 98
service catalog, 98
Service Function Chain (SFC), 98
service ID, 98
Service Level Agreement (SLA), 98
service project, 98
service provider, 98
service registration, 98
service token, 98
session back end, 98
session persistence, 98
session storage, 98
share, 98
share network, 99
Shared File Systems API, 99
Shared File Systems service (manila), 99
shared IP address, 99
shared IP group, 99
shared storage, 99
Sheepdog, 99
Simple Cloud Identity Management (SCIM), 99
Simple Protocol for Independent Computing Environments
(SPICE), 99
Single-root I/O Virtualization (SR-IOV), 99
SmokeStack, 99
snapshot, 99
soft reboot, 99
Software Development Lifecycle Automation service
(solum), 99
Software-defined networking (SDN), 99
SolidFire Volume Driver, 99
solum, 99
spread-first scheduler, 100
SQLAlchemy, 100
SQLite, 100
stack, 100
StackTach, 100
static IP address, 100
StaticWeb, 100
storage back end, 100
storage manager, 100
storage manager back end, 100
storage node, 100
storage services, 100
strategy, 100
subdomain, 100
subnet, 100
SUSE Linux Enterprise Server (SLES), 100
suspend, 100
swap, 100
swauth, 100
swift, 100
swift All in One (SAIO), 100
swift middleware, 100
swift proxy server, 100
swift storage node, 100
sync point, 100
sysadmin, 100
system usage, 101
T
tacker, 101
Telemetry service (telemetry), 101
TempAuth, 101
Tempest, 101
TempURL, 101
tenant, 101
Tenant API, 101
tenant endpoint, 101
tenant ID, 101
token, 101
Index 111
Architecture Design Guide (Release Version: 15.0.0)
token services, 101
tombstone, 101
topic publisher, 101
Torpedo, 101
transaction ID, 101
transient, 101
transient exchange, 101
transient message, 101
transient queue, 101
TripleO, 101
trove, 101
trusted platform module (TPM), 101
U
Ubuntu, 101
unscoped token, 102
updater, 102
user, 102
user data, 102
User Mode Linux (UML), 102
V
VIF UUID, 102
Virtual Central Processing Unit (vCPU), 102
Virtual Disk Image (VDI), 102
Virtual Extensible LAN (VXLAN), 102
Virtual Hard Disk (VHD), 102
virtual IP address (VIP), 102
virtual machine (VM), 102
virtual network, 102
Virtual Network Computing (VNC), 102
Virtual Network InterFace (VIF), 102
virtual networking, 102
virtual port, 102
virtual private network (VPN), 102
virtual server, 102
virtual switch (vSwitch), 102
virtual VLAN, 102
VirtualBox, 102
Vitrage, 102
VLAN manager, 103
VLAN network, 103
VM disk (VMDK), 103
VM image, 103
VM Remote Control (VMRC), 103
VMware API, 103
VMware NSX Neutron plug-in, 103
VNC proxy, 103
volume, 103
Volume API, 103
volume controller, 103
volume driver, 103
volume ID, 103
volume manager, 103
volume node, 103
volume plug-in, 103
volume worker, 103
vSphere, 103
W
Watcher, 103
weight, 103
weighted cost, 103
weighting, 103
worker, 103
Workflow service (mistral), 104
X
X.509, 104
Xen, 104
Xen API, 104
Xen Cloud Platform (XCP), 104
Xen Storage Manager Volume Driver, 104
XenServer, 104
XFS, 104
Z
zaqar, 104
ZeroMQ, 104
Zuul, 104
112 Index
Networking Guide
Release Version: 15.0.0
OpenStack contributors
May 10, 2017
CONTENTS
Abstract 1
Contents 2
Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Deployment examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
Migration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
Miscellaneous . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
Appendix 300
Community support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
Glossary 304
Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
Index 339
i
ABSTRACT
This guide targets OpenStack administrators seeking to deploy and manage OpenStack Networking (neutron).
This guide documents the OpenStack Ocata release.
1
CONTENTS
Conventions
The OpenStack documentation uses several typesetting conventions.
Notices
Notices take these forms:
Note: A comment with additional information that explains a part of the text.
Important: Something you must be aware of before proceeding.
Tip: An extra but helpful piece of practical advice.
Caution: Helpful information that prevents the user from making mistakes.
Warning: Critical information about the risk of data loss or security issues.
Command prompts
$ command
Any user, including the root user, can run commands that are prefixed with the $ prompt.
# command
The root user must run commands that are prefixed with the # prompt. You can also prefix these commands
with the sudo command, if available, to run them.
Introduction
The OpenStack Networking service provides an API that allows users to set up and define network connectivity
and addressing in the cloud. The project code-name for Networking services is neutron. OpenStack Networking
handles the creation and management of a virtual networking infrastructure, including networks, switches,
subnets, and routers for devices managed by the OpenStack Compute service (nova). Advanced services such
as firewalls or virtual private networks (VPNs) can also be used.
2
Networking Guide (Release Version: 15.0.0)
OpenStack Networking consists of the neutron-server, a database for persistent storage, and any number of
plug-in agents, which provide other services such as interfacing with native Linux networking mechanisms,
external devices, or SDN controllers.
OpenStack Networking is entirely standalone and can be deployed to a dedicated host. If your deployment uses
a controller host to run centralized Compute components, you can deploy the Networking server to that specific
host instead.
OpenStack Networking integrates with various OpenStack components:
• OpenStack Identity service (keystone) is used for authentication and authorization of API requests.
• OpenStack Compute service (nova) is used to plug each virtual NIC on the VM into a particular network.
• OpenStack Dashboard (horizon) is used by administrators and project users to create and manage network
services through a web-based graphical interface.
Note: The network address ranges used in this guide are chosen in accordance with RFC 5737 and RFC 3849,
and as such are restricted to the following:
IPv4:
• 192.0.2.0/24
• 198.51.100.0/24
• 203.0.113.0/24
IPv6:
• 2001:DB8::/32
The network address ranges in the examples of this guide should not be used for any purpose other than documentation.
Note: To reduce clutter, this guide removes command output without relevance to the particular action.
Basic networking
Ethernet
Ethernet is a networking protocol, specified by the IEEE 802.3 standard. Most wired network interface cards
(NICs) communicate using Ethernet.
In the OSI model of networking protocols, Ethernet occupies the second layer, which is known as the data link
layer. When discussing Ethernet, you will often hear terms such as local network, layer 2, L2, link layer and
data link layer.
In an Ethernet network, the hosts connected to the network communicate by exchanging frames. Every host
on an Ethernet network is uniquely identified by an address called the media access control (MAC) address. In
particular, every virtual machine instance in an OpenStack environment has a unique MAC address, which is
different from the MAC address of the compute host. A MAC address has 48 bits and is typically represented
as a hexadecimal string, such as 08:00:27:b9:88:74. The MAC address is hard-coded into the NIC by the
Introduction 3
Networking Guide (Release Version: 15.0.0)
manufacturer, although modern NICs allow you to change the MAC address programmatically. In Linux, you
can retrieve the MAC address of a NIC using the ip command:
$ ip link show eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT
,→group default qlen 1000
link/ether 08:00:27:b9:88:74 brd ff:ff:ff:ff:ff:ff
Conceptually, you can think of an Ethernet network as a single bus that each of the network hosts connects to.
In early implementations, an Ethernet network consisted of a single coaxial cable that hosts would tap into to
connect to the network. However, network hosts in modern Ethernet networks connect directly to a network
device called a switch. Still, this conceptual model is useful, and in network diagrams (including those generated
by the OpenStack dashboard) an Ethernet network is often depicted as if it was a single bus. You’ll sometimes
hear an Ethernet network referred to as a layer 2 segment.
In an Ethernet network, every host on the network can send a frame directly to every other host. An Ethernet
network also supports broadcasts so that one host can send a frame to every host on the network by sending
to the special MAC address ff:ff:ff:ff:ff:ff. ARP and DHCP are two notable protocols that use Ethernet
broadcasts. Because Ethernet networks support broadcasts, you will sometimes hear an Ethernet network
referred to as a broadcast domain.
When a NIC receives an Ethernet frame, by default the NIC checks to see if the destination MAC address
matches the address of the NIC (or the broadcast address), and the Ethernet frame is discarded if the MAC
address does not match. For a compute host, this behavior is undesirable because the frame may be intended
for one of the instances. NICs can be configured for promiscuous mode, where they pass all Ethernet frames
to the operating system, even if the MAC address does not match. Compute hosts should always have the
appropriate NICs configured for promiscuous mode.
As mentioned earlier, modern Ethernet networks use switches to interconnect the network hosts. A switch is
a box of networking hardware with a large number of ports that forward Ethernet frames from one connected
host to another. When hosts first send frames over the switch, the switch doesn’t know which MAC address
is associated with which port. If an Ethernet frame is destined for an unknown MAC address, the switch
broadcasts the frame to all ports. The switch learns which MAC addresses are at which ports by observing the
traffic. Once it knows which MAC address is associated with a port, it can send Ethernet frames to the correct
port instead of broadcasting. The switch maintains the mappings of MAC addresses to switch ports in a table
called a forwarding table or forwarding information base (FIB). Switches can be daisy-chained together, and
the resulting connection of switches and hosts behaves like a single network.
VLANs
VLAN is a networking technology that enables a single switch to act as if it was multiple independent switches.
Specifically, two hosts that are connected to the same switch but on different VLANs do not see each other’s
traffic. OpenStack is able to take advantage of VLANs to isolate the traffic of different projects, even if the
projects happen to have instances running on the same compute host. Each VLAN has an associated numerical
ID, between 1 and 4095. We say “VLAN 15” to refer to the VLAN with a numerical ID of 15.
To understand how VLANs work, let’s consider VLAN applications in a traditional IT environment, where
physical hosts are attached to a physical switch, and no virtualization is involved. Imagine a scenario where
you want three isolated networks but you only have a single physical switch. The network administrator would
choose three VLAN IDs, for example, 10, 11, and 12, and would configure the switch to associate switchports
with VLAN IDs. For example, switchport 2 might be associated with VLAN 10, switchport 3 might be associated
with VLAN 11, and so forth. When a switchport is configured for a specific VLAN, it is called an access
port. The switch is responsible for ensuring that the network traffic is isolated across the VLANs.
4 Introduction
Networking Guide (Release Version: 15.0.0)
Now consider the scenario that all of the switchports in the first switch become occupied, and so the organization
buys a second switch and connects it to the first switch to expand the available number of switchports. The
second switch is also configured to support VLAN IDs 10, 11, and 12. Now imagine host A connected to switch
1 on a port configured for VLAN ID 10 sends an Ethernet frame intended for host B connected to switch 2 on a
port configured for VLAN ID 10. When switch 1 forwards the Ethernet frame to switch 2, it must communicate
that the frame is associated with VLAN ID 10.
If two switches are to be connected together, and the switches are configured for VLANs, then the switchports
used for cross-connecting the switches must be configured to allow Ethernet frames from any VLAN to be
forwarded to the other switch. In addition, the sending switch must tag each Ethernet frame with the VLAN ID
so that the receiving switch can ensure that only hosts on the matching VLAN are eligible to receive the frame.
A switchport that is configured to pass frames from all VLANs and tag them with the VLAN IDs is called a
trunk port. IEEE 802.1Q is the network standard that describes how VLAN tags are encoded in Ethernet frames
when trunking is being used.
Note that if you are using VLANs on your physical switches to implement project isolation in your OpenStack
cloud, you must ensure that all of your switchports are configured as trunk ports.
It is important that you select a VLAN range not being used by your current network infrastructure. For example,
if you estimate that your cloud must support a maximum of 100 projects, pick a VLAN range outside
of that value, such as VLAN 200–299. OpenStack, and all physical network infrastructure that handles project
networks, must then support this VLAN range.
Trunking is used to connect between different switches. Each trunk uses a tag to identify which VLAN is in
use. This ensures that switches on the same VLAN can communicate.
Subnets and ARP
While NICs use MAC addresses to address network hosts, TCP/IP applications use IP addresses. The Address
Resolution Protocol (ARP) bridges the gap between Ethernet and IP by translating IP addresses into MAC
addresses.
IP addresses are broken up into two parts: a network number and a host identifier. Two hosts are on the same
subnet if they have the same network number. Recall that two hosts can only communicate directly over Ethernet
if they are on the same local network. ARP assumes that all machines that are in the same subnet are on the
same local network. Network administrators must take care when assigning IP addresses and netmasks to hosts
so that any two hosts that are in the same subnet are on the same local network, otherwise ARP does not work
properly.
To calculate the network number of an IP address, you must know the netmask associated with the address. A
netmask indicates how many of the bits in the 32-bit IP address make up the network number.
There are two syntaxes for expressing a netmask:
• dotted quad
• classless inter-domain routing (CIDR)
Consider an IP address of 192.168.1.5, where the first 24 bits of the address are the network number. In dotted
quad notation, the netmask would be written as 255.255.255.0. CIDR notation includes both the IP address
and netmask, and this example would be written as 192.168.1.5/24.
Note: Creating CIDR subnets including a multicast address or a loopback address cannot be used in an OpenStack
environment. For example, creating a subnet using 224.0.0.0/16 or 127.0.1.0/24 is not supported.
Introduction 5
Networking Guide (Release Version: 15.0.0)
Sometimes we want to refer to a subnet, but not any particular IP address on the subnet. A common convention
is to set the host identifier to all zeros to make reference to a subnet. For example, if a host’s IP address is
10.10.53.24/16, then we would say the subnet is 10.10.0.0/16.
To understand how ARP translates IP addresses to MAC addresses, consider the following example. Assume
host A has an IP address of 192.168.1.5/24 and a MAC address of fc:99:47:49:d4:a0, and wants to send
a packet to host B with an IP address of 192.168.1.7. Note that the network number is the same for both
hosts, so host A is able to send frames directly to host B.
The first time host A attempts to communicate with host B, the destination MAC address is not known. Host A
makes an ARP request to the local network. The request is a broadcast with a message like this:
To: everybody (ff:ff:ff:ff:ff:ff). I am looking for the computer who has IP address 192.168.1.7. Signed: MAC
address fc:99:47:49:d4:a0.
Host B responds with a response like this:
To: fc:99:47:49:d4:a0. I have IP address 192.168.1.7. Signed: MAC address 54:78:1a:86:00:a5.
Host A then sends Ethernet frames to host B.
You can initiate an ARP request manually using the arping command. For example, to send an ARP request
to IP address 10.30.0.132:
$ arping -I eth0 10.30.0.132
ARPING 10.30.0.132 from 10.30.0.131 eth0
Unicast reply from 10.30.0.132 [54:78:1A:86:1C:0B] 0.670ms
Unicast reply from 10.30.0.132 [54:78:1A:86:1C:0B] 0.722ms
Unicast reply from 10.30.0.132 [54:78:1A:86:1C:0B] 0.723ms
Sent 3 probes (1 broadcast(s))
Received 3 response(s)
To reduce the number of ARP requests, operating systems maintain an ARP cache that contains the mappings
of IP addresses to MAC address. On a Linux machine, you can view the contents of the ARP cache by using
the arp command:
$ arp -n
Address HWtype HWaddress Flags Mask Iface
10.0.2.3 ether 52:54:00:12:35:03 C eth0
10.0.2.2 ether 52:54:00:12:35:02 C eth0
DHCP
Hosts connected to a network use the Dynamic Host Configuration Protocol (DHCP) to dynamically obtain IP
addresses. A DHCP server hands out the IP addresses to network hosts, which are the DHCP clients.
DHCP clients locate the DHCP server by sending a UDP packet from port 68 to address 255.255.255.255 on
port 67. Address 255.255.255.255 is the local network broadcast address: all hosts on the local network see
the UDP packets sent to this address. However, such packets are not forwarded to other networks. Consequently,
the DHCP server must be on the same local network as the client, or the server will not receive the broadcast.
The DHCP server responds by sending a UDP packet from port 67 to port 68 on the client. The exchange looks
like this:
1. The client sends a discover (“I’m a client at MAC address 08:00:27:b9:88:74, I need an IP address”)
2. The server sends an offer (“OK 08:00:27:b9:88:74, I’m offering IP address 10.10.0.112”)
6 Introduction
Networking Guide (Release Version: 15.0.0)
3. The client sends a request (“Server 10.10.0.131, I would like to have IP 10.10.0.112”)
4. The server sends an acknowledgement (“OK 08:00:27:b9:88:74, IP 10.10.0.112 is yours”)
OpenStack uses a third-party program called dnsmasq to implement the DHCP server. Dnsmasq writes to the
syslog, where you can observe the DHCP request and replies:
Apr 23 15:53:46 c100-1 dhcpd: DHCPDISCOVER from 08:00:27:b9:88:74 via eth2
Apr 23 15:53:46 c100-1 dhcpd: DHCPOFFER on 10.10.0.112 to 08:00:27:b9:88:74 via eth2
Apr 23 15:53:48 c100-1 dhcpd: DHCPREQUEST for 10.10.0.112 (10.10.0.131) from
,→08:00:27:b9:88:74 via eth2
Apr 23 15:53:48 c100-1 dhcpd: DHCPACK on 10.10.0.112 to 08:00:27:b9:88:74 via eth2
When troubleshooting an instance that is not reachable over the network, it can be helpful to examine this log
to verify that all four steps of the DHCP protocol were carried out for the instance in question.
IP
The Internet Protocol (IP) specifies how to route packets between hosts that are connected to different local
networks. IP relies on special network hosts called routers or gateways. A router is a host that is connected to
at least two local networks and can forward IP packets from one local network to another. A router has multiple
IP addresses: one for each of the networks it is connected to.
In the OSI model of networking protocols IP occupies the third layer, known as the network layer. When
discussing IP, you will often hear terms such as layer 3, L3, and network layer.
A host sending a packet to an IP address consults its routing table to determine which machine on the local
network(s) the packet should be sent to. The routing table maintains a list of the subnets associated with each
local network that the host is directly connected to, as well as a list of routers that are on these local networks.
On a Linux machine, any of the following commands displays the routing table:
$ ip route show
$ route -n
$ netstat -rn
Here is an example of output from ip route show:
$ ip route show
default via 10.0.2.2 dev eth0
10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15
192.168.27.0/24 dev eth1 proto kernel scope link src 192.168.27.100
192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1
Line 1 of the output specifies the location of the default route, which is the effective routing rule if none of the
other rules match. The router associated with the default route (10.0.2.2 in the example above) is sometimes
referred to as the default gateway. A DHCP server typically transmits the IP address of the default gateway to
the DHCP client along with the client’s IP address and a netmask.
Line 2 of the output specifies that IPs in the 10.0.2.0/24 subnet are on the local network associated with the
network interface eth0.
Line 3 of the output specifies that IPs in the 192.168.27.0/24 subnet are on the local network associated with
the network interface eth1.
Line 4 of the output specifies that IPs in the 192.168.122.0/24 subnet are on the local network associated
with the network interface virbr0.
Introduction 7
Networking Guide (Release Version: 15.0.0)
The output of the route -n and netstat -rn commands are formatted in a slightly different way. This
example shows how the same routes would be formatted using these commands:
$ route -n
Kernel IP routing table
Destination Gateway Genmask Flags MSS Window irtt Iface
0.0.0.0 10.0.2.2 0.0.0.0 UG 0 0 0 eth0
10.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0
192.168.27.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1
192.168.122.0 0.0.0.0 255.255.255.0 U 0 0 0 virbr0
The ip route get command outputs the route for a destination IP address. From the below example, destination
IP address 10.0.2.14 is on the local network of eth0 and would be sent directly:
$ ip route get 10.0.2.14
10.0.2.14 dev eth0 src 10.0.2.15
The destination IP address 93.184.216.34 is not on any of the connected local networks and would be forwarded
to the default gateway at 10.0.2.2:
$ ip route get 93.184.216.34
93.184.216.34 via 10.0.2.2 dev eth0 src 10.0.2.15
It is common for a packet to hop across multiple routers to reach its final destination. On a Linux machine, the
traceroute and more recent mtr programs prints out the IP address of each router that an IP packet traverses
along its path to its destination.
TCP/UDP/ICMP
For networked software applications to communicate over an IP network, they must use a protocol layered atop
IP. These protocols occupy the fourth layer of the OSI model known as the transport layer or layer 4. See
the Protocol Numbers web page maintained by the Internet Assigned Numbers Authority (IANA) for a list of
protocols that layer atop IP and their associated numbers.
The Transmission Control Protocol (TCP) is the most commonly used layer 4 protocol in networked applications.
TCP is a connection-oriented protocol: it uses a client-server model where a client connects to a server,
where server refers to the application that receives connections. The typical interaction in a TCP-based application
proceeds as follows:
1. Client connects to server.
2. Client and server exchange data.
3. Client or server disconnects.
Because a network host may have multiple TCP-based applications running, TCP uses an addressing scheme
called ports to uniquely identify TCP-based applications. A TCP port is associated with a number in the range
1-65535, and only one application on a host can be associated with a TCP port at a time, a restriction that is
enforced by the operating system.
A TCP server is said to listen on a port. For example, an SSH server typically listens on port 22. For a client to
connect to a server using TCP, the client must know both the IP address of a server’s host and the server’s TCP
port.
The operating system of the TCP client application automatically assigns a port number to the client. The client
owns this port number until the TCP connection is terminated, after which the operating system reclaims the
8 Introduction
Networking Guide (Release Version: 15.0.0)
port number. These types of ports are referred to as ephemeral ports.
IANA maintains a registry of port numbersfor many TCP-based services, as well as services that use other layer
4 protocols that employ ports. Registering a TCP port number is not required, but registering a port number
is helpful to avoid collisions with other services. See firewalls and default ports in OpenStack Administrator
Guide for the default TCP ports used by various services involved in an OpenStack deployment.
The most common application programming interface (API) for writing TCP-based applications is called Berkeley
sockets, also known as BSD sockets or, simply, sockets. The sockets API exposes a stream oriented interface
for writing TCP applications. From the perspective of a programmer, sending data over a TCP connection is
similar to writing a stream of bytes to a file. It is the responsibility of the operating system’s TCP/IP implementation
to break up the stream of data into IP packets. The operating system is also responsible for automatically
retransmitting dropped packets, and for handling flow control to ensure that transmitted data does not overrun
the sender’s data buffers, receiver’s data buffers, and network capacity. Finally, the operating system is responsible
for re-assembling the packets in the correct order into a stream of data on the receiver’s side. Because
TCP detects and retransmits lost packets, it is said to be a reliable protocol.
The User Datagram Protocol (UDP) is another layer 4 protocol that is the basis of several well-known networking
protocols. UDP is a connectionless protocol: two applications that communicate over UDP do not need
to establish a connection before exchanging data. UDP is also an unreliable protocol. The operating system
does not attempt to retransmit or even detect lost UDP packets. The operating system also does not provide any
guarantee that the receiving application sees the UDP packets in the same order that they were sent in.
UDP, like TCP, uses the notion of ports to distinguish between different applications running on the same system.
Note, however, that operating systems treat UDP ports separately from TCP ports. For example, it is possible
for one application to be associated with TCP port 16543 and a separate application to be associated with UDP
port 16543.
Like TCP, the sockets API is the most common API for writing UDP-based applications. The sockets API
provides a message-oriented interface for writing UDP applications: a programmer sends data over UDP by
transmitting a fixed-sized message. If an application requires retransmissions of lost packets or a well-defined
ordering of received packets, the programmer is responsible for implementing this functionality in the application
code.
DHCP, the Domain Name System (DNS), the Network Time Protocol (NTP), and Virtual extensible local area
network (VXLAN) are examples of UDP-based protocols used in OpenStack deployments.
UDP has support for one-to-many communication: sending a single packet to multiple hosts. An application
can broadcast a UDP packet to all of the network hosts on a local network by setting the receiver IP address
as the special IP broadcast address 255.255.255.255. An application can also send a UDP packet to a set of
receivers using IP multicast. The intended receiver applications join a multicast group by binding a UDP socket
to a special IP address that is one of the valid multicast group addresses. The receiving hosts do not have to be
on the same local network as the sender, but the intervening routers must be configured to support IP multicast
routing. VXLAN is an example of a UDP-based protocol that uses IP multicast.
The Internet Control Message Protocol (ICMP) is a protocol used for sending control messages over an IP
network. For example, a router that receives an IP packet may send an ICMP packet back to the source if there
is no route in the router’s routing table that corresponds to the destination address (ICMP code 1, destination
host unreachable) or if the IP packet is too large for the router to handle (ICMP code 4, fragmentation required
and “don’t fragment” flag is set).
The ping and mtr Linux command-line tools are two examples of network utilities that use ICMP.
Network components
Introduction 9
Networking Guide (Release Version: 15.0.0)
Switches
Switches are Multi-Input Multi-Output (MIMO) devices that enable packets to travel from one node to another.
Switches connect hosts that belong to the same layer-2 network. Switches enable forwarding of the packet
received on one port (input) to another port (output) so that they reach the desired destination node. Switches
operate at layer-2 in the networking model. They forward the traffic based on the destination Ethernet address
in the packet header.
Routers
Routers are special devices that enable packets to travel from one layer-3 network to another. Routers enable
communication between two nodes on different layer-3 networks that are not directly connected to each other.
Routers operate at layer-3 in the networking model. They route the traffic based on the destination IP address
in the packet header.
Firewalls
Firewalls are used to regulate traffic to and from a host or a network. A firewall can be either a specialized
device connecting two networks or a software-based filtering mechanism implemented on an operating system.
Firewalls are used to restrict traffic to a host based on the rules defined on the host. They can filter packets
based on several criteria such as source IP address, destination IP address, port numbers, connection state, and
so on. It is primarily used to protect the hosts from unauthorized access and malicious attacks. Linux-based
operating systems implement firewalls through iptables.
Load balancers
Load balancers can be software-based or hardware-based devices that allow traffic to evenly be distributed
across several servers. By distributing the traffic across multiple servers, it avoids overload of a single server
thereby preventing a single point of failure in the product. This further improves the performance, network
throughput, and response time of the servers. Load balancers are typically used in a 3-tier architecture. In this
model, a load balancer receives a request from the front-end web server, which then forwards the request to
one of the available back-end database servers for processing. The response from the database server is passed
back to the web server for further processing.
Overlay (tunnel) protocols
Tunneling is a mechanism that makes transfer of payloads feasible over an incompatible delivery network. It
allows the network user to gain access to denied or insecure networks. Data encryption may be employed to
transport the payload, ensuring that the encapsulated user network data appears as public even though it is
private and can easily pass the conflicting network.
Generic routing encapsulation (GRE)
Generic routing encapsulation (GRE) is a protocol that runs over IP and is employed when delivery and payload
protocols are compatible but payload addresses are incompatible. For instance, a payload might think it is
running on a datalink layer but it is actually running over a transport layer using datagram protocol over IP.
GRE creates a private point-to-point connection and works by encapsulating a payload. GRE is a foundation
protocol for other tunnel protocols but the GRE tunnels provide only weak authentication.
10 Introduction
Networking Guide (Release Version: 15.0.0)
Virtual extensible local area network (VXLAN)
The purpose of VXLAN is to provide scalable network isolation. VXLAN is a Layer 2 overlay scheme on
a Layer 3 network. It allows an overlay layer-2 network to spread across multiple underlay layer-3 network
domains. Each overlay is termed a VXLAN segment. Only VMs within the same VXLAN segment can communicate.
Network namespaces
A namespace is a way of scoping a particular set of identifiers. Using a namespace, you can use the same
identifier multiple times in different namespaces. You can also restrict an identifier set visible to particular
processes.
For example, Linux provides namespaces for networking and processes, among other things. If a process
is running within a process namespace, it can only see and communicate with other processes in the same
namespace. So, if a shell in a particular process namespace ran ps waux, it would only show the other processes
in the same namespace.
Linux network namespaces
In a network namespace, the scoped ‘identifiers’ are network devices; so a given network device, such as eth0,
exists in a particular namespace. Linux starts up with a default network namespace, so if your operating system
does not do anything special, that is where all the network devices will be located. But it is also possible to
create further non-default namespaces, and create new devices in those namespaces, or to move an existing
device from one namespace to another.
Each network namespace also has its own routing table, and in fact this is the main reason for namespaces to
exist. A routing table is keyed by destination IP address, so network namespaces are what you need if you want
the same destination IP address to mean different things at different times - which is something that OpenStack
Networking requires for its feature of providing overlapping IP addresses in different virtual networks.
Each network namespace also has its own set of iptables (for both IPv4 and IPv6). So, you can apply different
security to flows with the same IP addressing in different namespaces, as well as different routing.
Any given Linux process runs in a particular network namespace. By default this is inherited from its parent
process, but a process with the right capabilities can switch itself into a different namespace; in practice this
is mostly done using the ip netns exec NETNS COMMAND... invocation, which starts COMMAND running in
the namespace named NETNS. Suppose such a process sends out a message to IP address A.B.C.D, the effect of
the namespace is that A.B.C.D will be looked up in that namespace’s routing table, and that will determine the
network device that the message is transmitted through.
Virtual routing and forwarding (VRF)
Virtual routing and forwarding is an IP technology that allows multiple instances of a routing table to coexist on
the same router at the same time. It is another name for the network namespace functionality described above.
Network address translation
Network Address Translation (NAT) is a process for modifying the source or destination addresses in the headers
of an IP packet while the packet is in transit. In general, the sender and receiver applications are not aware that
the IP packets are being manipulated.
Introduction 11
Networking Guide (Release Version: 15.0.0)
NAT is often implemented by routers, and so we will refer to the host performing NAT as a NAT router. However,
in OpenStack deployments it is typically Linux servers that implement the NAT functionality, not hardware
routers. These servers use the iptables software package to implement the NAT functionality.
There are multiple variations of NAT, and here we describe three kinds commonly found in OpenStack deployments.
SNAT
In Source Network Address Translation (SNAT), the NAT router modifies the IP address of the sender in IP
packets. SNAT is commonly used to enable hosts with private addresses to communicate with servers on the
public Internet.
RFC 1918 reserves the following three subnets as private addresses:
• 10.0.0.0/8
• 172.16.0.0/12
• 192.168.0.0/16
These IP addresses are not publicly routable, meaning that a host on the public Internet can not send an IP packet
to any of these addresses. Private IP addresses are widely used in both residential and corporate environments.
Often, an application running on a host with a private IP address will need to connect to a server on the public
Internet. An example is a user who wants to access a public website such as www.openstack.org. If the IP
packets reach the web server at www.openstack.org with a private IP address as the source, then the web server
cannot send packets back to the sender.
SNAT solves this problem by modifying the source IP address to an IP address that is routable on the public
Internet. There are different variations of SNAT; in the form that OpenStack deployments use, a NAT router
on the path between the sender and receiver replaces the packet’s source IP address with the router’s public
IP address. The router also modifies the source TCP or UDP port to another value, and the router maintains a
record of the sender’s true IP address and port, as well as the modified IP address and port.
When the router receives a packet with the matching IP address and port, it translates these back to the private
IP address and port, and forwards the packet along.
Because the NAT router modifies ports as well as IP addresses, this form of SNAT is sometimes referred to as
Port Address Translation (PAT). It is also sometimes referred to as NAT overload.
OpenStack uses SNAT to enable applications running inside of instances to connect out to the public Internet.
DNAT
In Destination Network Address Translation (DNAT), the NAT router modifies the IP address of the destination
in IP packet headers.
OpenStack uses DNAT to route packets from instances to the OpenStack metadata service. Applications running
inside of instances access the OpenStack metadata service by making HTTP GET requests to a web server with
IP address 169.254.169.254. In an OpenStack deployment, there is no host with this IP address. Instead,
OpenStack uses DNAT to change the destination IP of these packets so they reach the network interface that a
metadata service is listening on.
12 Introduction
Networking Guide (Release Version: 15.0.0)
One-to-one NAT
In one-to-one NAT, the NAT router maintains a one-to-one mapping between private IP addresses and public
IP addresses. OpenStack uses one-to-one NAT to implement floating IP addresses.
OpenStack Networking
OpenStack Networking allows you to create and manage network objects, such as networks, subnets, and ports,
which other OpenStack services can use. Plug-ins can be implemented to accommodate different networking
equipment and software, providing flexibility to OpenStack architecture and deployment.
The Networking service, code-named neutron, provides an API that lets you define network connectivity and
addressing in the cloud. The Networking service enables operators to leverage different networking technologies
to power their cloud networking. The Networking service also provides an API to configure and manage
a variety of network services ranging from L3 forwarding and NAT to load balancing, perimeter firewalls, and
virtual private networks.
It includes the following components:
API server The OpenStack Networking API includes support for Layer 2 networking and IP address management
(IPAM), as well as an extension for a Layer 3 router construct that enables routing between Layer 2
networks and gateways to external networks. OpenStack Networking includes a growing list of plug-ins
that enable interoperability with various commercial and open source network technologies, including
routers, switches, virtual switches and software-defined networking (SDN) controllers.
OpenStack Networking plug-in and agents Plugs and unplugs ports, creates networks or subnets, and provides
IP addressing. The chosen plug-in and agents differ depending on the vendor and technologies
used in the particular cloud. It is important to mention that only one plug-in can be used at a time.
Messaging queue Accepts and routes RPC requests between agents to complete API operations. Message
queue is used in the ML2 plug-in for RPC between the neutron server and neutron agents that run on
each hypervisor, in the ML2 mechanism drivers for Open vSwitch and Linux bridge.
Concepts
To configure rich network topologies, you can create and configure networks and subnets and instruct other
OpenStack services like Compute to attach virtual devices to ports on these networks. OpenStack Compute
is a prominent consumer of OpenStack Networking to provide connectivity for its instances. In particular,
OpenStack Networking supports each project having multiple private networks and enables projects to choose
their own IP addressing scheme, even if those IP addresses overlap with those that other projects use. There
are two types of network, project and provider networks. It is possible to share any of these types of networks
among projects as part of the network creation process.
Provider networks
Provider networks offer layer-2 connectivity to instances with optional support for DHCP and metadata services.
These networks connect, or map, to existing layer-2 networks in the data center, typically using VLAN (802.1q)
tagging to identify and separate them.
Provider networks generally offer simplicity, performance, and reliability at the cost of flexibility. By default
only administrators can create or update provider networks because they require configuration of physical netIntroduction
13
Networking Guide (Release Version: 15.0.0)
work infrastructure. It is possible to change the user who is allowed to create or update provider networks with
the following parameters of policy.json:
• create_network:provider:physical_network
• update_network:provider:physical_network
Warning: The creation and modification of provider networks enables use of physical network resources,
such as VLAN-s. Enable these changes only for trusted tenants.
Also, provider networks only handle layer-2 connectivity for instances, thus lacking support for features such
as routers and floating IP addresses.
In many cases, operators who are already familiar with virtual networking architectures that rely on physical
network infrastructure for layer-2, layer-3, or other services can seamlessly deploy the OpenStack Networking
service. In particular, provider networks appeal to operators looking to migrate from the Compute networking
service (nova-network) to the OpenStack Networking service. Over time, operators can build on this minimal
architecture to enable more cloud networking features.
In general, the OpenStack Networking software components that handle layer-3 operations impact performance
and reliability the most. To improve performance and reliability, provider networks move layer-3 operations to
the physical network infrastructure.
In one particular use case, the OpenStack deployment resides in a mixed environment with conventional virtualization
and bare-metal hosts that use a sizable physical network infrastructure. Applications that run inside
the OpenStack deployment might require direct layer-2 access, typically using VLANs, to applications outside
of the deployment.
Routed provider networks
Routed provider networks offer layer-3 connectivity to instances. These networks map to existing layer-3
networks in the data center. More specifically, the network maps to multiple layer-2 segments, each of which
is essentially a provider network. Each has a router gateway attached to it which routes traffic between them
and externally. The Networking service does not provide the routing.
Routed provider networks offer performance at scale that is difficult to achieve with a plain provider network
at the expense of guaranteed layer-2 connectivity.
See Routed provider networks for more information.
Self-service networks
Self-service networks primarily enable general (non-privileged) projects to manage networks without involving
administrators. These networks are entirely virtual and require virtual routers to interact with provider and
external networks such as the Internet. Self-service networks also usually provide DHCP and metadata services
to instances.
In most cases, self-service networks use overlay protocols such as VXLAN or GRE because they can support
many more networks than layer-2 segmentation using VLAN tagging (802.1q). Furthermore, VLANs typically
require additional configuration of physical network infrastructure.
14 Introduction
Networking Guide (Release Version: 15.0.0)
IPv4 self-service networks typically use private IP address ranges (RFC1918) and interact with provider networks
via source NAT on virtual routers. Floating IP addresses enable access to instances from provider networks
via destination NAT on virtual routers. IPv6 self-service networks always use public IP address ranges
and interact with provider networks via virtual routers with static routes.
The Networking service implements routers using a layer-3 agent that typically resides at least one network
node. Contrary to provider networks that connect instances to the physical network infrastructure at layer-2,
self-service networks must traverse a layer-3 agent. Thus, oversubscription or failure of a layer-3 agent or
network node can impact a significant quantity of self-service networks and instances using them. Consider
implementing one or more high-availability features to increase redundancy and performance of self-service
networks.
Users create project networks for connectivity within projects. By default, they are fully isolated and are not
shared with other projects. OpenStack Networking supports the following types of network isolation and overlay
technologies.
Flat All instances reside on the same network, which can also be shared with the hosts. No VLAN tagging or
other network segregation takes place.
VLAN Networking allows users to create multiple provider or project networks using VLAN IDs (802.1Q
tagged) that correspond to VLANs present in the physical network. This allows instances to communicate
with each other across the environment. They can also communicate with dedicated servers, firewalls,
load balancers, and other networking infrastructure on the same layer 2 VLAN.
GRE and VXLAN VXLAN and GRE are encapsulation protocols that create overlay networks to activate
and control communication between compute instances. A Networking router is required to allow traffic
to flow outside of the GRE or VXLAN project network. A router is also required to connect directlyconnected
project networks with external networks, including the Internet. The router provides the ability
to connect to instances directly from an external network using floating IP addresses.
Subnets
A block of IP addresses and associated configuration state. This is also known as the native IPAM (IP Address
Management) provided by the networking service for both project and provider networks. Subnets are used to
Introduction 15
Networking Guide (Release Version: 15.0.0)
allocate IP addresses when new ports are created on a network.
Subnet pools
End users normally can create subnets with any valid IP addresses without other restrictions. However, in some
cases, it is nice for the admin or the project to pre-define a pool of addresses from which to create subnets with
automatic allocation.
Using subnet pools constrains what addresses can be used by requiring that every subnet be within the defined
pool. It also prevents address reuse or overlap by two subnets from the same pool.
See Subnet pools for more information.
Ports
A port is a connection point for attaching a single device, such as the NIC of a virtual server, to a virtual network.
The port also describes the associated network configuration, such as the MAC and IP addresses to be used on
that port.
Routers
Routers provide virtual layer-3 services such as routing and NAT between self-service and provider networks
or among self-service networks belonging to a project. The Networking service uses a layer-3 agent to manage
routers via namespaces.
Security groups
Security groups provide a container for virtual firewall rules that control ingress (inbound to instances) and
egress (outbound from instances) network traffic at the port level. Security groups use a default deny policy
and only contain rules that allow specific traffic. Each port can reference one or more security groups in an
additive fashion. The firewall driver translates security group rules to a configuration for the underlying packet
filtering technology such as iptables.
Each project contains a default security group that allows all egress traffic and denies all ingress traffic. You
can change the rules in the default security group. If you launch an instance without specifying a security
group, the default security group automatically applies to it. Similarly, if you create a port without specifying
a security group, the default security group automatically applies to it.
Note: If you use the metadata service, removing the default egress rules denies access to TCP port 80 on
169.254.169.254, thus preventing instances from retrieving metadata.
Security group rules are stateful. Thus, allowing ingress TCP port 22 for secure shell automatically creates
rules that allow return egress traffic and ICMP error messages involving those TCP connections.
By default, all security groups contain a series of basic (sanity) and anti-spoofing rules that perform the following
actions:
16 Introduction
Networking Guide (Release Version: 15.0.0)
• Allow egress traffic only if it uses the source MAC and IP addresses of the port for the instance, source
MAC and IP combination in allowed-address-pairs, or valid MAC address (port or allowedaddress-pairs)
and associated EUI64 link-local IPv6 address.
• Allow egress DHCP discovery and request messages that use the source MAC address of the port for the
instance and the unspecified IPv4 address (0.0.0.0).
• Allow ingress DHCP and DHCPv6 responses from the DHCP server on the subnet so instances can
acquire IP addresses.
• Deny egress DHCP and DHCPv6 responses to prevent instances from acting as DHCP(v6) servers.
• Allow ingress/egress ICMPv6 MLD, neighbor solicitation, and neighbor discovery messages so instances
can discover neighbors and join multicast groups.
• Deny egress ICMPv6 router advertisements to prevent instances from acting as IPv6 routers and forwarding
IPv6 traffic for other instances.
• Allow egress ICMPv6 MLD reports (v1 and v2) and neighbor solicitation messages that use the source
MAC address of a particular instance and the unspecified IPv6 address (::). Duplicate address detection
(DAD) relies on these messages.
• Allow egress non-IP traffic from the MAC address of the port for the instance and any additional MAC
addresses in allowed-address-pairs on the port for the instance.
Although non-IP traffic, security groups do not implicitly allow all ARP traffic. Separate ARP filtering rules
prevent instances from using ARP to intercept traffic for another instance. You cannot disable or remove these
rules.
You can disable security groups including basic and anti-spoofing rules by setting the port attribute
port_security_enabled to False.
Extensions
The OpenStack Networking service is extensible. Extensions serve two purposes: they allow the introduction
of new features in the API without requiring a version change and they allow the introduction of vendor specific
niche functionality. Applications can programmatically list available extensions by performing a GET on the
/extensions URI. Note that this is a versioned request; that is, an extension available in one API version
might not be available in another.
DHCP
The optional DHCP service manages IP addresses for instances on provider and self-service networks. The
Networking service implements the DHCP service using an agent that manages qdhcp namespaces and the
dnsmasq service.
Metadata
The optional metadata service provides an API for instances to obtain metadata such as SSH keys.
Introduction 17
Networking Guide (Release Version: 15.0.0)
Service and component hierarchy
Server
• Provides API, manages database, etc.
Plug-ins
• Manages agents
Agents
• Provides layer 2/3 connectivity to instances
• Handles physical-virtual network transition
• Handles metadata, etc.
Layer 2 (Ethernet and Switching)
• Linux Bridge
• OVS
Layer 3 (IP and Routing)
• L3
• DHCP
Miscellaneous
• Metadata
Services
Routing services
VPNaaS
The Virtual Private Network-as-a-Service (VPNaaS) is a neutron extension that introduces the VPN feature set.
LBaaS
The Load-Balancer-as-a-Service (LBaaS) API provisions and configures load balancers. The reference implementation
is based on the HAProxy software load balancer.
18 Introduction
Networking Guide (Release Version: 15.0.0)
FWaaS
The Firewall-as-a-Service (FWaaS) API is an experimental API that enables early adopters and vendors to test
their networking implementations.
Firewall-as-a-Service (FWaaS)
The Firewall-as-a-Service (FWaaS) plug-in applies firewalls to OpenStack objects such as projects, routers, and
router ports.
Note: We anticipate this to expand to VM ports in the Ocata cycle.
The central concepts with OpenStack firewalls are the notions of a firewall policy and a firewall rule. A policy
is an ordered collection of rules. A rule specifies a collection of attributes (such as port ranges, protocol, and
IP addresses) that constitute match criteria and an action to take (allow or deny) on matched traffic. A policy
can be made public, so it can be shared across projects.
Firewalls are implemented in various ways, depending on the driver used. For example, an iptables driver
implements firewalls using iptable rules. An OpenVSwitch driver implements firewall rules using flow entries
in flow tables. A Cisco firewall driver manipulates NSX devices.
FWaaS v1
The original FWaaS implementation, v1, provides protection for routers. When a firewall is applied to a router,
all internal ports are protected.
The following diagram depicts FWaaS v1 protection. It illustrates the flow of ingress and egress traffic for the
VM2 instance:
Introduction 19
Networking Guide (Release Version: 15.0.0)
FWaaS v2
The newer FWaaS implementation, v2, provides a much more granular service. The notion of a firewall has
been replaced with firewall group to indicate that a firewall consists of two policies: an ingress policy and an
egress policy. A firewall group is applied not at the router level (all ports on a router) but at the port level.
Currently, router ports can be specified. For Ocata, VM ports can also be specified.
FWaaS v1 versus v2
The following table compares v1 and v2 features.
Feature v1 v2
Supports L3 firewalling for routers YES NO*
Supports L3 firewalling for router ports NO YES
Supports L2 firewalling (VM ports) NO NO**
CLI support YES YES
Horizon support YES NO
* A firewall group can be applied to all ports on a given router in order to effect this.
** This feature is planned for Ocata.
For further information, see v1 configuration guide or v2 configuration guide.
Configuration
ML2 plug-in
Architecture
The Modular Layer 2 (ML2) neutron plug-in is a framework allowing OpenStack Networking to simultaneously
use the variety of layer 2 networking technologies found in complex real-world data centers. The ML2
framework distinguishes between the two kinds of drivers that can be configured:
• Type drivers
Define how an OpenStack network is technically realized. Example: VXLAN
Each available network type is managed by an ML2 type driver. Type drivers maintain any needed
type-specific network state. They validate the type specific information for provider networks and are
responsible for the allocation of a free segment in project networks.
• Mechanism drivers
Define the mechanism to access an OpenStack network of a certain type. Example: Open vSwitch
mechanism driver.
The mechanism driver is responsible for taking the information established by the type driver and ensuring
that it is properly applied given the specific networking mechanisms that have been enabled.
Mechanism drivers can utilize L2 agents (via RPC) and/or interact directly with external devices or
controllers.
Multiple mechanism and type drivers can be used simultaneously to access different ports of the same virtual
network.
20 Configuration
Networking Guide (Release Version: 15.0.0)
ML2 driver support matrix
Table 1: Mechanism drivers and L2 agents
type driver / mech driver Flat VLAN VXLAN GRE
Open vSwitch yes yes yes yes
Linux bridge yes yes yes no
SRIOV yes yes no no
MacVTap yes yes no no
L2 population no no yes yes
Note: L2 population is a special mechanism driver that optimizes BUM (Broadcast, unknown destination
address, multicast) traffic in the overlay networks VXLAN and GRE. It needs to be used in conjunction with
either the Linux bridge or the Open vSwitch mechanism driver and cannot be used as standalone mechanism
driver. For more information, see the Mechanism drivers section below.
Configuration
Network type drivers
To enable type drivers in the ML2 plug-in. Edit the /etc/neutron/plugins/ml2/ml2_conf.ini file:
[ml2]
type_drivers = flat,vlan,vxlan,gre
For more details, see the Networking configuration options of Configuration Reference.
The following type drivers are available
• Flat
• VLAN
• GRE
• VXLAN
Provider network types
Provider networks provide connectivity like project networks. But only administrative (privileged) users can
manage those networks because they interface with the physical network infrastructure. More information about
provider networks see OpenStack Networking or the OpenStack Administrator Guide.
• Flat
The administrator needs to configure a list of physical network names that can be used for provider
networks. For more details, see the related section in the Configuration Reference.
• VLAN
The administrator needs to configure a list of physical network names that can be used for provider
networks. For more details, see the related section in the Configuration Reference.
Configuration 21
Networking Guide (Release Version: 15.0.0)
• GRE
No additional configuration required.
• VXLAN
The administrator can configure the VXLAN multicast group that should be used.
Note: VXLAN multicast group configuration is not applicable for the Open vSwitch agent.
As of today it is not used in the Linux bridge agent. The Linux bridge agent has its own agent specific
configuration option. Please see the following bug for more details: https://bugs.launchpad.net/neutron/
+bug/1523614
Project network types
Project networks provide connectivity to instances for a particular project. Regular (non-privileged) users can
manage project networks within the allocation that an administrator or operator defines for them. More information
about project and provider networks see OpenStack Networking or the OpenStack Administrator Guide.
Project network configurations are made in the /etc/neutron/plugins/ml2/ml2_conf.ini configuration
file on the neutron server:
• VLAN
The administrator needs to configure the range of VLAN IDs that can be used for project network allocation.
For more details, see the related section in the Configuration Reference.
• GRE
The administrator needs to configure the range of tunnel IDs that can be used for project network allocation.
For more details, see the related section in the Configuration Reference.
• VXLAN
The administrator needs to configure the range of VXLAN IDs that can be used for project network
allocation. For more details, see the related section in the Configuration Reference.
Note: Flat networks for project allocation are not supported. They only can exist as a provider network.
Mechanism drivers
To enable mechanism drivers in the ML2 plug-in, edit the /etc/neutron/plugins/ml2/ml2_conf.ini file
on the neutron server:
[ml2]
mechanism_drivers = ovs,l2pop
For more details, see the Configuration Reference.
• Linux bridge
22 Configuration
Networking Guide (Release Version: 15.0.0)
No additional configurations required for the mechanism driver. Additional agent configuration is required.
For details, see the related L2 agent section below.
• Open vSwitch
No additional configurations required for the mechanism driver. Additional agent configuration is required.
For details, see the related L2 agent section below.
• SRIOV
The administrator needs to define a list PCI hardware that shall be used by OpenStack. For more details,
see the related section in the Configuration Reference.
• MacVTap
No additional configurations required for the mechanism driver. Additional agent configuration is required.
Please see the related section.
• L2 population
The administrator can configure some optional configuration options. For more details, see the related
section in the Configuration Reference.
• Specialized
– Open source
External open source mechanism drivers exist as well as the neutron integrated reference implementations.
Configuration of those drivers is not part of this document. For example:
* OpenDaylight
* OpenContrail
– Proprietary (vendor)
External mechanism drivers from various vendors exist as well as the neutron integrated reference
implementations.
Configuration of those drivers is not part of this document.
Agents
L2 agent
An L2 agent serves layer 2 (Ethernet) network connectivity to OpenStack resources. It typically runs on each
Network Node and on each Compute Node.
• Open vSwitch agent
The Open vSwitch agent configures the Open vSwitch to realize L2 networks for OpenStack resources.
Configuration for the Open vSwitch agent is typically done in the openvswitch_agent.ini configuration
file. Make sure that on agent start you pass this configuration file as argument.
For a detailed list of configuration options, see the related section in the Configuration Reference.
• Linux bridge agent
The Linux bridge agent configures Linux bridges to realize L2 networks for OpenStack resources.
Configuration 23
Networking Guide (Release Version: 15.0.0)
Configuration for the Linux bridge agent is typically done in the linuxbridge_agent.ini configuration
file. Make sure that on agent start you pass this configuration file as argument.
For a detailed list of configuration options, see the related section in the Configuration Reference.
• SRIOV Nic Switch agent
The sriov nic switch agent configures PCI virtual functions to realize L2 networks for OpenStack instances.
Network attachments for other resources like routers, DHCP, and so on are not supported.
Configuration for the SRIOV nic switch agent is typically done in the sriov_agent.ini configuration
file. Make sure that on agent start you pass this configuration file as argument.
For a detailed list of configuration options, see the related section in the Configuration Reference.
• MacVTap agent
The MacVTap agent uses kernel MacVTap devices for realizing L2 networks for OpenStack instances.
Network attachments for other resources like routers, DHCP, and so on are not supported.
Configuration for the MacVTap agent is typically done in the macvtap_agent.ini configuration file.
Make sure that on agent start you pass this configuration file as argument.
For a detailed list of configuration options, see the related section in the Configuration Reference.
L3 agent
The L3 agent offers advanced layer 3 services, like virtual Routers and Floating IPs. It requires an L2 agent
running in parallel.
Configuration for the L3 agent is typically done in the l3_agent.ini configuration file. Make sure that on
agent start you pass this configuration file as argument.
For a detailed list of configuration options, see the related section in the Configuration Reference.
DHCP agent
The DHCP agent is responsible for DHCP and RADVD (Router Advertisement Daemon) services. It requires
a running L2 agent on the same node.
Configuration for the DHCP agent is typically done in the dhcp_agent.ini configuration file. Make sure that
on agent start you pass this configuration file as argument.
For a detailed list of configuration options, see the related section in the Configuration Reference.
Metadata agent
The Metadata agent allows instances to access cloud-init meta data and user data via the network. It requires a
running L2 agent on the same node.
Configuration for the Metadata agent is typically done in the metadata_agent.ini configuration file. Make
sure that on agent start you pass this configuration file as argument.
For a detailed list of configuration options, see the related section in the Configuration Reference.
24 Configuration
Networking Guide (Release Version: 15.0.0)
L3 metering agent
The L3 metering agent enables layer3 traffic metering. It requires a running L3 agent on the same node.
Configuration for the L3 metering agent is typically done in the metering_agent.ini configuration file.
Make sure that on agent start you pass this configuration file as argument.
For a detailed list of configuration options, see the related section in the Configuration Reference.
Security
L2 agents support some important security configurations.
• Security Groups
For more details, see the related section in the Configuration Reference.
• Arp Spoofing Prevention
Configured in the L2 agent configuration.
Reference implementations
Overview
In this section, the combination of a mechanism driver and an L2 agent is called ‘reference implementation’.
The following table lists these implementations:
Table 2: Mechanism drivers and L2 agents
Mechanism Driver L2 agent
Open vSwitch Open vSwitch agent
Linux bridge Linux bridge agent
SRIOV SRIOV nic switch agent
MacVTap MacVTap agent
L2 population Open vSwitch agent, Linux bridge agent
The following tables shows which reference implementations support which non-L2 neutron agents:
Table 3: Reference implementations and other agents
Reference Implementation L3 agent DHCP agent Metadata agent L3 Metering agent
Open vSwitch & Open vSwitch agent yes yes yes yes
Linux bridge & Linux bridge agent yes yes yes yes
SRIOV & SRIOV nic switch agent no no no no
MacVTap & MacVTap agent no no no no
Note: L2 population is not listed here, as it is not a standalone mechanism. If other agents are supported
depends on the conjunctive mechanism driver that is used for binding a port.
More information about L2 population see the OpenStack Manuals.
Configuration 25
Networking Guide (Release Version: 15.0.0)
Buying guide
This guide characterizes the L2 reference implementations that currently exist.
• Open vSwitch mechanism and Open vSwitch agent
Can be used for instance network attachments as well as for attachments of other network resources like
routers, DHCP, and so on.
• Linux bridge mechanism and Linux bridge agent
Can be used for instance network attachments as well as for attachments of other network resources like
routers, DHCP, and so on.
• SRIOV mechanism driver and SRIOV NIC switch agent
Can only be used for instance network attachments (device_owner = compute).
Is deployed besides an other mechanism driver and L2 agent such as OVS or Linux bridge. It offers
instances direct access to the network adapter through a PCI Virtual Function (VF). This gives an instance
direct access to hardware capabilities and high performance networking.
The cloud consumer can decide via the neutron APIs VNIC_TYPE attribute, if an instance gets a normal
OVS port or an SRIOV port.
Due to direct connection, some features are not available when using SRIOV. For example, DVR, security
groups, migration.
For more information see the SR-IOV.
• MacVTap mechanism driver and MacVTap agent
Can only be used for instance network attachments (device_owner = compute) and not for attachment of
other resources like routers, DHCP, and so on.
It is positioned as alternative to Open vSwitch or Linux bridge support on the compute node for internal
deployments.
MacVTap offers a direct connection with very little overhead between instances and down to the adapter.
You can use MacVTap agent on the compute node when you require a network connection that is performance
critical. It does not require specific hardware (like with SRIOV).
Due to the direct connection, some features are not available when using it on the compute node. For
example, DVR, security groups and arp-spoofing protection.
Address scopes
Address scopes build from subnet pools. While subnet pools provide a mechanism for controlling the allocation
of addresses to subnets, address scopes show where addresses can be routed between networks, preventing the
use of overlapping addresses in any two subnets. Because all addresses allocated in the address scope do not
overlap, neutron routers do not NAT between your projects’ network and your external network. As long as the
addresses within an address scope match, the Networking service performs simple routing between networks.
Accessing address scopes
Anyone with access to the Networking service can create their own address scopes. However, network administrators
can create shared address scopes, allowing other projects to create networks within that address
26 Configuration
Networking Guide (Release Version: 15.0.0)
scope.
Access to addresses in a scope are managed through subnet pools. Subnet pools can either be created in an
address scope, or updated to belong to an address scope.
With subnet pools, all addresses in use within the address scope are unique from the point of view of the address
scope owner. Therefore, add more than one subnet pool to an address scope if the pools have different owners,
allowing for delegation of parts of the address scope. Delegation prevents address overlap across the whole
scope. Otherwise, you receive an error if two pools have the same address ranges.
Each router interface is associated with an address scope by looking at subnets connected to the network. When
a router connects to an external network with matching address scopes, network traffic routes between without
Network address translation (NAT). The router marks all traffic connections originating from each interface
with its corresponding address scope. If traffic leaves an interface in the wrong scope, the router blocks the
traffic.
Backwards compatibility
Networks created before the Mitaka release do not contain explicitly named address scopes, unless the network
contains subnets from a subnet pool that belongs to a created or updated address scope. The Networking service
preserves backwards compatibility with pre-Mitaka networks through special address scope properties so that
these networks can perform advanced routing:
1. Unlimited address overlap is allowed.
2. Neutron routers, by default, will NAT traffic from internal networks to external networks.
3. Pre-Mitaka address scopes are not visible through the API. You cannot list address scopes or show details.
Scopes exist implicitly as a catch-all for addresses that are not explicitly scoped.
Create shared address scopes as an administrative user
This section shows how to set up shared address scopes to allow simple routing for project networks with the
same subnet pools.
Note: Irrelevant fields have been trimmed from the output of these commands for brevity.
1. Create IPv6 and IPv4 address scopes:
$ openstack address scope create --share --ip-version 6 address-scope-ip6
+------------+--------------------------------------+
| Field | Value |
+------------+--------------------------------------+
| headers | |
| id | 28424dfc-9abd-481b-afa3-1da97a8fead7 |
| ip_version | 6 |
| name | address-scope-ip6 |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| shared | True |
+------------+--------------------------------------+
Configuration 27
Networking Guide (Release Version: 15.0.0)
$ openstack address scope create --share --ip-version 4 address-scope-ip4
+------------+--------------------------------------+
| Field | Value |
+------------+--------------------------------------+
| headers | |
| id | 3193bd62-11b5-44dc-acf8-53180f21e9f2 |
| ip_version | 4 |
| name | address-scope-ip4 |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| shared | True |
+------------+--------------------------------------+
2. Create subnet pools specifying the name (or UUID) of the address scope that the subnet pool belongs
to. If you have existing subnet pools, use the openstack subnet pool set command to put them in
a new address scope:
$ openstack subnet pool create --address-scope address-scope-ip6 \
--share --pool-prefix 2001:db8:a583::/48 --default-prefix-length 64 \
subnet-pool-ip6
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| address_scope_id | 28424dfc-9abd-481b-afa3-1da97a8fead7 |
| created_at | 2016-12-13T22:53:30Z |
| default_prefixlen | 64 |
| default_quota | None |
| description | |
| id | a59ff52b-0367-41ff-9781-6318b927dd0e |
| ip_version | 6 |
| is_default | False |
| max_prefixlen | 128 |
| min_prefixlen | 64 |
| name | subnet-pool-ip6 |
| prefixes | 2001:db8:a583::/48 |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| revision_number | 1 |
| shared | True |
| updated_at | 2016-12-13T22:53:30Z |
+-------------------+--------------------------------------+
$ openstack subnet pool create --address-scope address-scope-ip4 \
--share --pool-prefix 203.0.113.0/24 --default-prefix-length 26 \
subnet-pool-ip4
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| address_scope_id | 3193bd62-11b5-44dc-acf8-53180f21e9f2 |
| created_at | 2016-12-13T22:55:09Z |
| default_prefixlen | 26 |
| default_quota | None |
| description | |
| id | d02af70b-d622-426f-8e60-ed9df2a8301f |
| ip_version | 4 |
| is_default | False |
| max_prefixlen | 32 |
28 Configuration
Networking Guide (Release Version: 15.0.0)
| min_prefixlen | 8 |
| name | subnet-pool-ip4 |
| prefixes | 203.0.113.0/24 |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| revision_number | 1 |
| shared | True |
| updated_at | 2016-12-13T22:55:09Z |
+-------------------+--------------------------------------+
3. Make sure that subnets on an external network are created from the subnet pools created above:
$ openstack subnet show ipv6-public-subnet
+-------------------+------------------------------------------+
| Field | Value |
+-------------------+------------------------------------------+
| allocation_pools | 2001:db8:a583::2-2001:db8:a583:0:ffff:ff |
| | ff:ffff:ffff |
| cidr | 2001:db8:a583::/64 |
| created_at | 2016-12-10T21:36:04Z |
| description | |
| dns_nameservers | |
| enable_dhcp | False |
| gateway_ip | 2001:db8:a583::1 |
| host_routes | |
| id | b333bf5a-758c-4b3f-97ec-5f12d9bfceb7 |
| ip_version | 6 |
| ipv6_address_mode | None |
| ipv6_ra_mode | None |
| name | ipv6-public-subnet |
| network_id | 05a8d31e-330b-4d96-a3fa-884b04abfa4c |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| revision_number | 2 |
| segment_id | None |
| service_types | |
| subnetpool_id | a59ff52b-0367-41ff-9781-6318b927dd0e |
| updated_at | 2016-12-10T21:36:04Z |
+-------------------+------------------------------------------+
$ openstack subnet show public-subnet
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| allocation_pools | 203.0.113.2-203.0.113.62 |
| cidr | 203.0.113.0/26 |
| created_at | 2016-12-10T21:35:52Z |
| description | |
| dns_nameservers | |
| enable_dhcp | False |
| gateway_ip | 203.0.113.1 |
| host_routes | |
| id | 7fd48240-3acc-4724-bc82-16c62857edec |
| ip_version | 4 |
| ipv6_address_mode | None |
| ipv6_ra_mode | None |
| name | public-subnet |
| network_id | 05a8d31e-330b-4d96-a3fa-884b04abfa4c |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
Configuration 29
Networking Guide (Release Version: 15.0.0)
| revision_number | 2 |
| segment_id | None |
| service_types | |
| subnetpool_id | d02af70b-d622-426f-8e60-ed9df2a8301f |
| updated_at | 2016-12-10T21:35:52Z |
+-------------------+--------------------------------------+
Routing with address scopes for non-privileged users
This section shows how non-privileged users can use address scopes to route straight to an external network
without NAT.
1. Create a couple of networks to host subnets:
$ openstack network create network1
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2016-12-13T23:21:01Z |
| description | |
| headers | |
| id | 1bcf3fe9-a0cb-4d88-a067-a4d7f8e635f0 |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| mtu | 1450 |
| name | network1 |
| port_security_enabled | True |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 94 |
| revision_number | 3 |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | [] |
| updated_at | 2016-12-13T23:21:01Z |
+---------------------------+--------------------------------------+
$ openstack network create network2
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2016-12-13T23:21:45Z |
| description | |
| headers | |
| id | 6c583603-c097-4141-9c5c-288b0e49c59f |
| ipv4_address_scope | None |
30 Configuration
Networking Guide (Release Version: 15.0.0)
| ipv6_address_scope | None |
| mtu | 1450 |
| name | network2 |
| port_security_enabled | True |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 81 |
| revision_number | 3 |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | [] |
| updated_at | 2016-12-13T23:21:45Z |
+---------------------------+--------------------------------------+
2. Create a subnet not associated with a subnet pool or an address scope:
$ openstack subnet create --network network1 --subnet-range \
198.51.100.0/26 subnet-ip4-1
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| allocation_pools | 198.51.100.2-198.51.100.62 |
| cidr | 198.51.100.0/26 |
| created_at | 2016-12-13T23:24:16Z |
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 198.51.100.1 |
| headers | |
| host_routes | |
| id | 66874039-d31b-4a27-85d7-14c89341bbb7 |
| ip_version | 4 |
| ipv6_address_mode | None |
| ipv6_ra_mode | None |
| name | subnet-ip4-1 |
| network_id | 1bcf3fe9-a0cb-4d88-a067-a4d7f8e635f0 |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| revision_number | 2 |
| service_types | |
| subnetpool_id | None |
| updated_at | 2016-12-13T23:24:16Z |
+-------------------+--------------------------------------+
$ openstack subnet create --network network1 --ipv6-ra-mode slaac \
--ipv6-address-mode slaac --ip-version 6 --subnet-range \
2001:db8:80d2:c4d3::/64 subnet-ip6-1
+-------------------+-----------------------------------------+
| Field | Value |
+-------------------+-----------------------------------------+
| allocation_pools | 2001:db8:80d2:c4d3::2-2001:db8:80d2:c4d |
| | 3:ffff:ffff:ffff:ffff |
| cidr | 2001:db8:80d2:c4d3::/64 |
| created_at | 2016-12-13T23:28:28Z |
| description | |
Configuration 31
Networking Guide (Release Version: 15.0.0)
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 2001:db8:80d2:c4d3::1 |
| headers | |
| host_routes | |
| id | a7551b23-2271-4a88-9c41-c84b048e0722 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | slaac |
| name | subnet-ip6-1 |
| network_id | 1bcf3fe9-a0cb-4d88-a067-a4d7f8e635f0 |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| revision_number | 2 |
| service_types | |
| subnetpool_id | None |
| updated_at | 2016-12-13T23:28:28Z |
+-------------------+-----------------------------------------+
3. Create a subnet using a subnet pool associated with an address scope from an external network:
$ openstack subnet create --subnet-pool subnet-pool-ip4 \
--network network2 subnet-ip4-2
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| allocation_pools | 203.0.113.2-203.0.113.62 |
| cidr | 203.0.113.0/26 |
| created_at | 2016-12-13T23:32:12Z |
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 203.0.113.1 |
| headers | |
| host_routes | |
| id | 12be8e8f-5871-4091-9e9e-4e0651b9677e |
| ip_version | 4 |
| ipv6_address_mode | None |
| ipv6_ra_mode | None |
| name | subnet-ip4-2 |
| network_id | 6c583603-c097-4141-9c5c-288b0e49c59f |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| revision_number | 2 |
| service_types | |
| subnetpool_id | d02af70b-d622-426f-8e60-ed9df2a8301f |
| updated_at | 2016-12-13T23:32:12Z |
+-------------------+--------------------------------------+
$ openstack subnet create --ip-version 6 --ipv6-ra-mode slaac \
--ipv6-address-mode slaac --subnet-pool subnet-pool-ip6 \
--network network2 subnet-ip6-2
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| allocation_pools | 2001:db8:a583::2-2001:db8:a583:0:fff |
| | f:ffff:ffff:ffff |
| cidr | 2001:db8:a583::/64 |
| created_at | 2016-12-13T23:31:17Z |
32 Configuration
Networking Guide (Release Version: 15.0.0)
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 2001:db8:a583::1 |
| headers | |
| host_routes | |
| id | b599c2be-e3cd-449c-ba39-3cfcc744c4be |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | slaac |
| name | subnet-ip6-2 |
| network_id | 6c583603-c097-4141-9c5c-288b0e49c59f |
| project_id | 098429d072d34d3596c88b7dbf7e91b6 |
| revision_number | 2 |
| service_types | |
| subnetpool_id | a59ff52b-0367-41ff-9781-6318b927dd0e |
| updated_at | 2016-12-13T23:31:17Z |
+-------------------+--------------------------------------+
By creating subnets from scoped subnet pools, the network is associated with the address scope.
$ openstack network show network2
+---------------------------+------------------------------+
| Field | Value |
+---------------------------+------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | nova |
| created_at | 2016-12-13T23:21:45Z |
| description | |
| id | 6c583603-c097-4141-9c5c- |
| | 288b0e49c59f |
| ipv4_address_scope | 3193bd62-11b5-44dc- |
| | acf8-53180f21e9f2 |
| ipv6_address_scope | 28424dfc-9abd-481b- |
| | afa3-1da97a8fead7 |
| mtu | 1450 |
| name | network2 |
| port_security_enabled | True |
| project_id | 098429d072d34d3596c88b7dbf7e |
| | 91b6 |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 81 |
| revision_number | 10 |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
| subnets | 12be8e8f-5871-4091-9e9e- |
| | 4e0651b9677e, b599c2be-e3cd- |
| | 449c-ba39-3cfcc744c4be |
| tags | [] |
| updated_at | 2016-12-13T23:32:12Z |
+---------------------------+------------------------------+
4. Connect a router to each of the project subnets that have been created, for example, using a router called
router1:
Configuration 33
Networking Guide (Release Version: 15.0.0)
$ openstack router add subnet router1 subnet-ip4-1
$ openstack router add subnet router1 subnet-ip4-2
$ openstack router add subnet router1 subnet-ip6-1
$ openstack router add subnet router1 subnet-ip6-2
Checking connectivity
This example shows how to check the connectivity between networks with address scopes.
1. Launch two instances, instance1 on network1 and instance2 on network2. Associate a floating IP
address to both instances.
2. Adjust security groups to allow pings and SSH (both IPv4 and IPv6):
$ openstack server list
+--------------+-----------+----------------------------------------------------------
,→-----------------+------------+
| ID | Name | Networks
,→ | Image Name |
+--------------+-----------+----------------------------------------------------------
,→-----------------+------------+
| 97e49c8e-... | instance1 | network1=2001:db8:80d2:c4d3:f816:3eff:fe52:b69f, 198.51.
,→100.3, 203.0.113.3| cirros |
| ceba9638-... | instance2 | network2=203.0.113.3,
,→2001:db8:a583:0:f816:3eff:fe42:1eeb, 203.0.113.4 | centos |
+--------------+-----------+----------------------------------------------------------
,→-----------------+------------+
Regardless of address scopes, the floating IPs can be pinged from the external network:
$ ping -c 1 203.0.113.3
1 packets transmitted, 1 received, 0% packet loss, time 0ms
$ ping -c 1 203.0.113.4
1 packets transmitted, 1 received, 0% packet loss, time 0ms
You can now ping instance2 directly because instance2 shares the same address scope as the external
network:
Note: BGP routing can be used to automatically set up a static route for your instances.
# ip route add 203.0.113.0/26 via 203.0.113.2
$ ping -c 1 203.0.113.3
1 packets transmitted, 1 received, 0% packet loss, time 0ms
# ip route add 2001:db8:a583::/64 via 2001:db8::1
$ ping6 -c 1 2001:db8:a583:0:f816:3eff:fe42:1eeb
1 packets transmitted, 1 received, 0% packet loss, time 0ms
You cannot ping instance1 directly because the address scopes do not match:
# ip route add 198.51.100.0/26 via 203.0.113.2
$ ping -c 1 198.51.100.3
1 packets transmitted, 0 received, 100% packet loss, time 0ms
34 Configuration
Networking Guide (Release Version: 15.0.0)
# ip route add 2001:db8:80d2:c4d3::/64 via 2001:db8::1
$ ping6 -c 1 2001:db8:80d2:c4d3:f816:3eff:fe52:b69f
1 packets transmitted, 0 received, 100% packet loss, time 0ms
If the address scopes match between networks then pings and other traffic route directly through. If the scopes
do not match between networks, the router either drops the traffic or applies NAT to cross scope boundaries.
Automatic allocation of network topologies
The auto-allocation feature introduced in Mitaka simplifies the procedure of setting up an external connectivity
for end-users, and is also known as Get Me A Network.
Previously, a user had to configure a range of networking resources to boot a server and get access to the Internet.
For example, the following steps are required:
• Create a network
• Create a subnet
• Create a router
• Uplink the router on an external network
• Downlink the router on the previously created subnet
These steps need to be performed on each logical segment that a VM needs to be connected to, and may require
networking knowledge the user might not have.
This feature is designed to automate the basic networking provisioning for projects. The steps to provision a
basic network are run during instance boot, making the networking setup transparent.
To make this possible, provide a default external network and default subnetpools (one for IPv4, or one for
IPv6, or one of each) so that the platform can choose what to do in lieu of input. Once these are in place, users
can boot their VMs without specifying any networking details. The Compute service will then use this feature
automatically to wire user VMs.
Enabling the deployment for auto-allocation
To use this feature, the neutron service must have the following extensions enabled:
• auto-allocated-topology
• subnet_allocation
• external-net
• router
Before the end-user can use the auto-allocation feature, the operator must create the resources that will be used
for the auto-allocated network topology creation. To perform this task, proceed with the following steps:
1. Set up a default external network
Setting up an external network is described in OpenStack Administrator Guide. Assuming the external
network to be used for the auto-allocation feature is named public, make it the default external network
with the following command:
Configuration 35
Networking Guide (Release Version: 15.0.0)
$ neutron net-update public --is-default=True
2. Create default subnetpools
The auto-allocation feature requires at least one default subnetpool. One for IPv4, or one for IPv6, or
one of each.
$ openstack subnet pool create --share --default \
--pool-prefix 192.0.2.0/24 --default-prefix-length 26 \
shared-default
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| address_scope_id | None |
| created_at | 2017-01-12T15:10:34Z |
| default_prefixlen | 26 |
| default_quota | None |
| description | |
| headers | |
| id | b41b7b9c-de57-4c19-b1c5-731985bceb7f |
| ip_version | 4 |
| is_default | True |
| max_prefixlen | 32 |
| min_prefixlen | 8 |
| name | shared-default |
| prefixes | 192.0.2.0/24 |
| project_id | 86acdbd1d72745fd8e8320edd7543400 |
| revision_number | 1 |
| shared | True |
| updated_at | 2017-01-12T15:10:34Z |
+-------------------+--------------------------------------+
$ openstack subnet pool create --share --default \
--pool-prefix 2001:db8:8000::/48 --default-prefix-length 64 \
default-v6
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| address_scope_id | None |
| created_at | 2017-01-12T15:14:35Z |
| default_prefixlen | 64 |
| default_quota | None |
| description | |
| headers | |
| id | 6f387016-17f0-4564-96ad-e34775b6ea14 |
| ip_version | 6 |
| is_default | True |
| max_prefixlen | 128 |
| min_prefixlen | 64 |
| name | default-v6 |
| prefixes | 2001:db8:8000::/48 |
| project_id | 86acdbd1d72745fd8e8320edd7543400 |
| revision_number | 1 |
| shared | True |
| updated_at | 2017-01-12T15:14:35Z |
36 Configuration
Networking Guide (Release Version: 15.0.0)
+-------------------+--------------------------------------+
Get Me A Network
In a deployment where the operator has set up the resources as described above, validate that users can get their
auto-allocated network topology as follows:
$ neutron auto-allocated-topology-show
+-----------+--------------------------------------+
| Field | Value |
+-----------+--------------------------------------+
| id | 8b835bfb-cae2-4acc-b53f-c16bb5f9a7d0 |
| tenant_id | 3a4e311bcb3545b9b7ad326f93194f8c |
+-----------+--------------------------------------+
Operators (and users with admin role) can get the auto-allocated topology for a project by specifying the project
ID:
$ neutron auto-allocated-topology-show 3a4e311bcb3545b9b7ad326f93194f8c
+-----------+--------------------------------------+
| Field | Value |
+-----------+--------------------------------------+
| id | 8b835bfb-cae2-4acc-b53f-c16bb5f9a7d0 |
| tenant_id | 3a4e311bcb3545b9b7ad326f93194f8c |
+-----------+--------------------------------------+
The ID returned by this command is a network which can be used for booting a VM.
$ openstack server create --flavor m1.small --image \
cirros-0.3.5-x86_64-uec --nic \
net-id=8b835bfb-cae2-4acc-b53f-c16bb5f9a7d0 vm1
The auto-allocated topology for a user never changes. In practice, when a user boots a server omitting the --
nic option, and not have any neutron network available, nova will invoke the API behind auto-allocatedtopology-show,
fetch the network UUID, and pass it on during the boot process.
Validating the requirements for auto-allocation
To validate that the required resources are correctly set up for auto-allocation, without actually provisioning
any resource, use the --dry-run option:
$ neutron auto-allocated-topology-show --dry-run
Deployment error: No default router:external network.
$ neutron net-update public --is-default=True
$ neutron auto-allocated-topology-show --dry-run
Deployment error: No default subnetpools defined.
$ neutron subnetpool-update shared-default --is-default=True
$ neutron auto-allocated-topology-show --dry-run
+---------+-------+
Configuration 37
Networking Guide (Release Version: 15.0.0)
| Field | Value |
+---------+-------+
| dry-run | pass |
+---------+-------+
The validation option behaves identically for all users. However, it is considered primarily an admin or service
utility since it is the operator who must set up the requirements.
Project resources created by auto-allocation
The auto-allocation feature creates one network topology in every project where it is used. The auto-allocated
network topology for a project contains the following resources:
Resource Name
network auto_allocated_network
subnet (IPv4) auto_allocated_subnet_v4
subnet (IPv6) auto_allocated_subnet_v6
router auto_allocated_router
Compatibility notes
Nova uses the auto-allocated-typology feature with API micro version 2.37 or later. This is because,
unlike the neutron feature which was implemented in the Mitaka release, the integration for nova was completed
during the Newton release cycle. Note that the CLI option --nic can be omitted regardless of the microversion
used as long as there is no more than one network available to the project, in which case nova fails with a
400 error because it does not know which network to use. Furthermore, nova does not start using the feature,
regardless of whether or not a user requests micro version 2.37 or later, unless all of the nova-compute services
are running Newton-level code.
Availability zones
An availability zone groups network nodes that run services like DHCP, L3, FW, and others. It is defined as an
agent’s attribute on the network node. This allows users to associate an availability zone with their resources
so that the resources get high availability.
Use case
An availability zone is used to make network resources highly available. The operators group the nodes that
are attached to different power sources under separate availability zones and configure scheduling for resources
with high availability so that they are scheduled on different availability zones.
Required extensions
The core plug-in must support the availability_zone extension. The core plug-in also must support the network_availability_zone
extension to schedule a network according to availability zones. The Ml2Plugin
supports it. The router service plug-in must support the router_availability_zone extension to schedule
a router according to the availability zones. The L3RouterPlugin supports it.
38 Configuration
Networking Guide (Release Version: 15.0.0)
$ openstack extension list --network -c Alias -c Name
+---------------------------+---------------------------+
| Name | Alias |
+---------------------------+---------------------------+
...
| Network Availability Zone | network_availability_zone |
...
| Availability Zone | availability_zone |
...
| Router Availability Zone | router_availability_zone |
...
+---------------------------+---------------------------+
Availability zone of agents
The availability_zone attribute can be defined in dhcp-agent and l3-agent. To define an availability
zone for each agent, set the value into [AGENT] section of /etc/neutron/dhcp_agent.ini or /etc/
neutron/l3_agent.ini:
[AGENT]
availability_zone = zone-1
To confirm the agent’s availability zone:
$ openstack network agent show 116cc128-4398-49af-a4ed-3e95494cd5fc
+---------------------+---------------------------------------------------+
| Field | Value |
+---------------------+---------------------------------------------------+
| admin_state_up | UP |
| agent_type | DHCP agent |
| alive | True |
| availability_zone | zone-1 |
| binary | neutron-dhcp-agent |
| configurations | dhcp_driver='neutron.agent.linux.dhcp.Dnsmasq', |
| | dhcp_lease_duration='86400', |
| | log_agent_heartbeats='False', networks='2', |
| | notifies_port_ready='True', ports='6', subnets='4 |
| created_at | 2016-12-14 00:25:54 |
| description | None |
| heartbeat_timestamp | 2016-12-14 06:20:24 |
| host | ankur-desktop |
| id | 116cc128-4398-49af-a4ed-3e95494cd5fc |
| started_at | 2016-12-14 00:25:54 |
| topic | dhcp_agent |
+---------------------+---------------------------------------------------+
$ openstack network agent show 9632309a-2aa4-4304-8603-c4de02c4a55f
+---------------------+-------------------------------------------------+
| Field | Value |
+---------------------+-------------------------------------------------+
| admin_state_up | UP |
| agent_type | L3 agent |
| alive | True |
| availability_zone | zone-1 |
| binary | neutron-l3-agent |
Configuration 39
Networking Guide (Release Version: 15.0.0)
| configurations | agent_mode='legacy', ex_gw_ports='2', |
| | external_network_bridge='', floating_ips='0', |
| | gateway_external_network_id='', |
| | handle_internal_only_routers='True', |
| | interface_driver='openvswitch', interfaces='4', |
| | log_agent_heartbeats='False', routers='2' |
| created_at | 2016-12-14 00:25:58 |
| description | None |
| heartbeat_timestamp | 2016-12-14 06:20:28 |
| host | ankur-desktop |
| id | 9632309a-2aa4-4304-8603-c4de02c4a55f |
| started_at | 2016-12-14 00:25:58 |
| topic | l3_agent |
+---------------------+-------------------------------------------------+
Availability zone related attributes
The following attributes are added into network and router:
Attribute name Access Required
Input type Description
availability_zone_hints
RW(POST
only)
No list of
string
availability zone candidates for the
resource
availability_zones RO N/A list of
string
availability zones for the resource
Use availability_zone_hints to specify the zone in which the resource is hosted:
$ openstack network create --availability-zone-hint zone-1 \
--availability-zone-hint zone-2 net1
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | zone-1 |
| | zone-2 |
| availability_zones | |
| created_at | 2016-12-14T06:23:36Z |
| description | |
| headers | |
| id | ad88e059-e7fa-4cf7-8857-6731a2a3a554 |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| mtu | 1450 |
| name | net1 |
| port_security_enabled | True |
| project_id | cfd1889ac7d64ad891d4f20aef9f8d7c |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 77 |
| revision_number | 3 |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
| subnets | |
40 Configuration
Networking Guide (Release Version: 15.0.0)
| tags | [] |
| updated_at | 2016-12-14T06:23:37Z |
+---------------------------+--------------------------------------+
$ openstack router create --ha --availability-zone-hint zone-1 \
--availability-zone-hint zone-2 router1
+-------------------------+--------------------------------------+
| Field | Value |
+-------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | zone-1 |
| | zone-2 |
| availability_zones | |
| created_at | 2016-12-14T06:25:40Z |
| description | |
| distributed | False |
| external_gateway_info | null |
| flavor_id | None |
| ha | False |
| headers | |
| id | ced10262-6cfe-47c1-8847-cd64276a868c |
| name | router1 |
| project_id | cfd1889ac7d64ad891d4f20aef9f8d7c |
| revision_number | 3 |
| routes | |
| status | ACTIVE |
| updated_at | 2016-12-14T06:25:40Z |
+-------------------------+--------------------------------------+
Availability zone is selected from default_availability_zones in /etc/neutron/neutron.conf if a
resource is created without availability_zone_hints:
default_availability_zones = zone-1,zone-2
To confirm the availability zone defined by the system:
$ openstack availability zone list
+-----------+-------------+
| Zone Name | Zone Status |
+-----------+-------------+
| zone-1 | available |
| zone-2 | available |
| zone-1 | available |
| zone-2 | available |
+-----------+-------------+
Look at the availability_zones attribute of each resource to confirm in which zone the resource is hosted:
$ openstack network show net1
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | zone-1 |
| | zone-2 |
| availability_zones | zone-1 |
Configuration 41
Networking Guide (Release Version: 15.0.0)
| | zone-2 |
| created_at | 2016-12-14T06:23:36Z |
| description | |
| headers | |
| id | ad88e059-e7fa-4cf7-8857-6731a2a3a554 |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| mtu | 1450 |
| name | net1 |
| port_security_enabled | True |
| project_id | cfd1889ac7d64ad891d4f20aef9f8d7c |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 77 |
| revision_number | 3 |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | [] |
| updated_at | 2016-12-14T06:23:37Z |
+---------------------------+--------------------------------------+
$ openstack router show router1
+-------------------------+--------------------------------------+
| Field | Value |
+-------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | zone-1 |
| | zone-2 |
| availability_zones | zone-1 |
| | zone-2 |
| created_at | 2016-12-14T06:25:40Z |
| description | |
| distributed | False |
| external_gateway_info | null |
| flavor_id | None |
| ha | False |
| headers | |
| id | ced10262-6cfe-47c1-8847-cd64276a868c |
| name | router1 |
| project_id | cfd1889ac7d64ad891d4f20aef9f8d7c |
| revision_number | 3 |
| routes | |
| status | ACTIVE |
| updated_at | 2016-12-14T06:25:40Z |
+-------------------------+--------------------------------------+
Note: The availability_zones attribute does not have a value until the resource is scheduled. Once the
Networking service schedules the resource to zones according to availability_zone_hints, availability_zones
shows in which zone the resource is hosted practically. The availability_zones may not match
availability_zone_hints. For example, even if you specify a zone with availability_zone_hints, all
agents of the zone may be dead before the resource is scheduled. In general, they should match, unless there
are failures or there is no capacity left in the zone requested.
42 Configuration
Networking Guide (Release Version: 15.0.0)
Availability zone aware scheduler
Network scheduler
Set AZAwareWeightScheduler to network_scheduler_driver in /etc/neutron/neutron.conf so that
the Networking service schedules a network according to the availability zone:
network_scheduler_driver = neutron.scheduler.dhcp_agent_scheduler.AZAwareWeightScheduler
dhcp_load_type = networks
The Networking service schedules a network to one of the agents within the selected zone as with
WeightScheduler. In this case, scheduler refers to dhcp_load_type as well.
Router scheduler
Set AZLeastRoutersScheduler to router_scheduler_driver in file /etc/neutron/neutron.conf so
that the Networking service schedules a router according to the availability zone:
router_scheduler_driver = neutron.scheduler.l3_agent_scheduler.AZLeastRoutersScheduler
The Networking service schedules a router to one of the agents within the selected zone as with LeastRouterScheduler.
Achieving high availability with availability zone
Although, the Networking service provides high availability for routers and high availability and fault tolerance
for networks’ DHCP services, availability zones provide an extra layer of protection by segmenting a Networking
service deployment in isolated failure domains. By deploying HA nodes across different availability zones,
it is guaranteed that network services remain available in face of zone-wide failures that affect the deployment.
This section explains how to get high availability with the availability zone for L3 and DHCP. You should
naturally set above configuration options for the availability zone.
L3 high availability
Set the following configuration options in file /etc/neutron/neutron.conf so that you get L3 high availability.
l3_ha = True
max_l3_agents_per_router = 3
HA routers are created on availability zones you selected when creating the router.
DHCP high availability
Set the following configuration options in file /etc/neutron/neutron.conf so that you get DHCP high
availability.
dhcp_agents_per_network = 2
DHCP services are created on availability zones you selected when creating the network.
Configuration 43
Networking Guide (Release Version: 15.0.0)
BGP dynamic routing
BGP dynamic routing enables advertisement of self-service (private) network prefixes to physical network
devices that support BGP such as routers, thus removing the conventional dependency on static routes. The
feature relies on address scopes and requires knowledge of their operation for proper deployment.
BGP dynamic routing consists of a service plug-in and an agent. The service plug-in implements the Networking
service extension and the agent manages BGP peering sessions. A cloud administrator creates and configures
a BGP speaker using the CLI or API and manually schedules it to one or more hosts running the agent. Agents
can reside on hosts with or without other Networking service agents. Prefix advertisement depends on the
binding of external networks to a BGP speaker and the address scope of external and internal IP address ranges
or subnets.
Note: Although self-service networks generally use private IP address ranges (RFC1918) for IPv4 subnets,
BGP dynamic routing can advertise any IPv4 address ranges.
Example configuration
The example configuration involves the following components:
• One BGP agent.
• One address scope containing IP address range 203.0.113.0/24 for provider networks, and IP address
ranges 10.0.1.0/24 and 10.0.2.0/24 for self-service networks.
• One provider network using IP address range 203.0.113.0/24.
44 Configuration
Networking Guide (Release Version: 15.0.0)
• Three self-service networks.
– Self-service networks 1 and 2 use IP address ranges inside of the address scope.
– Self-service network 3 uses a unique IP address range 10.0.3.0/24 to demonstrate that the BGP
speaker does not advertise prefixes outside of address scopes.
• Three routers. Each router connects one self-service network to the provider network.
– Router 1 contains IP addresses 203.0.113.11 and 10.0.1.1.
– Router 2 contains IP addresses 203.0.113.12 and 10.0.2.1.
– Router 3 contains IP addresses 203.0.113.13 and 10.0.3.1.
Note: The example configuration assumes sufficient knowledge about the Networking service, routing, and
BGP. For basic deployment of the Networking service, consult one of the Deployment examples. For more
information on BGP, see RFC 4271.
Controller node
• In the neutron.conf file, enable the conventional layer-3 and BGP dynamic routing service plug-ins:
[DEFAULT]
service_plugins = neutron_dynamic_routing.services.bgp.bgp_plugin.BgpPlugin,neutron.
,→services.l3_router.l3_router_plugin.L3RouterPlugin
Agent nodes
• In the bgp_dragent.ini file:
– Configure the driver.
[BGP]
bgp_speaker_driver = neutron_dynamic_routing.services.bgp.agent.driver.ryu.driver.
,→RyuBgpDriver
Note: The agent currently only supports the Ryu BGP driver.
– Configure the router ID.
[BGP]
bgp_router_id = ROUTER_ID
Replace ROUTER_ID with a suitable unique 32-bit number, typically an IPv4 address on the host
running the agent. For example, 192.0.2.2.
Verify service operation
1. Source the administrative project credentials.
Configuration 45
Networking Guide (Release Version: 15.0.0)
2. Verify presence and operation of each BGP dynamic routing agent.
$ neutron agent-list --agent-type="BGP dynamic routing agent"
+--------------------------------------+---------------------------+------------+-----
,→--------------+-------+----------------+---------------------------+
| id | agent_type | host |
,→availability_zone | alive | admin_state_up | binary |
+--------------------------------------+---------------------------+------------+-----
,→--------------+-------+----------------+---------------------------+
| 37729181-2224-48d8-89ef-16eca8e2f77e | BGP dynamic routing agent | controller |
,→ | :-) | True | neutron-bgp-dragent |
+--------------------------------------+---------------------------+------------+-----
,→--------------+-------+----------------+---------------------------+
Create the address scope and subnet pools
1. Create an address scope. The provider (external) and self-service networks must belong to the same
address scope for the agent to advertise those self-service network prefixes.
$ openstack address scope create --share --ip-version 4 bgp
+------------+--------------------------------------+
| Field | Value |
+------------+--------------------------------------+
| headers | |
| id | f71c958f-dbe8-49a2-8fb9-19c5f52a37f1 |
| ip_version | 4 |
| name | bgp |
| project_id | 86acdbd1d72745fd8e8320edd7543400 |
| shared | True |
+------------+--------------------------------------+
2. Create subnet pools. The provider and self-service networks use different pools.
• Create the provider network pool.
$ openstack subnet pool create --pool-prefix 203.0.113.0/24 \
--address-scope bgp provider
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| address_scope_id | f71c958f-dbe8-49a2-8fb9-19c5f52a37f1 |
| created_at | 2017-01-12T14:58:57Z |
| default_prefixlen | 8 |
| default_quota | None |
| description | |
| headers | |
| id | 63532225-b9a0-445a-9935-20a15f9f68d1 |
| ip_version | 4 |
| is_default | False |
| max_prefixlen | 32 |
| min_prefixlen | 8 |
| name | provider |
| prefixes | 203.0.113.0/24 |
| project_id | 86acdbd1d72745fd8e8320edd7543400 |
46 Configuration
Networking Guide (Release Version: 15.0.0)
| revision_number | 1 |
| shared | False |
| updated_at | 2017-01-12T14:58:57Z |
+-------------------+--------------------------------------+
• Create the self-service network pool.
$ openstack subnet pool create --pool-prefix 10.0.1.0/24 \
--pool-prefix 10.0.2.0/24 --address-scope bgp \
--share selfservice
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| address_scope_id | f71c958f-dbe8-49a2-8fb9-19c5f52a37f1 |
| created_at | 2017-01-12T15:02:31Z |
| default_prefixlen | 8 |
| default_quota | None |
| description | |
| headers | |
| id | 8d8270b1-b194-4b7e-914c-9c741dcbd49b |
| ip_version | 4 |
| is_default | False |
| max_prefixlen | 32 |
| min_prefixlen | 8 |
| name | selfservice |
| prefixes | 10.0.1.0/24, 10.0.2.0/24 |
| project_id | 86acdbd1d72745fd8e8320edd7543400 |
| revision_number | 1 |
| shared | True |
| updated_at | 2017-01-12T15:02:31Z |
+-------------------+--------------------------------------+
Create the provider and self-service networks
1. Create the provider network.
$ openstack network create provider --external --provider-physical-network \
provider --provider-network-type flat
Created a new network:
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2016-12-21T08:47:41Z |
| description | |
| headers | |
| id | 190ca651-2ee3-4a4b-891f-dedda47974fe |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| is_default | False |
| mtu | 1450 |
| name | provider |
Configuration 47
Networking Guide (Release Version: 15.0.0)
| port_security_enabled | True |
| project_id | c961a8f6d3654657885226378ade8220 |
| provider:network_type | flat |
| provider:physical_network | provider |
| provider:segmentation_id | 66 |
| revision_number | 3 |
| router:external | External |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | [] |
| updated_at | 2016-12-21T08:47:41Z |
+---------------------------+--------------------------------------+
2. Create a subnet on the provider network using an IP address range from the provider subnet pool.
$ neutron subnet-create --name provider --subnetpool provider \
--prefixlen 24 --allocation-pool start=203.0.113.11,end=203.0.113.254 \
--gateway 203.0.113.1 provider
Created a new subnet:
+-------------------+---------------------------------------------------+
| Field | Value |
+-------------------+---------------------------------------------------+
| allocation_pools | {"start": "203.0.113.11", "end": "203.0.113.254"} |
| cidr | 203.0.113.0/24 |
| created_at | 2016-03-17T23:17:16 |
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 203.0.113.1 |
| host_routes | |
| id | 8ed65d41-2b2a-4f3a-9f92-45adb266e01a |
| ip_version | 4 |
| ipv6_address_mode | |
| ipv6_ra_mode | |
| name | provider |
| network_id | 68ec148c-181f-4656-8334-8f4eb148689d |
| subnetpool_id | 3771c0e7-7096-46d3-a3bd-699c58e70259 |
| tenant_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
| updated_at | 2016-03-17T23:17:16 |
+-------------------+---------------------------------------------------+
Note: The IP address allocation pool starting at .11 improves clarity of the diagrams. You can safely
omit it.
3. Create the self-service networks.
$ openstack network create selfservice1
Created a new network:
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
48 Configuration
Networking Guide (Release Version: 15.0.0)
| created_at | 2016-12-21T08:49:38Z |
| description | |
| headers | |
| id | 9d842606-ef3d-4160-9ed9-e03fa63aed96 |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| mtu | 1450 |
| name | selfservice1 |
| port_security_enabled | True |
| project_id | c961a8f6d3654657885226378ade8220 |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 106 |
| revision_number | 3 |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | [] |
| updated_at | 2016-12-21T08:49:38Z |
+---------------------------+--------------------------------------+
$ openstack network create selfservice2
Created a new network:
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2016-12-21T08:50:05Z |
| description | |
| headers | |
| id | f85639e1-d23f-438e-b2b1-f40570d86b1c |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| mtu | 1450 |
| name | selfservice2 |
| port_security_enabled | True |
| project_id | c961a8f6d3654657885226378ade8220 |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 21 |
| revision_number | 3 |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | [] |
| updated_at | 2016-12-21T08:50:05Z |
+---------------------------+--------------------------------------+
$ openstack network create selfservice3
Created a new network:
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
Configuration 49
Networking Guide (Release Version: 15.0.0)
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2016-12-21T08:50:35Z |
| description | |
| headers | |
| id | eeccdb82-5cf4-4999-8ab3-e7dc99e7d43b |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| mtu | 1450 |
| name | selfservice3 |
| port_security_enabled | True |
| project_id | c961a8f6d3654657885226378ade8220 |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 86 |
| revision_number | 3 |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | [] |
| updated_at | 2016-12-21T08:50:35Z |
+---------------------------+--------------------------------------+
4. Create a subnet on the first two self-service networks using an IP address range from the self-service
subnet pool.
$ neutron subnet-create --name selfservice1 --subnetpool selfservice \
--prefixlen 24 selfservice1
Created a new subnet:
+-------------------+--------------------------------------------+
| Field | Value |
+-------------------+--------------------------------------------+
| allocation_pools | {"start": "10.0.1.2", "end": "10.0.1.254"} |
| cidr | 10.0.1.0/24 |
| created_at | 2016-03-17T23:20:20 |
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 10.0.1.1 |
| host_routes | |
| id | 8edd3dc2-df40-4d71-816e-a4586d61c809 |
| ip_version | 4 |
| ipv6_address_mode | |
| ipv6_ra_mode | |
| name | selfservice1 |
| network_id | be79de1e-5f56-11e6-9dfb-233e41cec48c |
| subnetpool_id | c7e9737a-cfd3-45b5-a861-d1cee1135a92 |
| tenant_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
| updated_at | 2016-03-17T23:20:20 |
+-------------------+--------------------------------------------+
$ neutron subnet-create --name selfservice2 --subnetpool selfservice \
--prefixlen 24 selfservice2
Created a new subnet:
+-------------------+--------------------------------------------+
50 Configuration
Networking Guide (Release Version: 15.0.0)
| Field | Value |
+-------------------+--------------------------------------------+
| allocation_pools | {"start": "10.0.2.2", "end": "10.0.2.254"} |
| cidr | 10.0.2.0/24 |
| created_at | 2016-03-17T23:20:20 |
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 10.0.2.1 |
| host_routes | |
| id | 8edd3dc2-df40-4d71-816e-a4586d61c809 |
| ip_version | 4 |
| ipv6_address_mode | |
| ipv6_ra_mode | |
| name | selfservice2 |
| network_id | c1fd9846-5f56-11e6-a8ac-0f998d9cc0a2 |
| subnetpool_id | c7e9737a-cfd3-45b5-a861-d1cee1135a92 |
| tenant_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
| updated_at | 2016-03-17T23:20:20 |
+-------------------+--------------------------------------------+
5. Create a subnet on the last self-service network using an IP address range outside of the address scope.
$ neutron subnet-create --name subnet3 selfservice3 10.0.3.0/24
Created a new subnet:
+-------------------+--------------------------------------------+
| Field | Value |
+-------------------+--------------------------------------------+
| allocation_pools | {"start": "10.0.3.2", "end": "10.0.3.254"} |
| cidr | 10.0.3.0/24 |
| created_at | 2016-03-17T23:20:20 |
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 10.0.3.1 |
| host_routes | |
| id | cd9f9156-5f59-11e6-aeec-172ec7ee939a |
| ip_version | 4 |
| ipv6_address_mode | |
| ipv6_ra_mode | |
| name | selfservice3 |
| network_id | c283dc1c-5f56-11e6-bfb6-efc30e1eb73b |
| subnetpool_id | |
| tenant_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
| updated_at | 2016-03-17T23:20:20 |
+-------------------+--------------------------------------------+
Create and configure the routers
1. Create the routers.
$ openstack router create router1
+-------------------------+--------------------------------------+
| Field | Value |
+-------------------------+--------------------------------------+
Configuration 51
Networking Guide (Release Version: 15.0.0)
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2017-01-10T13:15:19Z |
| description | |
| distributed | False |
| external_gateway_info | null |
| flavor_id | None |
| ha | False |
| headers | |
| id | 3f6f4ef8-63be-11e6-bbb3-2fbcef363ab8 |
| name | router1 |
| project_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
| revision_number | 1 |
| routes | |
| status | ACTIVE |
| updated_at | 2017-01-10T13:15:19Z |
+-------------------------+--------------------------------------+
$ openstack router create router2
+-------------------------+--------------------------------------+
| Field | Value |
+-------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2017-01-10T13:15:19Z |
| description | |
| distributed | False |
| external_gateway_info | null |
| flavor_id | None |
| ha | False |
| headers | |
| id | 3fd21a60-63be-11e6-9c95-5714c208c499 |
| name | router2 |
| project_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
| revision_number | 1 |
| routes | |
| status | ACTIVE |
| updated_at | 2017-01-10T13:15:19Z |
+-------------------------+--------------------------------------+
$ openstack router create router3
+-------------------------+--------------------------------------+
| Field | Value |
+-------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2017-01-10T13:15:19Z |
| description | |
| distributed | False |
| external_gateway_info | null |
| flavor_id | None |
| ha | False |
| headers | |
| id | 40069a4c-63be-11e6-9ecc-e37c1eaa7e84 |
52 Configuration
Networking Guide (Release Version: 15.0.0)
| name | router3 |
| project_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
| revision_number | 1 |
| routes | |
| status | ACTIVE |
| updated_at | 2017-01-10T13:15:19Z |
+-------------------------+--------------------------------------+
2. For each router, add one self-service subnet as an interface on the router.
$ neutron router-interface-add router1 selfservice1
Added interface 90e3880a-5f5c-11e6-914c-9f3e20c8c151 to router router1.
$ neutron router-interface-add router2 selfservice2
Added interface 91628362-5f5c-11e6-826a-7322fb03a821 to router router2.
$ neutron router-interface-add router3 selfservice3
Added interface 91d51044-5f5c-11e6-bf55-ffd180541cc2 to router router3.
3. Add the provider network as a gateway on each router.
$ neutron router-gateway-set router1 provider
Set gateway for router router1
$ neutron router-gateway-set router2 provider
Set gateway for router router2
$ neutron router-gateway-set router3 provider
Set gateway for router router3
Create and configure the BGP speaker
The BGP speaker advertises the next-hop IP address for eligible self-service networks and floating IP addresses
for instances using those networks.
1. Create the BGP speaker.
$ neutron bgp-speaker-create --ip-version 4 \
--local-as LOCAL_AS bgpspeaker
Created a new bgp_speaker:
+-----------------------------------+--------------------------------------+
| Field | Value |
+-----------------------------------+--------------------------------------+
| advertise_floating_ip_host_routes | True |
| advertise_tenant_networks | True |
| id | 5f227f14-4f46-4eca-9524-fc5a1eabc358 |
| ip_version | 4 |
| local_as | 1234 |
| name | bgpspeaker |
| networks | |
| peers | |
| tenant_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
+-----------------------------------+--------------------------------------+
Configuration 53
Networking Guide (Release Version: 15.0.0)
Replace LOCAL_AS with an appropriate local autonomous system number. The example configuration
uses AS 1234.
2. A BGP speaker requires association with a provider network to determine eligible prefixes. The association
builds a list of all virtual routers with gateways on provider and self-service networks in the same
address scope so the BGP speaker can advertise self-service network prefixes with the corresponding
router as the next-hop IP address. Associate the BGP speaker with the provider network.
$ neutron bgp-speaker-network-add bgpspeaker provider
Added network provider to BGP speaker bgpspeaker.
3. Verify association of the provider network with the BGP speaker.
$ neutron bgp-speaker-show bgpspeaker
+-----------------------------------+--------------------------------------+
| Field | Value |
+-----------------------------------+--------------------------------------+
| advertise_floating_ip_host_routes | True |
| advertise_tenant_networks | True |
| id | 5f227f14-4f46-4eca-9524-fc5a1eabc358 |
| ip_version | 4 |
| local_as | 1234 |
| name | bgpspeaker |
| networks | 68ec148c-181f-4656-8334-8f4eb148689d |
| peers | |
| tenant_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
+-----------------------------------+--------------------------------------+
4. Verify the prefixes and next-hop IP addresses that the BGP speaker advertises.
$ neutron bgp-speaker-advertiseroute-list bgpspeaker
+-------------+--------------+
| destination | next_hop |
+-------------+--------------+
| 10.0.1.0/24 | 203.0.113.11 |
| 10.0.2.0/24 | 203.0.113.12 |
+-------------+--------------+
5. Create a BGP peer.
$ neutron bgp-peer-create --peer-ip 192.0.2.1 \
--remote-as REMOTE_AS bgppeer
Created a new bgp_peer:
+-----------+--------------------------------------+
| Field | Value |
+-----------+--------------------------------------+
| auth_type | none |
| id | 35c89ca0-ac5a-4298-a815-0b073c2362e9 |
| name | bgppeer |
| peer_ip | 192.0.2.1 |
| remote_as | 4321 |
| tenant_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
+-----------+--------------------------------------+
Replace REMOTE_AS with an appropriate remote autonomous system number. The example configuration
uses AS 4321 which triggers EBGP peering.
54 Configuration
Networking Guide (Release Version: 15.0.0)
Note: The host containing the BGP agent must have layer-3 connectivity to the provider router.
6. Add a BGP peer to the BGP speaker.
$ neutron bgp-speaker-peer-add bgpspeaker bgppeer
Added BGP peer bgppeer to BGP speaker bgpspeaker.
7. Verify addition of the BGP peer to the BGP speaker.
$ neutron bgp-speaker-show bgpspeaker
+-----------------------------------+--------------------------------------+
| Field | Value |
+-----------------------------------+--------------------------------------+
| advertise_floating_ip_host_routes | True |
| advertise_tenant_networks | True |
| id | 5f227f14-4f46-4eca-9524-fc5a1eabc358 |
| ip_version | 4 |
| local_as | 1234 |
| name | bgpspeaker |
| networks | 68ec148c-181f-4656-8334-8f4eb148689d |
| peers | 35c89ca0-ac5a-4298-a815-0b073c2362e9 |
| tenant_id | b3ac05ef10bf441fbf4aa17f16ae1e6d |
+-----------------------------------+--------------------------------------+
Note: After creating a peering session, you cannot change the local or remote autonomous system
numbers.
Schedule the BGP speaker to an agent
1. Unlike most agents, BGP speakers require manual scheduling to an agent. BGP speakers only form
peering sessions and begin prefix advertisement after scheduling to an agent. Schedule the BGP speaker
to agent 37729181-2224-48d8-89ef-16eca8e2f77e.
$ neutron bgp-dragent-speaker-add 37729181-2224-48d8-89ef-16eca8e2f77e bgpspeaker
Associated BGP speaker bgpspeaker to the Dynamic Routing agent.
2. Verify scheduling of the BGP speaker to the agent.
$ neutron bgp-dragent-list-hosting-speaker bgpspeaker
+--------------------------------------+------------+----------------+-------+
| id | host | admin_state_up | alive |
+--------------------------------------+------------+----------------+-------+
| 37729181-2224-48d8-89ef-16eca8e2f77e | controller | True | :-) |
+--------------------------------------+------------+----------------+-------+
$ neutron bgp-speaker-list-on-dragent 37729181-2224-48d8-89ef-16eca8e2f77e
+--------------------------------------+------------+----------+------------+
| id | name | local_as | ip_version |
+--------------------------------------+------------+----------+------------+
| 5f227f14-4f46-4eca-9524-fc5a1eabc358 | bgpspeaker | 1234 | 4 |
+--------------------------------------+------------+----------+------------+
Configuration 55
Networking Guide (Release Version: 15.0.0)
Prefix advertisement
BGP dynamic routing advertises prefixes for self-service networks and host routes for floating IP addresses.
Advertisement of a self-service network requires satisfying the following conditions:
• The external and self-service network reside in the same address scope.
• The router contains an interface on the self-service subnet and a gateway on the external network.
• The BGP speaker associates with the external network that provides a gateway on the router.
• The BGP speaker has the advertise_tenant_networks attribute set to True.
Advertisement of a floating IP address requires satisfying the following conditions:
• The router with the floating IP address binding contains a gateway on an external network with the BGP
speaker association.
• The BGP speaker has the advertise_floating_ip_host_routes attribute set to True.
56 Configuration
Networking Guide (Release Version: 15.0.0)
Operation with Distributed Virtual Routers (DVR)
In deployments using DVR, the BGP speaker advertises floating IP addresses and self-service networks differently.
For floating IP addresses, the BGP speaker advertises the floating IP agent gateway on the corresponding
compute node as the next-hop IP address. For self-service networks using SNAT, the BGP speaker advertises
the DVR SNAT node as the next-hop IP address.
For example, consider the following components:
1. A provider network using IP address range 203.0.113.0/24, and supporting floating IP addresses
203.0.113.101, 203.0.113.102, and 203.0.113.103.
2. A self-service network using IP address range 10.0.1.0/24.
3. The SNAT gateway resides on 203.0.113.11.
4. The floating IP agent gateways (one per compute node) reside on 203.0.113.12, 203.0.113.13, and
203.0.113.14.
5. Three instances, one per compute node, each with a floating IP address.
$ neutron bgp-speaker-advertiseroute-list bgpspeaker
+------------------+--------------+
| destination | next_hop |
Configuration 57
Networking Guide (Release Version: 15.0.0)
+------------------+--------------+
| 10.0.1.0/24 | 203.0.113.11 |
| 203.0.113.101/32 | 203.0.113.12 |
| 203.0.113.102/32 | 203.0.113.13 |
| 203.0.113.103/32 | 203.0.113.14 |
+------------------+--------------+
Note: DVR lacks support for routing directly to a fixed IP address via the floating IP agent gateway port and
thus prevents the BGP speaker from advertising fixed IP addresses.
You can also identify floating IP agent gateways in your environment to assist with verifying operation of the
BGP speaker.
$ neutron port-list --device_owner="network:floatingip_agent_gateway"
+--------------------------------------+------+-------------------+-------------------------
,→-------------------------------------------------------------------------------+
| id | name | mac_address | fixed_ips
,→ |
+--------------------------------------+------+-------------------+-------------------------
,→-------------------------------------------------------------------------------+
| 87cf2970-4970-462e-939e-00e808295dfa | | fa:16:3e:7c:68:e3 | {"subnet_id": "8ed65d41-
,→2b2a-4f3a-9f92-45adb266e01a", "ip_address": "203.0.113.12"} |
| 8d218440-0d2e-49d0-8a7b-3266a6146dc1 | | fa:16:3e:9d:78:cf | {"subnet_id": "8ed65d41-
,→2b2a-4f3a-9f92-45adb266e01a", "ip_address": "203.0.113.13"} |
| 87cf2970-4970-462e-939e-00e802281dfa | | fa:16:3e:6b:18:e0 | {"subnet_id": "8ed65d41-
,→2b2a-4f3a-9f92-45adb266e01a", "ip_address": "203.0.113.14"} |
+--------------------------------------+------+-------------------+-------------------------
,→-------------------------------------------------------------------------------+
IPv6
BGP dynamic routing supports peering via IPv6 and advertising IPv6 prefixes.
• To enable peering via IPv6, create a BGP peer and use an IPv6 address for peer_ip.
• To enable advertising IPv6 prefixes, create an address scope with ip_version=6 and a BGP speaker
with ip_version=6.
Note: DVR with IPv6 functions similarly to DVR with IPv4.
High availability
BGP dynamic routing supports scheduling a BGP speaker to multiple agents which effectively multiplies prefix
advertisements to the same peer. If an agent fails, the peer continues to receive advertisements from one or more
operational agents.
1. Show available dynamic routing agents.
$ neutron agent-list --agent-type="BGP dynamic routing agent"
+--------------------------------------+---------------------------+----------+-------
,→------------+-------+----------------+---------------------------+
58 Configuration
Networking Guide (Release Version: 15.0.0)
| id | agent_type | host |
,→availability_zone | alive | admin_state_up | binary |
+--------------------------------------+---------------------------+----------+-------
,→------------+-------+----------------+---------------------------+
| 37729181-2224-48d8-89ef-16eca8e2f77e | BGP dynamic routing agent | bgp-ha1 |
,→ | :-) | True | neutron-bgp-dragent |
| 1a2d33bb-9321-30a2-76ab-22eff3d2f56a | BGP dynamic routing agent | bgp-ha2 |
,→ | :-) | True | neutron-bgp-dragent |
+--------------------------------------+---------------------------+----------+-------
,→------------+-------+----------------+---------------------------+
2. Schedule BGP speaker to multiple agents.
$ neutron bgp-dragent-speaker-add 37729181-2224-48d8-89ef-16eca8e2f77e bgpspeaker
Associated BGP speaker bgpspeaker to the Dynamic Routing agent.
$ neutron bgp-dragent-speaker-add 1a2d33bb-9321-30a2-76ab-22eff3d2f56a bgpspeaker
Associated BGP speaker bgpspeaker to the Dynamic Routing agent.
$ neutron bgp-dragent-list-hosting-speaker bgpspeaker
+--------------------------------------+---------+----------------+-------+
| id | host | admin_state_up | alive |
+--------------------------------------+---------+----------------+-------+
| 37729181-2224-48d8-89ef-16eca8e2f77e | bgp-ha1 | True | :-) |
| 1a2d33bb-9321-30a2-76ab-22eff3d2f56a | bgp-ha2 | True | :-) |
+--------------------------------------+---------+----------------+-------+
$ neutron bgp-speaker-list-on-dragent 37729181-2224-48d8-89ef-16eca8e2f77e
+--------------------------------------+------------+----------+------------+
| id | name | local_as | ip_version |
+--------------------------------------+------------+----------+------------+
| 5f227f14-4f46-4eca-9524-fc5a1eabc358 | bgpspeaker | 1234 | 4 |
+--------------------------------------+------------+----------+------------+
$ neutron bgp-speaker-list-on-dragent 1a2d33bb-9321-30a2-76ab-22eff3d2f56a
+--------------------------------------+------------+----------+------------+
| id | name | local_as | ip_version |
+--------------------------------------+------------+----------+------------+
| 5f227f14-4f46-4eca-9524-fc5a1eabc358 | bgpspeaker | 1234 | 4 |
+--------------------------------------+------------+----------+------------+
High-availability for DHCP
This section describes how to use the agent management (alias agent) and scheduler (alias agent_scheduler)
extensions for DHCP agents scalability and HA.
Note: Use the neutron ext-list client command to check if these extensions are enabled. Check agent
and agent_scheduler are included in the output.
$ openstack extension list --network -c Name -c Alias
+-------------------------------------------------------------+---------------------------+
| Name | Alias |
+-------------------------------------------------------------+---------------------------+
| Default Subnetpools | default-subnetpools |
Configuration 59
Networking Guide (Release Version: 15.0.0)
| Network IP Availability | network-ip-availability |
| Network Availability Zone | network_availability_zone |
| Auto Allocated Topology Services | auto-allocated-topology |
| Neutron L3 Configurable external gateway mode | ext-gw-mode |
| Port Binding | binding |
| Neutron Metering | metering |
| agent | agent |
| Subnet Allocation | subnet_allocation |
| L3 Agent Scheduler | l3_agent_scheduler |
| Tag support | tag |
| Neutron external network | external-net |
| Neutron Service Flavors | flavors |
| Network MTU | net-mtu |
| Availability Zone | availability_zone |
| Quota management support | quotas |
| HA Router extension | l3-ha |
| Provider Network | provider |
| Multi Provider Network | multi-provider |
| Address scope | address-scope |
| Neutron Extra Route | extraroute |
| Subnet service types | subnet-service-types |
| Resource timestamps | standard-attr-timestamp |
| Neutron Service Type Management | service-type |
| Router Flavor Extension | l3-flavors |
| Tag support for resources: subnet, subnetpool, port, router | tag-ext |
| Neutron Extra DHCP opts | extra_dhcp_opt |
| Resource revision numbers | standard-attr-revisions |
| Pagination support | pagination |
| Sorting support | sorting |
| security-group | security-group |
| DHCP Agent Scheduler | dhcp_agent_scheduler |
| Router Availability Zone | router_availability_zone |
| RBAC Policies | rbac-policies |
| standard-attr-description | standard-attr-description |
| Neutron L3 Router | router |
| Allowed Address Pairs | allowed-address-pairs |
| project_id field enabled | project-id |
| Distributed Virtual Router | dvr |
+-------------------------------------------------------------+---------------------------+
60 Configuration
Networking Guide (Release Version: 15.0.0)
Demo setup
There will be three hosts in the setup.
Host Description
OpenStack
controller host -
controlnode
Runs the Networking, Identity, and Compute services that are required to deploy
VMs. The node must have at least one network interface that is connected to the
Management Network. Note that nova-network should not be running because it
is replaced by Neutron.
HostA Runs nova-compute, the Neutron L2 agent and DHCP agent
HostB Same as HostA
Configuration
controlnode: neutron server
1. Neutron configuration file /etc/neutron/neutron.conf:
[DEFAULT]
core_plugin = linuxbridge
rabbit_host = controlnode
allow_overlapping_ips = True
host = controlnode
agent_down_time = 5
dhcp_agents_per_network = 1
Configuration 61
Networking Guide (Release Version: 15.0.0)
Note: In the above configuration, we use dhcp_agents_per_network = 1 for this demonstration.
In usual deployments, we suggest setting dhcp_agents_per_network to more than one to match the
number of DHCP agents in your deployment. See Enabling DHCP high availability by default.
2. Update the plug-in configuration file /etc/neutron/plugins/linuxbridge/linuxbridge_conf.
ini:
[vlans]
tenant_network_type = vlan
network_vlan_ranges = physnet1:1000:2999
[database]
connection = mysql://root:root@127.0.0.1:3306/neutron_linux_bridge
retry_interval = 2
[linux_bridge]
physical_interface_mappings = physnet1:eth0
HostA and HostB: L2 agent
1. Neutron configuration file /etc/neutron/neutron.conf:
[DEFAULT]
rabbit_host = controlnode
rabbit_password = openstack
# host = HostB on hostb
host = HostA
2. Update the plug-in configuration file /etc/neutron/plugins/linuxbridge/linuxbridge_conf.
ini:
[vlans]
tenant_network_type = vlan
network_vlan_ranges = physnet1:1000:2999
[database]
connection = mysql://root:root@127.0.0.1:3306/neutron_linux_bridge
retry_interval = 2
[linux_bridge]
physical_interface_mappings = physnet1:eth0
3. Update the nova configuration file /etc/nova/nova.conf:
[DEFAULT]
use_neutron=True
firewall_driver=nova.virt.firewall.NoopFirewallDriver
[neutron]
admin_username=neutron
admin_password=servicepassword
admin_auth_url=http://controlnode:35357/v2.0/
auth_strategy=keystone
admin_tenant_name=servicetenant
url=http://203.0.113.10:9696/
HostA and HostB: DHCP agent
• Update the DHCP configuration file /etc/neutron/dhcp_agent.ini:
62 Configuration
Networking Guide (Release Version: 15.0.0)
[DEFAULT]
interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver
Prerequisites for demonstration
Admin role is required to use the agent management and scheduler extensions. Ensure you run the following
commands under a project with an admin role.
To experiment, you need VMs and a neutron network:
$ openstack server list
+--------------------------------------+-----------+--------+----------------+------------+
| ID | Name | Status | Networks | Image Name |
+--------------------------------------+-----------+--------+----------------+------------+
| c394fcd0-0baa-43ae-a793-201815c3e8ce | myserver1 | ACTIVE | net1=192.0.2.3 | cirros |
| 2d604e05-9a6c-4ddb-9082-8a1fbdcc797d | myserver2 | ACTIVE | net1=192.0.2.4 | ubuntu |
| c7c0481c-3db8-4d7a-a948-60ce8211d585 | myserver3 | ACTIVE | net1=192.0.2.5 | centos |
+--------------------------------------+-----------+--------+----------------+------------+
$ openstack network list
+--------------------------------------+------+--------------------------------------+
| ID | Name | Subnets |
+--------------------------------------+------+--------------------------------------+
| 89dca1c6-c7d4-4f7a-b730-549af0fb6e34 | net1 | f6c832e3-9968-46fd-8e45-d5cf646db9d1 |
+--------------------------------------+------+--------------------------------------+
Managing agents in neutron deployment
1. List all agents:
$ openstack network agent list
+--------------------------------------+--------------------+-------+-----------------
,→--+-------+-------+---------------------------+
| ID | Agent Type | Host | Availability
,→Zone | Alive | State | Binary |
+--------------------------------------+--------------------+-------+-----------------
,→--+-------+-------+---------------------------+
| 22467163-01ea-4231-ba45-3bd316f425e6 | Linux bridge agent | HostA | None
,→ | True | UP | neutron-metering-agent |
| 2444c54d-0d28-460c-ab0f-cd1e6b5d3c7b | DHCP agent | HostA | None
,→ | True | UP | neutron-openvswitch-agent |
| 3066d20c-9f8f-440c-ae7c-a40ffb4256b6 | Linux bridge agent | HostB | nova
,→ | True | UP | neutron-l3-agent |
| 55569f4e-6f31-41a6-be9d-526efce1f7fe | DHCP agent | HostB | nova
,→ | True | UP | neutron-l3-agent |
+--------------------------------------+--------------------+-------+-----------------
,→--+-------+-------+---------------------------+
Every agent that supports these extensions will register itself with the neutron server when it starts up.
The output shows information for four agents. The alive field shows :-) if the agent reported its state
within the period defined by the agent_down_time option in the neutron.conf file. Otherwise the
alive is xxx.
Configuration 63
Networking Guide (Release Version: 15.0.0)
2. List DHCP agents that host a specified network:
$ neutron dhcp-agent-list-hosting-net net1
+--------------------------------------+-------+----------------+-------+
| id | host | admin_state_up | alive |
+--------------------------------------+-------+----------------+-------+
| a0c1c21c-d4f4-4577-9ec7-908f2d48622d | HostA | True | :-) |
+--------------------------------------+-------+----------------+-------+
3. List the networks hosted by a given DHCP agent:
This command is to show which networks a given dhcp agent is managing.
$ neutron net-list-on-dhcp-agent a0c1c21c-d4f4-4577-9ec7-908f2d48622d
+--------------------------------------+------+---------------------------------------
,→------------+
| id | name | subnets
,→ |
+--------------------------------------+------+---------------------------------------
,→------------+
| 89dca1c6-c7d4-4f7a-b730-549af0fb6e34 | net1 | f6c832e3-9968-46fd-8e45-d5cf646db9d1
,→192.0.2.0/24 |
+--------------------------------------+------+---------------------------------------
,→------------+
4. Show agent details.
The openstack network agent show command shows details for a specified agent:
$ openstack network agent show 22467163-01ea-4231-ba45-3bd316f425e6
+---------------------+---------------------------------------------------------------
,→----------+
| Field | Value
,→ |
+---------------------+---------------------------------------------------------------
,→----------+
| admin_state_up | UP
,→ |
| agent_type | Metering agent
,→ |
| alive | False
,→ |
| availability_zone | None
,→ |
| binary | neutron-metering-agent
,→ |
| configurations | measure_interval='30', metering_driver='neutron.services.
,→metering.drive |
| | rs.noop.noop_driver.NoopMeteringDriver', report_interval='300
,→' |
| created_at | 2016-10-08 15:17:14
,→ |
| description | None
,→ |
| heartbeat_timestamp | 2016-10-24 13:53:35
,→ |
| host | HostA
,→ |
64 Configuration
Networking Guide (Release Version: 15.0.0)
| id | 22467163-01ea-4231-ba45-3bd316f425e6
,→ |
| started_at | 2016-10-08 15:17:14
,→ |
| topic | dhcp_agent
,→ |
+---------------------+---------------------------------------------------------------
,→----------+
In this output, heartbeat_timestamp is the time on the neutron server. You do not need to synchronize
all agents to this time for this extension to run correctly. configurations describes the static configuration
for the agent or run time data. This agent is a DHCP agent and it hosts one network, one subnet,
and three ports.
Different types of agents show different details. The following output shows information for a Linux
bridge agent:
$ openstack network agent show 22467163-01ea-4231-ba45-3bd316f425e6
+---------------------+---------------------------------------------------------------
,→----------+
| Field | Value
,→ |
+---------------------+---------------------------------------------------------------
,→----------+
| admin_state_up | UP
,→ |
| agent_type | Metering agent
,→ |
| alive | False
,→ |
| availability_zone | None
,→ |
| binary | neutron-linuxbridge-agent
,→ |
| configurations | measure_interval='30', metering_driver='neutron.services.
,→metering.drive |
| | rs.noop.noop_driver.NoopMeteringDriver', report_interval='300
,→' |
| created_at | 2016-10-08 15:17:14
,→ |
| description | None
,→ |
| heartbeat_timestamp | 2016-10-24 13:53:35
,→ |
| host | HostB
,→ |
| id | 22467163-01ea-4231-ba45-3bd316f425e6
,→ |
| started_at | 2016-10-08 15:17:14
,→ |
| topic | dhcp_agent
,→ |
+---------------------+---------------------------------------------------------------
,→----------+
The output shows bridge-mapping and the number of virtual network devices on this L2 agent.
Configuration 65
Networking Guide (Release Version: 15.0.0)
Managing assignment of networks to DHCP agent
A single network can be assigned to more than one DHCP agents and one DHCP agent can host more than one
network. You can add a network to a DHCP agent and remove one from it.
1. Default scheduling.
When you create a network with one port, the network will be scheduled to an active DHCP agent. If
many active DHCP agents are running, select one randomly. You can design more sophisticated scheduling
algorithms in the same way as nova-schedule later on.
$ neutron net-create net2
$ neutron subnet-create net2 198.51.100.0/24 --name subnet2
$ neutron port-create net2
$ openstack network agent list --agent-type dhcp --host qiaomin-free
+--------------------------------------+------------+-------+-------------------+-----
,→--+-------+--------------------+
| ID | Agent Type | Host | Availability Zone |
,→Alive | State | Binary |
+--------------------------------------+------------+-------+-------------------+-----
,→--+-------+--------------------+
| e838ef5c-75b1-4b12-84da-7bdbd62f1040 | DHCP agent | HostA | nova |
,→True | UP | neutron-dhcp-agent |
+--------------------------------------+------------+-------+-------------------+-----
,→--+-------+--------------------+
It is allocated to DHCP agent on HostA. If you want to validate the behavior through the dnsmasq
command, you must create a subnet for the network because the DHCP agent starts the dnsmasq service
only if there is a DHCP.
2. Assign a network to a given DHCP agent.
To add another DHCP agent to host the network, run this command:
$ neutron dhcp-agent-network-add f28aa126-6edb-4ea5-a81e-8850876bc0a8 net2
Added network net2 to dhcp agent
$ openstack network agent list --agent-type dhcp --host qiaomin-free
+--------------------------------------+------------+-------+-------------------+-----
,→--+-------+--------------------+
| ID | Agent Type | Host | Availability Zone |
,→Alive | State | Binary |
+--------------------------------------+------------+-------+-------------------+-----
,→--+-------+--------------------+
| e838ef5c-75b1-4b12-84da-7bdbd62f1040 | DHCP agent | HostA | nova |
,→True | UP | neutron-dhcp-agent |
| f28aa126-6edb-4ea5-a81e-8850876bc0a8 | DHCP agent | HostB | nova |
,→True | UP | neutron-dhcp-agent |
+--------------------------------------+------------+-------+-------------------+-----
,→--+-------+--------------------+
Both DHCP agents host the net2 network.
3. Remove a network from a specified DHCP agent.
This command is the sibling command for the previous one. Remove net2 from the DHCP agent for
HostA:
66 Configuration
Networking Guide (Release Version: 15.0.0)
$ neutron dhcp-agent-network-remove a0c1c21c-d4f4-4577-9ec7-908f2d48622d \
net2
Removed network net2 to dhcp agent
$ neutron dhcp-agent-list-hosting-net net2
+--------------------------------------+-------+----------------+-------+
| id | host | admin_state_up | alive |
+--------------------------------------+-------+----------------+-------+
| f28aa126-6edb-4ea5-a81e-8850876bc0a8 | HostB | True | :-) |
+--------------------------------------+-------+----------------+-------+
You can see that only the DHCP agent for HostB is hosting the net2 network.
HA of DHCP agents
Boot a VM on net2. Let both DHCP agents host net2. Fail the agents in turn to see if the VM can still get the
desired IP.
1. Boot a VM on net2:
$ openstack network list
+--------------------------------------+------+--------------------------------------+
| ID | Name | Subnets |
+--------------------------------------+------+--------------------------------------+
| 89dca1c6-c7d4-4f7a-b730-549af0fb6e34 | net1 | f6c832e3-9968-46fd-8e45-d5cf646db9d1 |
| 9b96b14f-71b8-4918-90aa-c5d705606b1a | net2 | 6979b71a-0ae8-448c-aa87-65f68eedcaaa |
+--------------------------------------+------+--------------------------------------+
$ openstack server create --image tty --flavor 1 myserver4 \
--nic net-id=9b96b14f-71b8-4918-90aa-c5d705606b1a
...
$ openstack server list
+--------------------------------------+-----------+--------+-------------------+-----
,→-------+
| ID | Name | Status | Networks |
,→Image Name |
+--------------------------------------+-----------+--------+-------------------+-----
,→-------+
| c394fcd0-0baa-43ae-a793-201815c3e8ce | myserver1 | ACTIVE | net1=192.0.2.3 |
,→cirros |
| 2d604e05-9a6c-4ddb-9082-8a1fbdcc797d | myserver2 | ACTIVE | net1=192.0.2.4 |
,→ubuntu |
| c7c0481c-3db8-4d7a-a948-60ce8211d585 | myserver3 | ACTIVE | net1=192.0.2.5 |
,→centos |
| f62f4731-5591-46b1-9d74-f0c901de567f | myserver4 | ACTIVE | net2=198.51.100.2 |
,→cirros1 |
+--------------------------------------+-----------+--------+-------------------+-----
,→-------+
2. Make sure both DHCP agents hosting net2:
Use the previous commands to assign the network to agents.
$ neutron dhcp-agent-list-hosting-net net2
+--------------------------------------+-------+----------------+-------+
| id | host | admin_state_up | alive |
+--------------------------------------+-------+----------------+-------+
| a0c1c21c-d4f4-4577-9ec7-908f2d48622d | HostA | True | :-) |
Configuration 67
Networking Guide (Release Version: 15.0.0)
| f28aa126-6edb-4ea5-a81e-8850876bc0a8 | HostB | True | :-) |
+--------------------------------------+-------+----------------+-------+
To test the HA of DHCP agent:
1. Log in to the myserver4 VM, and run udhcpc, dhclient or other DHCP client.
2. Stop the DHCP agent on HostA. Besides stopping the neutron-dhcp-agent binary, you must stop the
dnsmasq processes.
3. Run a DHCP client in VM to see if it can get the wanted IP.
4. Stop the DHCP agent on HostB too.
5. Run udhcpc in the VM; it cannot get the wanted IP.
6. Start DHCP agent on HostB. The VM gets the wanted IP again.
Disabling and removing an agent
An administrator might want to disable an agent if a system hardware or software upgrade is planned. Some
agents that support scheduling also support disabling and enabling agents, such as L3 and DHCP agents. After
the agent is disabled, the scheduler does not schedule new resources to the agent.
After the agent is disabled, you can safely remove the agent. Even after disabling the agent, resources on the
agent are kept assigned. Ensure you remove the resources on the agent before you delete the agent.
Disable the DHCP agent on HostA before you stop it:
$ neutron agent-update a0c1c21c-d4f4-4577-9ec7-908f2d48622d --admin-state-up False
$ neutron agent-list
$ openstack network agent list
+--------------------------------------+--------------------+-------+-------------------+---
,→----+-------+---------------------------+
| ID | Agent Type | Host | Availability Zone |
,→Alive | State | Binary |
+--------------------------------------+--------------------+-------+-------------------+---
,→----+-------+---------------------------+
| 22467163-01ea-4231-ba45-3bd316f425e6 | Linux bridge agent | HostA | None |
,→True | UP | neutron-metering-agent |
| 2444c54d-0d28-460c-ab0f-cd1e6b5d3c7b | DHCP agent | HostA | None |
,→True | UP | neutron-openvswitch-agent |
| 3066d20c-9f8f-440c-ae7c-a40ffb4256b6 | Linux bridge agent | HostB | nova |
,→True | UP | neutron-l3-agent |
| 55569f4e-6f31-41a6-be9d-526efce1f7fe | DHCP agent | HostB | nova |
,→True | UP | neutron-l3-agent |
+--------------------------------------+--------------------+-------+-------------------+---
,→----+-------+---------------------------+
After you stop the DHCP agent on HostA, you can delete it by the following command:
$ openstack network agent delete 2444c54d-0d28-460c-ab0f-cd1e6b5d3c7b
$ openstack network agent list
+--------------------------------------+--------------------+-------+-------------------+---
,→----+-------+---------------------------+
| ID | Agent Type | Host | Availability Zone |
,→Alive | State | Binary |
+--------------------------------------+--------------------+-------+-------------------+---
,→----+-------+---------------------------+
68 Configuration
Networking Guide (Release Version: 15.0.0)
| 22467163-01ea-4231-ba45-3bd316f425e6 | Linux bridge agent | HostA | None |
,→True | UP | neutron-metering-agent |
| 3066d20c-9f8f-440c-ae7c-a40ffb4256b6 | Linux bridge agent | HostB | nova |
,→True | UP | neutron-l3-agent |
| 55569f4e-6f31-41a6-be9d-526efce1f7fe | DHCP agent | HostB | nova |
,→True | UP | neutron-l3-agent |
+--------------------------------------+--------------------+-------+-------------------+---
,→----+-------+---------------------------+
After deletion, if you restart the DHCP agent, it appears on the agent list again.
Enabling DHCP high availability by default
You can control the default number of DHCP agents assigned to a network by setting the following configuration
option in the file /etc/neutron/neutron.conf.
dhcp_agents_per_network = 3
DNS integration
This page serves as a guide for how to use the DNS integration functionality of the Networking service. The
functionality described covers DNS from two points of view:
• The internal DNS functionality offered by the Networking service and its interaction with the Compute
service.
• Integration of the Compute service and the Networking service with an external DNSaaS (DNS-as-aService).
Users can control the behavior of the Networking service in regards to DNS using two attributes associated
with ports, networks, and floating IPs. The following table shows the attributes available for each one of these
resources:
Resource dns_name dns_domain
Ports Yes No
Networks No Yes
Floating IPs Yes Yes
The Networking service internal DNS resolution
The Networking service enables users to control the name assigned to ports by the internal DNS. To enable this
functionality, do the following:
1. Edit the /etc/neutron/neutron.conf file and assign a value different to openstacklocal (its default
value) to the dns_domain parameter in the [default] section. As an example:
dns_domain = example.org.
2. Add dns to extension_drivers in the [ml2] section of /etc/neutron/plugins/ml2/ml2_conf.
ini. The following is an example:
Configuration 69
Networking Guide (Release Version: 15.0.0)
[ml2]
extension_drivers = port_security,dns
After re-starting the neutron-server, users will be able to assign a dns_name attribute to their ports.
Note: The enablement of this functionality is prerequisite for the enablement of the Networking service integration
with an external DNS service, which is described in detail in Configuring OpenStack Networking for
integration with an external DNS service.
The following illustrates the creation of a port with my-port in its dns_name attribute.
Note: The name assigned to the port by the Networking service internal DNS is now visible in the response
in the dns_assignment attribute.
$ neutron port-create my-net --dns_name my-port
Created a new port:
+-----------------------+-------------------------------------------------------------------
,→----------------+
| Field | Value
,→ |
+-----------------------+-------------------------------------------------------------------
,→----------------+
| admin_state_up | True
,→ |
| allowed_address_pairs |
,→ |
| binding:vnic_type | normal
,→ |
| device_id |
,→ |
| device_owner |
,→ |
| dns_assignment | {"hostname": "my-port", "ip_address": "10.0.1.3", "fqdn": "my-
,→port.example.org."} |
| dns_name | my-port
,→ |
| fixed_ips | {"subnet_id":"6141b474-56cd-430f-b731-71660bb79b79", "ip_address
,→": "10.0.1.3"} |
| id | fb3c10f4-017e-420c-9be1-8f8c557ae21f
,→ |
| mac_address | fa:16:3e:aa:9b:e1
,→ |
| name |
,→ |
| network_id | bf2802a0-99a0-4e8c-91e4-107d03f158ea
,→ |
| port_security_enabled | True
,→ |
| security_groups | 1f0ddd73-7e3c-48bd-a64c-7ded4fe0e635
,→ |
| status | DOWN
,→ |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725
,→ |
70 Configuration
Networking Guide (Release Version: 15.0.0)
+-----------------------+-------------------------------------------------------------------
,→----------------+
When this functionality is enabled, it is leveraged by the Compute service when creating instances. When
allocating ports for an instance during boot, the Compute service populates the dns_name attributes of these
ports with the hostname attribute of the instance, which is a DNS sanitized version of its display name. As
a consequence, at the end of the boot process, the allocated ports will be known in the dnsmasq associated to
their networks by their instance hostname.
The following is an example of an instance creation, showing how its hostname populates the dns_name
attribute of the allocated port:
$ openstack server create --image cirros --flavor 42 \
--nic net-id=37aaff3a-6047-45ac-bf4f-a825e56fd2b3 my_vm
+--------------------------------------+----------------------------------------------------
,→------------+
| Field | Value
,→ |
+--------------------------------------+----------------------------------------------------
,→------------+
| OS-DCF:diskConfig | MANUAL
,→ |
| OS-EXT-AZ:availability_zone |
,→ |
| OS-EXT-STS:power_state | 0
,→ |
| OS-EXT-STS:task_state | scheduling
,→ |
| OS-EXT-STS:vm_state | building
,→ |
| OS-SRV-USG:launched_at | -
,→ |
| OS-SRV-USG:terminated_at | -
,→ |
| accessIPv4 |
,→ |
| accessIPv6 |
,→ |
| adminPass | dB45Zvo8Jpfe
,→ |
| config_drive |
,→ |
| created | 2016-02-05T21:35:04Z
,→ |
| flavor | m1.nano (42)
,→ |
| hostId |
,→ |
| id | 66c13cb4-3002-4ab3-8400-7efc2659c363
,→ |
| image | cirros-0.3.5-x86_64-uec(b9d981eb-d21c-4ce2-9dbc-
,→dd38f3d9015f) |
| key_name | -
,→ |
| locked | False
,→ |
| metadata | {}
,→ |
Configuration 71
Networking Guide (Release Version: 15.0.0)
| name | my_vm
,→ |
| os-extended-volumes:volumes_attached | []
,→ |
| progress | 0
,→ |
| security_groups | default
,→ |
| status | BUILD
,→ |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725
,→ |
| updated | 2016-02-05T21:35:04Z
,→ |
| user_id | 8bb6e578cba24e7db9d3810633124525
,→ |
+--------------------------------------+----------------------------------------------------
,→------------+
$ neutron port-list --device_id 66c13cb4-3002-4ab3-8400-7efc2659c363
+--------------------------------------+------+-------------------+-------------------------
,→--------------------------------------------------------------+
| id | name | mac_address | fixed_ips
,→ |
+--------------------------------------+------+-------------------+-------------------------
,→--------------------------------------------------------------+
| b3ecc464-1263-44a7-8c38-2d8a52751773 | | fa:16:3e:a8:ce:b8 | {"subnet_id": "277eca5d-
,→9869-474b-960e-6da5951d09f7", "ip_address": "172.24.5.8"} |
| | | | {"subnet_id": "eab47748-
,→3f0a-4775-a09f-b0c24bb64bc4", "ip_address":"2001:db8:10::8"} |
+--------------------------------------+------+-------------------+-------------------------
,→--------------------------------------------------------------+
$ neutron port-show b3ecc464-1263-44a7-8c38-2d8a52751773
+-----------------------+-------------------------------------------------------------------
,→--------------------+
| Field | Value
,→ |
+-----------------------+-------------------------------------------------------------------
,→--------------------+
| admin_state_up | True
,→ |
| allowed_address_pairs |
,→ |
| binding:vnic_type | normal
,→ |
| device_id | 66c13cb4-3002-4ab3-8400-7efc2659c363
,→ |
| device_owner | compute:None
,→ |
| dns_assignment | {"hostname": "my-vm", "ip_address": "172.24.5.8", "fqdn": "my-vm.
,→example.org."} |
| | {"hostname": "my-vm", "ip_address": "2001:db8:10::8", "fqdn": "my-
,→vm.example.org."} |
| dns_name | my-vm
,→ |
| extra_dhcp_opts |
,→ |
72 Configuration
Networking Guide (Release Version: 15.0.0)
| fixed_ips | {"subnet_id": "277eca5d-9869-474b-960e-6da5951d09f7", "ip_address
,→": "172.24.5.8"} |
| | {"subnet_id": "eab47748-3f0a-4775-a09f-b0c24bb64bc4", "ip_address
,→": "2001:db8:10::8"} |
| id | b3ecc464-1263-44a7-8c38-2d8a52751773
,→ |
| mac_address | fa:16:3e:a8:ce:b8
,→ |
| name |
,→ |
| network_id | 37aaff3a-6047-45ac-bf4f-a825e56fd2b3
,→ |
| port_security_enabled | True
,→ |
| security_groups | 1f0ddd73-7e3c-48bd-a64c-7ded4fe0e635
,→ |
| status | ACTIVE
,→ |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725
,→ |
+-----------------------+-------------------------------------------------------------------
,→--------------------+
In the above example notice that:
• The name given to the instance by the user, my_vm, is sanitized by the Compute service and becomes
my-vm as the port’s dns_name.
• The port’s dns_assignment attribute shows that its FQDN is my-vm.example.org. in the Networking
service internal DNS, which is the result of concatenating the port’s dns_name with the value configured
in the dns_domain parameter in neutron.conf, as explained previously.
• The dns_assignment attribute also shows that the port’s hostname in the Networking service internal
DNS is my-vm.
• Instead of having the Compute service create the port for the instance, the user might have created it and
assigned a value to its dns_name attribute. In this case, the value assigned to the dns_name attribute
must be equal to the value that Compute service will assign to the instance’s hostname, in this example
my-vm. Otherwise, the instance boot will fail.
Integration with an external DNS service
Users can also integrate the Networking and Compute services with an external DNS. To accomplish this, the
users have to:
1. Enable the functionality described in The Networking service internal DNS resolution.
2. Configure an external DNS driver. The Networking service provides a driver reference implementation
based on the OpenStack DNS service. It is expected that third party vendors will provide other implementations
in the future. For detailed configuration instructions, see Configuring OpenStack Networking
for integration with an external DNS service.
Once the neutron-server has been configured and restarted, users will have functionality that covers three
use cases, described in the following sections. In each of the use cases described below:
• The examples assume the OpenStack DNS service as the external DNS.
Configuration 73
Networking Guide (Release Version: 15.0.0)
• A, AAAA and PTR records will be created in the DNS service.
• Before executing any of the use cases, the user must create in the DNS service under his project a DNS
zone where the A and AAAA records will be created. For the description of the use cases below, it is
assumed the zone example.org. was created previously.
• The PTR records will be created in zones owned by a project with admin privileges. See Configuring
OpenStack Networking for integration with an external DNS service for more details.
Use case 1: Ports are published directly in the external DNS service
In this case, the user is creating ports or booting instances on a network that is accessible externally. The steps
to publish the port in the external DNS service are the following:
1. Assign a valid domain name to the network’s dns_domain attribute. This name must end with a period
(.).
2. Boot an instance specifying the externally accessible network. Alternatively, create a port on the externally
accessible network specifying a valid value to its dns_name attribute. If the port is going to be used
for an instance boot, the value assigned to dns_name must be equal to the hostname that the Compute
service will assign to the instance. Otherwise, the boot will fail.
Once these steps are executed, the port’s DNS data will be published in the external DNS service. This is an
example:
$ neutron net-list
+--------------------------------------+----------+-----------------------------------------
,→-----------------+
| id | name | subnets
,→ |
+--------------------------------------+----------+-----------------------------------------
,→-----------------+
| 41fa3995-9e4a-4cd9-bb51-3e5424f2ff2a | public | a67cfdf7-9d5d-406f-8a19-3f38e4fc3e74
,→ |
| | | cbd8c6dc-ca81-457e-9c5d-f8ece7ef67f8
,→ |
| 37aaff3a-6047-45ac-bf4f-a825e56fd2b3 | external | 277eca5d-9869-474b-960e-6da5951d09f7
,→172.24.5.0/24 |
| | | eab47748-3f0a-4775-a09f-b0c24bb64bc4
,→2001:db8:10::/64 |
| bf2802a0-99a0-4e8c-91e4-107d03f158ea | my-net | 6141b474-56cd-430f-b731-71660bb79b79 10.
,→0.1.0/24 |
| 38c5e950-b450-4c30-83d4-ee181c28aad3 | private | 43414c53-62ae-49bc-aa6c-c9dd7705818a
,→fda4:653e:71b0::/64 |
| | | 5b9282a1-0be1-4ade-b478-7868ad2a16ff 10.
,→0.0.0/24 |
+--------------------------------------+----------+-----------------------------------------
,→-----------------+
$ neutron net-update 37aaff3a-6047-45ac-bf4f-a825e56fd2b3 --dns_domain example.org.
Updated network: 37aaff3a-6047-45ac-bf4f-a825e56fd2b3
$ neutron net-show 37aaff3a-6047-45ac-bf4f-a825e56fd2b3
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | True |
74 Configuration
Networking Guide (Release Version: 15.0.0)
| availability_zone_hints | |
| availability_zones | nova |
| dns_domain | example.org. |
| id | 37aaff3a-6047-45ac-bf4f-a825e56fd2b3 |
| mtu | 1450 |
| name | external |
| port_security_enabled | True |
| provider:network_type | vlan |
| provider:physical_network | |
| provider:segmentation_id | 2016 |
| router:external | False |
| shared | True |
| status | ACTIVE |
| subnets | eab47748-3f0a-4775-a09f-b0c24bb64bc4 |
| | 277eca5d-9869-474b-960e-6da5951d09f7 |
| tenant_id | 04fc2f83966245dba907efb783f8eab9 |
+---------------------------+--------------------------------------+
$ designate record-list example.org.
+--------------------------------------+------+--------------+------------------------------
,→-----------------------------------------+
| id | type | name | data
,→ |
+--------------------------------------+------+--------------+------------------------------
,→-----------------------------------------+
| 10a36008-6ecf-47c3-b321-05652a929b04 | SOA | example.org. | ns1.devstack.org. malavall.
,→us.ibm.com. 1454729414 3600 600 86400 3600 |
| 56ca0b88-e343-4c98-8faa-19746e169baf | NS | example.org. | ns1.devstack.org.
,→ |
+--------------------------------------+------+--------------+------------------------------
,→-----------------------------------------+
$ neutron port-create 37aaff3a-6047-45ac-bf4f-a825e56fd2b3 --dns_name my-vm
Created a new port:
+-----------------------+-------------------------------------------------------------------
,→--------------------+
| Field | Value
,→ |
+-----------------------+-------------------------------------------------------------------
,→--------------------+
| admin_state_up | True
,→ |
| allowed_address_pairs |
,→ |
| binding:vnic_type | normal
,→ |
| device_id |
,→ |
| device_owner |
,→ |
| dns_assignment | {"hostname": "my-vm", "ip_address": "172.24.5.9", "fqdn": "my-vm.
,→example.org."} |
| | {"hostname": "my-vm", "ip_address": "2001:db8:10::9", "fqdn": "my-
,→vm.example.org."} |
| dns_name | my-vm
,→ |
| fixed_ips | {"subnet_id": "277eca5d-9869-474b-960e-6da5951d09f7", "ip_address
,→": "172.24.5.9"} |
Configuration 75
Networking Guide (Release Version: 15.0.0)
| | {"subnet_id": "eab47748-3f0a-4775-a09f-b0c24bb64bc4", "ip_address
,→": "2001:db8:10::9"} |
| id | 04be331b-dc5e-410a-9103-9c8983aeb186
,→ |
| mac_address | fa:16:3e:0f:4b:e4
,→ |
| name |
,→ |
| network_id | 37aaff3a-6047-45ac-bf4f-a825e56fd2b3
,→ |
| port_security_enabled | True
,→ |
| security_groups | 1f0ddd73-7e3c-48bd-a64c-7ded4fe0e635
,→ |
| status | DOWN
,→ |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725
,→ |
+-----------------------+-------------------------------------------------------------------
,→--------------------+
$ designate record-list example.org.
+--------------------------------------+------+--------------------+------------------------
,→-----------------------------------------------+
| id | type | name | data
,→ |
+--------------------------------------+------+--------------------+------------------------
,→-----------------------------------------------+
| 10a36008-6ecf-47c3-b321-05652a929b04 | SOA | example.org. | ns1.devstack.org.
,→malavall.us.ibm.com. 1455563035 3600 600 86400 3600 |
| 56ca0b88-e343-4c98-8faa-19746e169baf | NS | example.org. | ns1.devstack.org.
,→ |
| 3593591b-181f-4beb-9ab7-67fad7413b37 | A | my-vm.example.org. | 172.24.5.9
,→ |
| 5649c68f-7a88-48f5-9f87-ccb1f6ae67ca | AAAA | my-vm.example.org. | 2001:db8:10::9
,→ |
+--------------------------------------+------+--------------------+------------------------
,→-----------------------------------------------+
$ openstack server create --image cirros --flavor 42 \
--nic port-id=04be331b-dc5e-410a-9103-9c8983aeb186 my_vm
+--------------------------------------+----------------------------------------------------
,→------------+
| Field | Value
,→ |
+--------------------------------------+----------------------------------------------------
,→------------+
| OS-DCF:diskConfig | MANUAL
,→ |
| OS-EXT-AZ:availability_zone |
,→ |
| OS-EXT-STS:power_state | 0
,→ |
| OS-EXT-STS:task_state | scheduling
,→ |
| OS-EXT-STS:vm_state | building
,→ |
76 Configuration
Networking Guide (Release Version: 15.0.0)
| OS-SRV-USG:launched_at | -
,→ |
| OS-SRV-USG:terminated_at | -
,→ |
| accessIPv4 |
,→ |
| accessIPv6 |
,→ |
| adminPass | TDc9EpBT3B9W
,→ |
| config_drive |
,→ |
| created | 2016-02-15T19:10:43Z
,→ |
| flavor | m1.nano (42)
,→ |
| hostId |
,→ |
| id | 62c19691-d1c7-4d7b-a88e-9cc4d95d4f41
,→ |
| image | cirros-0.3.5-x86_64-uec (b9d981eb-d21c-4ce2-9dbc-
,→dd38f3d9015f) |
| key_name | -
,→ |
| locked | False
,→ |
| metadata | {}
,→ |
| name | my_vm
,→ |
| os-extended-volumes:volumes_attached | []
,→ |
| progress | 0
,→ |
| security_groups | default
,→ |
| status | BUILD
,→ |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725
,→ |
| updated | 2016-02-15T19:10:43Z
,→ |
| user_id | 8bb6e578cba24e7db9d3810633124525
,→ |
+--------------------------------------+----------------------------------------------------
,→------------+
$ openstack server list
+--------------------------------------+-------+--------+------------+-------------+--------
,→-----------------------------+------------+
| ID | Name | Status | Task State | Power State |
,→Networks | Image Name |
+--------------------------------------+-------+--------+------------+-------------+--------
,→-----------------------------+------------+
| 62c19691-d1c7-4d7b-a88e-9cc4d95d4f41 | my_vm | ACTIVE | - | Running |
,→external=172.24.5.9, 2001:db8:10::9 | cirros |
+--------------------------------------+-------+--------+------------+-------------+--------
,→-----------------------------+------------+
Configuration 77
Networking Guide (Release Version: 15.0.0)
In this example the port is created manually by the user and then used to boot an instance. Notice that:
• The port’s data was visible in the DNS service as soon as it was created.
• See Performance considerations for an explanation of the potential performance impact associated with
this use case.
Following are the PTR records created for this example. Note that for IPv4, the value of
ipv4_ptr_zone_prefix_size is 24. In the case of IPv6, the value of ipv6_ptr_zone_prefix_size is 116. For more
details, see Configuring OpenStack Networking for integration with an external DNS service:
$ designate record-list 5.24.172.in-addr.arpa.
+--------------------------------------+------+--------------------------+------------------
,→---------------------------------------------------+
| id | type | name | data
,→ |
+--------------------------------------+------+--------------------------+------------------
,→---------------------------------------------------+
| ab7ada72-7e64-4bed-913e-04718a80fafc | NS | 5.24.172.in-addr.arpa. | ns1.devstack.org.
,→ |
| 28346a94-790c-4ae1-9f7b-069d98d9efbd | SOA | 5.24.172.in-addr.arpa. | ns1.devstack.org.
,→ admin.example.org. 1455563035 3600 600 86400 3600 |
| cfcaf537-844a-4c1b-9b5f-464ff07dca33 | PTR | 9.5.24.172.in-addr.arpa. | my-vm.example.
,→org. |
+--------------------------------------+------+--------------------------+------------------
,→---------------------------------------------------+
$ designate record-list 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1.0.0.8.b.d.0.1.0.0.2.ip6.arpa.
+--------------------------------------+------+---------------------------------------------
,→------------------------------+-----------------------------------------------------------
,→----------+
| id | type | name
,→ | data
,→ |
+--------------------------------------+------+---------------------------------------------
,→------------------------------+-----------------------------------------------------------
,→----------+
| d8923354-13eb-4bd9-914a-0a2ae5f95989 | SOA | 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1.0.0.8.
,→b.d.0.1.0.0.2.ip6.arpa. | ns1.devstack.org. admin.example.org. 1455563036 3600 600
,→86400 3600 |
| 72e60acd-098d-41ea-9771-5b6546c9c06f | NS | 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1.0.0.8.
,→b.d.0.1.0.0.2.ip6.arpa. | ns1.devstack.org.
,→ |
| 877e0215-2ddf-4d01-a7da-47f1092dfd56 | PTR | 9.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1.
,→0.0.8.b.d.0.1.0.0.2.ip6.arpa. | my-vm.example.org.
,→ |
+--------------------------------------+------+---------------------------------------------
,→------------------------------+-----------------------------------------------------------
,→----------+
See Configuring OpenStack Networking for integration with an external DNS service for detailed instructions
on how to create the externally accessible network.
78 Configuration
Networking Guide (Release Version: 15.0.0)
Use case 2: Floating IPs are published with associated port DNS attributes
In this use case, the address of a floating IP is published in the external DNS service in conjunction with the
dns_name of its associated port and the dns_domain of the port’s network. The steps to execute in this use
case are the following:
1. Assign a valid domain name to the network’s dns_domain attribute. This name must end with a period
(.).
2. Boot an instance or alternatively, create a port specifying a valid value to its dns_name attribute. If
the port is going to be used for an instance boot, the value assigned to dns_name must be equal to the
hostname that the Compute service will assign to the instance. Otherwise, the boot will fail.
3. Create a floating IP and associate it to the port.
Following is an example of these steps:
$ neutron net-update 38c5e950-b450-4c30-83d4-ee181c28aad3 --dns_domain example.org.
Updated network: 38c5e950-b450-4c30-83d4-ee181c28aad3
$ neutron net-show 38c5e950-b450-4c30-83d4-ee181c28aad3
+-------------------------+--------------------------------------+
| Field | Value |
+-------------------------+--------------------------------------+
| admin_state_up | True |
| availability_zone_hints | |
| availability_zones | nova |
| dns_domain | example.org. |
| id | 38c5e950-b450-4c30-83d4-ee181c28aad3 |
| mtu | 1450 |
| name | private |
| port_security_enabled | True |
| router:external | False |
| shared | False |
| status | ACTIVE |
| subnets | 43414c53-62ae-49bc-aa6c-c9dd7705818a |
| | 5b9282a1-0be1-4ade-b478-7868ad2a16ff |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725 |
+-------------------------+--------------------------------------+
$ openstack server create --image cirros --flavor 42 \
--nic net-id=38c5e950-b450-4c30-83d4-ee181c28aad3 my_vm
+--------------------------------------+----------------------------------------------------
,→------------+
| Field | Value
,→ |
+--------------------------------------+----------------------------------------------------
,→------------+
| OS-DCF:diskConfig | MANUAL
,→ |
| OS-EXT-AZ:availability_zone |
,→ |
| OS-EXT-STS:power_state | 0
,→ |
| OS-EXT-STS:task_state | scheduling
,→ |
| OS-EXT-STS:vm_state | building
,→ |
Configuration 79
Networking Guide (Release Version: 15.0.0)
| OS-SRV-USG:launched_at | -
,→ |
| OS-SRV-USG:terminated_at | -
,→ |
| accessIPv4 |
,→ |
| accessIPv6 |
,→ |
| adminPass | oTLQLR3Kezmt
,→ |
| config_drive |
,→ |
| created | 2016-02-15T19:27:34Z
,→ |
| flavor | m1.nano (42)
,→ |
| hostId |
,→ |
| id | 43f328bb-b2d1-4cf1-a36f-3b2593397cb1
,→ |
| image | cirros-0.3.5-x86_64-uec (b9d981eb-d21c-4ce2-9dbc-
,→dd38f3d9015f) |
| key_name | -
,→ |
| locked | False
,→ |
| metadata | {}
,→ |
| name | my_vm
,→ |
| os-extended-volumes:volumes_attached | []
,→ |
| progress | 0
,→ |
| security_groups | default
,→ |
| status | BUILD
,→ |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725
,→ |
| updated | 2016-02-15T19:27:34Z
,→ |
| user_id | 8bb6e578cba24e7db9d3810633124525
,→ |
+--------------------------------------+----------------------------------------------------
,→------------+
$ openstack server list
+--------------------------------------+-------+--------+------------+-------------+--------
,→-------------------------------------------------+------------+
| ID | Name | Status | Task State | Power State |
,→Networks | Image Name |
+--------------------------------------+-------+--------+------------+-------------+--------
,→-------------------------------------------------+------------+
| 43f328bb-b2d1-4cf1-a36f-3b2593397cb1 | my_vm | ACTIVE | - | Running |
,→private=fda4:653e:71b0:0:f816:3eff:fe16:b5f2, 10.0.0.15 | cirros |
+--------------------------------------+-------+--------+------------+-------------+--------
,→-------------------------------------------------+------------+
80 Configuration
Networking Guide (Release Version: 15.0.0)
$ neutron port-list --device_id 43f328bb-b2d1-4cf1-a36f-3b2593397cb1
+--------------------------------------+------+-------------------+-------------------------
,→------------------------------------------------------------------------------------+
| id | name | mac_address | fixed_ips
,→ |
+--------------------------------------+------+-------------------+-------------------------
,→------------------------------------------------------------------------------------+
| da0b1f75-c895-460f-9fc1-4d6ec84cf85f | | fa:16:3e:16:b5:f2 | {"subnet_id": "5b9282a1-
,→0be1-4ade-b478-7868ad2a16ff", "ip_address": "10.0.0.15"} |
| | | | {"subnet_id": "43414c53-
,→62ae-49bc-aa6c-c9dd7705818a", "ip_address": "fda4:653e:71b0:0:f816:3eff:fe16:b5f2"} |
+--------------------------------------+------+-------------------+-------------------------
,→------------------------------------------------------------------------------------+
$ neutron port-show da0b1f75-c895-460f-9fc1-4d6ec84cf85f
+-----------------------+-------------------------------------------------------------------
,→------------------------------------------+
| Field | Value
,→ |
+-----------------------+-------------------------------------------------------------------
,→------------------------------------------+
| admin_state_up | True
,→ |
| allowed_address_pairs |
,→ |
| binding:vnic_type | normal
,→ |
| device_id | 43f328bb-b2d1-4cf1-a36f-3b2593397cb1
,→ |
| device_owner | compute:None
,→ |
| dns_assignment | {"hostname": "my-vm", "ip_address": "10.0.0.15", "fqdn": "my-vm.
,→example.org."} |
| | {"hostname": "my-vm", "ip_address":
,→"fda4:653e:71b0:0:f816:3eff:fe16:b5f2", "fqdn": "my-vm.example.org."} |
| dns_name | my-vm
,→ |
| extra_dhcp_opts |
,→ |
| fixed_ips | {"subnet_id": "5b9282a1-0be1-4ade-b478-7868ad2a16ff", "ip_address
,→": "10.0.0.15"} |
| | {"subnet_id": "43414c53-62ae-49bc-aa6c-c9dd7705818a", "ip_address
,→": "fda4:653e:71b0:0:f816:3eff:fe16:b5f2"} |
| id | da0b1f75-c895-460f-9fc1-4d6ec84cf85f
,→ |
| mac_address | fa:16:3e:16:b5:f2
,→ |
| name |
,→ |
| network_id | 38c5e950-b450-4c30-83d4-ee181c28aad3
,→ |
| port_security_enabled | True
,→ |
| security_groups | 1f0ddd73-7e3c-48bd-a64c-7ded4fe0e635
,→ |
| status | ACTIVE
,→ |
Configuration 81
Networking Guide (Release Version: 15.0.0)
| tenant_id | d5660cb1e6934612a01b4fb2fb630725
,→ |
+-----------------------+-------------------------------------------------------------------
,→------------------------------------------+
$ designate record-list example.org.
+--------------------------------------+------+--------------+------------------------------
,→-----------------------------------------+
| id | type | name | data
,→ |
+--------------------------------------+------+--------------+------------------------------
,→-----------------------------------------+
| 10a36008-6ecf-47c3-b321-05652a929b04 | SOA | example.org. | ns1.devstack.org. malavall.
,→us.ibm.com. 1455563783 3600 600 86400 3600 |
| 56ca0b88-e343-4c98-8faa-19746e169baf | NS | example.org. | ns1.devstack.org.
,→ |
+--------------------------------------+------+--------------+------------------------------
,→-----------------------------------------+
$ neutron floatingip-create 41fa3995-9e4a-4cd9-bb51-3e5424f2ff2a \
--port_id da0b1f75-c895-460f-9fc1-4d6ec84cf85f
Created a new floatingip:
+---------------------+--------------------------------------+
| Field | Value |
+---------------------+--------------------------------------+
| dns_domain | |
| dns_name | |
| fixed_ip_address | 10.0.0.15 |
| floating_ip_address | 172.24.4.4 |
| floating_network_id | 41fa3995-9e4a-4cd9-bb51-3e5424f2ff2a |
| id | e78f6eb1-a35f-4a90-941d-87c888d5fcc7 |
| port_id | da0b1f75-c895-460f-9fc1-4d6ec84cf85f |
| router_id | 970ebe83-c4a3-4642-810e-43ab7b0c2b5f |
| status | DOWN |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725 |
+---------------------+--------------------------------------+
$ designate record-list example.org.
+--------------------------------------+------+--------------------+------------------------
,→-----------------------------------------------+
| id | type | name | data
,→ |
+--------------------------------------+------+--------------------+------------------------
,→-----------------------------------------------+
| 10a36008-6ecf-47c3-b321-05652a929b04 | SOA | example.org. | ns1.devstack.org.
,→malavall.us.ibm.com. 1455564861 3600 600 86400 3600 |
| 56ca0b88-e343-4c98-8faa-19746e169baf | NS | example.org. | ns1.devstack.org.
,→ |
| 5ff53fd0-3746-48da-b9c9-77ed3004ec67 | A | my-vm.example.org. | 172.24.4.4
,→ |
+--------------------------------------+------+--------------------+------------------------
,→-----------------------------------------------+
In this example, notice that the data is published in the DNS service when the floating IP is associated to the
port.
Following are the PTR records created for this example. Note that for IPv4, the value of
82 Configuration
Networking Guide (Release Version: 15.0.0)
ipv4_ptr_zone_prefix_size is 24. For more details, see Configuring OpenStack Networking for integration
with an external DNS service:
$ designate record-list 4.24.172.in-addr.arpa.
+--------------------------------------+------+--------------------------+------------------
,→---------------------------------------------------+
| id | type | name | data
,→ |
+--------------------------------------+------+--------------------------+------------------
,→---------------------------------------------------+
| 2dd0b894-25fa-4563-9d32-9f13bd67f329 | NS | 4.24.172.in-addr.arpa. | ns1.devstack.org.
,→ |
| 47b920f1-5eff-4dfa-9616-7cb5b7cb7ca6 | SOA | 4.24.172.in-addr.arpa. | ns1.devstack.org.
,→ admin.example.org. 1455564862 3600 600 86400 3600 |
| fb1edf42-abba-410c-8397-831f45fd0cd7 | PTR | 4.4.24.172.in-addr.arpa. | my-vm.example.
,→org. |
+--------------------------------------+------+--------------------------+------------------
,→---------------------------------------------------+
Use case 3: Floating IPs are published in the external DNS service
In this use case, the user assigns dns_name and dns_domain attributes to a floating IP when it is created. The
floating IP data becomes visible in the external DNS service as soon as it is created. The floating IP can be
associated with a port on creation or later on. The following example shows a user booting an instance and then
creating a floating IP associated to the port allocated for the instance:
$ neutron net-show 38c5e950-b450-4c30-83d4-ee181c28aad3
+-------------------------+--------------------------------------+
| Field | Value |
+-------------------------+--------------------------------------+
| admin_state_up | True |
| availability_zone_hints | |
| availability_zones | nova |
| dns_domain | example.org. |
| id | 38c5e950-b450-4c30-83d4-ee181c28aad3 |
| mtu | 1450 |
| name | private |
| port_security_enabled | True |
| router:external | False |
| shared | False |
| status | ACTIVE |
| subnets | 43414c53-62ae-49bc-aa6c-c9dd7705818a |
| | 5b9282a1-0be1-4ade-b478-7868ad2a16ff |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725 |
+-------------------------+--------------------------------------+
$ openstack server create --image cirros --flavor 42 \
--nic net-id=38c5e950-b450-4c30-83d4-ee181c28aad3 my_vm
+--------------------------------------+----------------------------------------------------
,→------------+
| Field | Value
,→ |
+--------------------------------------+----------------------------------------------------
,→------------+
| OS-DCF:diskConfig | MANUAL
,→ |
Configuration 83
Networking Guide (Release Version: 15.0.0)
| OS-EXT-AZ:availability_zone |
,→ |
| OS-EXT-STS:power_state | 0
,→ |
| OS-EXT-STS:task_state | scheduling
,→ |
| OS-EXT-STS:vm_state | building
,→ |
| OS-SRV-USG:launched_at | -
,→ |
| OS-SRV-USG:terminated_at | -
,→ |
| accessIPv4 |
,→ |
| accessIPv6 |
,→ |
| adminPass | HLXGznYqXM4J
,→ |
| config_drive |
,→ |
| created | 2016-02-15T19:42:44Z
,→ |
| flavor | m1.nano (42)
,→ |
| hostId |
,→ |
| id | 71fb4ac8-eed8-4644-8113-0641962bb125
,→ |
| image | cirros-0.3.5-x86_64-uec (b9d981eb-d21c-4ce2-9dbc-
,→dd38f3d9015f) |
| key_name | -
,→ |
| locked | False
,→ |
| metadata | {}
,→ |
| name | my_vm
,→ |
| os-extended-volumes:volumes_attached | []
,→ |
| progress | 0
,→ |
| security_groups | default
,→ |
| status | BUILD
,→ |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725
,→ |
| updated | 2016-02-15T19:42:44Z
,→ |
| user_id | 8bb6e578cba24e7db9d3810633124525
,→ |
+--------------------------------------+----------------------------------------------------
,→------------+
$ openstack server list
+--------------------------------------+-------+--------+------------+-------------+--------
,→-------------------------------------------------+------------+
84 Configuration
Networking Guide (Release Version: 15.0.0)
| ID | Name | Status | Task State | Power State |
,→Networks | Image Name |
+--------------------------------------+-------+--------+------------+-------------+--------
,→-------------------------------------------------+------------+
| 71fb4ac8-eed8-4644-8113-0641962bb125 | my_vm | ACTIVE | - | Running |
,→private=fda4:653e:71b0:0:f816:3eff:fe24:8614, 10.0.0.16 | cirros |
+--------------------------------------+-------+--------+------------+-------------+--------
,→-------------------------------------------------+------------+
$ neutron port-list --device_id 71fb4ac8-eed8-4644-8113-0641962bb125
+--------------------------------------+------+-------------------+-------------------------
,→------------------------------------------------------------------------------------+
| id | name | mac_address | fixed_ips
,→ |
+--------------------------------------+------+-------------------+-------------------------
,→------------------------------------------------------------------------------------+
| 1e7033fb-8e9d-458b-89ed-8312cafcfdcb | | fa:16:3e:24:86:14 | {"subnet_id": "5b9282a1-
,→0be1-4ade-b478-7868ad2a16ff", "ip_address": "10.0.0.16"} |
| | | | {"subnet_id": "43414c53-
,→62ae-49bc-aa6c-c9dd7705818a", "ip_address": "fda4:653e:71b0:0:f816:3eff:fe24:8614"} |
+--------------------------------------+------+-------------------+-------------------------
,→------------------------------------------------------------------------------------+
$ neutron port-show 1e7033fb-8e9d-458b-89ed-8312cafcfdcb
+-----------------------+-------------------------------------------------------------------
,→------------------------------------------+
| Field | Value
,→ |
+-----------------------+-------------------------------------------------------------------
,→------------------------------------------+
| admin_state_up | True
,→ |
| allowed_address_pairs |
,→ |
| binding:vnic_type | normal
,→ |
| device_id | 71fb4ac8-eed8-4644-8113-0641962bb125
,→ |
| device_owner | compute:None
,→ |
| dns_assignment | {"hostname": "my-vm", "ip_address": "10.0.0.16", "fqdn": "my-vm.
,→example.org."} |
| | {"hostname": "my-vm", "ip_address":
,→"fda4:653e:71b0:0:f816:3eff:fe24:8614", "fqdn": "my-vm.example.org."} |
| dns_name | my-vm
,→ |
| extra_dhcp_opts |
,→ |
| fixed_ips | {"subnet_id": "5b9282a1-0be1-4ade-b478-7868ad2a16ff", "ip_address
,→": "10.0.0.16"} |
| | {"subnet_id": "43414c53-62ae-49bc-aa6c-c9dd7705818a", "ip_address
,→": "fda4:653e:71b0:0:f816:3eff:fe24:8614"} |
| id | 1e7033fb-8e9d-458b-89ed-8312cafcfdcb
,→ |
| mac_address | fa:16:3e:24:86:14
,→ |
| name |
,→ |
Configuration 85
Networking Guide (Release Version: 15.0.0)
| network_id | 38c5e950-b450-4c30-83d4-ee181c28aad3
,→ |
| port_security_enabled | True
,→ |
| security_groups | 1f0ddd73-7e3c-48bd-a64c-7ded4fe0e635
,→ |
| status | ACTIVE
,→ |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725
,→ |
+-----------------------+-------------------------------------------------------------------
,→------------------------------------------+
$ designate record-list example.org.
+--------------------------------------+------+--------------+------------------------------
,→-----------------------------------------+
| id | type | name | data
,→ |
+--------------------------------------+------+--------------+------------------------------
,→-----------------------------------------+
| 10a36008-6ecf-47c3-b321-05652a929b04 | SOA | example.org. | ns1.devstack.org. malavall.
,→us.ibm.com. 1455565110 3600 600 86400 3600 |
| 56ca0b88-e343-4c98-8faa-19746e169baf | NS | example.org. | ns1.devstack.org.
,→ |
+--------------------------------------+------+--------------+------------------------------
,→-----------------------------------------+
$ neutron floatingip-create 41fa3995-9e4a-4cd9-bb51-3e5424f2ff2a \
--dns_domain example.org. --dns_name my-floatingip
Created a new floatingip:
+---------------------+--------------------------------------+
| Field | Value |
+---------------------+--------------------------------------+
| dns_domain | example.org. |
| dns_name | my-floatingip |
| fixed_ip_address | |
| floating_ip_address | 172.24.4.5 |
| floating_network_id | 41fa3995-9e4a-4cd9-bb51-3e5424f2ff2a |
| id | 9f23a9c6-eceb-42eb-9f45-beb58c473728 |
| port_id | |
| router_id | |
| status | DOWN |
| tenant_id | d5660cb1e6934612a01b4fb2fb630725 |
+---------------------+--------------------------------------+
$ designate record-list example.org.
+--------------------------------------+------+----------------------------+----------------
,→-------------------------------------------------------+
| id | type | name | data
,→ |
+--------------------------------------+------+----------------------------+----------------
,→-------------------------------------------------------+
| 10a36008-6ecf-47c3-b321-05652a929b04 | SOA | example.org. | ns1.devstack.
,→org. malavall.us.ibm.com. 1455566486 3600 600 86400 3600 |
| 56ca0b88-e343-4c98-8faa-19746e169baf | NS | example.org. | ns1.devstack.
,→org. |
| 8884c56f-3ef5-446e-ae4d-8053cc8bc2b4 | A | my-floatingip.example.org. | 172.24.4.5
,→ |
86 Configuration
Networking Guide (Release Version: 15.0.0)
+--------------------------------------+------+----------------------------+----------------
,→-------------------------------------------------------+
Note that in this use case:
• The dns_name and dns_domain attributes of a floating IP must be specified together on creation. They
cannot be assigned to the floating IP separately.
• The dns_name and dns_domain of a floating IP have precedence, for purposes of being published in
the external DNS service, over the dns_name of its associated port and the dns_domain of the port’s
network, whether they are specified or not. Only the dns_name and the dns_domain of the floating IP
are published in the external DNS service.
Following are the PTR records created for this example. Note that for IPv4, the value of
ipv4_ptr_zone_prefix_size is 24. For more details, see Configuring OpenStack Networking for integration
with an external DNS service:
$ designate record-list 4.24.172.in-addr.arpa.
+--------------------------------------+------+--------------------------+------------------
,→---------------------------------------------------+
| id | type | name | data
,→ |
+--------------------------------------+------+--------------------------+------------------
,→---------------------------------------------------+
| 2dd0b894-25fa-4563-9d32-9f13bd67f329 | NS | 4.24.172.in-addr.arpa. | ns1.devstack.org.
,→ |
| 47b920f1-5eff-4dfa-9616-7cb5b7cb7ca6 | SOA | 4.24.172.in-addr.arpa. | ns1.devstack.org.
,→ admin.example.org. 1455566487 3600 600 86400 3600 |
| 589a0171-e77a-4ab6-ba6e-23114f2b9366 | PTR | 5.4.24.172.in-addr.arpa. | my-floatingip.
,→example.org. |
+--------------------------------------+------+--------------------------+------------------
,→---------------------------------------------------+
Performance considerations
Only for Use case 1: Ports are published directly in the external DNS service, if the port binding extension
is enabled in the Networking service, the Compute service will execute one additional port update operation
when allocating the port for the instance during the boot process. This may have a noticeable adverse effect in
the performance of the boot process that must be evaluated before adoption of this use case.
Configuring OpenStack Networking for integration with an external DNS service
The first step to configure the integration with an external DNS service is to enable the functionality described
in The Networking service internal DNS resolution. Once this is done, the user has to take the following steps
and restart neutron-server.
1. Edit the [default] section of /etc/neutron/neutron.conf and specify the external DNS service
driver to be used in parameter external_dns_driver. The valid options are defined in namespace
neutron.services.external_dns_drivers. The following example shows how to set up the driver
for the OpenStack DNS service:
external_dns_driver = designate
Configuration 87
Networking Guide (Release Version: 15.0.0)
2. If the OpenStack DNS service is the target external DNS, the [designate] section of /etc/neutron/
neutron.conf must define the following parameters:
• url: the OpenStack DNS service public endpoint URL.
• allow_reverse_dns_lookup: a boolean value specifying whether to enable or not the creation
of reverse lookup (PTR) records.
• admin_auth_url: the Identity service admin authorization endpoint url. This endpoint will be
used by the Networking service to authenticate as an admin user to create and update reverse lookup
(PTR) zones.
• admin_username: the admin user to be used by the Networking service to create and update
reverse lookup (PTR) zones.
• admin_password: the password of the admin user to be used by Networking service to create and
update reverse lookup (PTR) zones.
• admin_tenant_name: the project of the admin user to be used by the Networking service to create
and update reverse lookup (PTR) zones.
• ipv4_ptr_zone_prefix_size: the size in bits of the prefix for the IPv4 reverse lookup (PTR)
zones.
• ipv6_ptr_zone_prefix_size: the size in bits of the prefix for the IPv6 reverse lookup (PTR)
zones.
• insecure: Disable SSL certificate validation. By default, certificates are validated.
• cafile: Path to a valid Certificate Authority (CA) certificate.
The following is an example:
[designate]
url = http://55.114.111.93:9001/v2
admin_auth_url = http://55.114.111.93:35357/v2.0
admin_username = neutron
admin_password = x5G90074
admin_tenant_name = service
allow_reverse_dns_lookup = True
ipv4_ptr_zone_prefix_size = 24
ipv6_ptr_zone_prefix_size = 116
cafile = /etc/ssl/certs/my_ca_cert
Configuration of the externally accessible network for use case 1
In Use case 1: Ports are published directly in the external DNS service, the externally accessible network must
meet the following requirements:
• The network cannot have attribute router:external set to True.
• The network type can be FLAT, VLAN, GRE, VXLAN or GENEVE.
• For network types VLAN, GRE, VXLAN or GENEVE, the segmentation ID must be outside the ranges
assigned to tenant networks.
88 Configuration
Networking Guide (Release Version: 15.0.0)
Name resolution for instances
The Networking service offers several methods to configure name resolution (DNS) for instances. Most deployments
should implement case 1 or 2. Case 3 requires security considerations to prevent leaking internal
DNS information to instances.
Case 1: Each virtual network uses unique DNS resolver(s)
In this case, the DHCP agent offers one or more unique DNS resolvers to instances via DHCP on each virtual
network. You can configure a DNS resolver when creating or updating a subnet. To configure more than one
DNS resolver, use a comma between each value.
• Configure a DNS resolver when creating a subnet.
$ neutron subnet-create --dns-nameserver DNS_RESOLVER
Replace DNS_RESOLVER with the IP address of a DNS resolver reachable from the virtual network. For
example:
$ neutron subnet-create --dns-nameserver 8.8.8.8,8.8.4.4
Note: This command requires other options outside the scope of this content.
• Configure a DNS resolver on an existing subnet.
$ neutron subnet-update --dns-nameserver DNS_RESOLVER SUBNET_ID_OR_NAME
Replace DNS_RESOLVER with the IP address of a DNS resolver reachable from the virtual network and
SUBNET_ID_OR_NAME with the UUID or name of the subnet. For example, using the selfservice
subnet:
$ neutron subnet-update --dns-nameserver 8.8.8.8,8.8.4.4 selfservice
Case 2: All virtual networks use same DNS resolver(s)
In this case, the DHCP agent offers the same DNS resolver(s) to instances via DHCP on all virtual networks.
• In the dhcp_agent.ini file, configure one or more DNS resolvers. To configure more than one DNS
resolver, use a comma between each value.
[DEFAULT]
dnsmasq_dns_servers = DNS_RESOLVER
Replace DNS_RESOLVER with the IP address of a DNS resolver reachable from all virtual networks. For
example:
[DEFAULT]
dnsmasq_dns_servers = 8.8.8.8, 8.8.4.4
Configuration 89
Networking Guide (Release Version: 15.0.0)
Note: You must configure this option for all eligible DHCP agents and restart them to activate the
values.
Case 3: All virtual networks use DNS resolver(s) on the host
In this case, the DHCP agent offers the DNS resolver(s) in the resolv.conf file on the host running the DHCP
agent via DHCP to instances on all virtual networks.
• In the dhcp_agent.ini file, enable advertisement of the DNS resolver(s) on the host.
[DEFAULT]
dnsmasq_local_resolv = True
Note: You must configure this option for all eligible DHCP agents and restart them to activate the
values.
Distributed Virtual Routing with VRRP
Open vSwitch: High availability using DVR supports augmentation using Virtual Router Redundancy Protocol
(VRRP). Using this configuration, virtual routers support both the --distributed and --ha options.
Similar to legacy HA routers, DVR/SNAT HA routers provide a quick fail over of the SNAT service to a backup
DVR/SNAT router on an l3-agent running on a different node.
SNAT high availability is implemented in a manner similar to the Linux bridge: High availability using VRRP
and Open vSwitch: High availability using VRRP examples where keepalived uses VRRP to provide quick
failover of SNAT services.
During normal operation, the master router periodically transmits heartbeat packets over a hidden project network
that connects all HA routers for a particular project.
If the DVR/SNAT backup router stops receiving these packets, it assumes failure of the master DVR/SNAT
router and promotes itself to master router by configuring IP addresses on the interfaces in the snat namespace.
In environments with more than one backup router, the rules of VRRP are followed to select a new master router.
Warning: There is a known bug with keepalived v1.2.15 and earlier which can cause packet loss
when max_l3_agents_per_router is set to 3 or more. Therefore, we recommend that you upgrade to
keepalived v1.2.16 or greater when using this feature.
Note: Experimental feature or incomplete documentation.
Configuration example
The basic deployment model consists of one controller node, two or more network nodes, and multiple computes
nodes.
90 Configuration
Networking Guide (Release Version: 15.0.0)
Controller node configuration
1. Add the following to /etc/neutron/neutron.conf:
[DEFAULT]
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = True
router_distributed = True
l3_ha = True
l3_ha_net_cidr = 169.254.192.0/18
max_l3_agents_per_router = 3
When the router_distributed = True flag is configured, routers created by all users are distributed.
Without it, only privileged users can create distributed routers by using --distributed True.
Similarly, when the l3_ha = True flag is configured, routers created by all users default to HA.
It follows that with these two flags set to True in the configuration file, routers created by all users will
default to distributed HA routers (DVR HA).
The same can explicitly be accomplished by a user with administrative credentials setting the flags in the
neutron router-create command:
$ neutron router-create name-of-router --distributed=True --ha=True
Note: The max_l3_agents_per_router determine the number of backup DVR/SNAT routers which will
be instantiated.
2. Add the following to /etc/neutron/plugins/ml2/ml2_conf.ini:
[ml2]
type_drivers = flat,vxlan
tenant_network_types = vxlan
mechanism_drivers = openvswitch,l2population
extension_drivers = port_security
[ml2_type_flat]
flat_networks = external
[ml2_type_vxlan]
vni_ranges = MIN_VXLAN_ID:MAX_VXLAN_ID
Replace MIN_VXLAN_ID and MAX_VXLAN_ID with VXLAN ID minimum and maximum values suitable
for your environment.
Note: The first value in the tenant_network_types option becomes the default project network type
when a regular user creates a network.
Configuration 91
Networking Guide (Release Version: 15.0.0)
Network nodes
1. Configure the Open vSwitch agent. Add the following to /etc/neutron/plugins/ml2/ml2_conf.
ini:
[ovs]
local_ip = TUNNEL_INTERFACE_IP_ADDRESS
bridge_mappings = external:br-ex
[agent]
enable_distributed_routing = True
tunnel_types = vxlan
l2_population = True
Replace TUNNEL_INTERFACE_IP_ADDRESS with the IP address of the interface that handles VXLAN
project networks.
2. Configure the L3 agent. Add the following to /etc/neutron/l3_agent.ini:
[DEFAULT]
ha_vrrp_auth_password = password
interface_driver = openvswitch
external_network_bridge =
agent_mode = dvr_snat
Note: The external_network_bridge option intentionally contains no value.
Compute nodes
1. Configure the Open vSwitch agent. Add the following to /etc/neutron/plugins/ml2/ml2_conf.
ini:
[ovs]
local_ip = TUNNEL_INTERFACE_IP_ADDRESS
bridge_mappings = external:br-ex
[agent]
enable_distributed_routing = True
tunnel_types = vxlan
l2_population = True
[securitygroup]
firewall_driver = neutron.agent.linux.iptables_firewall.
,→OVSHybridIptablesFirewallDriver
2. Configure the L3 agent. Add the following to /etc/neutron/l3_agent.ini:
[DEFAULT]
interface_driver = openvswitch
external_network_bridge =
agent_mode = dvr
92 Configuration
Networking Guide (Release Version: 15.0.0)
Replace TUNNEL_INTERFACE_IP_ADDRESS with the IP address of the interface that handles VXLAN
project networks.
Keepalived VRRP health check
The health of your keepalived instances can be automatically monitored via a bash script that verifies connectivity
to all available and configured gateway addresses. In the event that connectivity is lost, the master
router is rescheduled to another node.
If all routers lose connectivity simultaneously, the process of selecting a new master router will be repeated in
a round-robin fashion until one or more routers have their connectivity restored.
To enable this feature, edit the l3_agent.ini file:
ha_vrrp_health_check_interval = 30
Where ha_vrrp_health_check_interval indicates how often in seconds the health check should run. The
default value is 0, which indicates that the check should not run at all.
Known limitations
• Migrating a router from distributed only, HA only, or legacy to distributed HA is not supported at this
time. The router must be created as distributed HA. The reverse direction is also not supported. You
cannot reconfigure a distributed HA router to be only distributed, only HA, or legacy.
• There are certain scenarios where l2pop and distributed HA routers do not interact in an expected manner.
These situations are the same that affect HA only routers and l2pop.
IPAM configuration
Note: Experimental feature or incomplete documentation.
Starting with the Liberty release, OpenStack Networking includes a pluggable interface for the IP Address
Management (IPAM) function. This interface creates a driver framework for the allocation and de-allocation of
subnets and IP addresses, enabling the integration of alternate IPAM implementations or third-party IP Address
Management systems.
The basics
In Liberty and Mitaka, the IPAM implementation within OpenStack Networking provided a pluggable and
non-pluggable flavor. As of Newton, the non-pluggable flavor is no longer available. Instead, it is completely
replaced with a reference driver implementation of the pluggable framework. All data will be automatically
migrated during the upgrade process, unless you have previously configured a pluggable IPAM driver. In that
case, no migration is necessary.
To configure a driver other than the reference driver, specify it in the neutron.conf file. Do this after the
migration is complete. There is no need to specify any value if you wish to use the reference driver.
Configuration 93
Networking Guide (Release Version: 15.0.0)
ipam_driver = ipam-driver-name
There is no need to specify any value if you wish to use the reference driver, though specifying internal will
explicitly choose the reference driver. The documentation for any alternate drivers will include the value to use
when specifying that driver.
Known limitations
• The driver interface is designed to allow separate drivers for each subnet pool. However, the current
implementation allows only a single IPAM driver system-wide.
• Third-party drivers must provide their own migration mechanisms to convert existing OpenStack installations
to their IPAM.
IPv6
This section describes the following items:
• How to enable dual-stack (IPv4 and IPv6 enabled) instances.
• How those instances receive an IPv6 address.
• How those instances communicate across a router to other subnets or the internet.
• How those instances interact with other OpenStack services.
Enabling a dual-stack network in OpenStack Networking simply requires creating a subnet with the
ip_version field set to 6, then the IPv6 attributes (ipv6_ra_mode and ipv6_address_mode) set. The
ipv6_ra_mode and ipv6_address_mode will be described in detail in the next section. Finally, the subnets
cidr needs to be provided.
This section does not include the following items:
• Single stack IPv6 project networking
• OpenStack control communication between servers and services over an IPv6 network.
• Connection to the OpenStack APIs via an IPv6 transport network
• IPv6 multicast
• IPv6 support in conjunction with any out of tree routers, switches, services or agents whether in physical
or virtual form factors.
Neutron subnets and the IPv6 API attributes
As of Juno, the OpenStack Networking service (neutron) provides two new attributes to the subnet object, which
allows users of the API to configure IPv6 subnets.
There are two IPv6 attributes:
• ipv6_ra_mode
• ipv6_address_mode
These attributes can be set to the following values:
94 Configuration
Networking Guide (Release Version: 15.0.0)
• slaac
• dhcpv6-stateful
• dhcpv6-stateless
The attributes can also be left unset.
IPv6 addressing
The ipv6_address_mode attribute is used to control how addressing is handled by OpenStack. There are
a number of different ways that guest instances can obtain an IPv6 address, and this attribute exposes these
choices to users of the Networking API.
Router advertisements
The ipv6_ra_mode attribute is used to control router advertisements for a subnet.
The IPv6 Protocol uses Internet Control Message Protocol packets (ICMPv6) as a way to distribute information
about networking. ICMPv6 packets with the type flag set to 134 are called “Router Advertisement” packets,
which contain information about the router and the route that can be used by guest instances to send network
traffic.
The ipv6_ra_mode is used to specify if the Networking service should generate Router Advertisement packets
for a subnet.
Configuration 95
Networking Guide (Release Version: 15.0.0)
ipv6_ra_mode and ipv6_address_mode combinations
ipv6 ra
mode
ipv6
address
mode
radvd
A,M,O
External
Router
A,M,O
Description
N/S N/S Off Not
Defined
Backwards compatibility with pre-Juno IPv6 behavior.
N/S slaac Off 1,0,0 Guest instance obtains IPv6 address from non-OpenStack
router using SLAAC.
N/S dhcpv6-
stateful
Off 0,1,1 Not currently implemented in the reference implementation.
N/S dhcpv6-
stateless
Off 1,0,1 Not currently implemented in the reference implementation.
slaac N/S 1,0,0 Off Not currently implemented in the reference implementation.
dhcpv6-
stateful
N/S 0,1,1 Off Not currently implemented in the reference implementation.
dhcpv6-
stateless
N/S 1,0,1 Off Not currently implemented in the reference implementation.
slaac slaac 1,0,0 Off Guest instance obtains IPv6 address from OpenStack managed
radvd using SLAAC.
dhcpv6-
stateful
dhcpv6-
stateful
0,1,1 Off Guest instance obtains IPv6 address from dnsmasq using
DHCPv6 stateful and optional info from dnsmasq using
DHCPv6.
dhcpv6-
stateless
dhcpv6-
stateless
1,0,1 Off Guest instance obtains IPv6 address from OpenStack managed
radvd using SLAAC and optional info from dnsmasq using
DHCPv6.
slaac dhcpv6-
stateful
Invalid combination.
slaac dhcpv6-
stateless
Invalid combination.
dhcpv6-
stateful
slaac Invalid combination.
dhcpv6-
stateful
dhcpv6-
stateless
Invalid combination.
dhcpv6-
stateless
slaac Invalid combination.
dhcpv6-
stateless
dhcpv6-
stateful
Invalid combination.
Project network considerations
Dataplane
Both the Linux bridge and the Open vSwitch dataplane modules support forwarding IPv6 packets amongst the
guests and router ports. Similar to IPv4, there is no special configuration or setup required to enable the dataplane
to properly forward packets from the source to the destination using IPv6. Note that these dataplanes will
forward Link-local Address (LLA) packets between hosts on the same network just fine without any participation
or setup by OpenStack components after the ports are all connected and MAC addresses learned.
96 Configuration
Networking Guide (Release Version: 15.0.0)
Addresses for subnets
There are three methods currently implemented for a subnet to get its cidr in OpenStack:
1. Direct assignment during subnet creation via command line or Horizon
2. Referencing a subnet pool during subnet creation
3. Using a Prefix Delegation (PD) client to request a prefix for a subnet from a PD server
In the future, additional techniques could be used to allocate subnets to projects, for example, use of an external
IPAM module.
Address modes for ports
Note: An external DHCPv6 server in theory could override the full address OpenStack assigns based on the
EUI-64 address, but that would not be wise as it would not be consistent through the system.
IPv6 supports three different addressing schemes for address configuration and for providing optional network
information.
Stateless Address Auto Configuration (SLAAC) Address configuration using Router Advertisement (RA).
DHCPv6-stateless Address configuration using RA and optional information using DHCPv6.
DHCPv6-stateful Address configuration and optional information using DHCPv6.
OpenStack can be setup such that OpenStack Networking directly provides RA, DHCP relay and DHCPv6
address and optional information for their networks or this can be delegated to external routers and services
based on the drivers that are in use. There are two neutron subnet attributes - ipv6_ra_mode and
ipv6_address_mode – that determine how IPv6 addressing and network information is provided to project
instances:
• ipv6_ra_mode: Determines who sends RA.
• ipv6_address_mode: Determines how instances obtain IPv6 address, default gateway, or optional information.
For the above two attributes to be effective, enable_dhcp of the subnet object must be set to True.
Using SLAAC for addressing
When using SLAAC, the currently supported combinations for ipv6_ra_mode and ipv6_address_mode are
as follows.
ipv6_ra_mode ipv6_address_modeResult
Not
specified.
SLAAC Addresses are assigned using EUI-64, and an external router will be
used for routing.
SLAAC SLAAC Address are assigned using EUI-64, and OpenStack Networking
provides routing.
Setting ipv6_ra_mode to slaac will result in OpenStack Networking routers being configured to send RA
packets, when they are created. This results in the following values set for the address configuration flags in
the RA messages:
Configuration 97
Networking Guide (Release Version: 15.0.0)
• Auto Configuration Flag = 1
• Managed Configuration Flag = 0
• Other Configuration Flag = 0
New or existing neutron networks that contain a SLAAC enabled IPv6 subnet will result in all neutron ports
attached to the network receiving IPv6 addresses. This is because when RA broadcast messages are sent out on
a neutron network, they are received by all IPv6 capable ports on the network, and each port will then configure
an IPv6 address based on the information contained in the RA packet. In some cases, an IPv6 SLAAC address
will be added to a port, in addition to other IPv4 and IPv6 addresses that the port already has been assigned.
DHCPv6
For DHCPv6, the currently supported combinations are as follows:
ipv6_ra_mode ipv6_address_mode Result
DHCPv6-
stateless
DHCPv6-
stateless
Addresses are assigned through RAs (see SLAAC above) and optional
information is delivered through DHCPv6.
DHCPv6-
stateful
DHCPv6-
stateful
Addresses and optional information are assigned using DHCPv6.
Setting DHCPv6-stateless for ipv6_ra_mode configures the neutron router with radvd agent to send RAs. The
list below captures the values set for the address configuration flags in the RA packet in this scenario. Similarly,
setting DHCPv6-stateless for ipv6_address_mode configures neutron DHCP implementation to provide the
additional network information.
• Auto Configuration Flag = 1
• Managed Configuration Flag = 0
• Other Configuration Flag = 1
Setting DHCPv6-stateful for ipv6_ra_mode configures the neutron router with radvd agent to send RAs. The
list below captures the values set for the address configuration flags in the RA packet in this scenario. Similarly,
setting DHCPv6-stateful for ipv6_address_mode configures neutron DHCP implementation to provide
addresses and additional network information through DHCPv6.
• Auto Configuration Flag = 0
• Managed Configuration Flag = 1
• Other Configuration Flag = 1
Router support
The behavior of the neutron router for IPv6 is different than for IPv4 in a few ways.
Internal router ports, that act as default gateway ports for a network, will share a common port for all IPv6
subnets associated with the network. This implies that there will be an IPv6 internal router interface with
multiple IPv6 addresses from each of the IPv6 subnets associated with the network and a separate IPv4 internal
router interface for the IPv4 subnet. On the other hand, external router ports are allowed to have a dual-stack
configuration with both an IPv4 and an IPv6 address assigned to them.
Neutron project networks that are assigned Global Unicast Address (GUA) prefixes and addresses don’t require
NAT on the neutron router external gateway port to access the outside world. As a consequence of the lack of
NAT the external router port doesn’t require a GUA to send and receive to the external networks. This implies
98 Configuration
Networking Guide (Release Version: 15.0.0)
a GUA IPv6 subnet prefix is not necessarily needed for the neutron external network. By default, a IPv6
LLA associated with the external gateway port can be used for routing purposes. To handle this scenario, the
implementation of router-gateway-set API in neutron has been modified so that an IPv6 subnet is not required
for the external network that is associated with the neutron router. The LLA address of the upstream router can
be learned in two ways.
1. In the absence of an upstream RA support, ipv6_gateway flag can be set with the external router gateway
LLA in the neutron L3 agent configuration file. This also requires that no subnet is associated with that
port.
2. The upstream router can send an RA and the neutron router will automatically learn the next-hop LLA,
provided again that no subnet is assigned and the ipv6_gateway flag is not set.
Effectively the ipv6_gateway flag takes precedence over an RA that is received from the upstream router. If
it is desired to use a GUA next hop that is accomplished by allocating a subnet to the external router port and
assigning the upstream routers GUA address as the gateway for the subnet.
Note: It should be possible for projects to communicate with each other on an isolated network (a network
without a router port) using LLA with little to no participation on the part of OpenStack. The authors of this
section have not proven that to be true for all scenarios.
Note: When using the neutron L3 agent in a configuration where it is auto-configuring an IPv6 address via
SLAAC, and the agent is learning its default IPv6 route from the ICMPv6 Router Advertisement, it may be
necessary to set the net.ipv6.conf.<physical_interface>.accept_ra sysctl to the value 2 in order for
routing to function correctly. For a more detailed description, please see the bug.
Neutron’s Distributed Router feature and IPv6
IPv6 does work when the Distributed Virtual Router functionality is enabled, but all ingress/egress traffic is via
the centralized router (hence, not distributed). More work is required to fully enable this functionality.
Advanced services
VPNaaS
VPNaaS supports IPv6, but support in Kilo and prior releases will have some bugs that may limit how it can
be used. More thorough and complete testing and bug fixing is being done as part of the Liberty release. IPv6-
based VPN-as-a-Service is configured similar to the IPv4 configuration. Either or both the peer_address and
the peer_cidr can specified as an IPv6 address. The choice of addressing modes and router modes described
above should not impact support.
LBaaS
TODO
Configuration 99
Networking Guide (Release Version: 15.0.0)
FWaaS
FWaaS allows creation of IPv6 based rules.
NAT & Floating IPs
At the current time OpenStack Networking does not provide any facility to support any flavor of NAT with
IPv6. Unlike IPv4 there is no current embedded support for floating IPs with IPv6. It is assumed that the IPv6
addressing amongst the projects is using GUAs with no overlap across the projects.
Security considerations
Configuring interfaces of the guest
OpenStack currently doesn’t support the privacy extensions defined by RFC 4941. The interface identifier and
DUID used must be directly derived from the MAC as described in RFC 2373. The compute hosts must not be
setup to utilize the privacy extensions when generating their interface identifier.
There is no provisions for an IPv6-based metadata service similar to what is provided for IPv4. In the case of
dual stacked guests though it is always possible to use the IPv4 metadata service instead.
Unlike IPv4 the MTU of a given network can be conveyed in the RA messages sent by the router as well as in
the DHCP messages.
OpenStack control & management network considerations
As of the Kilo release, considerable effort has gone in to ensuring the project network can handle dual stack
IPv6 and IPv4 transport across the variety of configurations described above. OpenStack control network can
be run in a dual stack configuration and OpenStack API endpoints can be accessed via an IPv6 network. At this
time, Open vSwitch (OVS) tunnel types - STT, VXLAN, GRE, support both IPv4 and IPv6 endpoints.
Prefix delegation
From the Liberty release onwards, OpenStack Networking supports IPv6 prefix delegation. This section describes
the configuration and workflow steps necessary to use IPv6 prefix delegation to provide automatic
allocation of subnet CIDRs. This allows you as the OpenStack administrator to rely on an external (to the
OpenStack Networking service) DHCPv6 server to manage your project network prefixes.
Note: Prefix delegation became available in the Liberty release, it is not available in the Kilo release. HA and
DVR routers are not currently supported by this feature.
Configuring OpenStack Networking for prefix delegation
To enable prefix delegation, edit the /etc/neutron/neutron.conf file. If you are running OpenStack Liberty,
make the following change:
100 Configuration
Networking Guide (Release Version: 15.0.0)
default_ipv6_subnet_pool = prefix_delegation
Otherwise if you are running OpenStack Mitaka, make this change:
ipv6_pd_enabled = True
Note: If you are not using the default dibbler-based driver for prefix delegation, then you also need to set the
driver in /etc/neutron/neutron.conf:
pd_dhcp_driver = <class path to driver>
Drivers other than the default one may require extra configuration, please refer to Extra configuration
This tells OpenStack Networking to use the prefix delegation mechanism for subnet allocation when the user
does not provide a CIDR or subnet pool id when creating a subnet.
Requirements
To use this feature, you need a prefix delegation capable DHCPv6 server that is reachable from your OpenStack
Networking node(s). This could be software running on the OpenStack Networking node(s) or elsewhere, or a
physical router. For the purposes of this guide we are using the open-source DHCPv6 server, Dibbler. Dibbler
is available in many Linux package managers, or from source at tomaszmrugalski/dibbler.
When using the reference implementation of the OpenStack Networking prefix delegation driver, Dibbler must
also be installed on your OpenStack Networking node(s) to serve as a DHCPv6 client. Version 1.0.1 or higher
is required.
This guide assumes that you are running a Dibbler server on the network node where the external network
bridge exists. If you already have a prefix delegation capable DHCPv6 server in place, then you can skip the
following section.
Configuring the Dibbler server
After installing Dibbler, edit the /etc/dibbler/server.conf file:
script "/var/lib/dibbler/pd-server.sh"
iface "br-ex" {
pd-class {
pd-pool 2001:db8:2222::/48
pd-length 64
}
}
The options used in the configuration file above are:
• script Points to a script to be run when a prefix is delegated or released. This is only needed if you
want instances on your subnets to have external network access. More on this below.
• iface The name of the network interface on which to listen for prefix delegation messages.
Configuration 101
Networking Guide (Release Version: 15.0.0)
• pd-pool The larger prefix from which you want your delegated prefixes to come. The example given
is sufficient if you do not need external network access, otherwise a unique globally routable prefix is
necessary.
• pd-length The length that delegated prefixes will be. This must be 64 to work with the current OpenStack
Networking reference implementation.
To provide external network access to your instances, your Dibbler server also needs to create new routes for
each delegated prefix. This is done using the script file named in the config file above. Edit the /var/lib/
dibbler/pd-server.sh file:
if [ "$PREFIX1" != "" ]; then
if [ "$1" == "add" ]; then
sudo ip -6 route add ${PREFIX1}/64 via $REMOTE_ADDR dev $IFACE
fi
if [ "$1" == "delete" ]; then
sudo ip -6 route del ${PREFIX1}/64 via $REMOTE_ADDR dev $IFACE
fi
fi
The variables used in the script file above are:
• $PREFIX1 The prefix being added/deleted by the Dibbler server.
• $1 The operation being performed.
• $REMOTE_ADDR The IP address of the requesting Dibbler client.
• $IFACE The network interface upon which the request was received.
The above is all you need in this scenario, but more information on installing, configuring, and running Dibbler
is available in the Dibbler user guide, at Dibbler – a portable DHCPv6.
To start your Dibbler server, run:
# dibbler-server run
Or to run in headless mode:
# dibbler-server start
When using DevStack, it is important to start your server after the stack.sh script has finished to ensure that
the required network interfaces have been created.
User workflow
First, create a network and IPv6 subnet:
$ openstack network create ipv6-pd
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2017-01-25T19:26:01Z |
| description | |
102 Configuration
Networking Guide (Release Version: 15.0.0)
| headers | |
| id | 4b782725-6abe-4a2d-b061-763def1bb029 |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| mtu | 1450 |
| name | ipv6-pd |
| port_security_enabled | True |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 46 |
| revision_number | 3 |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | [] |
| updated_at | 2017-01-25T19:26:01Z |
+---------------------------+--------------------------------------+
$ openstack subnet create --ip-version 6 --ipv6-ra-mode slaac \
--ipv6-address-mode slaac --use-default-subnet-pool \
--network ipv6-pd ipv6-pd-1
+------------------------+--------------------------------------+
| Field | Value |
+------------------------+--------------------------------------+
| allocation_pools | ::2-::ffff:ffff:ffff:ffff |
| cidr | ::/64 |
| created_at | 2017-01-25T19:31:53Z |
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | ::1 |
| headers | |
| host_routes | |
| id | 1319510d-c92c-4532-bf5d-8bcf3da761a1 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | slaac |
| name | ipv6-pd-1 |
| network_id | 4b782725-6abe-4a2d-b061-763def1bb029 |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| revision_number | 2 |
| service_types | |
| subnetpool_id | prefix_delegation |
| updated_at | 2017-01-25T19:31:53Z |
| use_default_subnetpool | True |
+------------------------+--------------------------------------+
The subnet is initially created with a temporary CIDR before one can be assigned by prefix delegation. Any
number of subnets with this temporary CIDR can exist without raising an overlap error. The subnetpool_id is
automatically set to prefix_delegation.
To trigger the prefix delegation process, create a router interface between this subnet and a router with an active
interface on the external network:
Configuration 103
Networking Guide (Release Version: 15.0.0)
$ openstack router add subnet router1 ipv6-pd-1
The prefix delegation mechanism then sends a request via the external network to your prefix delegation server,
which replies with the delegated prefix. The subnet is then updated with the new prefix, including issuing new
IP addresses to all ports:
$ openstack subnet show ipv6-pd-1
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| allocation_pools | 2001:db8:2222:6977::2-2001:db8:2222: |
| | 6977:ffff:ffff:ffff:ffff |
| cidr | 2001:db8:2222:6977::/64 |
| created_at | 2017-01-25T19:31:53Z |
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 2001:db8:2222:6977::1 |
| host_routes | |
| id | 1319510d-c92c-4532-bf5d-8bcf3da761a1 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | slaac |
| name | ipv6-pd-1 |
| network_id | 4b782725-6abe-4a2d-b061-763def1bb029 |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| revision_number | 4 |
| service_types | |
| subnetpool_id | prefix_delegation |
| updated_at | 2017-01-25T19:35:26Z |
+-------------------+--------------------------------------+
If the prefix delegation server is configured to delegate globally routable prefixes and setup routes, then any
instance with a port on this subnet should now have external network access.
Deleting the router interface causes the subnet to be reverted to the temporary CIDR, and all ports have their
IPs updated. Prefix leases are released and renewed automatically as necessary.
References
The following link provides a great step by step tutorial on setting up IPv6 with OpenStack: Tenant IPV6
deployment in OpenStack Kilo release.
Extra configuration
Neutron dhcpv6_pd_agent
To enable the driver for the dhcpv6_pd_agent, set pd_dhcp_driver to this in /etc/neutron/neutron.conf:
pd_dhcp_driver = neutron_pd_agent
To allow the neutron-pd-agent to communicate with prefix delegation servers, you must set which network
interface to use for external communication. In DevStack the default for this is br-ex:
104 Configuration
Networking Guide (Release Version: 15.0.0)
pd_interface = br-ex
Once you have stacked run the command below to start the neutron-pd-agent:
neutron-pd-agent --config-file /etc/neutron/neutron.conf
Load Balancer as a Service (LBaaS)
The Networking service offers a load balancer feature called “LBaaS v2” through the neutron-lbaas service
plug-in.
LBaaS v2 adds the concept of listeners to the LBaaS v1 load balancers. LBaaS v2 allows you to configure
multiple listener ports on a single load balancer IP address.
There are two reference implementations of LBaaS v2. The one is an agent based implementation with
HAProxy. The agents handle the HAProxy configuration and manage the HAProxy daemon. Another LBaaS
v2 implementation, Octavia, has a separate API and separate worker processes that build load balancers within
virtual machines on hypervisors that are managed by the Compute service. You do not need an agent for Octavia.
Note: LBaaS v1 was removed in the Newton release. These links provide more details about how LBaaS v1
works and how to configure it:
• Load-Balancer-as-a-Service (LBaaS) overview
• Basic Load-Balancer-as-a-Service operations
Warning: Currently, no migration path exists between v1 and v2 load balancers. If you choose to switch
from v1 to v2, you must recreate all load balancers, pools, and health monitors.
LBaaS v2 Concepts
LBaaS v2 has several new concepts to understand:
Configuration 105
Networking Guide (Release Version: 15.0.0)
Load balancer The load balancer occupies a neutron network port and has an IP address assigned from a
subnet.
Listener Load balancers can listen for requests on multiple ports. Each one of those ports is specified by a
listener.
Pool A pool holds a list of members that serve content through the load balancer.
Member Members are servers that serve traffic behind a load balancer. Each member is specified by the IP
address and port that it uses to serve traffic.
Health monitor Members may go offline from time to time and health monitors divert traffic away from members
that are not responding properly. Health monitors are associated with pools.
LBaaS v2 has multiple implementations via different service plug-ins. The two most common implementations
use either an agent or the Octavia services. Both implementations use the LBaaS v2 API.
Configurations
Configuring LBaaS v2 with an agent
1. Add the LBaaS v2 service plug-in to the service_plugins configuration directive in /etc/neutron/
neutron.conf. The plug-in list is comma-separated:
service_plugins = [existing service plugins],neutron_lbaas.services.loadbalancer.
,→plugin.LoadBalancerPluginv2
2. Add the LBaaS v2 service provider to the service_provider configuration directive within the [service_providers]
section in /etc/neutron/neutron_lbaas.conf:
service_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.
,→HaproxyOnHostPluginDriver:default
106 Configuration
Networking Guide (Release Version: 15.0.0)
If you have existing service providers for other networking service plug-ins, such as VPNaaS or FWaaS,
add the service_provider line shown above in the [service_providers] section as a separate line.
These configuration directives are repeatable and are not comma-separated.
3. Select the driver that manages virtual interfaces in /etc/neutron/lbaas_agent.ini:
[DEFAULT]
interface_driver = INTERFACE_DRIVER
Replace INTERFACE_DRIVER with the interface driver that the layer-2 agent in your environment uses.
For example, openvswitch for Open vSwitch or linuxbridge for Linux bridge.
4. Run the neutron-lbaas database migration:
neutron-db-manage --subproject neutron-lbaas upgrade head
5. If you have deployed LBaaS v1, stop the LBaaS v1 agent now. The v1 and v2 agents cannot run
simultaneously.
6. Start the LBaaS v2 agent:
neutron-lbaasv2-agent \
--config-file /etc/neutron/neutron.conf \
--config-file /etc/neutron/lbaas_agent.ini
7. Restart the Network service to activate the new configuration. You are now ready to create load balancers
with the LBaaS v2 agent.
Configuring LBaaS v2 with Octavia
Octavia provides additional capabilities for load balancers, including using a compute driver to build instances
that operate as load balancers. The Hands on Lab - Install and Configure OpenStack Octavia session at the
OpenStack Summit in Tokyo provides an overview of Octavia.
The DevStack documentation offers a simple method to deploy Octavia and test the service with redundant
load balancer instances. If you already have Octavia installed and configured within your environment, you
can configure the Network service to use Octavia:
1. Add the LBaaS v2 service plug-in to the service_plugins configuration directive in /etc/neutron/
neutron.conf. The plug-in list is comma-separated:
service_plugins = [existing service plugins],neutron_lbaas.services.loadbalancer.
,→plugin.LoadBalancerPluginv2
2. Add the Octavia service provider to the service_provider configuration directive within the [service_providers]
section in /etc/neutron/neutron_lbaas.conf:
service_provider = LOADBALANCERV2:Octavia:neutron_lbaas.drivers.octavia.driver.
,→OctaviaDriver:default
Ensure that the LBaaS v1 and v2 service providers are removed from the [service_providers] section.
They are not used with Octavia. Verify that all LBaaS agents are stopped.
3. Restart the Network service to activate the new configuration. You are now ready to create and manage
load balancers with Octavia.
Configuration 107
Networking Guide (Release Version: 15.0.0)
Add LBaaS panels to Dashboard
The Dashboard panels for managing LBaaS v2 are available starting with the Mitaka release.
1. Clone the neutron-lbaas-dashboard repository and check out the release branch that matches the installed
version of Dashboard:
$ git clone https://git.openstack.org/openstack/neutron-lbaas-dashboard
$ cd neutron-lbaas-dashboard
$ git checkout OPENSTACK_RELEASE
2. Install the Dashboard panel plug-in:
$ python setup.py install
3. Copy the _1481_project_ng_loadbalancersv2_panel.py file from the neutron-lbaasdashboard/enabled
directory into the Dashboard openstack_dashboard/local/enabled
directory.
This step ensures that Dashboard can find the plug-in when it enumerates all of its available panels.
4. Enable the plug-in in Dashboard by editing the local_settings.py file and setting enable_lb to True
in the OPENSTACK_NEUTRON_NETWORK dictionary.
5. If Dashboard is configured to compress static files for better performance (usually set through COMPRESS_OFFLINE
in local_settings.py), optimize the static files again:
$ ./manage.py collectstatic
$ ./manage.py compress
6. Restart Apache to activate the new panel:
$ sudo service apache2 restart
To find the panel, click on Project in Dashboard, then click the Network drop-down menu and select Load
Balancers.
LBaaS v2 operations
The same neutron commands are used for LBaaS v2 with an agent or with Octavia.
Building an LBaaS v2 load balancer
1. Start by creating a load balancer on a network. In this example, the private network is an isolated
network with two web server instances:
$ neutron lbaas-loadbalancer-create --name test-lb private-subnet
2. You can view the load balancer status and IP address with the neutron lbaas-loadbalancer-show
command:
$ neutron lbaas-loadbalancer-show test-lb
+---------------------+------------------------------------------------+
| Field | Value |
108 Configuration
Networking Guide (Release Version: 15.0.0)
+---------------------+------------------------------------------------+
| admin_state_up | True |
| description | |
| id | 7780f9dd-e5dd-43a9-af81-0d2d1bd9c386 |
| listeners | {"id": "23442d6a-4d82-40ee-8d08-243750dbc191"} |
| | {"id": "7e0d084d-6d67-47e6-9f77-0115e6cf9ba8"} |
| name | test-lb |
| operating_status | ONLINE |
| provider | haproxy |
| provisioning_status | ACTIVE |
| tenant_id | fbfce4cb346c4f9097a977c54904cafd |
| vip_address | 192.0.2.22 |
| vip_port_id | 9f8f8a75-a731-4a34-b622-864907e1d556 |
| vip_subnet_id | f1e7827d-1bfe-40b6-b8f0-2d9fd946f59b |
+---------------------+------------------------------------------------+
3. Update the security group to allow traffic to reach the new load balancer. Create a new security group
along with ingress rules to allow traffic into the new load balancer. The neutron port for the load balancer
is shown as vip_port_id above.
Create a security group and rules to allow TCP port 80, TCP port 443, and all ICMP traffic:
$ neutron security-group-create lbaas
$ neutron security-group-rule-create \
--direction ingress \
--protocol tcp \
--port-range-min 80 \
--port-range-max 80 \
--remote-ip-prefix 0.0.0.0/0 \
lbaas
$ neutron security-group-rule-create \
--direction ingress \
--protocol tcp \
--port-range-min 443 \
--port-range-max 443 \
--remote-ip-prefix 0.0.0.0/0 \
lbaas
$ neutron security-group-rule-create \
--direction ingress \
--protocol icmp \
lbaas
Apply the security group to the load balancer’s network port using vip_port_id from the neutron
lbaas-loadbalancer-show command:
$ neutron port-update \
--security-group lbaas \
9f8f8a75-a731-4a34-b622-864907e1d556
Adding an HTTP listener
1. With the load balancer online, you can add a listener for plaintext HTTP traffic on port 80:
$ neutron lbaas-listener-create \
--name test-lb-http \
Configuration 109
Networking Guide (Release Version: 15.0.0)
--loadbalancer test-lb \
--protocol HTTP \
--protocol-port 80
This load balancer is active and ready to serve traffic on 192.0.2.22.
2. Verify that the load balancer is responding to pings before moving further:
$ ping -c 4 192.0.2.22
PING 192.0.2.22 (192.0.2.22) 56(84) bytes of data.
64 bytes from 192.0.2.22: icmp_seq=1 ttl=62 time=0.410 ms
64 bytes from 192.0.2.22: icmp_seq=2 ttl=62 time=0.407 ms
64 bytes from 192.0.2.22: icmp_seq=3 ttl=62 time=0.396 ms
64 bytes from 192.0.2.22: icmp_seq=4 ttl=62 time=0.397 ms
--- 192.0.2.22 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 2997ms
rtt min/avg/max/mdev = 0.396/0.402/0.410/0.020 ms
3. You can begin building a pool and adding members to the pool to serve HTTP content on port 80. For
this example, the web servers are 192.0.2.16 and 192.0.2.17:
$ neutron lbaas-pool-create \
--name test-lb-pool-http \
--lb-algorithm ROUND_ROBIN \
--listener test-lb-http \
--protocol HTTP
$ neutron lbaas-member-create \
--name test-lb-http-member-1 \
--subnet private-subnet \
--address 192.0.2.16 \
--protocol-port 80 \
test-lb-pool-http
$ neutron lbaas-member-create \
--name test-lb-http-member-2 \
--subnet private-subnet \
--address 192.0.2.17 \
--protocol-port 80 \
test-lb-pool-http
4. You can use curl to verify connectivity through the load balancers to your web servers:
$ curl 192.0.2.22
web2
$ curl 192.0.2.22
web1
$ curl 192.0.2.22
web2
$ curl 192.0.2.22
web1
In this example, the load balancer uses the round robin algorithm and the traffic alternates between the
web servers on the backend.
5. You can add a health monitor so that unresponsive servers are removed from the pool:
110 Configuration
Networking Guide (Release Version: 15.0.0)
$ neutron lbaas-healthmonitor-create \
--name test-lb-http-monitor \
--delay 5 \
--max-retries 2 \
--timeout 10 \
--type HTTP \
--pool test-lb-pool-http
In this example, the health monitor removes the server from the pool if it fails a health check at two fivesecond
intervals. When the server recovers and begins responding to health checks again, it is added to
the pool once again.
Adding an HTTPS listener
You can add another listener on port 443 for HTTPS traffic. LBaaS v2 offers SSL/TLS termination at the load
balancer, but this example takes a simpler approach and allows encrypted connections to terminate at each
member server.
1. Start by creating a listener, attaching a pool, and then adding members:
$ neutron lbaas-listener-create \
--name test-lb-https \
--loadbalancer test-lb \
--protocol HTTPS \
--protocol-port 443
$ neutron lbaas-pool-create \
--name test-lb-pool-https \
--lb-algorithm LEAST_CONNECTIONS \
--listener test-lb-https \
--protocol HTTPS
$ neutron lbaas-member-create \
--name test-lb-https-member-1 \
--subnet private-subnet \
--address 192.0.2.16 \
--protocol-port 443 \
test-lb-pool-https
$ neutron lbaas-member-create \
--name test-lb-https-member-2 \
--subnet private-subnet \
--address 192.0.2.17 \
--protocol-port 443 \
test-lb-pool-https
2. You can also add a health monitor for the HTTPS pool:
$ neutron lbaas-healthmonitor-create \
--name test-lb-https-monitor \
--delay 5 \
--max-retries 2 \
--timeout 10 \
--type HTTPS \
--pool test-lb-pool-https
The load balancer now handles traffic on ports 80 and 443.
Configuration 111
Networking Guide (Release Version: 15.0.0)
Associating a floating IP address
Load balancers that are deployed on a public or provider network that are accessible to external clients do not
need a floating IP address assigned. External clients can directly access the virtual IP address (VIP) of those
load balancers.
However, load balancers deployed onto private or isolated networks need a floating IP address assigned if they
must be accessible to external clients. To complete this step, you must have a router between the private and
public networks and an available floating IP address.
You can use the neutron lbaas-loadbalancer-show command from the beginning of this section to locate
the vip_port_id. The vip_port_id is the ID of the network port that is assigned to the load balancer. You
can associate a free floating IP address to the load balancer using neutron floatingip-associate:
$ neutron floatingip-associate FLOATINGIP_ID LOAD_BALANCER_PORT_ID
Setting quotas for LBaaS v2
Quotas are available for limiting the number of load balancers and load balancer pools. By default, both quotas
are set to 10.
You can adjust quotas using the neutron quota-update command:
$ neutron quota-update --tenant-id TENANT_UUID --loadbalancer 25
$ neutron quota-update --tenant-id TENANT_UUID --pool 50
A setting of -1 disables the quota for a tenant.
Retrieving load balancer statistics
The LBaaS v2 agent collects four types of statistics for each load balancer every six seconds. Users can query
these statistics with the neutron lbaas-loadbalancer-stats command:
$ neutron lbaas-loadbalancer-stats test-lb
+--------------------+----------+
| Field | Value |
+--------------------+----------+
| active_connections | 0 |
| bytes_in | 40264557 |
| bytes_out | 71701666 |
| total_connections | 384601 |
+--------------------+----------+
The active_connections count is the total number of connections that were active at the time the agent
polled the load balancer. The other three statistics are cumulative since the load balancer was last started. For
example, if the load balancer restarts due to a system error or a configuration change, these statistics will be
reset.
Macvtap mechanism driver
The Macvtap mechanism driver for the ML2 plug-in generally increases network performance of instances.
112 Configuration
Networking Guide (Release Version: 15.0.0)
Consider the following attributes of this mechanism driver to determine practicality in your environment:
• Supports only instance ports. Ports for DHCP and layer-3 (routing) services must use another mechanism
driver such as Linux bridge or Open vSwitch (OVS).
• Supports only untagged (flat) and tagged (VLAN) networks.
• Lacks support for security groups including basic (sanity) and anti-spoofing rules.
• Lacks support for layer-3 high-availability mechanisms such as Virtual Router Redundancy Protocol
(VRRP) and Distributed Virtual Routing (DVR).
• Only compute resources can be attached via macvtap. Attaching other resources like DHCP, Routers and
others is not supported. Therefore run either OVS or linux bridge in VLAN or flat mode on the controller
node.
• Instance migration requires the same values for the physical_interface_mapping configuration
option on each compute node. For more information, see https://bugs.launchpad.net/neutron/+bug/
1550400.
Prerequisites
You can add this mechanism driver to an existing environment using either the Linux bridge or OVS mechanism
drivers with only provider networks or provider and self-service networks. You can change the configuration
of existing compute nodes or add compute nodes with the Macvtap mechanism driver. The example configuration
assumes addition of compute nodes with the Macvtap mechanism driver to the Linux bridge: Self-service
networks or Open vSwitch: Self-service networks deployment examples.
Add one or more compute nodes with the following components:
• Three network interfaces: management, provider, and overlay.
• OpenStack Networking Macvtap layer-2 agent and any dependencies.
Note: To support integration with the deployment examples, this content configures the Macvtap mechanism
driver to use the overlay network for untagged (flat) or tagged (VLAN) networks in addition to overlay networks
such as VXLAN. Your physical network infrastructure must support VLAN (802.1q) tagging on the overlay
network.
Architecture
The Macvtap mechanism driver only applies to compute nodes. Otherwise, the environment resembles the
prerequisite deployment example.
Configuration 113
Networking Guide (Release Version: 15.0.0)
114 Configuration
Networking Guide (Release Version: 15.0.0)
Example configuration
Use the following example configuration as a template to add support for the Macvtap mechanism driver to an
existing operational environment.
Controller node
1. In the ml2_conf.ini file:
• Add macvtap to mechanism drivers.
[ml2]
mechanism_drivers = macvtap
• Configure network mappings.
Configuration 115
Networking Guide (Release Version: 15.0.0)
[ml2_type_flat]
flat_networks = provider,macvtap
[ml2_type_vlan]
network_vlan_ranges = provider,macvtap:VLAN_ID_START:VLAN_ID_END
Note: Use of macvtap is arbitrary. Only the self-service deployment examples require VLAN ID
ranges. Replace VLAN_ID_START and VLAN_ID_END with appropriate numerical values.
2. Restart the following services:
• Server
Network nodes
No changes.
Compute nodes
1. Install the Networking service Macvtap layer-2 agent.
2. In the neutron.conf file, configure common options:
[DEFAULT]
core_plugin = ml2
auth_strategy = keystone
[database]
# ...
[keystone_authtoken]
# ...
[nova]
# ...
[agent]
# ...
See the Installation Tutorials and Guides and Configuration Reference for your OpenStack release
to obtain the appropriate additional configuration for the [DEFAULT], [database], [keystone_authtoken],
[nova], and [agent] sections.
3. In the macvtap_agent.ini file, configure the layer-2 agent.
[macvtap]
physical_interface_mappings = macvtap:MACVTAP_INTERFACE
[securitygroup]
firewall_driver = noop
116 Configuration
Networking Guide (Release Version: 15.0.0)
Replace MACVTAP_INTERFACE with the name of the underlying interface that handles Macvtap mechanism
driver interfaces. If using a prerequisite deployment example, replace MACVTAP_INTERFACE with
the name of the underlying interface that handles overlay networks. For example, eth1.
4. Start the following services:
• Macvtap agent
Verify service operation
1. Source the administrative project credentials.
2. Verify presence and operation of the agents:
$ openstack network agent list
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| ID | Agent Type | Host | Availability
,→Zone | Alive | State | Binary |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| 31e1bc1b-c872-4429-8fc3-2c8eba52634e | Metadata agent | compute1 | None
,→ | True | UP | neutron-metadata-agent |
| 378f5550-feee-42aa-a1cb-e548b7c2601f | Open vSwitch agent | compute1 | None
,→ | True | UP | neutron-openvswitch-agent |
| 7d2577d0-e640-42a3-b303-cb1eb077f2b6 | L3 agent | compute1 | nova
,→ | True | UP | neutron-l3-agent |
| d5d7522c-ad14-4c63-ab45-f6420d6a81dd | Metering agent | compute1 | None
,→ | True | UP | neutron-metering-agent |
| e838ef5c-75b1-4b12-84da-7bdbd62f1040 | DHCP agent | compute1 | nova
,→ | True | UP | neutron-dhcp-agent |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
Create initial networks
This mechanism driver simply changes the virtual network interface driver for instances. Thus, you can reference
the Create initial networks content for the prerequisite deployment example.
Verify network operation
This mechanism driver simply changes the virtual network interface driver for instances. Thus, you can reference
the Verify network operation content for the prerequisite deployment example.
Network traffic flow
This mechanism driver simply removes the Linux bridge handling security groups on the compute nodes. Thus,
you can reference the network traffic flow scenarios for the prerequisite deployment example.
Configuration 117
Networking Guide (Release Version: 15.0.0)
MTU considerations
The Networking service uses the MTU of the underlying physical network to calculate the MTU for virtual
network components including instance network interfaces. By default, it assumes a standard 1500-byte MTU
for the underlying physical network.
The Networking service only references the underlying physical network MTU. Changing the underlying physical
network device MTU requires configuration of physical network devices such as switches and routers.
Jumbo frames
The Networking service supports underlying physical networks using jumbo frames and also enables instances
to use jumbo frames minus any overlay protocol overhead. For example, an underlying physical network with
a 9000-byte MTU yields a 8950-byte MTU for instances using a VXLAN network with IPv4 endpoints. Using
IPv6 endpoints for overlay networks adds 20 bytes of overhead for any protocol.
The Networking service supports the following underlying physical network architectures. Case 1 refers to the
most common architecture. In general, architectures should avoid cases 2 and 3.
Note: You can trigger MTU recalculation for existing networks by changing the MTU configuration and
restarting the neutron-server service. However, propagating MTU calculations to the data plane may require
users to delete and recreate ports on the network.
When using the Open vSwitch or Linux bridge drivers, new MTU calculations will be propogated automatically
after restarting the l3-agent service.
Case 1
For typical underlying physical network architectures that implement a single MTU value, you can leverage
jumbo frames using two options, one in the neutron.conf file and the other in the ml2_conf.ini file. Most
environments should use this configuration.
For example, referencing an underlying physical network with a 9000-byte MTU:
1. In the neutron.conf file:
[DEFAULT]
global_physnet_mtu = 9000
2. In the ml2_conf.ini file:
[ml2]
path_mtu = 9000
Case 2
Some underlying physical network architectures contain multiple layer-2 networks with different MTU values.
You can configure each flat or VLAN provider network in the bridge or interface mapping options of the layer-2
agent to reference a unique MTU value.
118 Configuration
Networking Guide (Release Version: 15.0.0)
For example, referencing a 4000-byte MTU for provider2, a 1500-byte MTU for provider3, and a 9000-byte
MTU for other networks using the Open vSwitch agent:
1. In the neutron.conf file:
[DEFAULT]
global_physnet_mtu = 9000
2. In the openvswitch_agent.ini file:
[ovs]
bridge_mappings = provider1:eth1,provider2:eth2,provider3:eth3
3. In the ml2_conf.ini file:
[ml2]
physical_network_mtus = provider2:4000,provider3:1500
path_mtu = 9000
Case 3
Some underlying physical network architectures contain a unique layer-2 network for overlay networks using
protocols such as VXLAN and GRE.
For example, referencing a 4000-byte MTU for overlay networks and a 9000-byte MTU for other networks:
1. In the neutron.conf file:
[DEFAULT]
global_physnet_mtu = 9000
2. In the ml2_conf.ini file:
[ml2]
path_mtu = 4000
Note: Other networks including provider networks and flat or VLAN self-service networks assume the
value of the global_physnet_mtu option.
Instance network interfaces (VIFs)
The DHCP agent provides an appropriate MTU value to instances using IPv4, while the L3 agent provides an
appropriate MTU value to instances using IPv6. IPv6 uses RA via the L3 agent because the DHCP agent only
supports IPv4. Instances using IPv4 and IPv6 should obtain the same MTU value regardless of method.
Open vSwitch with DPDK datapath
This page serves as a guide for how to use the OVS with DPDK datapath functionality available in the Networking
service as of the Mitaka release.
Configuration 119
Networking Guide (Release Version: 15.0.0)
The basics
Open vSwitch (OVS) provides support for a Data Plane Development Kit (DPDK) datapath since OVS 2.2, and
a DPDK-backed vhost-user virtual interface since OVS 2.4. The DPDK datapath provides lower latency and
higher performance than the standard kernel OVS datapath, while DPDK-backed vhost-user interfaces can
connect guests to this datapath. For more information on DPDK, refer to the DPDK website.
OVS with DPDK, or OVS-DPDK, can be used to provide high-performance networking between instances on
OpenStack compute nodes.
Prerequisites
Using DPDK in OVS requires the following minimum software versions:
• OVS 2.4
• DPDK 2.0
• QEMU 2.1.0
• libvirt 1.2.13
Multiqueue support is available if the following newer versions are used:
• OVS 2.5
• DPDK 2.2
• QEMU 2.5
• libvirt 1.2.17
In both cases, install and configure Open vSwitch with DPDK support for each node. For more information,
see the OVS-DPDK installation guide.
Using vhost-user interfaces
Once OVS is correctly configured with DPDK support, vhost-user interfaces are completely transparent to
the guest. However, guests must request large pages. This can be done through flavors. For example:
$ openstack flavor set m1.large --property hw:mem_page_size=large
For more information about the syntax for hw:mem_page_size, refer to the Flavors guide.
Note: vhost-user requires file descriptor-backed shared memory. Currently, the only way to request this
is by requesting large pages. This is why instances spawned on hosts with OVS-DPDK must request large
pages. The aggregate flavor affinity filter can be used to associate flavors with large page support to hosts with
OVS-DPDK support.
Create and add vhost-user network interfaces to instances in the same fashion as conventional interfaces.
These interfaces can use the kernel virtio-net driver or a DPDK-compatible driver in the guest
$ openstack server create --nic net-id=$net_id ... testserver
120 Configuration
Networking Guide (Release Version: 15.0.0)
Known limitations
• This feature is only supported when using the libvirt compute driver, and the KVM/QEMU hypervisor.
• Large pages are required for each instance running on hosts with OVS-DPDK. If large pages are not
present in the guest, the interface will appear but will not function.
• Expect performance degradation of services using tap devices: these devices do not support DPDK.
Example services include DVR, FWaaS, or LBaaS.
Native Open vSwitch firewall driver
Note: Experimental feature or incomplete documentation.
Historically, Open vSwitch (OVS) could not interact directly with iptables to implement security groups. Thus,
the OVS agent and Compute service use a Linux bridge between each instance (VM) and the OVS integration
bridge br-int to implement security groups. The Linux bridge device contains the iptables rules pertaining to
the instance. In general, additional components between instances and physical network infrastructure cause
scalability and performance problems. To alleviate such problems, the OVS agent includes an optional firewall
driver that natively implements security groups as flows in OVS rather than Linux bridge and iptables, thus
increasing scalability and performance.
Prerequisites
The native OVS firewall implementation requires kernel and user space support for conntrack, thus requiring
minimum versions of the Linux kernel and Open vSwitch. All cases require Open vSwitch version 2.5 or newer.
• Kernel version 4.3 or newer includes conntrack support.
• Kernel version 3.3, but less than 4.3, does not include conntrack support and requires building the OVS
modules.
Enable the native OVS firewall driver
• On nodes running the Open vSwitch agent, edit the openvswitch_agent.ini file and enable the firewall
driver.
[securitygroup]
firewall_driver = openvswitch
For more information, see the developer documentation and the video.
Quality of Service (QoS)
QoS is defined as the ability to guarantee certain network requirements like bandwidth, latency, jitter, and
reliability in order to satisfy a Service Level Agreement (SLA) between an application provider and end users.
Network devices such as switches and routers can mark traffic so that it is handled with a higher priority to fulfill
the QoS conditions agreed under the SLA. In other cases, certain network traffic such as Voice over IP (VoIP)
and video streaming needs to be transmitted with minimal bandwidth constraints. On a system without network
Configuration 121
Networking Guide (Release Version: 15.0.0)
QoS management, all traffic will be transmitted in a “best-effort” manner making it impossible to guarantee
service delivery to customers.
QoS is an advanced service plug-in. QoS is decoupled from the rest of the OpenStack Networking code on
multiple levels and it is available through the ml2 extension driver.
Details about the DB models, API extension, and use cases are out of the scope of this guide but can be found
in the Neutron QoS specification.
Supported QoS rule types
Any plug-in or ml2 mechanism driver can claim support for some QoS rule types by providing a plug-in/driver
class property called supported_qos_rule_types that returns a list of strings that correspond to QoS rule
types.
Note: Bandwidth limit is supported on OVS, Linux bridge, and SR-IOV mechanism drivers. For the Newton
release onward DSCP marking is supported on the OVS and Linux bridge mechanism drivers, and the minimum
bandwidth rules on the SR-IOV NIC mechanism driver.
In the most simple case, the property can be represented by a simple Python list defined on the class.
For an ml2 plug-in, the list of supported QoS rule types is defined as a common subset of rules supported by
all active mechanism drivers.
Note: The list of supported rule types reported by core plug-in is not enforced when accessing QoS rule
resources. This is mostly because then we would not be able to create any rules while at least one ml2 driver
lacks support for QoS (at the moment of writing, only macvtap is such a driver).
Configuration
To enable the service, follow the steps below:
On network nodes:
1. Add the QoS service to the service_plugins setting in /etc/neutron/neutron.conf. For example:
service_plugins = \
neutron.services.l3_router.l3_router_plugin.L3RouterPlugin,
neutron.services.metering.metering_plugin.MeteringPlugin,
neutron.services.qos.qos_plugin.QoSPlugin
2. Optionally, set the needed notification_drivers in the [qos] section in /etc/neutron/neutron.
conf (message_queue is the default).
3. In /etc/neutron/plugins/ml2/ml2_conf.ini, add qos to extension_drivers in the [ml2] section.
For example:
[ml2]
extension_drivers = port_security, qos
4. If the Open vSwitch agent is being used, set extensions to qos in the [agent] section of /etc/
neutron/plugins/ml2/openvswitch_agent.ini. For example:
122 Configuration
Networking Guide (Release Version: 15.0.0)
[agent]
extensions = qos
On compute nodes:
1. In /etc/neutron/plugins/ml2/openvswitch_agent.ini, add qos to the extensions setting in
the [agent] section. For example:
[agent]
extensions = qos
Note: QoS currently works with ml2 only (SR-IOV, Open vSwitch, and linuxbridge are drivers that are enabled
for QoS in Mitaka release).
Trusted projects policy.json configuration
If projects are trusted to administrate their own QoS policies in your cloud, neutron’s file policy.json can be
modified to allow this.
Modify /etc/neutron/policy.json policy entries as follows:
"get_policy": "rule:regular_user",
"create_policy": "rule:regular_user",
"update_policy": "rule:regular_user",
"delete_policy": "rule:regular_user",
"get_rule_type": "rule:regular_user",
To enable bandwidth limit rule:
"get_policy_bandwidth_limit_rule": "rule:regular_user",
"create_policy_bandwidth_limit_rule": "rule:regular_user",
"delete_policy_bandwidth_limit_rule": "rule:regular_user",
"update_policy_bandwidth_limit_rule": "rule:regular_user",
To enable DSCP marking rule:
"get_policy_dscp_marking_rule": "rule:regular_user",
"create_dscp_marking_rule": "rule:regular_user",
"delete_dscp_marking_rule": "rule:regular_user",
"update_dscp_marking_rule": "rule:regular_user",
To enable minimum bandwidth rule:
"get_policy_minimum_bandwidth_rule": "rule:regular_user",
"create_policy_minimum_bandwidth_rule": "rule:regular_user",
"delete_policy_minimum_bandwidth_rule": "rule:regular_user",
"update_policy_minimum_bandwidth_rule": "rule:regular_user",
User workflow
QoS policies are only created by admins with the default policy.json. Therefore, you should have the cloud
operator set them up on behalf of the cloud projects.
Configuration 123
Networking Guide (Release Version: 15.0.0)
If projects are trusted to create their own policies, check the trusted projects policy.json configuration section.
First, create a QoS policy and its bandwidth limit rule:
$ neutron qos-policy-create bw-limiter
Created a new policy:
+-------------+--------------------------------------+
| Field | Value |
+-------------+--------------------------------------+
| description | |
| id | 0ee1c673-5671-40ca-b55f-4cd4bbd999c7 |
| name | bw-limiter |
| rules | |
| shared | False |
| tenant_id | 85b859134de2428d94f6ee910dc545d8 |
+-------------+--------------------------------------+
$ neutron qos-bandwidth-limit-rule-create bw-limiter --max-kbps 3000 \
--max-burst-kbps 300
Created a new bandwidth_limit_rule:
+----------------+--------------------------------------+
| Field | Value |
+----------------+--------------------------------------+
| id | 92ceb52f-170f-49d0-9528-976e2fee2d6f |
| max_burst_kbps | 300 |
| max_kbps | 3000 |
+----------------+--------------------------------------+
Note: The burst value is given in kilobits, not in kilobits per second as the name of the parameter might suggest.
This is an amount of data which can be sent before the bandwidth limit applies.
Note: The QoS implementation requires a burst value to ensure proper behavior of bandwidth limit rules in
the Open vSwitch and Linux bridge agents. If you do not provide a value, it defaults to 80% of the bandwidth
limit which works for typical TCP traffic.
Second, associate the created policy with an existing neutron port. In order to do this, user extracts the port id
to be associated to the already created policy. In the next example, we will assign the bw-limiter policy to
the VM with IP address 192.0.2.1.
$ neutron port-list
+--------------------------------------+-----------------------------------+
| id | fixed_ips |
+--------------------------------------+-----------------------------------+
| 0271d1d9-1b16-4410-bd74-82cdf6dcb5b3 | { ... , "ip_address": "192.0.2.1"}|
| 88101e57-76fa-4d12-b0e0-4fc7634b874a | { ... , "ip_address": "192.0.2.3"}|
| e04aab6a-5c6c-4bd9-a600-33333551a668 | { ... , "ip_address": "192.0.2.2"}|
+--------------------------------------+-----------------------------------+
$ neutron port-update 88101e57-76fa-4d12-b0e0-4fc7634b874a --qos-policy bw-limiter
Updated port: 88101e57-76fa-4d12-b0e0-4fc7634b874a
124 Configuration
Networking Guide (Release Version: 15.0.0)
In order to detach a port from the QoS policy, simply update again the port configuration.
$ neutron port-update 88101e57-76fa-4d12-b0e0-4fc7634b874a --no-qos-policy
Updated port: 88101e57-76fa-4d12-b0e0-4fc7634b874a
Ports can be created with a policy attached to them too.
$ neutron port-create private --qos-policy-id bw-limiter
Created a new port:
+-----------------------+--------------------------------------------------+
| Field | Value |
+-----------------------+--------------------------------------------------+
| admin_state_up | True |
| allowed_address_pairs | |
| binding:vnic_type | normal |
| device_id | |
| device_owner | |
| dns_assignment | {"hostname": "host-192-0-2-4", ... } |
| dns_name | |
| fixed_ips | {"subnet_id": |
| | "fabaf9b6-7a84-43b6-9d23-543591b531b8", |
| | "ip_address": "192.0.2.4"} |
| id | c3cb8faa-db36-429d-bd25-6003fafe63c5 |
| mac_address | fa:16:3e:02:65:15 |
| name | |
| network_id | 4920548d-1a6c-4d67-8de4-06501211587c |
| port_security_enabled | True |
| qos_policy_id | 0ee1c673-5671-40ca-b55f-4cd4bbd999c7 |
| security_groups | b9cecbc5-a136-4032-b196-fb3eb091fff2 |
| status | DOWN |
| tenant_id | 85b859134de2428d94f6ee910dc545d8 |
+-----------------------+--------------------------------------------------+
You can attach networks to a QoS policy. The meaning of this is that any compute port connected to the network
will use the network policy by default unless the port has a specific policy attached to it. Network owned ports
like DHCP and router ports are excluded from network policy application.
In order to attach a QoS policy to a network, update an existing network, or initially create the network attached
to the policy.
$ neutron net-update private --qos-policy bw-limiter
Updated network: private
Note: Configuring the proper burst value is very important. If the burst value is set too low, bandwidth usage
will be throttled even with a proper bandwidth limit setting. This issue is discussed in various documentation
sources, for example in Juniper’s documentation. Burst value for TCP traffic can be set as 80% of desired
bandwidth limit value. For example, if the bandwidth limit is set to 1000kbps then enough burst value will be
800kbit. If the configured burst value is too low, achieved bandwidth limit will be lower than expected. If the
configured burst value is too high, too few packets could be limited and achieved bandwidth limit would be
higher than expected.
Configuration 125
Networking Guide (Release Version: 15.0.0)
Administrator enforcement
Administrators are able to enforce policies on project ports or networks. As long as the policy is not shared, the
project is not be able to detach any policy attached to a network or port.
If the policy is shared, the project is able to attach or detach such policy from its own ports and networks.
Rule modification
You can modify rules at runtime. Rule modifications will be propagated to any attached port.
$ neutron qos-bandwidth-limit-rule-update \
92ceb52f-170f-49d0-9528-976e2fee2d6f bw-limiter \
--max-kbps 2000 --max-burst-kbps 200
Updated bandwidth_limit_rule: 92ceb52f-170f-49d0-9528-976e2fee2d6f
$ neutron qos-bandwidth-limit-rule-show \
92ceb52f-170f-49d0-9528-976e2fee2d6f bw-limiter
+----------------+--------------------------------------+
| Field | Value |
+----------------+--------------------------------------+
| id | 92ceb52f-170f-49d0-9528-976e2fee2d6f |
| max_burst_kbps | 200 |
| max_kbps | 2000 |
+----------------+--------------------------------------+
Just like with bandwidth limiting, create a policy for DSCP marking rule:
$ neutron qos-policy-create dscp-marking
Created a new policy:
+-------------+--------------------------------------+
| Field | Value |
+-------------+--------------------------------------+
| description | |
| id | 8569fb4d-3d63-483e-b49a-9f9290d794f4 |
| name | dscp-marking |
| rules | |
| shared | False |
| tenant_id | 85b859134de2428d94f6ee910dc545d8 |
+-------------+--------------------------------------+
You can create, update, list, delete, and show DSCP markings with the neutron client:
$ neutron qos-dscp-marking-rule-create dscp-marking --dscp-mark 26
Created a new dscp marking rule
+----------------+--------------------------------------+
| Field | Value |
+----------------+--------------------------------------+
| id | 115e4f70-8034-4176-8fe9-2c47f8878a7d |
| dscp_mark | 26 |
+----------------+--------------------------------------+
126 Configuration
Networking Guide (Release Version: 15.0.0)
$ neutron qos-dscp-marking-rule-update \
115e4f70-8034-4176-8fe9-2c47f8878a7d dscp-marking --dscp-mark 22
Updated dscp_rule: 115e4f70-8034-4176-8fe9-2c47f8878a7d
$ neutron qos-dscp-marking-rule-list dscp-marking
+--------------------------------------+----------------------------------+
| id | dscp_mark |
+--------------------------------------+----------------------------------+
| 115e4f70-8034-4176-8fe9-2c47f8878a7d | 22 |
+--------------------------------------+----------------------------------+
$ neutron qos-dscp-marking-rule-show \
115e4f70-8034-4176-8fe9-2c47f8878a7d dscp-marking
+----------------+--------------------------------------+
| Field | Value |
+----------------+--------------------------------------+
| id | 115e4f70-8034-4176-8fe9-2c47f8878a7d |
| dscp_mark | 22 |
+----------------+--------------------------------------+
$ neutron qos-dscp-marking-rule-delete \
115e4f70-8034-4176-8fe9-2c47f8878a7d dscp-marking
Deleted dscp_rule: 115e4f70-8034-4176-8fe9-2c47f8878a7d
You can also include minimum bandwidth rules in your policy:
$ openstack network qos policy create bandwidth-control
+-------------+--------------------------------------+
| Field | Value |
+-------------+--------------------------------------+
| description | |
| id | 8491547e-add1-4c6c-a50e-42121237256c |
| name | bandwidth-control |
| project_id | 7cc5a84e415d48e69d2b06aa67b317d8 |
| rules | [] |
| shared | False |
+-------------+--------------------------------------+
$ openstack network qos rule create bandwidth-control \
--type minimum-bandwidth --min-kbps 1000 --egress
+------------+--------------------------------------+
| Field | Value |
+------------+--------------------------------------+
| direction | egress |
| id | da858b32-44bc-43c9-b92b-cf6e2fa836ab |
| min_kbps | 1000 |
| name | None |
| project_id | |
+------------+--------------------------------------+
A policy with a minimum bandwidth ensures best efforts are made to provide no less than the specified bandwidth
to each port on which the rule is applied. However, as this feature is not yet integrated with the Compute
scheduler, minimum bandwidth cannot be guaranteed.
It is also possible to combine several rules in one policy:
Configuration 127
Networking Guide (Release Version: 15.0.0)
$ openstack network qos rule create bandwidth-control \
--type bandwidth-limit --max-kbps 50000 --max-burst-kbits 50000
+----------------+--------------------------------------+
| Field | Value |
+----------------+--------------------------------------+
| id | 0db48906-a762-4d32-8694-3f65214c34a6 |
| max_burst_kbps | 50000 |
| max_kbps | 50000 |
| name | None |
| project_id | |
+----------------+--------------------------------------+
$ openstack network qos policy show bandwidth-control
+-------------+-------------------------------------------------------------------+
| Field | Value |
+-------------+-------------------------------------------------------------------+
| description | |
| id | 8491547e-add1-4c6c-a50e-42121237256c |
| name | bandwidth-control |
| project_id | 7cc5a84e415d48e69d2b06aa67b317d8 |
| rules | [{u'max_kbps': 50000, u'type': u'bandwidth_limit', |
| | u'id': u'0db48906-a762-4d32-8694-3f65214c34a6', |
| | u'max_burst_kbps': 50000, |
| | u'qos_policy_id': u'8491547e-add1-4c6c-a50e-42121237256c'}, |
| | {u'direction': |
| | u'egress', u'min_kbps': 1000, u'type': u'minimum_bandwidth', |
| | u'id': u'da858b32-44bc-43c9-b92b-cf6e2fa836ab', |
| | u'qos_policy_id': u'8491547e-add1-4c6c-a50e-42121237256c'}] |
| shared | False |
+-------------+-------------------------------------------------------------------+
Role-Based Access Control (RBAC)
The Role-Based Access Control (RBAC) policy framework enables both operators and users to grant access to
resources for specific projects.
Supported objects for sharing with specific projects
Currently, the access that can be granted using this feature is supported by:
• Regular port creation permissions on networks (since Liberty).
• Binding QoS policies permissions to networks or ports (since Mitaka).
• Attaching router gateways to networks (since Mitaka).
Sharing an object with specific projects
Sharing an object with a specific project is accomplished by creating a policy entry that permits the target project
the access_as_shared action on that object.
128 Configuration
Networking Guide (Release Version: 15.0.0)
Sharing a network with specific projects
Create a network to share:
$ openstack network create secret_network
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2017-01-25T20:16:40Z |
| description | |
| dns_domain | None |
| id | f55961b9-3eb8-42eb-ac96-b97038b568de |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| is_default | None |
| mtu | 1450 |
| name | secret_network |
| port_security_enabled | True |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 9 |
| qos_policy_id | None |
| revision_number | 3 |
| router:external | Internal |
| segments | None |
| shared | False |
| status | ACTIVE |
| subnets | |
| updated_at | 2017-01-25T20:16:40Z |
+---------------------------+--------------------------------------+
Create the policy entry using the openstack network rbac create command (in this example, the ID of
the project we want to share with is b87b2fc13e0248a4a031d38e06dc191d):
$ openstack network rbac create --target-project \
b87b2fc13e0248a4a031d38e06dc191d --action access_as_shared \
--type network f55961b9-3eb8-42eb-ac96-b97038b568de
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| action | access_as_shared |
| id | f93efdbf-f1e0-41d2-b093-8328959d469e |
| name | None |
| object_id | f55961b9-3eb8-42eb-ac96-b97038b568de |
| object_type | network |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| target_project_id | b87b2fc13e0248a4a031d38e06dc191d |
+-------------------+--------------------------------------+
The target-project parameter specifies the project that requires access to the network. The action parameter
specifies what the project is allowed to do. The type parameter says that the target object is a network.
The final parameter is the ID of the network we are granting access to.
Configuration 129
Networking Guide (Release Version: 15.0.0)
Project b87b2fc13e0248a4a031d38e06dc191d will now be able to see the network when running openstack
network list and openstack network show and will also be able to create ports on that network. No other
users (other than admins and the owner) will be able to see the network.
To remove access for that project, delete the policy that allows it using the openstack network rbac delete
command:
$ openstack network rbac delete f93efdbf-f1e0-41d2-b093-8328959d469e
If that project has ports on the network, the server will prevent the policy from being deleted until the ports
have been deleted:
$ openstack network rbac delete f93efdbf-f1e0-41d2-b093-8328959d469e
RBAC policy on object f93efdbf-f1e0-41d2-b093-8328959d469e
cannot be removed because other objects depend on it.
This process can be repeated any number of times to share a network with an arbitrary number of projects.
Sharing a QoS policy with specific projects
Create a QoS policy to share:
$ openstack network qos policy create secret_policy
+-------------+--------------------------------------+
| Field | Value |
+-------------+--------------------------------------+
| description | |
| id | 1f730d69-1c45-4ade-a8f2-89070ac4f046 |
| name | secret_policy |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| rules | [] |
| shared | False |
+-------------+--------------------------------------+
Create the RBAC policy entry using the openstack network rbac create command (in this example, the
ID of the project we want to share with is be98b82f8fdf46b696e9e01cebc33fd9):
$ openstack network rbac create --target-project \
be98b82f8fdf46b696e9e01cebc33fd9 --action access_as_shared \
--type qos_policy 1f730d69-1c45-4ade-a8f2-89070ac4f046
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| action | access_as_shared |
| id | 8828e38d-a0df-4c78-963b-e5f215d3d550 |
| name | None |
| object_id | 1f730d69-1c45-4ade-a8f2-89070ac4f046 |
| object_type | qos_policy |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| target_project_id | be98b82f8fdf46b696e9e01cebc33fd9 |
+-------------------+--------------------------------------+
The target-project parameter specifies the project that requires access to the QoS policy. The action
parameter specifies what the project is allowed to do. The type parameter says that the target object is a QoS
policy. The final parameter is the ID of the QoS policy we are granting access to.
130 Configuration
Networking Guide (Release Version: 15.0.0)
Project be98b82f8fdf46b696e9e01cebc33fd9 will now be able to see the QoS policy when running openstack
network qos policy list and openstack network qos policy show and will also be able to
bind it to its ports or networks. No other users (other than admins and the owner) will be able to see the QoS
policy.
To remove access for that project, delete the RBAC policy that allows it using the openstack network rbac
delete command:
$ openstack network rbac delete 8828e38d-a0df-4c78-963b-e5f215d3d550
If that project has ports or networks with the QoS policy applied to them, the server will not delete the RBAC
policy until the QoS policy is no longer in use:
$ openstack network rbac delete 8828e38d-a0df-4c78-963b-e5f215d3d550
RBAC policy on object 8828e38d-a0df-4c78-963b-e5f215d3d550
cannot be removed because other objects depend on it.
This process can be repeated any number of times to share a qos-policy with an arbitrary number of projects.
How the ‘shared’ flag relates to these entries
As introduced in other guide entries, neutron provides a means of making an object (network, qos-policy)
available to every project. This is accomplished using the shared flag on the supported object:
$ openstack network create global_network --share
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2017-01-25T20:32:06Z |
| description | |
| dns_domain | None |
| id | 84a7e627-573b-49da-af66-c9a65244f3ce |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| is_default | None |
| mtu | 1450 |
| name | global_network |
| port_security_enabled | True |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 7 |
| qos_policy_id | None |
| revision_number | 3 |
| router:external | Internal |
| segments | None |
| shared | True |
| status | ACTIVE |
| subnets | |
| updated_at | 2017-01-25T20:32:07Z |
+---------------------------+--------------------------------------+
Configuration 131
Networking Guide (Release Version: 15.0.0)
This is the equivalent of creating a policy on the network that permits every project to perform the action
access_as_shared on that network. Neutron treats them as the same thing, so the policy entry for that network
should be visible using the openstack network rbac list command:
$ openstack network rbac list
+-------------------------------+-------------+--------------------------------+
| ID | Object Type | Object ID |
+-------------------------------+-------------+--------------------------------+
| 58a5ee31-2ad6-467d- | qos_policy | 1f730d69-1c45-4ade- |
| 8bb8-8c2ae3dd1382 | | a8f2-89070ac4f046 |
| 27efbd79-f384-4d89-9dfc- | network | 84a7e627-573b-49da- |
| 6c4a606ceec6 | | af66-c9a65244f3ce |
+-------------------------------+-------------+--------------------------------+
Use the neutron rbac-show command to see the details:
$ openstack network rbac show 27efbd79-f384-4d89-9dfc-6c4a606ceec6
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| action | access_as_shared |
| id | 27efbd79-f384-4d89-9dfc-6c4a606ceec6 |
| name | None |
| object_id | 84a7e627-573b-49da-af66-c9a65244f3ce |
| object_type | network |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| target_project_id | * |
+-------------------+--------------------------------------+
The output shows that the entry allows the action access_as_shared on object 84a7e627-573b-49da-af66-
c9a65244f3ce of type network to target_tenant *, which is a wildcard that represents all projects.
Currently, the shared flag is just a mapping to the underlying RBAC policies for a network. Setting the flag
to True on a network creates a wildcard RBAC entry. Setting it to False removes the wildcard entry.
When you run openstack network list or openstack network show, the shared flag is calculated by
the server based on the calling project and the RBAC entries for each network. For QoS objects use openstack
network qos policy list or openstack network qos policy show respectively. If there is a wildcard
entry, the shared flag is always set to True. If there are only entries that share with specific projects, only the
projects the object is shared to will see the flag as True and the rest will see the flag as False.
Allowing a network to be used as an external network
To make a network available as an external network for specific projects rather than all projects, use the access_as_external
action.
1. Create a network that you want to be available as an external network:
$ openstack network create secret_external_network
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2017-01-25T20:36:59Z |
132 Configuration
Networking Guide (Release Version: 15.0.0)
| description | |
| dns_domain | None |
| id | 802d4e9e-4649-43e6-9ee2-8d052a880cfb |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| is_default | None |
| mtu | 1450 |
| name | secret_external_network |
| port_security_enabled | True |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| proider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 21 |
| qos_policy_id | None |
| revision_number | 3 |
| router:external | Internal |
| segments | None |
| shared | False |
| status | ACTIVE |
| subnets | |
| updated_at | 2017-01-25T20:36:59Z |
+---------------------------+--------------------------------------+
2. Create a policy entry using the openstack network rbac create command (in this example, the ID
of the project we want to share with is 838030a7bf3c4d04b4b054c0f0b2b17c):
$ openstack network rbac create --target-project \
838030a7bf3c4d04b4b054c0f0b2b17c --action access_as_external \
--type network 802d4e9e-4649-43e6-9ee2-8d052a880cfb
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| action | access_as_external |
| id | afdd5b8d-b6f5-4a15-9817-5231434057be |
| name | None |
| object_id | 802d4e9e-4649-43e6-9ee2-8d052a880cfb |
| object_type | network |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| target_project_id | 838030a7bf3c4d04b4b054c0f0b2b17c |
+-------------------+--------------------------------------+
The target-project parameter specifies the project that requires access to the network. The action parameter
specifies what the project is allowed to do. The type parameter indicates that the target object is a network.
The final parameter is the ID of the network we are granting external access to.
Now project 838030a7bf3c4d04b4b054c0f0b2b17c is able to see the network when running openstack
network list and openstack network show and can attach router gateway ports to that network. No other
users (other than admins and the owner) are able to see the network.
To remove access for that project, delete the policy that allows it using the openstack network rbac delete
command:
$ openstack network rbac delete afdd5b8d-b6f5-4a15-9817-5231434057be
If that project has router gateway ports attached to that network, the server prevents the policy from being
deleted until the ports have been deleted:
Configuration 133
Networking Guide (Release Version: 15.0.0)
$ openstack network rbac delete afdd5b8d-b6f5-4a15-9817-5231434057be
RBAC policy on object afdd5b8d-b6f5-4a15-9817-5231434057be
cannot be removed because other objects depend on it.
This process can be repeated any number of times to make a network available as external to an arbitrary number
of projects.
If a network is marked as external during creation, it now implicitly creates a wildcard RBAC policy granting
everyone access to preserve previous behavior before this feature was added.
$ openstack network create global_external_network --external
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| created_at | 2017-01-25T20:41:44Z |
| description | |
| dns_domain | None |
| id | 72a257a2-a56e-4ac7-880f-94a4233abec6 |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| is_default | None |
| mtu | 1450 |
| name | global_external_network |
| port_security_enabled | True |
| project_id | 61b7eba037fd41f29cfba757c010faff |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 69 |
| qos_policy_id | None |
| revision_number | 4 |
| router:external | External |
| segments | None |
| shared | False |
| status | ACTIVE |
| subnets | |
| updated_at | 2017-01-25T20:41:44Z |
+---------------------------+--------------------------------------+
In the output above the standard router:external attribute is External as expected. Now a wildcard policy
is visible in the RBAC policy listings:
$ openstack network rbac list --long -c ID -c Action
+--------------------------------------+--------------------+
| ID | Action |
+--------------------------------------+--------------------+
| b694e541-bdca-480d-94ec-eda59ab7d71a | access_as_external |
+--------------------------------------+--------------------+
You can modify or delete this policy with the same constraints as any other RBAC access_as_external
policy.
134 Configuration
Networking Guide (Release Version: 15.0.0)
Preventing regular users from sharing objects with each other
The default policy.json file will not allow regular users to share objects with every other project using a
wildcard; however, it will allow them to share objects with specific project IDs.
If an operator wants to prevent normal users from doing this, the "create_rbac_policy": entry in policy.
json can be adjusted from "" to "rule:admin_only".
Routed provider networks
Note: Experimental feature. Use of this feature requires the OpenStack client version 3.3 or newer.
Before routed provider networks, the Networking service could not present a multi-segment layer-3 network as
a single entity. Thus, each operator typically chose one of the following architectures:
• Single large layer-2 network
• Multiple smaller layer-2 networks
Single large layer-2 networks become complex at scale and involve significant failure domains.
Multiple smaller layer-2 networks scale better and shrink failure domains, but leave network selection to the
user. Without additional information, users cannot easily differentiate these networks.
A routed provider network enables a single provider network to represent multiple layer-2 networks (broadcast
domains) or segments and enables the operator to present one network to users. However, the particular IP
addresses available to an instance depend on the segment of the network available on the particular compute
node.
Similar to conventional networking, layer-2 (switching) handles transit of traffic between ports on the same
segment and layer-3 (routing) handles transit of traffic between segments.
Each segment requires at least one subnet that explicitly belongs to that segment. The association between a
segment and a subnet distinguishes a routed provider network from other types of networks. The Networking
service enforces that either zero or all subnets on a particular network associate with a segment. For example,
attempting to create a subnet without a segment on a network containing subnets with segments generates an
error.
The Networking service does not provide layer-3 services between segments. Instead, it relies on physical
network infrastructure to route subnets. Thus, both the Networking service and physical network infrastructure
must contain configuration for routed provider networks, similar to conventional provider networks. In the
future, implementation of dynamic routing protocols may ease configuration of routed networks.
Limitations
The Newton implementation contains the following limitations:
• The Compute scheduler lacks awareness of IP address resources on a routed network. Thus, the scheduler
could exhaust the IP addresses in one segment before assigning IP addresses from another segment. The
Ocata release of the Compute scheduler should provide the capability of scheduling around segments
without available IP addresses. In Newton, the Compute scheduler selects any compute node on the
provider network. If the segment on that compute node lacks available IP addresses, port binding fails
and the Compute scheduler chooses another compute node without regard to segments. Rescheduling
Configuration 135
Networking Guide (Release Version: 15.0.0)
continues up to the maximum number of retries. Operators should monitor IP usage and add subnets to
segments prior to exhaustion of IP addresses. For more information, see the blueprint.
• A routed provider network cannot also function as a router:external networks which prevents compatibility
with floating IPv4 addresses. Additional routing, possibly using BGP dynamic routing, could
address this issue in the future.
Prerequisites
Routed provider networks require additional prerequisites over conventional provider networks. We recommend
using the following procedure:
1. Begin with segments. The Networking service defines a segment using the following components:
• Unique physical network name
• Segmentation type
• Segmentation ID
For example, provider1, VLAN, and 2016. See the API reference for more information.
Within a network, use a unique physical network name for each segment which enables reuse of the same
segmentation details between subnets. For example, using the same VLAN ID across all segments of a
particular provider network. Similar to conventional provider networks, the operator must provision the
layer-2 physical network infrastructure accordingly.
2. Implement routing between segments.
The Networking service does not provision routing among segments. The operator must implement
routing among segments of a provider network. Each subnet on a segment must contain the gateway
address of the router interface on that particular subnet. For example:
Segment Version Addresses Gateway
segment1 4 203.0.113.0/24 203.0.113.1
segment1 6 fd00:203:0:113::/64 fd00:203:0:113::1
segment2 4 198.51.100.0/24 198.51.100.1
segment2 6 fd00:198:51:100::/64 fd00:198:51:100::1
3. Map segments to compute nodes.
Routed provider networks imply that compute nodes reside on different segments. The operator must
ensure that every compute host that is supposed to participate in a router provider network has direct
connectivity to one of its segments.
Host Rack Physical Network
compute0001 rack 1 segment 1
compute0002 rack 1 segment 1
... ... ...
compute0101 rack 2 segment 2
compute0102 rack 2 segment 2
compute0102 rack 2 segment 2
... ... ...
4. Deploy DHCP agents.
Unlike conventional provider networks, a DHCP agent cannot support more than one segment within a
network. The operator must deploy at least one DHCP agent per segment. Consider deploying DHCP
136 Configuration
Networking Guide (Release Version: 15.0.0)
agents on compute nodes containing the segments rather than one or more network nodes to reduce node
count.
Host Rack Physical Network
network0001 rack 1 segment 1
network0002 rack 2 segment 2
... ... ...
5. Configure communication of the Networking service with the Compute scheduler.
An instance with an interface with an IPv4 address in a routed provider network must be placed by
the Compute scheduler in a host that has access to a segment with available IPv4 addresses. To make
this possible, the Networking service communicates to the Compute scheduler the inventory of IPv4
addresses associated with each segment of a routed provider network. The operator must configure
the authentication credentials that the Networking service will use to communicate with the Compute
scheduler’s placement API. Please see below an example configuration.
Note: Coordination between the Networking service and the Compute scheduler is not necessary for
IPv6 subnets as a consequence of their large address spaces.
Note: The coordination between the Networking service and the Compute scheduler requires the following
minimum API micro-versions.
• Compute service API: 2.41
• Placement API: 1.1
Example configuration
Controller node
1. Enable the segments service plug-in by appending segments to the list of service_plugins in the
neutron.conf file on all nodes running the neutron-server service:
[DEFAULT]
# ...
service_plugins = ..., segments
2. Add a placement section to the neutron.conf file with authentication credentials for the Compute
service placement API:
[placement]
auth_uri = http://192.0.2.72/identity
project_domain_name = Default
project_name = service
user_domain_name = Default
password = apassword
username = nova
auth_url = http://192.0.2.72/identity_admin
auth_type = password
region_name = RegionOne
Configuration 137
Networking Guide (Release Version: 15.0.0)
3. Restart the neutron-server service.
Network or compute nodes
• Configure the layer-2 agent on each node to map one or more segments to the appropriate physical
network bridge or interface and restart the agent.
Create a routed provider network
The following steps create a routed provider network with two segments. Each segment contains one IPv4
subnet and one IPv6 subnet.
1. Source the administrative project credentials.
2. Create a VLAN provider network which includes a default segment. In this example, the network uses
the provider1 physical network with VLAN ID 2016.
$ openstack network create --share --provider-physical-network provider1 \
--provider-network-type vlan --provider-segment 2016 multisegment1
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| id | 6ab19caa-dda9-4b3d-abc4-5b8f435b98d9 |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| l2_adjacency | True |
| mtu | 1500 |
| name | multisegment1 |
| port_security_enabled | True |
| provider:network_type | vlan |
| provider:physical_network | provider1 |
| provider:segmentation_id | 2016 |
| router:external | Internal |
| shared | True |
| status | ACTIVE |
| subnets | |
| tags | [] |
+---------------------------+--------------------------------------+
3. Rename the default segment to segment1.
$ openstack network segment list --network multisegment1
+--------------------------------------+----------+-----------------------------------
,→---+--------------+---------+
| ID | Name | Network
,→ | Network Type | Segment |
+--------------------------------------+----------+-----------------------------------
,→---+--------------+---------+
| 43e16869-ad31-48e4-87ce-acf756709e18 | None | 6ab19caa-dda9-4b3d-abc4-
,→5b8f435b98d9 | vlan | 2016 |
+--------------------------------------+----------+-----------------------------------
,→---+--------------+---------+
138 Configuration
Networking Guide (Release Version: 15.0.0)
$ openstack network segment set --name segment1 43e16869-ad31-48e4-87ce-acf756709e18
Note: This command provides no output.
4. Create a second segment on the provider network. In this example, the segment uses the provider2
physical network with VLAN ID 2016.
$ openstack network segment create --physical-network provider2 \
--network-type vlan --segment 2016 --network multisegment1 segment2
+------------------+--------------------------------------+
| Field | Value |
+------------------+--------------------------------------+
| description | None |
| headers | |
| id | 053b7925-9a89-4489-9992-e164c8cc8763 |
| name | segment2 |
| network_id | 6ab19caa-dda9-4b3d-abc4-5b8f435b98d9 |
| network_type | vlan |
| physical_network | provider2 |
| segmentation_id | 2016 |
+------------------+--------------------------------------+
5. Verify that the network contains the segment1 and segment2 segments.
$ openstack network segment list --network multisegment1
+--------------------------------------+----------+-----------------------------------
,→---+--------------+---------+
| ID | Name | Network
,→ | Network Type | Segment |
+--------------------------------------+----------+-----------------------------------
,→---+--------------+---------+
| 053b7925-9a89-4489-9992-e164c8cc8763 | segment2 | 6ab19caa-dda9-4b3d-abc4-
,→5b8f435b98d9 | vlan | 2016 |
| 43e16869-ad31-48e4-87ce-acf756709e18 | segment1 | 6ab19caa-dda9-4b3d-abc4-
,→5b8f435b98d9 | vlan | 2016 |
+--------------------------------------+----------+-----------------------------------
,→---+--------------+---------+
6. Create subnets on the segment1 segment. In this example, the IPv4 subnet uses 203.0.113.0/24 and the
IPv6 subnet uses fd00:203:0:113::/64.
$ openstack subnet create \
--network multisegment1 --network-segment segment1 \
--ip-version 4 --subnet-range 203.0.113.0/24 \
multisegment1-segment1-v4
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| allocation_pools | 203.0.113.2-203.0.113.254 |
| cidr | 203.0.113.0/24 |
| enable_dhcp | True |
| gateway_ip | 203.0.113.1 |
| id | c428797a-6f8e-4cb1-b394-c404318a2762 |
| ip_version | 4 |
| name | multisegment1-segment1-v4 |
Configuration 139
Networking Guide (Release Version: 15.0.0)
| network_id | 6ab19caa-dda9-4b3d-abc4-5b8f435b98d9 |
| segment_id | 43e16869-ad31-48e4-87ce-acf756709e18 |
+-------------------+--------------------------------------+
$ openstack subnet create \
--network multisegment1 --network-segment segment1 \
--ip-version 6 --subnet-range fd00:203:0:113::/64 \
--ipv6-address-mode slaac multisegment1-segment1-v6
+-------------------+------------------------------------------------------+
| Field | Value |
+-------------------+------------------------------------------------------+
| allocation_pools | fd00:203:0:113::2-fd00:203:0:113:ffff:ffff:ffff:ffff |
| cidr | fd00:203:0:113::/64 |
| enable_dhcp | True |
| gateway_ip | fd00:203:0:113::1 |
| id | e41cb069-9902-4c01-9e1c-268c8252256a |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | None |
| name | multisegment1-segment1-v6 |
| network_id | 6ab19caa-dda9-4b3d-abc4-5b8f435b98d9 |
| segment_id | 43e16869-ad31-48e4-87ce-acf756709e18 |
+-------------------+------------------------------------------------------+
Note: By default, IPv6 subnets on provider networks rely on physical network infrastructure for stateless
address autoconfiguration (SLAAC) and router advertisement.
7. Create subnets on the segment2 segment. In this example, the IPv4 subnet uses 198.51.100.0/24 and
the IPv6 subnet uses fd00:198:51:100::/64.
$ openstack subnet create \
--network multisegment1 --network-segment segment2 \
--ip-version 4 --subnet-range 198.51.100.0/24 \
multisegment1-segment2-v4
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| allocation_pools | 198.51.100.2-198.51.100.254 |
| cidr | 198.51.100.0/24 |
| enable_dhcp | True |
| gateway_ip | 198.51.100.1 |
| id | 242755c2-f5fd-4e7d-bd7a-342ca95e50b2 |
| ip_version | 4 |
| name | multisegment1-segment2-v4 |
| network_id | 6ab19caa-dda9-4b3d-abc4-5b8f435b98d9 |
| segment_id | 053b7925-9a89-4489-9992-e164c8cc8763 |
+-------------------+--------------------------------------+
$ openstack subnet create \
--network multisegment1 --network-segment segment2 \
--ip-version 6 --subnet-range fd00:198:51:100::/64 \
--ipv6-address-mode slaac multisegment1-segment2-v6
+-------------------+--------------------------------------------------------+
| Field | Value |
+-------------------+--------------------------------------------------------+
140 Configuration
Networking Guide (Release Version: 15.0.0)
| allocation_pools | fd00:198:51:100::2-fd00:198:51:100:ffff:ffff:ffff:ffff |
| cidr | fd00:198:51:100::/64 |
| enable_dhcp | True |
| gateway_ip | fd00:198:51:100::1 |
| id | b884c40e-9cfe-4d1b-a085-0a15488e9441 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | None |
| name | multisegment1-segment2-v6 |
| network_id | 6ab19caa-dda9-4b3d-abc4-5b8f435b98d9 |
| segment_id | 053b7925-9a89-4489-9992-e164c8cc8763 |
+-------------------+--------------------------------------------------------+
8. Verify that each IPv4 subnet associates with at least one DHCP agent.
$ neutron dhcp-agent-list-hosting-net multisegment1
+--------------------------------------+-------------+----------------+-------+
| id | host | admin_state_up | alive |
+--------------------------------------+-------------+----------------+-------+
| c904ed10-922c-4c1a-84fd-d928abaf8f55 | compute0001 | True | :-) |
| e0b22cc0-d2a6-4f1c-b17c-27558e20b454 | compute0101 | True | :-) |
+--------------------------------------+-------------+----------------+-------+
9. Verify that inventories were created for each segment IPv4 subnet in the Compute service placement API
(for the sake of brevity, only one of the segments is shown in this example).
$ SEGMENT_ID=053b7925-9a89-4489-9992-e164c8cc8763
$ curl -s -X GET \
http://localhost/placement/resource_providers/$SEGMENT_ID/inventories \
-H "Content-type: application/json" \
-H "X-Auth-Token: $TOKEN" \
-H "Openstack-Api-Version: placement 1.1"
{
"resource_provider_generation": 1,
"inventories": {
"allocation_ratio": 1,
"total": 254,
"reserved": 2,
"step_size": 1,
"min_unit": 1,
"max_unit": 1
}
}
Note: As of the writing of this guide, there is not placement API CLI client, so the curl command is
used for this example.
10. Verify that host aggregates were created for each segment in the Compute service (for the sake of brevity,
only one of the segments is shown in this example).
$ nova aggregate-list
+----+---------------------------------------------------------+-------------------+
| Id | Name | Availability Zone |
+----+---------------------------------------------------------+-------------------+
Configuration 141
Networking Guide (Release Version: 15.0.0)
| 10 | Neutron segment id 053b7925-9a89-4489-9992-e164c8cc8763 | |
+----+---------------------------------------------------------+-------------------+
11. Launch one or more instances. Each instance obtains IP addresses according to the segment it uses on
the particular compute node.
Note: Creating a port and passing it to an instance yields a different behavior than conventional networks.
The Networking service defers assignment of IP addresses to the port until the particular compute
node becomes apparent. For example:
$ openstack port create --network multisegment1 port1
+-----------------------+--------------------------------------+
| Field | Value |
+-----------------------+--------------------------------------+
| admin_state_up | UP |
| binding_vnic_type | normal |
| id | 6181fb47-7a74-4add-9b6b-f9837c1c90c4 |
| ip_allocation | deferred |
| mac_address | fa:16:3e:34:de:9b |
| name | port1 |
| network_id | 6ab19caa-dda9-4b3d-abc4-5b8f435b98d9 |
| port_security_enabled | True |
| security_groups | e4fcef0d-e2c5-40c3-a385-9c33ac9289c5 |
| status | DOWN |
+-----------------------+--------------------------------------+
Service function chaining
Service function chain (SFC) essentially refers to the software-defined networking (SDN) version of policybased
routing (PBR). In many cases, SFC involves security, although it can include a variety of other features.
Fundamentally, SFC routes packets through one or more service functions instead of conventional routing that
routes packets using destination IP address. Service functions essentially emulate a series of physical network
devices with cables linking them together.
A basic example of SFC involves routing packets from one location to another through a firewall that lacks a
“next hop” IP address from a conventional routing perspective. A more complex example involves an ordered
series of service functions, each implemented using multiple instances (VMs). Packets must flow through one
instance and a hashing algorithm distributes flows across multiple instances at each hop.
Architecture
All OpenStack Networking services and OpenStack Compute instances connect to a virtual network via ports
making it possible to create a traffic steering model for service chaining using only ports. Including these ports
in a port chain enables steering of traffic through one or more instances providing service functions.
A port chain, or service function path, consists of the following:
• A set of ports that define the sequence of service functions.
• A set of flow classifiers that specify the classified traffic flows entering the chain.
142 Configuration
Networking Guide (Release Version: 15.0.0)
If a service function involves a pair of ports, the first port acts as the ingress port of the service function and the
second port acts as the egress port. If both ports use the same value, they function as a single virtual bidirectional
port.
A port chain is a unidirectional service chain. The first port acts as the head of the service function chain and
the second port acts as the tail of the service function chain. A bidirectional service function chain consists of
two unidirectional port chains.
A flow classifier can only belong to one port chain to prevent ambiguity as to which chain should handle packets
in the flow. A check prevents such ambiguity. However, you can associate multiple flow classifiers with a port
chain because multiple flows can request the same service function path.
Currently, SFC lacks support for multi-project service functions.
The port chain plug-in supports backing service providers including the OVS driver and a variety of SDN
controller drivers. The common driver API enables different drivers to provide different implementations for
the service chain path rendering.
See the developer documentation for more information.
Resources
Port chain
• id - Port chain ID
• tenant_id - Project ID
• name - Readable name
• description - Readable description
Configuration 143
Networking Guide (Release Version: 15.0.0)
• port_pair_groups - List of port pair group IDs
• flow_classifiers - List of flow classifier IDs
• chain_parameters - Dictionary of chain parameters
A port chain consists of a sequence of port pair groups. Each port pair group is a hop in the port chain. A group
of port pairs represents service functions providing equivalent functionality. For example, a group of firewall
service functions.
A flow classifier identifies a flow. A port chain can contain multiple flow classifiers. Omitting the flow classifier
effectively prevents steering of traffic through the port chain.
The chain_parameters attribute contains one or more parameters for the port chain. Currently, it only supports
a correlation parameter that defaults to mpls for consistency with Open vSwitch (OVS) capabilities. Future
values for the correlation parameter may include the network service header (NSH).
Port pair group
• id - Port pair group ID
• tenant_id - Project ID
• name - Readable name
• description - Readable description
• port_pairs - List of service function port pairs
A port pair group may contain one or more port pairs. Multiple port pairs enable load balancing/distribution
over a set of functionally equivalent service functions.
Port pair
• id - Port pair ID
• tenant_id - Project ID
• name - Readable name
• description - Readable description
• ingress - Ingress port
• egress - Egress port
• service_function_parameters - Dictionary of service function parameters
A port pair represents a service function instance that includes an ingress and egress port. A service function
containing a bidirectional port uses the same ingress and egress port.
The service_function_parameters attribute includes one or more parameters for the service function. Currently,
it only supports a correlation parameter that determines association of a packet with a chain. This parameter
defaults to none for legacy service functions that lack support for correlation such as the NSH. If set to
none, the data plane implementation must provide service function proxy functionality.
144 Configuration
Networking Guide (Release Version: 15.0.0)
Flow classifier
• id - Flow classifier ID
• tenant_id - Project ID
• name - Readable name
• description - Readable description
• ethertype - Ethertype (IPv4/IPv6)
• protocol - IP protocol
• source_port_range_min - Minimum source protocol port
• source_port_range_max - Maximum source protocol port
• destination_port_range_min - Minimum destination protocol port
• destination_port_range_max - Maximum destination protocol port
• source_ip_prefix - Source IP address or prefix
• destination_ip_prefix - Destination IP address or prefix
• logical_source_port - Source port
• logical_destination_port - Destination port
• l7_parameters - Dictionary of L7 parameters
A combination of the source attributes defines the source of the flow. A combination of the destination attributes
defines the destination of the flow. The l7_parameters attribute is a place holder that may be used
to support flow classification using layer 7 fields, such as a URL. If unspecified, the logical_source_port
and logical_destination_port attributes default to none, the ethertype attribute defaults to IPv4, and
all other attributes default to a wildcard value.
Operations
Create a port chain
The following example uses the neutron command-line interface (CLI) to create a port chain consisting of three
service function instances to handle HTTP (TCP) traffic flows from 192.0.2.11:1000 to 198.51.100.11:80.
• Instance 1
– Name: vm1
– Function: Firewall
– Port pair: [p1, p2]
• Instance 2
– Name: vm2
– Function: Firewall
– Port pair: [p3, p4]
• Instance 3
Configuration 145
Networking Guide (Release Version: 15.0.0)
– Name: vm3
– Function: Intrusion detection system (IDS)
– Port pair: [p5, p6]
Note: The example network net1 must exist before creating ports on it.
1. Source the credentials of the project that owns the net1 network.
2. Create ports on network net1 and record the UUID values.
$ openstack port create p1 --network net1
$ openstack port create p2 --network net1
$ openstack port create p3 --network net1
$ openstack port create p4 --network net1
$ openstack port create p5 --network net1
$ openstack port create p6 --network net1
3. Launch service function instance vm1 using ports p1 and p2, vm2 using ports p3 and p4, and vm3 using
ports p5 and p6.
$ openstack server create --nic port-id=P1_ID --nic port-id=P2_ID vm1
$ openstack server create --nic port-id=P3_ID --nic port-id=P4_ID vm2
$ openstack server create --nic port-id=P5_ID --nic port-id=P6_ID vm3
Replace P1_ID, P2_ID, P3_ID, P4_ID, P5_ID, and P6_ID with the UUIDs of the respective ports.
Note: This command requires additional options to successfully launch an instance. See the CLI reference
for more information.
Alternatively, you can launch each instance with one network interface and attach additional ports later.
4. Create flow classifier FC1 that matches the appropriate packet headers.
$ neutron flow-classifier-create \
--description "HTTP traffic from 192.0.2.11 to 198.51.100.11" \
--ethertype IPv4 \
--source-ip-prefix 192.0.2.11/32 \
--destination-ip-prefix 198.51.100.11/32 \
--protocol tcp \
--source-port 1000:1000 \
--destination-port 80:80 FC1
5. Create port pair PP1 with ports p1 and p2, PP2 with ports p3 and p4, and PP3 with ports p5 and p6.
$ neutron port-pair-create \
--description "Firewall SF instance 1" \
--ingress p1 \
--egress p2 PP1
$ neutron port-pair-create \
--description "Firewall SF instance 2" \
--ingress p3 \
--egress p4 PP2
146 Configuration
Networking Guide (Release Version: 15.0.0)
$ neutron port-pair-create \
--description "IDS SF instance" \
--ingress p5 \
--egress p6 PP3
6. Create port pair group PPG1 with port pair PP1 and PP2 and PPG2 with port pair PP3.
$ neutron port-pair-group-create \
--port-pair PP1 --port-pair PP2 PPG1
$ neutron port-pair-group-create \
--port-pair PP3 PPG2
Note: You can repeat the --port-pair option for multiple port pairs of functionally equivalent service
functions.
7. Create port chain PC1 with port pair groups PPG1 and PPG2 and flow classifier FC1.
$ neutron port-chain-create \
--port-pair-group PPG1 --port-pair-group PPG2 \
--flow-classifier FC1 PC1
Note: You can repeat the --port-pair-group option to specify additional port pair groups in the port
chain. A port chain must contain at least one port pair group.
You can repeat the --flow-classifier option to specify multiple flow classifiers for a port chain.
Each flow classifier identifies a flow.
Update a port chain or port pair group
• Use the neutron port-chain-update command to dynamically add or remove port pair groups or
flow classifiers on a port chain.
– For example, add port pair group PPG3 to port chain PC1:
$ neutron port-chain-update \
--port-pair-group PPG1 --port-pair-group PPG2 --port-pair-group PPG3 \
--flow-classifier FC1 PC1
– For example, add flow classifier FC2 to port chain PC1:
$ neutron port-chain-update \
--port-pair-group PPG1 --port-pair-group PPG2 \
--flow-classifier FC1 --flow-classifier FC2 PC1
SFC steers traffic matching the additional flow classifier to the port pair groups in the port chain.
• Use the neutron port-pair-group-update command to perform dynamic scale-out or scale-in operations
by adding or removing port pairs on a port pair group.
Configuration 147
Networking Guide (Release Version: 15.0.0)
$ neutron port-pair-group-update \
--port-pair PP1 --port-pair PP2 --port-pair PP4 PPG1
SFC performs load balancing/distribution over the additional service functions in the port pair group.
SR-IOV
The purpose of this page is to describe how to enable SR-IOV functionality available in OpenStack (using
OpenStack Networking). This functionality was first introduced in the OpenStack Juno release. This page
intends to serve as a guide for how to configure OpenStack Networking and OpenStack Compute to create
SR-IOV ports.
The basics
PCI-SIG Single Root I/O Virtualization and Sharing (SR-IOV) functionality is available in OpenStack since
the Juno release. The SR-IOV specification defines a standardized mechanism to virtualize PCIe devices. This
mechanism can virtualize a single PCIe Ethernet controller to appear as multiple PCIe devices. Each device
can be directly assigned to an instance, bypassing the hypervisor and virtual switch layer. As a result, users are
able to achieve low latency and near-line wire speed.
The following terms are used throughout this document:
Term Definition
PF Physical Function. The physical Ethernet controller that supports SR-IOV.
VF Virtual Function. The virtual PCIe device created from a physical Ethernet controller.
SR-IOV agent
The SR-IOV agent allows you to set the admin state of ports, configure port security (enable and disable spoof
checking), and configure QoS rate limiting. You must include the SR-IOV agent on each compute node using
SR-IOV ports.
Note: The SR-IOV agent was optional before Mitaka, and was not enabled by default before Liberty.
Note: The ability to control port security and QoS rate limit settings was added in Liberty.
Supported Ethernet controllers
The following manufacturers are known to work:
• Intel
• Mellanox
• QLogic
148 Configuration
Networking Guide (Release Version: 15.0.0)
For information on Mellanox SR-IOV Ethernet ConnectX-3/ConnectX-3 Pro cards, see Mellanox: How To
Configure SR-IOV VFs.
For information on QLogic SR-IOV Ethernet cards, see User’s Guide OpenStack Deployment with SR-IOV
Configuration.
Using SR-IOV interfaces
In order to enable SR-IOV, the following steps are required:
1. Create Virtual Functions (Compute)
2. Whitelist PCI devices in nova-compute (Compute)
3. Configure neutron-server (Controller)
4. Configure nova-scheduler (Controller)
5. Enable neutron sriov-agent (Compute)
We recommend using VLAN provider networks for segregation. This way you can combine instances without
SR-IOV ports and instances with SR-IOV ports on a single network.
Note: Throughout this guide, eth3 is used as the PF and physnet2 is used as the provider network configured
as a VLAN range. These ports may vary in different environments.
Create Virtual Functions (Compute)
Create the VFs for the network interface that will be used for SR-IOV. We use eth3 as PF, which is also used
as the interface for the VLAN provider network and has access to the private networks of all machines.
Note: The steps detail how to create VFs using Mellanox ConnectX-4 and newer/Intel SR-IOV Ethernet cards
on an Intel system. Steps may differ for different hardware configurations.
1. Ensure SR-IOV and VT-d are enabled in BIOS.
2. Enable IOMMU in Linux by adding intel_iommu=on to the kernel parameters, for example, using
GRUB.
3. On each compute node, create the VFs via the PCI SYS interface:
# echo '8' > /sys/class/net/eth3/device/sriov_numvfs
Note: On some PCI devices, observe that when changing the amount of VFs you receive the error
Device or resource busy. In this case, you must first set sriov_numvfs to 0, then set it to your
new value.
Configuration 149
Networking Guide (Release Version: 15.0.0)
Warning: Alternatively, you can create VFs by passing the max_vfs to the kernel module of your
network interface. However, the max_vfs parameter has been deprecated, so the PCI SYS interface
is the preferred method.
You can determine the maximum number of VFs a PF can support:
# cat /sys/class/net/eth3/device/sriov_totalvfs
63
4. Verify that the VFs have been created and are in up state:
# lspci | grep Ethernet
82:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network
,→Connection (rev 01)
82:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network
,→Connection (rev 01)
82:10.0 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual
,→Function (rev 01)
82:10.2 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual
,→Function (rev 01)
82:10.4 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual
,→Function (rev 01)
82:10.6 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual
,→Function (rev 01)
82:11.0 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual
,→Function (rev 01)
82:11.2 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual
,→Function (rev 01)
82:11.4 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual
,→Function (rev 01)
82:11.6 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual
,→Function (rev 01)
# ip link show eth3
8: eth3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT
,→qlen 1000
link/ether a0:36:9f:8f:3f:b8 brd ff:ff:ff:ff:ff:ff
vf 0 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
vf 1 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
vf 2 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
vf 3 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
vf 4 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
vf 5 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
vf 6 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
vf 7 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
If the interfaces are down, set them to up before launching a guest, otherwise the instance will fail to
spawn:
# ip link set eth3 up
5. Persist created VFs on reboot:
# echo "echo '7' > /sys/class/net/eth3/device/sriov_numvfs" >> /etc/rc.local
150 Configuration
Networking Guide (Release Version: 15.0.0)
Note: The suggested way of making PCI SYS settings persistent is through the sysfsutils tool.
However, this is not available by default on many major distributions.
Whitelist PCI devices nova-compute (Compute)
1. Configure which PCI devices the nova-compute service may use. Edit the nova.conf file:
[default]
pci_passthrough_whitelist = { "devname": "eth3", "physical_network": "physnet2"}
This tells the Compute service that all VFs belonging to eth3 are allowed to be passed through to instances
and belong to the provider network physnet2.
Alternatively the pci_passthrough_whitelist parameter also supports whitelisting by:
• PCI address: The address uses the same syntax as in lspci and an asterisk (*) can be used to match
anything.
pci_passthrough_whitelist = { "address": "[[[[<domain>]:]<bus>]:][<slot>][.[
,→<function>]]", "physical_network": "physnet2" }
For example, to match any domain, bus 0a, slot 00, and all functions:
pci_passthrough_whitelist = { "address": "*:0a:00.*", "physical_network":
,→"physnet2" }
• PCI vendor_id and product_id as displayed by the Linux utility lspci.
pci_passthrough_whitelist = { "vendor_id": "<id>", "product_id": "<id>",
,→"physical_network": "physnet2" }
If the device defined by the PCI address or devname corresponds to an SR-IOV PF, all VFs under the PF
will match the entry. Multiple pci_passthrough_whitelist entries per host are supported.
2. Restart the nova-compute service for the changes to go into effect.
Configure neutron-server (Controller)
1. Add sriovnicswitch as mechanism driver. Edit the ml2_conf.ini file on each controller:
mechanism_drivers = openvswitch,sriovnicswitch
2. Add the ml2_conf_sriov.ini file as parameter to the neutron-server service. Edit the appropriate
initialization script to configure the neutron-server service to load the SR-IOV configuration file:
--config-file /etc/neutron/neutron.conf
--config-file /etc/neutron/plugin.ini
--config-file /etc/neutron/plugins/ml2/ml2_conf_sriov.ini
3. Restart the neutron-server service.
Configuration 151
Networking Guide (Release Version: 15.0.0)
Configure nova-scheduler (Controller)
1. On every controller node running the nova-scheduler service, add PciPassthroughFilter
to scheduler_default_filters to enable PciPassthroughFilter by default. Also ensure
scheduler_available_filters parameter under the [DEFAULT] section in nova.conf is set to
all_filters to enable all filters provided by the Compute service.
[DEFAULT]
scheduler_default_filters = RetryFilter, AvailabilityZoneFilter, RamFilter,
,→ComputeFilter, ComputeCapabilitiesFilter, ImagePropertiesFilter,
,→ServerGroupAntiAffinityFilter, ServerGroupAffinityFilter, PciPassthroughFilter
scheduler_available_filters = nova.scheduler.filters.all_filters
2. Restart the nova-scheduler service.
Enable neutron sriov-agent (Compute)
1. Install the SR-IOV agent.
2. Edit the sriov_agent.ini file on each compute node. For example:
[securitygroup]
firewall_driver = neutron.agent.firewall.NoopFirewallDriver
[sriov_nic]
physical_device_mappings = physnet2:eth3
exclude_devices =
Note: The physical_device_mappings parameter is not limited to be a 1-1 mapping between physical
networks and NICs. This enables you to map the same physical network to more than one NIC. For
example, if physnet2 is connected to eth3 and eth4, then physnet2:eth3,physnet2:eth4 is a valid
option.
The exclude_devices parameter is empty, therefore, all the VFs associated with eth3 may be configured
by the agent. To exclude specific VFs, add them to the exclude_devices parameter as follows:
exclude_devices = eth1:0000:07:00.2;0000:07:00.3,eth2:0000:05:00.1;0000:05:00.2
3. Ensure the neutron sriov-agent runs successfully:
# neutron-sriov-nic-agent \
--config-file /etc/neutron/neutron.conf \
--config-file /etc/neutron/plugins/ml2/sriov_agent.ini
4. Enable the neutron sriov-agent service.
If installing from source, you must configure a daemon file for the init system manually.
(Optional) FDB L2 agent extension
Forwarding DataBase (FDB) population is an L2 agent extension to OVS agent or Linux bridge. Its objective is
to update the FDB table for existing instance using normal port. This enables communication between SR-IOV
152 Configuration
Networking Guide (Release Version: 15.0.0)
instances and normal instances. The use cases of the FDB population extension are:
• Direct port and normal port instances reside on the same compute node.
• Direct port instance that uses floating IP address and network node are located on the same host.
For additional information describing the problem, refer to: Virtual switching technologies and Linux bridge.
1. Edit the ovs_agent.ini or linuxbridge_agent.ini file on each compute node. For example:
[agent]
extensions = fdb
2. Add the FDB section and the shared_physical_device_mappings parameter. This parameter maps
each physical port to its physical network name. Each physical network can be mapped to several ports:
[FDB]
shared_physical_device_mappings = physnet1:p1p1, physnet1:p1p2
Launching instances with SR-IOV ports
Once configuration is complete, you can launch instances with SR-IOV ports.
1. Get the id of the network where you want the SR-IOV port to be created:
$ net_id=`neutron net-show net04 | grep "\ id\ " | awk '{ print $4 }'`
2. Create the SR-IOV port. vnic_type=direct is used here, but other options include normal, directphysical,
and macvtap:
$ port_id=`neutron port-create $net_id --name sriov_port --binding:vnic_type direct |
,→grep "\ id\ " | awk '{ print $4 }'`
3. Create the instance. Specify the SR-IOV port created in step two for the NIC:
$ openstack server create --flavor m1.large --image ubuntu_14.04 --nic port-id=$port_
,→id test-sriov
Note: There are two ways to attach VFs to an instance. You can create an SR-IOV port or use the
pci_alias in the Compute service. For more information about using pci_alias, refer to nova-api
configuration.
SR-IOV with InfiniBand
The support for SR-IOV with InfiniBand allows a Virtual PCI device (VF) to be directly mapped to the guest,
allowing higher performance and advanced features such as RDMA (remote direct memory access). To use this
feature, you must:
1. Use InfiniBand enabled network adapters.
2. Run InfiniBand subnet managers to enable InfiniBand fabric.
Configuration 153
Networking Guide (Release Version: 15.0.0)
All InfiniBand networks must have a subnet manager running for the network to function. This is true
even when doing a simple network of two machines with no switch and the cards are plugged in backto-back.
A subnet manager is required for the link on the cards to come up. It is possible to have more
than one subnet manager. In this case, one of them will act as the master, and any other will act as a slave
that will take over when the master subnet manager fails.
3. Install the ebrctl utility on the compute nodes.
Check that ebrctl is listed somewhere in /etc/nova/rootwrap.d/*:
$ grep 'ebrctl' /etc/nova/rootwrap.d/*
If ebrctl does not appear in any of the rootwrap files, add this to the /etc/nova/rootwrap.d/
compute.filters file in the [Filters] section.
[Filters]
ebrctl: CommandFilter, ebrctl, root
Known limitations
• When using Quality of Service (QoS), max_burst_kbps (burst over max_kbps) is not supported. In
addition, max_kbps is rounded to Mbps.
• Security groups are not supported when using SR-IOV, thus, the firewall driver must be disabled. This
can be done in the neutron.conf file.
[securitygroup]
firewall_driver = neutron.agent.firewall.NoopFirewallDriver
• SR-IOV is not integrated into the OpenStack Dashboard (horizon). Users must use the CLI or API to
configure SR-IOV interfaces.
• Live migration is not supported for instances with SR-IOV ports.
Note: SR-IOV features may require a specific NIC driver version, depending on the vendor. Intel NICs,
for example, require ixgbe version 4.4.6 or greater, and ixgbevf version 3.2.2 or greater.
Subnet pools
Subnet pools have been made available since the Kilo release. It is a simple feature that has the potential to
improve your workflow considerably. It also provides a building block from which other new features will be
built in to OpenStack Networking.
To see if your cloud has this feature available, you can check that it is listed in the supported aliases. You can
do this with the OpenStack client.
$ openstack extension list | grep subnet_allocation
| Subnet Allocation | subnet_allocation | Enables allocation of subnets
from a subnet pool
,→ |
154 Configuration
Networking Guide (Release Version: 15.0.0)
Why you need them
Before Kilo, Networking had no automation around the addresses used to create a subnet. To create one, you
had to come up with the addresses on your own without any help from the system. There are valid use cases
for this but if you are interested in the following capabilities, then subnet pools might be for you.
First, would not it be nice if you could turn your pool of addresses over to Neutron to take care of? When you
need to create a subnet, you just ask for addresses to be allocated from the pool. You do not have to worry about
what you have already used and what addresses are in your pool. Subnet pools can do this.
Second, subnet pools can manage addresses across projects. The addresses are guaranteed not to overlap. If the
addresses come from an externally routable pool then you know that all of the projects have addresses which
are routable and unique. This can be useful in the following scenarios.
1. IPv6 since OpenStack Networking has no IPv6 floating IPs.
2. Routing directly to a project network from an external network.
How they work
A subnet pool manages a pool of addresses from which subnets can be allocated. It ensures that there is no
overlap between any two subnets allocated from the same pool.
As a regular project in an OpenStack cloud, you can create a subnet pool of your own and use it to manage your
own pool of addresses. This does not require any admin privileges. Your pool will not be visible to any other
project.
If you are an admin, you can create a pool which can be accessed by any regular project. Being a shared
resource, there is a quota mechanism to arbitrate access.
Quotas
Subnet pools have a quota system which is a little bit different than other quotas in Neutron. Other quotas in
Neutron count discrete instances of an object against a quota. Each time you create something like a router,
network, or a port, it uses one from your total quota.
With subnets, the resource is the IP address space. Some subnets take more of it than others. For example,
203.0.113.0/24 uses 256 addresses in one subnet but 198.51.100.224/28 uses only 16. If address space is limited,
the quota system can encourage efficient use of the space.
With IPv4, the default_quota can be set to the number of absolute addresses any given project is allowed to
consume from the pool. For example, with a quota of 128, I might get 203.0.113.128/26, 203.0.113.224/28,
and still have room to allocate 48 more addresses in the future.
With IPv6 it is a little different. It is not practical to count individual addresses. To avoid ridiculously large
numbers, the quota is expressed in the number of /64 subnets which can be allocated. For example, with
a default_quota of 3, I might get 2001:db8:c18e:c05a::/64, 2001:db8:221c:8ef3::/64, and still have room to
allocate one more prefix in the future.
Default subnet pools
Beginning with Mitaka, a subnet pool can be marked as the default. This is handled with a new extension.
Configuration 155
Networking Guide (Release Version: 15.0.0)
$ openstack extension list | grep default-subnetpools
| Default Subnetpools | default-subnetpools | Provides ability to mark
and use a subnetpool as the default
,→ |
An administrator can mark a pool as default. Only one pool from each address family can be marked default.
$ openstack subnet pool set --default 74348864-f8bf-4fc0-ab03-81229d189467
If there is a default, it can be requested by passing --use-default-subnetpool instead of --subnet-pool
SUBNETPOOL.
Demo
If you have access to an OpenStack Kilo or later based neutron, you can play with this feature now. Give it a
try. All of the following commands work equally as well with IPv6 addresses.
First, as admin, create a shared subnet pool:
$ openstack subnet pool create --share --pool-prefix 203.0.113.0/24 \
--default-prefix-length 26 demo-subnetpool4
+-------------------+--------------------------------+
| Field | Value |
+-------------------+--------------------------------+
| address_scope_id | None |
| created_at | 2016-12-14T07:21:26Z |
| default_prefixlen | 26 |
| default_quota | None |
| description | |
| headers | |
| id | d3aefb76-2527-43d4-bc21-0ec253 |
| | 908545 |
| ip_version | 4 |
| is_default | False |
| max_prefixlen | 32 |
| min_prefixlen | 8 |
| name | demo-subnetpool4 |
| prefixes | 203.0.113.0/24 |
| project_id | cfd1889ac7d64ad891d4f20aef9f8d |
| | 7c |
| revision_number | 1 |
| shared | True |
| updated_at | 2016-12-14T07:21:26Z |
+-------------------+--------------------------------+
The default_prefix_length defines the subnet size you will get if you do not specify --prefix-length
when creating a subnet.
Do essentially the same thing for IPv6 and there are now two subnet pools. Regular projects can see them. (the
output is trimmed a bit for display)
$ openstack subnet pool list
+------------------+------------------+--------------------+
| ID | Name | Prefixes |
+------------------+------------------+--------------------+
156 Configuration
Networking Guide (Release Version: 15.0.0)
| 2b7cc19f-0114-4e | demo-subnetpool | 2001:db8:a583::/48 |
| f4-ad86-c1bb91fc | | |
| d1f9 | | |
| d3aefb76-2527-43 | demo-subnetpool4 | 203.0.113.0/24 |
| d4-bc21-0ec25390 | | |
| 8545 | | |
+------------------+------------------+--------------------+
Now, use them. It is easy to create a subnet from a pool:
$ openstack subnet create --ip-version 4 --subnet-pool \
demo-subnetpool4 --network demo-network1 demo-subnet1
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| allocation_pools | 203.0.113.194-203.0.113.254 |
| cidr | 203.0.113.192/26 |
| created_at | 2016-12-14T07:33:13Z |
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 203.0.113.193 |
| headers | |
| host_routes | |
| id | 8d4fbae3-076c-4c08-b2dd-2d6175115a5e |
| ip_version | 4 |
| ipv6_address_mode | None |
| ipv6_ra_mode | None |
| name | demo-subnet1 |
| network_id | 6b377f77-ce00-4ff6-8676-82343817470d |
| project_id | cfd1889ac7d64ad891d4f20aef9f8d7c |
| revision_number | 2 |
| service_types | |
| subnetpool_id | d3aefb76-2527-43d4-bc21-0ec253908545 |
| updated_at | 2016-12-14T07:33:13Z |
+-------------------+--------------------------------------+
You can request a specific subnet from the pool. You need to specify a subnet that falls within the pool’s
prefixes. If the subnet is not already allocated, the request succeeds. You can leave off the IP version because
it is deduced from the subnet pool.
$ openstack subnet create --subnet-pool demo-subnetpool4 \
--network demo-network1 --subnet-range 203.0.113.128/26 subnet2
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| allocation_pools | 203.0.113.130-203.0.113.190 |
| cidr | 203.0.113.128/26 |
| created_at | 2016-12-14T07:27:40Z |
| description | |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 203.0.113.129 |
| headers | |
| host_routes | |
| id | d32814e3-cf46-4371-80dd-498a80badfba |
| ip_version | 4 |
Configuration 157
Networking Guide (Release Version: 15.0.0)
| ipv6_address_mode | None |
| ipv6_ra_mode | None |
| name | subnet2 |
| network_id | 6b377f77-ce00-4ff6-8676-82343817470d |
| project_id | cfd1889ac7d64ad891d4f20aef9f8d7c |
| revision_number | 2 |
| service_types | |
| subnetpool_id | d3aefb76-2527-43d4-bc21-0ec253908545 |
| updated_at | 2016-12-14T07:27:40Z |
+-------------------+--------------------------------------+
If the pool becomes exhausted, load some more prefixes:
$ openstack subnet pool set --pool-prefix \
198.51.100.0/24 demo-subnetpool4
$ openstack subnet pool show demo-subnetpool4
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| address_scope_id | None |
| created_at | 2016-12-14T07:21:26Z |
| default_prefixlen | 26 |
| default_quota | None |
| description | |
| id | d3aefb76-2527-43d4-bc21-0ec253908545 |
| ip_version | 4 |
| is_default | False |
| max_prefixlen | 32 |
| min_prefixlen | 8 |
| name | demo-subnetpool4 |
| prefixes | 198.51.100.0/24, 203.0.113.0/24 |
| project_id | cfd1889ac7d64ad891d4f20aef9f8d7c |
| revision_number | 2 |
| shared | True |
| updated_at | 2016-12-14T07:30:32Z |
+-------------------+--------------------------------------+
Service subnets
Service subnets enable operators to define valid port types for each subnet on a network without limiting networks
to one subnet or manually creating ports with a specific subnet ID. Using this feature, operators can
ensure that ports for instances and router interfaces, for example, always use different subnets.
Operation
Define one or more service types for one or more subnets on a particular network. Each service type must
correspond to a valid device owner within the port model in order for it to be used.
During IP allocation, the IPAM driver returns an address from a subnet with a service type matching the port
device owner. If no subnets match, or all matching subnets lack available IP addresses, the IPAM driver attempts
to use a subnet without any service types to preserve compatibility. If all subnets on a network have a service
type, the IPAM driver cannot preserve compatibility. However, this feature enables strict IP allocation from
subnets with a matching device owner. If multiple subnets contain the same service type, or a subnet without
158 Configuration
Networking Guide (Release Version: 15.0.0)
a service type exists, the IPAM driver selects the first subnet with a matching service type. For example, a
floating IP agent gateway port uses the following selection process:
• network:floatingip_agent_gateway
• None
Note: Ports with the device owner network:dhcp are exempt from the above IPAM logic for subnets with
dhcp_enabled set to True. This preserves the existing automatic DHCP port creation behaviour for DHCPenabled
subnets.
Creating or updating a port with a specific subnet skips this selection process and explicitly uses the given
subnet.
Usage
Note: Creating a subnet with a service type requires administrative privileges.
Example 1 - Proof-of-concept
This following example is not typical of an actual deployment. It is shown to allow users to experiment with
configuring service subnets.
1. Create a network.
$ openstack network create demo-net1
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | UP |
| availability_zone_hints | |
| availability_zones | |
| description | |
| headers | |
| id | b5b729d8-31cc-4d2c-8284-72b3291fec02 |
| ipv4_address_scope | None |
| ipv6_address_scope | None |
| mtu | 1450 |
| name | demo-net1 |
| port_security_enabled | True |
| project_id | a3db43cd0f224242a847ab84d091217d |
| provider:network_type | vxlan |
| provider:physical_network | None |
| provider:segmentation_id | 110 |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | [] |
+---------------------------+--------------------------------------+
Configuration 159
Networking Guide (Release Version: 15.0.0)
2. Create a subnet on the network with one or more service types. For example, the compute:nova service
type enables instances to use this subnet.
$ openstack subnet create demo-subnet1 --subnet-range 192.0.2.0/24 \
--service-type 'compute:nova' --network demo-net1
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| id | 6e38b23f-0b27-4e3c-8e69-fd23a3df1935 |
| ip_version | 4 |
| cidr | 192.0.2.0/24 |
| name | demo-subnet1 |
| network_id | b5b729d8-31cc-4d2c-8284-72b3291fec02 |
| service_types | ['compute:nova'] |
| tenant_id | a8b3054cc1214f18b1186b291525650f |
+-------------------+--------------------------------------+
3. Optionally, create another subnet on the network with a different service type. For example, the compute:foo
arbitrary service type.
$ openstack subnet create demo-subnet2 --subnet-range 198.51.100.0/24 \
--service-type 'compute:foo' --network demo-net1
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| id | ea139dcd-17a3-4f0a-8cca-dff8b4e03f8a |
| ip_version | 4 |
| cidr | 198.51.100.0/24 |
| name | demo-subnet2 |
| network_id | b5b729d8-31cc-4d2c-8284-72b3291fec02 |
| service_types | ['compute:foo'] |
| tenant_id | a8b3054cc1214f18b1186b291525650f |
+-------------------+--------------------------------------+
4. Launch an instance using the network. For example, using the cirros image and m1.tiny flavor.
$ openstack server create demo-instance1 --flavor m1.tiny \
--image cirros --nic net-id=b5b729d8-31cc-4d2c-8284-72b3291fec02
+--------------------------------------+----------------------------------------------
,→-+
| Field | Value
,→ |
+--------------------------------------+----------------------------------------------
,→-+
| OS-DCF:diskConfig | MANUAL
,→ |
| OS-EXT-AZ:availability_zone |
,→ |
| OS-EXT-SRV-ATTR:host | None
,→ |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None
,→ |
| OS-EXT-SRV-ATTR:instance_name | instance-00000009
,→ |
| OS-EXT-STS:power_state | 0
,→ |
| OS-EXT-STS:task_state | scheduling
,→ |
160 Configuration
Networking Guide (Release Version: 15.0.0)
| OS-EXT-STS:vm_state | building
,→ |
| OS-SRV-USG:launched_at | None
,→ |
| OS-SRV-USG:terminated_at | None
,→ |
| accessIPv4 |
,→ |
| accessIPv6 |
,→ |
| addresses |
,→ |
| adminPass | Fn85skabdxBL
,→ |
| config_drive |
,→ |
| created | 2016-09-19T15:07:42Z
,→ |
| flavor | m1.tiny (1)
,→ |
| hostId |
,→ |
| id | 04222b73-1a6e-4c2a-9af4-ef3d17d521ff
,→ |
| image | cirros (4aaec87d-c655-4856-8618-
,→b2dada3a2b11) |
| key_name | None
,→ |
| name | demo-instance1
,→ |
| os-extended-volumes:volumes_attached | []
,→ |
| progress | 0
,→ |
| project_id | d44c19e056674381b86430575184b167
,→ |
| properties |
,→ |
| security_groups | [{u'name': u'default'}]
,→ |
| status | BUILD
,→ |
| updated | 2016-09-19T15:07:42Z
,→ |
| user_id | 331afbeb322d4c559a181e19051ae362
,→ |
+--------------------------------------+----------------------------------------------
,→-+
5. Check the instance status. The Networks field contains an IP address from the subnet having the compute:nova
service type.
$ openstack server list
+--------------------------------------+-----------------+---------+------------------
,→---+
| ID | Name | Status | Networks
,→ |
Configuration 161
Networking Guide (Release Version: 15.0.0)
+--------------------------------------+-----------------+---------+------------------
,→---+
| 20181f46-5cd2-4af8-9af0-f4cf5c983008 | demo-instance1 | ACTIVE | demo-net1=192.0.
,→2.3 |
+--------------------------------------+-----------------+---------+------------------
,→---+
Example 2 - DVR configuration
The following example outlines how you can configure service subnets in a DVR-enabled deployment, with
the goal of minimizing public IP address consumption. This example uses three subnets on the same external
network:
• 192.0.2.0/24 for instance floating IP addresses
• 198.51.100.0/24 for floating IP agent gateway IPs configured on compute nodes
• 203.0.113.0/25 for all other IP allocations on the external network
This example uses again the private network, demo-net1 (b5b729d8-31cc-4d2c-8284-72b3291fec02) which
was created in Example 1 - Proof-of-concept.
1. Create an external network:
$ openstack network create --external demo-ext-net
2. Create a subnet on the external network for the instance floating IP addresses. This uses the network:floatingip
service type.
$ openstack subnet create demo-floating-ip-subnet \
--subnet-range 192.0.2.0/24 --no-dhcp \
--service-type 'network:floatingip' --network demo-ext-net
3. Create a subnet on the external network for the floating IP agent gateway IP addresses, which are configured
by DVR on compute nodes. This will use the network:floatingip_agent_gateway service
type.
$ openstack subnet create demo-floating-ip-agent-gateway-subnet \
--subnet-range 198.51.100.0/24 --no-dhcp \
--service-type 'network:floatingip_agent_gateway' \
--network demo-ext-net
4. Create a subnet on the external network for all other IP addresses allocated on the external network. This
will not use any service type. It acts as a fall back for allocations that do not match either of the above
two service subnets.
$ openstack subnet create demo-other-subnet \
--subnet-range 203.0.113.0/25 --no-dhcp \
--network demo-ext-net
5. Create a router:
$ openstack router create demo-router
6. Add an interface to the router on demo-subnet1:
162 Configuration
Networking Guide (Release Version: 15.0.0)
$ openstack router add subnet demo-router demo-subnet1
7. Set the external gateway for the router, which will create an interface and allocate an IP address on
demo-ext-net:
$ neutron router-gateway-set demo-router demo-ext-net
8. Launch an instance on a private network and retrieve the neutron port ID that was allocated. As above,
use the cirros image and m1.tiny flavor:
$ openstack server create demo-instance1 --flavor m1.tiny \
--image cirros --nic net-id=b5b729d8-31cc-4d2c-8284-72b3291fec02
$ openstack port list --server demo-instance1
+--------------------------------------+------+-------------------+-------------------
,→-------------------------------+--------+
| ID | Name | MAC Address | Fixed IP
,→Addresses | Status |
+--------------------------------------+------+-------------------+-------------------
,→-------------------------------+--------+
| a752bb24-9bf2-4d37-b9d6-07da69c86f19 | | fa:16:3e:99:54:32 | ip_address='203.0.
,→113.130', | ACTIVE |
| | | | subnet_id=
,→'6e38b23f-0b27-4e3c-8e69-fd23a3df1935' | |
+--------------------------------------+------+-------------------+-------------------
,→-------------------------------+--------+
9. Associate a floating IP with the instance port and verify it was allocated an IP address from the correct
subnet:
$ openstack floating ip create --port \
a752bb24-9bf2-4d37-b9d6-07da69c86f19 demo-ext-net
+---------------------+--------------------------------------+
| Field | Value |
+---------------------+--------------------------------------+
| fixed_ip_address | 203.0.113.130 |
| floating_ip_address | 192.0.2.12 |
| floating_network_id | 02d236d5-dad9-4082-bb6b-5245f9f84d13 |
| id | f15cae7f-5e05-4b19-bd25-4bb71edcf3de |
| port_id | a752bb24-9bf2-4d37-b9d6-07da69c86f19 |
| project_id | d44c19e056674381b86430575184b167 |
| router_id | 5a8ca19f-3703-4f81-bc29-db6bc2f528d6 |
| status | ACTIVE |
+---------------------+--------------------------------------+
10. As the admin user, verify the neutron routers are allocated IP addresses from their correct subnets. Use
openstack port list to find ports associated with the routers.
First, the router gateway external port:
$ neutron port-show f148ffeb-3c26-4067-bc5f-5c3dfddae2f5
+-----------------------+-------------------------------------------------------------
,→-------------+
| Field | Value
,→ |
+-----------------------+-------------------------------------------------------------
,→-------------+
| admin_state_up | UP
,→ |
Configuration 163
Networking Guide (Release Version: 15.0.0)
| device_id | 5a8ca19f-3703-4f81-bc29-db6bc2f528d6
,→ |
| device_owner | network:router_gateway
,→ |
| extra_dhcp_opts |
,→ |
| fixed_ips | ip_address='203.0.113.11',
,→ |
| | subnet_id='67c251d9-2b7a-4200-99f6-e13785b0334d'
,→ |
| id | f148ffeb-3c26-4067-bc5f-5c3dfddae2f5
,→ |
| mac_address | fa:16:3e:2c:0f:69
,→ |
| network_id | 02d236d5-dad9-4082-bb6b-5245f9f84d13
,→ |
| project_id |
,→ |
| status | ACTIVE
,→ |
+-----------------------+-------------------------------------------------------------
,→-------------+
Second, the router floating IP agent gateway external port:
$ neutron port-show a2d1e756-8ae1-4f96-9aa1-e7ea16a6a68a
+-----------------------+-------------------------------------------------------------
,→-------------+
| Field | Value
,→ |
+-----------------------+-------------------------------------------------------------
,→-------------+
| admin_state_up | UP
,→ |
| device_id | 3d0c98eb-bca3-45cc-8aa4-90ae3deb0844
,→ |
| device_owner | network:floatingip_agent_gateway
,→ |
| extra_dhcp_opts |
,→ |
| fixed_ips | ip_address='198.51.100.10',
,→ |
| | subnet_id='67c251d9-2b7a-4200-99f6-e13785b0334d'
,→ |
| id | a2d1e756-8ae1-4f96-9aa1-e7ea16a6a68a
,→ |
| mac_address | fa:16:3e:f4:5d:fa
,→ |
| network_id | 02d236d5-dad9-4082-bb6b-5245f9f84d13
,→ |
| project_id |
,→ |
| status | ACTIVE
,→ |
+-----------------------+-------------------------------------------------------------
,→-------------+
164 Configuration
Networking Guide (Release Version: 15.0.0)
Trunking
The network trunk service allows multiple networks to be connected to an instance using a single virtual NIC
(vNIC). Multiple networks can be presented to an instance by connecting it to a single port.
Operation
Network trunking consists of a service plug-in and a set of drivers that manage trunks on different layer-2
mechanism drivers. Users can create a port, associate it with a trunk, and launch an instance on that port. Users
can dynamically attach and detach additional networks without disrupting operation of the instance.
Every trunk has a parent port and can have any number of subports. The parent port is the port that the trunk
is associated with. Users create instances and specify the parent port of the trunk when launching instances
attached to a trunk.
The network presented by the subport is the network of the associated port. When creating a subport, a segmentation-id
may be required by the driver. segmentation-id defines the segmentation ID on which the
subport network is presented to the instance. segmentation-type may be required by certain drivers like
OVS, although at this time only vlan is supported as a segmentation-type.
Note: The segmentation-type and segmentation-id parameters are optional in the Networking API.
However, all drivers as of the Newton release require both to be provided when adding a subport to a trunk.
Future drivers may be implemented without this requirement.
The segmentation-type and segmentation-id specified by the user on the subports is intentionally decoupled
from the segmentation-type and ID of the networks. For example, it is possible to configure the Networking
service with tenant_network_types = vxlan and still create subports with segmentation_type
= vlan. The Networking service performs remapping as necessary.
Example configuration
The ML2 plug-in supports trunking with the following mechanism drivers:
• Open vSwitch (OVS)
• Linux bridge
• Open Virtual Network (OVN)
When using a segmentation-type of vlan, the OVS and Linux bridge drivers present the network of the
parent port as the untagged VLAN and all subports as tagged VLANs.
Controller node
• In the neutron.conf file, enable the trunk service plug-in:
[DEFAULT]
service_plugins = trunk
Configuration 165
Networking Guide (Release Version: 15.0.0)
Verify service operation
1. Source the administrative project credentials and list the enabled extensions.
2. Use the command openstack extension list --network to verify that the Trunk Extension and
Trunk port details extensions are enabled.
Workflow
At a high level, the basic steps to launching an instance on a trunk are the following:
1. Create networks and subnets for the trunk and subports
2. Create the trunk
3. Add subports to the trunk
4. Launch an instance on the trunk
Create networks and subnets for the trunk and subports
Create the appropriate networks for the trunk and subports that will be added to the trunk. Create subnets on
these networks to ensure the desired layer-3 connectivity over the trunk.
Create the trunk
• Create a parent port for the trunk.
$ openstack port create --network project-net-A trunk
+-------------------+-----------------------------------------------------------------
,→-------+
| Field | Value
,→ |
+-------------------+-----------------------------------------------------------------
,→-------+
| admin_state_up | UP
,→ |
| binding_vif_type | unbound
,→ |
| binding_vnic_type | normal
,→ |
| fixed_ips | ip_address='192.0.2.7',subnet_id='8b957198-d3cf-4953-8449-
,→ad4e4dd712cc' |
| id | 73fb9d54-43a7-4bb1-a8dc-569e0e0a0a38
,→ |
| mac_address | fa:16:3e:dd:c4:d1
,→ |
| name | trunk
,→ |
| network_id | 1b47d3e7-cda5-48e4-b0c8-d20bd7e35f55
,→ |
+-------------------+-----------------------------------------------------------------
,→-------+
166 Configuration
Networking Guide (Release Version: 15.0.0)
• Create the trunk using --parent-port to reference the port from the previous step:
$ openstack network trunk create --parent-port 73fb9d54-43a7-4bb1-a8dc-569e0e0a0a38
,→trunk
+-----------------+--------------------------------------+
| Field | Value |
+-----------------+--------------------------------------+
| admin_state_up | UP |
| id | fdf02fcb-1844-45f1-9d9b-e4c2f522c164 |
| name | trunk |
| port_id | 73fb9d54-43a7-4bb1-a8dc-569e0e0a0a38 |
| sub_ports | |
+-----------------+--------------------------------------+
Add subports to the trunk
Subports can be added to a trunk in two ways: creating the trunk with subports or adding subports to an existing
trunk.
• Create trunk with subports:
This method entails creating the trunk with subports specified at trunk creation.
$ openstack port create --network project-net-A trunk-parent
+-------------------+-----------------------------------------------------------------
,→-------+
| Field | Value
,→ |
+-------------------+-----------------------------------------------------------------
,→-------+
| admin_state_up | UP
,→ |
| binding_vif_type | unbound
,→ |
| binding_vnic_type | normal
,→ |
| fixed_ips | ip_address='192.0.2.7',subnet_id='8b957198-d3cf-4953-8449-
,→ad4e4dd712cc' |
| id | 73fb9d54-43a7-4bb1-a8dc-569e0e0a0a38
,→ |
| mac_address | fa:16:3e:dd:c4:d1
,→ |
| name | trunk-parent
,→ |
| network_id | 1b47d3e7-cda5-48e4-b0c8-d20bd7e35f55
,→ |
+-------------------+-----------------------------------------------------------------
,→-------+
$ openstack port create --network trunked-net subport1
+-------------------+-----------------------------------------------------------------
,→-------+
| Field | Value
,→ |
+-------------------+-----------------------------------------------------------------
,→-------+
Configuration 167
Networking Guide (Release Version: 15.0.0)
| admin_state_up | UP
,→ |
| binding_vif_type | unbound
,→ |
| binding_vnic_type | normal
,→ |
| fixed_ips | ip_address='192.0.2.8',subnet_id='2a860e2c-922b-437b-a149-
,→b269a8c9b120' |
| id | 91f9dde8-80a4-4506-b5da-c287feb8f5d8
,→ |
| mac_address | fa:16:3e:ba:f0:4d
,→ |
| name | subport1
,→ |
| network_id | aef78ec5-16e3-4445-b82d-b2b98c6a86d9
,→ |
+-------------------+-----------------------------------------------------------------
,→-------+
$ openstack network trunk create \
--parent-port 73fb9d54-43a7-4bb1-a8dc-569e0e0a0a38 \
--subport port=91f9dde8-80a4-4506-b5da-c287feb8f5d8, \
segmentation-type=vlan,segmentation-id=100
+----------------+--------------------------------------------------------------------
,→-----------------------------+
| Field | Value
,→ |
+----------------+--------------------------------------------------------------------
,→-----------------------------+
| admin_state_up | UP
,→ |
| id | 61d8e620-fe3a-4d8f-b9e6-e1b0dea6d9e3
,→ |
| name | trunk
,→ |
| port_id | 73fb9d54-43a7-4bb1-a8dc-569e0e0a0a38
,→ |
| sub_ports | port_id='73fb9d54-43a7-4bb1-a8dc-569e0e0a0a38', segmentation_id=
,→'100', segmentation_type='vlan' |
+----------------+--------------------------------------------------------------------
,→-----------------------------+
• Add subports to an existing trunk:
This method entails creating a trunk, then adding subports to the trunk after it has already been created.
$ openstack network trunk set --subport \
port=91f9dde8-80a4-4506-b5da-c287feb8f5d8, \
segmentation-type=vlan, \
segmentation-id=100 61d8e620-fe3a-4d8f-b9e6-e1b0dea6d9e3
Note: The command provides no output.
$ openstack network trunk show 61d8e620-fe3a-4d8f-b9e6-e1b0dea6d9e3
+----------------+--------------------------------------------------------------------
,→-----------------------------+
168 Configuration
Networking Guide (Release Version: 15.0.0)
| Field | Value
,→ |
+----------------+--------------------------------------------------------------------
,→-----------------------------+
| admin_state_up | UP
,→ |
| id | 61d8e620-fe3a-4d8f-b9e6-e1b0dea6d9e3
,→ |
| name | trunk
,→ |
| port_id | 73fb9d54-43a7-4bb1-a8dc-569e0e0a0a38
,→ |
| sub_ports | port_id='73fb9d54-43a7-4bb1-a8dc-569e0e0a0a38', segmentation_id=
,→'100', segmentation_type='vlan' |
+----------------+--------------------------------------------------------------------
,→-----------------------------+
Launch an instance on the trunk
• Show trunk details to get the port_id of the trunk.
$ openstack network trunk show 61d8e620-fe3a-4d8f-b9e6-e1b0dea6d9e3
+----------------+--------------------------------------+
| Field | Value |
+----------------+--------------------------------------+
| admin_state_up | UP |
| id | 61d8e620-fe3a-4d8f-b9e6-e1b0dea6d9e3 |
| name | trunk |
| port_id | 73fb9d54-43a7-4bb1-a8dc-569e0e0a0a38 |
| sub_ports | |
+----------------+--------------------------------------+
• Launch the instance by specifying port-id using the value of port_id from the trunk details. Launching
an instance on a subport is not supported.
Using trunks and subports inside an instance
When configuring instances to use a subport, ensure that the interface on the instance is set to use the MAC
address assigned to the port by the Networking service. Instances are not made aware of changes made to the
trunk after they are active. For example, when a subport with a segmentation-type of vlan is added to a
trunk, any operations specific to the instance operating system that allow the instance to send and receive traffic
on the new VLAN must be handled outside of the Networking service.
When creating subports, the MAC address of the trunk parent port can be set on the subport. This will allow
VLAN subinterfaces inside an instance launched on a trunk to be configured without explicitly setting a MAC
address. Although unique MAC addresses can be used for subports, this can present issues with ARP spoof
protections and the native OVS firewall driver. If the native OVS firewall driver is to be used, we recommend
that the MAC address of the parent port be re-used on all subports.
Trunk states
• ACTIVE
Configuration 169
Networking Guide (Release Version: 15.0.0)
The trunk is ACTIVE when both the logical and physical resources have been created. This means that all
operations within the Networking and Compute services have completed and the trunk is ready for use.
• DOWN
A trunk is DOWN when it is first created without an instance launched on it, or when the instance associated
with the trunk has been deleted.
• DEGRADED
A trunk can be in a DEGRADED state when a temporary failure during the provisioning process is encountered.
This includes situations where a subport add or remove operation fails. When in a degraded state,
the trunk is still usable and some subports may be usable as well. Operations that cause the trunk to go
into a DEGRADED state can be retried to fix temporary failures and move the trunk into an ACTIVE state.
• ERROR
A trunk is in ERROR state if the request leads to a conflict or an error that cannot be fixed by retrying the
request. The ERROR status can be encountered if the network is not compatible with the trunk configuration
or the binding process leads to a persistent failure. When a trunk is in ERROR state, it must be
brought to a sane state (ACTIVE), or else requests to add subports will be rejected.
• BUILD
A trunk is in BUILD state while the resources associated with the trunk are in the process of being provisioned.
Once the trunk and all of the subports have been provisioned successfully, the trunk transitions
to ACTIVE. If there was a partial failure, the trunk transitions to DEGRADED.
When admin_state is set to DOWN, the user is blocked from performing operations on the trunk. admin_state
is set by the user and should not be used to monitor the health of the trunk.
Limitations and issues
• See bugs for more information.
Note: For general configuration, see the Configuration Reference.
Deployment examples
The following deployment examples provide building blocks of increasing architectural complexity using the
Networking service reference architecture which implements the Modular Layer 2 (ML2) plug-in and either
the Open vSwitch (OVS) or Linux bridge mechanism drivers. Both mechanism drivers support the same basic
features such as provider networks, self-service networks, and routers. However, more complex features often
require a particular mechanism driver. Thus, you should consider the requirements (or goals) of your cloud
before choosing a mechanism driver.
After choosing a mechanism driver, the deployment examples generally include the following building blocks:
1. Provider (public/external) networks using IPv4 and IPv6
2. Self-service (project/private/internal) networks including routers using IPv4 and IPv6
3. High-availability features
4. Other features such as BGP dynamic routing
170 Deployment examples
Networking Guide (Release Version: 15.0.0)
Prerequisites
Prerequisites, typically hardware requirements, generally increase with each building block. Each building
block depends on proper deployment and operation of prior building blocks. For example, the first building
block (provider networks) only requires one controller and two compute nodes, the second building block (selfservice
networks) adds a network node, and the high-availability building blocks typically add a second network
node for a total of five nodes. Each building block could also require additional infrastructure or changes to
existing infrastructure such as networks.
For basic configuration of prerequisites, see the Ocata Install Tutorials and Guides.
Note: Example commands using the openstack client assume version 3.2.0 or higher.
Nodes
The deployment examples refer one or more of the following nodes:
• Controller: Contains control plane components of OpenStack services and their dependencies.
– Two network interfaces: management and provider.
– Operational SQL server with databases necessary for each OpenStack service.
– Operational message queue service.
– Operational OpenStack Identity (keystone) service.
– Operational OpenStack Image Service (glance).
– Operational management components of the OpenStack Compute (nova) service with appropriate
configuration to use the Networking service.
– OpenStack Networking (neutron) server service and ML2 plug-in.
• Network: Contains the OpenStack Networking service layer-3 (routing) component. High availability
options may include additional components.
– Three network interfaces: management, overlay, and provider.
– OpenStack Networking layer-2 (switching) agent, layer-3 agent, and any dependencies.
• Compute: Contains the hypervisor component of the OpenStack Compute service and the OpenStack
Networking layer-2, DHCP, and metadata components. High-availability options may include additional
components.
– Two network interfaces: management and provider.
– Operational hypervisor components of the OpenStack Compute (nova) service with appropriate
configuration to use the Networking service.
– OpenStack Networking layer-2 agent, DHCP agent, metadata agent, and any dependencies.
Each building block defines the quantity and types of nodes including the components on each node.
Note: You can virtualize these nodes for demonstration, training, or proof-of-concept purposes. However, you
must use physical hosts for evaluation of performance or scaling.
Deployment examples 171
Networking Guide (Release Version: 15.0.0)
Networks and network interfaces
The deployment examples refer to one or more of the following networks and network interfaces:
• Management: Handles API requests from clients and control plane traffic for OpenStack services including
their dependencies.
• Overlay: Handles self-service networks using an overlay protocol such as VXLAN or GRE.
• Provider: Connects virtual and physical networks at layer-2. Typically uses physical network infrastructure
for switching/routing traffic to external networks such as the Internet.
Note: For best performance, 10+ Gbps physical network infrastructure should support jumbo frames.
For illustration purposes, the configuration examples typically reference the following IP address ranges:
• Management network: 10.0.0.0/24
• Overlay (tunnel) network: 10.0.1.0/24
• Provider network 1:
– IPv4: 203.0.113.0/24
– IPv6: fd00:203:0:113::/64
• Provider network 2:
– IPv4: 192.0.2.0/24
– IPv6: fd00:192:0:2::/64
• Self-service networks:
– IPv4: 192.168.0.0/16 in /24 segments
– IPv6: fd00:192:168::/48 in /64 segments
You may change them to work with your particular network infrastructure.
Mechanism drivers
Linux bridge mechanism driver
The Linux bridge mechanism driver uses only Linux bridges and veth pairs as interconnection devices. A
layer-2 agent manages Linux bridges on each compute node and any other node that provides layer-3 (routing),
DHCP, metadata, or other network services.
Linux bridge: Provider networks
The provider networks architecture example provides layer-2 connectivity between instances and the physical
network infrastructure using VLAN (802.1q) tagging. It supports one untagged (flat) network and and up to
4095 tagged (VLAN) networks. The actual quantity of VLAN networks depends on the physical network
infrastructure. For more information on provider networks, see Provider networks.
172 Deployment examples
Networking Guide (Release Version: 15.0.0)
Prerequisites
One controller node with the following components:
• Two network interfaces: management and provider.
• OpenStack Networking server service and ML2 plug-in.
Two compute nodes with the following components:
• Two network interfaces: management and provider.
• OpenStack Networking Linux bridge layer-2 agent, DHCP agent, metadata agent, and any dependencies.
Note: Larger deployments typically deploy the DHCP and metadata agents on a subset of compute nodes
to increase performance and redundancy. However, too many agents can overwhelm the message bus. Also,
to further simplify any deployment, you can omit the metadata agent and use a configuration drive to provide
metadata to instances.
Architecture
Deployment examples 173
Networking Guide (Release Version: 15.0.0)
The following figure shows components and connectivity for one untagged (flat) network. In this particular
case, the instance resides on the same compute node as the DHCP agent for the network. If the DHCP agent
resides on another compute node, the latter only contains a DHCP namespace and Linux bridge with a port on
the provider physical network interface.
The following figure describes virtual connectivity among components for two tagged (VLAN) networks. Essentially,
each network uses a separate bridge that contains a port on the VLAN sub-interface on the provider
physical network interface. Similar to the single untagged network case, the DHCP agent may reside on a
different compute node.
174 Deployment examples
Networking Guide (Release Version: 15.0.0)
Note: These figures omit the controller node because it does not handle instance network traffic.
Example configuration
Use the following example configuration as a template to deploy provider networks in your environment.
Controller node
1. Install the Networking service components that provides the neutron-server service and ML2 plug-in.
2. In the neutron.conf file:
• Configure common options:
[DEFAULT]
core_plugin = ml2
auth_strategy = keystone
[database]
Deployment examples 175
Networking Guide (Release Version: 15.0.0)
# ...
[keystone_authtoken]
# ...
[nova]
# ...
[agent]
# ...
See the Installation Tutorials and Guides and Configuration Reference for your OpenStack release
to obtain the appropriate additional configuration for the [DEFAULT], [database], [keystone_authtoken],
[nova], and [agent] sections.
• Disable service plug-ins because provider networks do not require any. However, this breaks portions
of the dashboard that manage the Networking service. See the Ocata Install Tutorials and
Guides for more information.
[DEFAULT]
service_plugins =
• Enable two DHCP agents per network so both compute nodes can provide DHCP service provider
networks.
[DEFAULT]
dhcp_agents_per_network = 2
• If necessary, configure MTU.
3. In the ml2_conf.ini file:
• Configure drivers and network types:
[ml2]
type_drivers = flat,vlan
tenant_network_types =
mechanism_drivers = linuxbridge
extension_drivers = port_security
• Configure network mappings:
[ml2_type_flat]
flat_networks = provider
[ml2_type_vlan]
network_vlan_ranges = provider
Note: The tenant_network_types option contains no value because the architecture does not
support self-service networks.
Note: The provider value in the network_vlan_ranges option lacks VLAN ID ranges to
support use of arbitrary VLAN IDs.
176 Deployment examples
Networking Guide (Release Version: 15.0.0)
4. Populate the database.
# su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf \
--config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
5. Start the following services:
• Server
Compute nodes
1. Install the Networking service Linux bridge layer-2 agent.
2. In the neutron.conf file, configure common options:
[DEFAULT]
core_plugin = ml2
auth_strategy = keystone
[database]
# ...
[keystone_authtoken]
# ...
[nova]
# ...
[agent]
# ...
See the Installation Tutorials and Guides and Configuration Reference for your OpenStack release
to obtain the appropriate additional configuration for the [DEFAULT], [database], [keystone_authtoken],
[nova], and [agent] sections.
3. In the linuxbridge_agent.ini file, configure the Linux bridge agent:
[linux_bridge]
physical_interface_mappings = provider:PROVIDER_INTERFACE
[vxlan]
enable_vxlan = False
[securitygroup]
firewall_driver = iptables
Replace PROVIDER_INTERFACE with the name of the underlying interface that handles provider networks.
For example, eth1.
4. In the dhcp_agent.ini file, configure the DHCP agent:
[DEFAULT]
interface_driver = linuxbridge
enable_isolated_metadata = True
force_metadata = True
Deployment examples 177
Networking Guide (Release Version: 15.0.0)
Note: The force_metadata option forces the DHCP agent to provide a host route to the metadata
service on 169.254.169.254 regardless of whether the subnet contains an interface on a router, thus
maintaining similar and predictable metadata behavior among subnets.
5. In the metadata_agent.ini file, configure the metadata agent:
[DEFAULT]
nova_metadata_ip = controller
metadata_proxy_shared_secret = METADATA_SECRET
The value of METADATA_SECRET must match the value of the same option in the [neutron] section of
the nova.conf file.
6. Start the following services:
• Linux bridge agent
• DHCP agent
• Metadata agent
Verify service operation
1. Source the administrative project credentials.
2. Verify presence and operation of the agents:
$ openstack network agent list
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| ID | Agent Type | Host | Availability
,→Zone | Alive | State | Binary |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| 09de6af6-c5f1-4548-8b09-18801f068c57 | Linux bridge agent | compute2 |
,→ | True | UP | neutron-linuxbridge-agent |
| 188945d1-9e70-4803-a276-df924e0788a4 | Linux bridge agent | compute1 |
,→ | True | UP | neutron-linuxbridge-agent |
| e76c440d-d5f6-4316-a674-d689630b629e | DHCP agent | compute1 | nova
,→ | True | UP | neutron-dhcp-agent |
| e67367de-6657-11e6-86a4-931cd04404bb | DHCP agent | compute2 | nova
,→ | True | UP | neutron-dhcp-agent |
| e8174cae-6657-11e6-89f0-534ac6d0cb5c | Metadata agent | compute1 |
,→ | True | UP | neutron-metadata-agent |
| ece49ec6-6657-11e6-bafb-c7560f19197d | Metadata agent | compute2 |
,→ | True | UP | neutron-metadata-agent |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
Create initial networks
The configuration supports one flat or multiple VLAN provider networks. For simplicity, the following procedure
creates one flat provider network.
178 Deployment examples
Networking Guide (Release Version: 15.0.0)
1. Source the administrative project credentials.
2. Create a flat network.
$ openstack network create --share --provider-physical-network provider \
--provider-network-type flat provider1
+---------------------------+-----------+-
| Field | Value |
+---------------------------+-----------+
| admin_state_up | UP |
| mtu | 1500 |
| name | provider1 |
| port_security_enabled | True |
| provider:network_type | flat |
| provider:physical_network | provider |
| provider:segmentation_id | None |
| router:external | Internal |
| shared | True |
| status | ACTIVE |
+---------------------------+-----------+
Note: The share option allows any project to use this network. To limit access to provider networks,
see Role-Based Access Control (RBAC).
Note: To create a VLAN network instead of a flat network, change --provider:network_type
flat to --provider-network-type vlan and add --provider-segment with a value referencing
the VLAN ID.
3. Create a IPv4 subnet on the provider network.
$ openstack subnet create --subnet-range 203.0.113.0/24 --gateway 203.0.113.1 \
--network provider1 --allocation-pool start=203.0.113.11,end=203.0.113.250 \
--dns-nameserver 8.8.4.4 provider1-v4
+-------------------+----------------------------+
| Field | Value |
+-------------------+----------------------------+
| allocation_pools | 203.0.113.11-203.0.113.250 |
| cidr | 203.0.113.0/24 |
| dns_nameservers | 8.8.4.4 |
| enable_dhcp | True |
| gateway_ip | 203.0.113.1 |
| ip_version | 4 |
| name | provider1-v4 |
+-------------------+----------------------------+
Note: Enabling DHCP causes the Networking service to provide DHCP which can interfere with existing
DHCP services on the physical network infrastructure.
4. Create a IPv6 subnet on the provider network.
Deployment examples 179
Networking Guide (Release Version: 15.0.0)
$ openstack subnet create --subnet-range fd00:203:0:113::/64 --gateway
,→fd00:203:0:113::1 \
--ip-version 6 --ipv6-address-mode slaac --network provider1 \
--dns-nameserver 2001:4860:4860::8844 provider1-v6
+-------------------+------------------------------------------------------+
| Field | Value |
+-------------------+------------------------------------------------------+
| allocation_pools | fd00:203:0:113::2-fd00:203:0:113:ffff:ffff:ffff:ffff |
| cidr | fd00:203:0:113::/64 |
| dns_nameservers | 2001:4860:4860::8844 |
| enable_dhcp | True |
| gateway_ip | fd00:203:0:113::1 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | None |
| name | provider1-v6 |
+-------------------+------------------------------------------------------+
Note: The Networking service uses the layer-3 agent to provide router advertisement. Provider networks
rely on physical network infrastructure for layer-3 services rather than the layer-3 agent. Thus,
the physical network infrastructure must provide router advertisement on provider networks for proper
operation of IPv6.
Verify network operation
1. On each compute node, verify creation of the qdhcp namespace.
# ip netns
qdhcp-8b868082-e312-4110-8627-298109d4401c
2. Source a regular (non-administrative) project credentials.
3. Create the appropriate security group rules to allow ping and SSH access instances using the network.
$ openstack security group rule create --proto icmp default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv4 |
| protocol | icmp |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+
$ openstack security group rule create --ethertype IPv6 --proto ipv6-icmp default
+-----------+-----------+
| Field | Value |
+-----------+-----------+
| direction | ingress |
| ethertype | IPv6 |
| protocol | ipv6-icmp |
+-----------+-----------+
180 Deployment examples
Networking Guide (Release Version: 15.0.0)
$ openstack security group rule create --proto tcp --dst-port 22 default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv4 |
| port_range_max | 22 |
| port_range_min | 22 |
| protocol | tcp |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+
$ openstack security group rule create --ethertype IPv6 --proto tcp --dst-port 22
,→default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv6 |
| port_range_max | 22 |
| port_range_min | 22 |
| protocol | tcp |
+------------------+-----------+
4. Launch an instance with an interface on the provider network. For example, a CirrOS image using flavor
ID 1.
$ openstack server create --flavor 1 --image cirros \
--nic net-id=NETWORK_ID provider-instance1
Replace NETWORK_ID with the ID of the provider network.
5. Determine the IPv4 and IPv6 addresses of the instance.
$ openstack server list
+--------------------------------------+--------------------+--------+----------------
,→--------------------------------------------+------------+
| ID | Name | Status | Networks
,→ | Image Name |
+--------------------------------------+--------------------+--------+----------------
,→--------------------------------------------+------------+
| 018e0ae2-b43c-4271-a78d-62653dd03285 | provider-instance1 | ACTIVE | provider1=203.
,→0.113.13, fd00:203:0:113:f816:3eff:fe58:be4e | cirros |
+--------------------------------------+--------------------+--------+----------------
,→--------------------------------------------+------------+
6. On the controller node or any host with access to the provider network, ping the IPv4 and IPv6 addresses
of the instance.
$ ping -c 4 203.0.113.13
PING 203.0.113.13 (203.0.113.13) 56(84) bytes of data.
64 bytes from 203.0.113.13: icmp_req=1 ttl=63 time=3.18 ms
64 bytes from 203.0.113.13: icmp_req=2 ttl=63 time=0.981 ms
64 bytes from 203.0.113.13: icmp_req=3 ttl=63 time=1.06 ms
64 bytes from 203.0.113.13: icmp_req=4 ttl=63 time=0.929 ms
--- 203.0.113.13 ping statistics ---
Deployment examples 181
Networking Guide (Release Version: 15.0.0)
4 packets transmitted, 4 received, 0% packet loss, time 3002ms
rtt min/avg/max/mdev = 0.929/1.539/3.183/0.951 ms
$ ping6 -c 4 fd00:203:0:113:f816:3eff:fe58:be4e
PING fd00:203:0:113:f816:3eff:fe58:be4e(fd00:203:0:113:f816:3eff:fe58:be4e) 56 data
,→bytes
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=1 ttl=64 time=1.25 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=2 ttl=64 time=0.683 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=3 ttl=64 time=0.762 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=4 ttl=64 time=0.486 ms
--- fd00:203:0:113:f816:3eff:fe58:be4e ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 2999ms
rtt min/avg/max/mdev = 0.486/0.796/1.253/0.282 ms
7. Obtain access to the instance.
8. Test IPv4 and IPv6 connectivity to the Internet or other external network.
Network traffic flow
The following sections describe the flow of network traffic in several common scenarios. North-south network
traffic travels between an instance and external network such as the Internet. East-west network traffic travels
between instances on the same or different networks. In all scenarios, the physical network infrastructure
handles switching and routing among provider networks and external networks such as the Internet. Each case
references one or more of the following components:
• Provider network 1 (VLAN)
– VLAN ID 101 (tagged)
– IP address ranges 203.0.113.0/24 and fd00:203:0:113::/64
– Gateway (via physical network infrastructure)
* IP addresses 203.0.113.1 and fd00:203:0:113:0::1
• Provider network 2 (VLAN)
– VLAN ID 102 (tagged)
– IP address range 192.0.2.0/24 and fd00:192:0:2::/64
– Gateway
* IP addresses 192.0.2.1 and fd00:192:0:2::1
• Instance 1
– IP addresses 203.0.113.101 and fd00:203:0:113:0::101
• Instance 2
– IP addresses 192.0.2.101 and fd00:192:0:2:0::101
North-south scenario: Instance with a fixed IP address
• The instance resides on compute node 1 and uses provider network 1.
182 Deployment examples
Networking Guide (Release Version: 15.0.0)
• The instance sends a packet to a host on the Internet.
The following steps involve compute node 1.
1. The instance interface (1) forwards the packet to the provider bridge instance port (2) via veth pair.
2. Security group rules (3) on the provider bridge handle firewalling and connection tracking for the packet.
3. The VLAN sub-interface port (4) on the provider bridge forwards the packet to the physical network
interface (5).
4. The physical network interface (5) adds VLAN tag 101 to the packet and forwards it to the physical
network infrastructure switch (6).
The following steps involve the physical network infrastructure:
1. The switch removes VLAN tag 101 from the packet and forwards it to the router (7).
2. The router routes the packet from the provider network (8) to the external network (9) and forwards the
packet to the switch (10).
3. The switch forwards the packet to the external network (11).
4. The external network (12) receives the packet.
Deployment examples 183
Networking Guide (Release Version: 15.0.0)
Note: Return traffic follows similar steps in reverse.
East-west scenario 1: Instances on the same network
Instances on the same network communicate directly between compute nodes containing those instances.
• Instance 1 resides on compute node 1 and uses provider network 1.
• Instance 2 resides on compute node 2 and uses provider network 1.
• Instance 1 sends a packet to instance 2.
The following steps involve compute node 1:
1. The instance 1 interface (1) forwards the packet to the provider bridge instance port (2) via veth pair.
2. Security group rules (3) on the provider bridge handle firewalling and connection tracking for the packet.
3. The VLAN sub-interface port (4) on the provider bridge forwards the packet to the physical network
interface (5).
4. The physical network interface (5) adds VLAN tag 101 to the packet and forwards it to the physical
network infrastructure switch (6).
The following steps involve the physical network infrastructure:
1. The switch forwards the packet from compute node 1 to compute node 2 (7).
The following steps involve compute node 2:
1. The physical network interface (8) removes VLAN tag 101 from the packet and forwards it to the VLAN
sub-interface port (9) on the provider bridge.
2. Security group rules (10) on the provider bridge handle firewalling and connection tracking for the packet.
3. The provider bridge instance port (11) forwards the packet to the instance 2 interface (12) via veth pair.
184 Deployment examples
Networking Guide (Release Version: 15.0.0)
Note: Return traffic follows similar steps in reverse.
East-west scenario 2: Instances on different networks
Instances communicate via router on the physical network infrastructure.
• Instance 1 resides on compute node 1 and uses provider network 1.
• Instance 2 resides on compute node 1 and uses provider network 2.
• Instance 1 sends a packet to instance 2.
Note: Both instances reside on the same compute node to illustrate how VLAN tagging enables multiple
logical layer-2 networks to use the same physical layer-2 network.
Deployment examples 185
Networking Guide (Release Version: 15.0.0)
The following steps involve the compute node:
1. The instance 1 interface (1) forwards the packet to the provider bridge instance port (2) via veth pair.
2. Security group rules (3) on the provider bridge handle firewalling and connection tracking for the packet.
3. The VLAN sub-interface port (4) on the provider bridge forwards the packet to the physical network
interface (5).
4. The physical network interface (5) adds VLAN tag 101 to the packet and forwards it to the physical
network infrastructure switch (6).
The following steps involve the physical network infrastructure:
1. The switch removes VLAN tag 101 from the packet and forwards it to the router (7).
2. The router routes the packet from provider network 1 (8) to provider network 2 (9).
3. The router forwards the packet to the switch (10).
4. The switch adds VLAN tag 102 to the packet and forwards it to compute node 1 (11).
The following steps involve the compute node:
1. The physical network interface (12) removes VLAN tag 102 from the packet and forwards it to the VLAN
sub-interface port (13) on the provider bridge.
2. Security group rules (14) on the provider bridge handle firewalling and connection tracking for the packet.
3. The provider bridge instance port (15) forwards the packet to the instance 2 interface (16) via veth pair.
186 Deployment examples
Networking Guide (Release Version: 15.0.0)
Note: Return traffic follows similar steps in reverse.
Linux bridge: Self-service networks
This architecture example augments Linux bridge: Provider networks to support a nearly limitless quantity of
entirely virtual networks. Although the Networking service supports VLAN self-service networks, this example
focuses on VXLAN self-service networks. For more information on self-service networks, see Self-service
networks.
Deployment examples 187
Networking Guide (Release Version: 15.0.0)
Note: The Linux bridge agent lacks support for other overlay protocols such as GRE and Geneve.
Prerequisites
Add one network node with the following components:
• Three network interfaces: management, provider, and overlay.
• OpenStack Networking Linux bridge layer-2 agent, layer-3 agent, and any dependencies.
Modify the compute nodes with the following components:
• Add one network interface: overlay.
Note: You can keep the DHCP and metadata agents on each compute node or move them to the network node.
188 Deployment examples
Networking Guide (Release Version: 15.0.0)
Architecture
The following figure shows components and connectivity for one self-service network and one untagged (flat)
provider network. In this particular case, the instance resides on the same compute node as the DHCP agent for
the network. If the DHCP agent resides on another compute node, the latter only contains a DHCP namespace
Deployment examples 189
Networking Guide (Release Version: 15.0.0)
and Linux bridge with a port on the overlay physical network interface.
Example configuration
Use the following example configuration as a template to add support for self-service networks to an existing
operational environment that supports provider networks.
Controller node
1. In the neutron.conf file:
• Enable routing and allow overlapping IP address ranges.
[DEFAULT]
service_plugins = router
allow_overlapping_ips = True
2. In the ml2_conf.ini file:
• Add vxlan to type drivers and project network types.
190 Deployment examples
Networking Guide (Release Version: 15.0.0)
[ml2]
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
• Enable the layer-2 population mechanism driver.
[ml2]
mechanism_drivers = linuxbridge,l2population
• Configure the VXLAN network ID (VNI) range.
[ml2_type_vxlan]
vni_ranges = VNI_START:VNI_END
Replace VNI_START and VNI_END with appropriate numerical values.
3. Restart the following services:
• Server
Network node
1. Install the Networking service layer-3 agent.
2. In the neutron.conf file, configure common options:
[DEFAULT]
core_plugin = ml2
auth_strategy = keystone
[database]
# ...
[keystone_authtoken]
# ...
[nova]
# ...
[agent]
# ...
See the Installation Tutorials and Guides and Configuration Reference for your OpenStack release
to obtain the appropriate additional configuration for the [DEFAULT], [database], [keystone_authtoken],
[nova], and [agent] sections.
3. In the linuxbridge_agent.ini file, configure the layer-2 agent.
[linux_bridge]
physical_interface_mappings = provider:PROVIDER_INTERFACE
[vxlan]
enable_vxlan = True
l2_population = True
local_ip = OVERLAY_INTERFACE_IP_ADDRESS
Deployment examples 191
Networking Guide (Release Version: 15.0.0)
[securitygroup]
firewall_driver = iptables
Replace PROVIDER_INTERFACE with the name of the underlying interface that handles provider networks.
For example, eth1.
Replace OVERLAY_INTERFACE_IP_ADDRESS with the IP address of the interface that handles VXLAN
overlays for self-service networks.
4. In the l3_agent.ini file, configure the layer-3 agent.
[DEFAULT]
interface_driver = linuxbridge
external_network_bridge =
Note: The external_network_bridge option intentionally contains no value.
5. Start the following services:
• Linux bridge agent
• Layer-3 agent
Compute nodes
1. In the linuxbridge_agent.ini file, enable VXLAN support including layer-2 population.
[vxlan]
enable_vxlan = True
l2_population = True
local_ip = OVERLAY_INTERFACE_IP_ADDRESS
Replace OVERLAY_INTERFACE_IP_ADDRESS with the IP address of the interface that handles VXLAN
overlays for self-service networks.
2. Restart the following services:
• Linux bridge agent
Verify service operation
1. Source the administrative project credentials.
2. Verify presence and operation of the agents.
$ openstack network agent list
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| ID | Agent Type | Host | Availability
,→Zone | Alive | State | Binary |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| 09de6af6-c5f1-4548-8b09-18801f068c57 | Linux bridge agent | compute2 |
,→ | True | UP | neutron-linuxbridge-agent |
192 Deployment examples
Networking Guide (Release Version: 15.0.0)
| 188945d1-9e70-4803-a276-df924e0788a4 | Linux bridge agent | compute1 |
,→ | True | UP | neutron-linuxbridge-agent |
| e76c440d-d5f6-4316-a674-d689630b629e | DHCP agent | compute1 | nova
,→ | True | UP | neutron-dhcp-agent |
| e67367de-6657-11e6-86a4-931cd04404bb | DHCP agent | compute2 | nova
,→ | True | UP | neutron-dhcp-agent |
| e8174cae-6657-11e6-89f0-534ac6d0cb5c | Metadata agent | compute1 |
,→ | True | UP | neutron-metadata-agent |
| ece49ec6-6657-11e6-bafb-c7560f19197d | Metadata agent | compute2 |
,→ | True | UP | neutron-metadata-agent |
| 598f6357-4331-4da5-a420-0f5be000bec9 | L3 agent | network1 | nova
,→ | True | UP | neutron-l3-agent |
| f4734e0f-bcd5-4922-a19d-e31d56b0a7ae | Linux bridge agent | network1 |
,→ | True | UP | neutron-linuxbridge-agent |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
Create initial networks
The configuration supports multiple VXLAN self-service networks. For simplicity, the following procedure
creates one self-service network and a router with a gateway on the flat provider network. The router uses NAT
for IPv4 network traffic and directly routes IPv6 network traffic.
Note: IPv6 connectivity with self-service networks often requires addition of static routes to nodes and physical
network infrastructure.
1. Source the administrative project credentials.
2. Update the provider network to support external connectivity for self-service networks.
$ openstack network set --external provider1
Note: This command provides no output.
3. Source a regular (non-administrative) project credentials.
4. Create a self-service network.
$ openstack network create selfservice1
+-------------------------+--------------+
| Field | Value |
+-------------------------+--------------+
| admin_state_up | UP |
| mtu | 1450 |
| name | selfservice1 |
| port_security_enabled | True |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
+-------------------------+--------------+
5. Create a IPv4 subnet on the self-service network.
Deployment examples 193
Networking Guide (Release Version: 15.0.0)
$ openstack subnet create --subnet-range 192.0.2.0/24 \
--network selfservice1 --dns-nameserver 8.8.4.4 selfservice1-v4
+-------------------+---------------------------+
| Field | Value |
+-------------------+---------------------------+
| allocation_pools | 192.0.2.2-192.0.2.254 |
| cidr | 192.0.2.0/24 |
| dns_nameservers | 8.8.4.4 |
| enable_dhcp | True |
| gateway_ip | 192.0.2.1 |
| ip_version | 4 |
| name | selfservice1-v4 |
+-------------------+---------------------------+
6. Create a IPv6 subnet on the self-service network.
$ openstack subnet create --subnet-range fd00:192:0:2::/64 --ip-version 6 \
--ipv6-ra-mode slaac --ipv6-address-mode slaac --network selfservice1 \
--dns-nameserver 2001:4860:4860::8844 selfservice1-v6
+-------------------+------------------------------------------------------+
| Field | Value |
+-------------------+------------------------------------------------------+
| allocation_pools | fd00:192:0:2::2-fd00:192:0:2:ffff:ffff:ffff:ffff |
| cidr | fd00:192:0:2::/64 |
| dns_nameservers | 2001:4860:4860::8844 |
| enable_dhcp | True |
| gateway_ip | fd00:192:0:2::1 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | slaac |
| name | selfservice1-v6 |
+-------------------+------------------------------------------------------+
7. Create a router.
$ openstack router create router1
+-----------------------+---------+
| Field | Value |
+-----------------------+---------+
| admin_state_up | UP |
| name | router1 |
| status | ACTIVE |
+-----------------------+---------+
8. Add the IPv4 and IPv6 subnets as interfaces on the router.
$ openstack router add subnet router1 selfservice1-v4
$ openstack router add subnet router1 selfservice1-v6
Note: These commands provide no output.
9. Add the provider network as the gateway on the router.
$ neutron router-gateway-set router1 provider1
Set gateway for router router1
194 Deployment examples
Networking Guide (Release Version: 15.0.0)
Verify network operation
1. On each compute node, verify creation of a second qdhcp namespace.
# ip netns
qdhcp-8b868082-e312-4110-8627-298109d4401c
qdhcp-8fbc13ca-cfe0-4b8a-993b-e33f37ba66d1
2. On the network node, verify creation of the qrouter namespace.
# ip netns
qrouter-17db2a15-e024-46d0-9250-4cd4d336a2cc
3. Source a regular (non-administrative) project credentials.
4. Create the appropriate security group rules to allow ping and SSH access instances using the network.
$ openstack security group rule create --proto icmp default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv4 |
| protocol | icmp |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+
$ openstack security group rule create --ethertype IPv6 --proto ipv6-icmp default
+-----------+-----------+
| Field | Value |
+-----------+-----------+
| direction | ingress |
| ethertype | IPv6 |
| protocol | ipv6-icmp |
+-----------+-----------+
$ openstack security group rule create --proto tcp --dst-port 22 default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv4 |
| port_range_max | 22 |
| port_range_min | 22 |
| protocol | tcp |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+
$ openstack security group rule create --ethertype IPv6 --proto tcp --dst-port 22
,→default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv6 |
| port_range_max | 22 |
| port_range_min | 22 |
Deployment examples 195
Networking Guide (Release Version: 15.0.0)
| protocol | tcp |
+------------------+-----------+
5. Launch an instance with an interface on the self-service network. For example, a CirrOS image using
flavor ID 1.
$ openstack server create --flavor 1 --image cirros --nic net-id=NETWORK_ID
,→selfservice-instance1
Replace NETWORK_ID with the ID of the self-service network.
6. Determine the IPv4 and IPv6 addresses of the instance.
$ openstack server list
+--------------------------------------+-----------------------+--------+-------------
,→-------------------------------------------------+
| ID | Name | Status | Networks
,→ |
+--------------------------------------+-----------------------+--------+-------------
,→-------------------------------------------------+
| c055cdb0-ebb4-4d65-957c-35cbdbd59306 | selfservice-instance1 | ACTIVE |
,→selfservice1=192.0.2.4, fd00:192:0:2:f816:3eff:fe30:9cb0 |
+--------------------------------------+-----------------------+--------+-------------
,→-------------------------------------------------+
Warning: The IPv4 address resides in a private IP address range (RFC1918). Thus, the Networking
service performs source network address translation (SNAT) for the instance to access external networks
such as the Internet. Access from external networks such as the Internet to the instance requires
a floating IPv4 address. The Networking service performs destination network address translation
(DNAT) from the floating IPv4 address to the instance IPv4 address on the self-service network. On
the other hand, the Networking service architecture for IPv6 lacks support for NAT due to the significantly
larger address space and complexity of NAT. Thus, floating IP addresses do not exist for
IPv6 and the Networking service only performs routing for IPv6 subnets on self-service networks.
In other words, you cannot rely on NAT to “hide” instances with IPv4 and IPv6 addresses or only
IPv6 addresses and must properly implement security groups to restrict access.
7. On the controller node or any host with access to the provider network, ping the IPv6 address of the
instance.
$ ping6 -c 4 fd00:192:0:2:f816:3eff:fe30:9cb0
PING fd00:192:0:2:f816:3eff:fe30:9cb0(fd00:192:0:2:f816:3eff:fe30:9cb0) 56 data bytes
64 bytes from fd00:192:0:2:f816:3eff:fe30:9cb0: icmp_seq=1 ttl=63 time=2.08 ms
64 bytes from fd00:192:0:2:f816:3eff:fe30:9cb0: icmp_seq=2 ttl=63 time=1.88 ms
64 bytes from fd00:192:0:2:f816:3eff:fe30:9cb0: icmp_seq=3 ttl=63 time=1.55 ms
64 bytes from fd00:192:0:2:f816:3eff:fe30:9cb0: icmp_seq=4 ttl=63 time=1.62 ms
--- fd00:192:0:2:f816:3eff:fe30:9cb0 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3004ms
rtt min/avg/max/mdev = 1.557/1.788/2.085/0.217 ms
8. Optionally, enable IPv4 access from external networks such as the Internet to the instance.
(a) Create a floating IPv4 address on the provider network.
196 Deployment examples
Networking Guide (Release Version: 15.0.0)
$ openstack floating ip create provider1
+-------------+--------------------------------------+
| Field | Value |
+-------------+--------------------------------------+
| fixed_ip | None |
| id | 22a1b088-5c9b-43b4-97f3-970ce5df77f2 |
| instance_id | None |
| ip | 203.0.113.16 |
| pool | provider1 |
+-------------+--------------------------------------+
(b) Associate the floating IPv4 address with the instance.
$ openstack server add floating ip selfservice-instance1 203.0.113.16
Note: This command provides no output.
(c) On the controller node or any host with access to the provider network, ping the floating IPv4
address of the instance.
$ ping -c 4 203.0.113.16
PING 203.0.113.16 (203.0.113.16) 56(84) bytes of data.
64 bytes from 203.0.113.16: icmp_seq=1 ttl=63 time=3.41 ms
64 bytes from 203.0.113.16: icmp_seq=2 ttl=63 time=1.67 ms
64 bytes from 203.0.113.16: icmp_seq=3 ttl=63 time=1.47 ms
64 bytes from 203.0.113.16: icmp_seq=4 ttl=63 time=1.59 ms
--- 203.0.113.16 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3005ms
rtt min/avg/max/mdev = 1.473/2.040/3.414/0.798 ms
9. Obtain access to the instance.
10. Test IPv4 and IPv6 connectivity to the Internet or other external network.
Network traffic flow
The following sections describe the flow of network traffic in several common scenarios. North-south network
traffic travels between an instance and external network such as the Internet. East-west network traffic travels
between instances on the same or different networks. In all scenarios, the physical network infrastructure
handles switching and routing among provider networks and external networks such as the Internet. Each case
references one or more of the following components:
• Provider network (VLAN)
– VLAN ID 101 (tagged)
• Self-service network 1 (VXLAN)
– VXLAN ID (VNI) 101
• Self-service network 2 (VXLAN)
– VXLAN ID (VNI) 102
Deployment examples 197
Networking Guide (Release Version: 15.0.0)
• Self-service router
– Gateway on the provider network
– Interface on self-service network 1
– Interface on self-service network 2
• Instance 1
• Instance 2
North-south scenario 1: Instance with a fixed IP address
For instances with a fixed IPv4 address, the network node performs SNAT on north-south traffic passing from
self-service to external networks such as the Internet. For instances with a fixed IPv6 address, the network node
performs conventional routing of traffic between self-service and external networks.
• The instance resides on compute node 1 and uses self-service network 1.
• The instance sends a packet to a host on the Internet.
The following steps involve compute node 1:
1. The instance interface (1) forwards the packet to the self-service bridge instance port (2) via veth pair.
2. Security group rules (3) on the self-service bridge handle firewalling and connection tracking for the
packet.
3. The self-service bridge forwards the packet to the VXLAN interface (4) which wraps the packet using
VNI 101.
4. The underlying physical interface (5) for the VXLAN interface forwards the packet to the network node
via the overlay network (6).
The following steps involve the network node:
1. The underlying physical interface (7) for the VXLAN interface forwards the packet to the VXLAN
interface (8) which unwraps the packet.
2. The self-service bridge router port (9) forwards the packet to the self-service network interface (10) in
the router namespace.
• For IPv4, the router performs SNAT on the packet which changes the source IP address to the router
IP address on the provider network and sends it to the gateway IP address on the provider network
via the gateway interface on the provider network (11).
• For IPv6, the router sends the packet to the next-hop IP address, typically the gateway IP address
on the provider network, via the provider gateway interface (11).
3. The router forwards the packet to the provider bridge router port (12).
4. The VLAN sub-interface port (13) on the provider bridge forwards the packet to the provider physical
network interface (14).
5. The provider physical network interface (14) adds VLAN tag 101 to the packet and forwards it to the
Internet via physical network infrastructure (15).
198 Deployment examples
Networking Guide (Release Version: 15.0.0)
Note: Return traffic follows similar steps in reverse. However, without a floating IPv4 address, hosts on the
provider or external networks cannot originate connections to instances on the self-service network.
North-south scenario 2: Instance with a floating IPv4 address
For instances with a floating IPv4 address, the network node performs SNAT on north-south traffic passing from
the instance to external networks such as the Internet and DNAT on north-south traffic passing from external
networks to the instance. Floating IP addresses and NAT do not apply to IPv6. Thus, the network node routes
IPv6 traffic in this scenario.
• The instance resides on compute node 1 and uses self-service network 1.
• A host on the Internet sends a packet to the instance.
The following steps involve the network node:
1. The physical network infrastructure (1) forwards the packet to the provider physical network interface
Deployment examples 199
Networking Guide (Release Version: 15.0.0)
(2).
2. The provider physical network interface removes VLAN tag 101 and forwards the packet to the VLAN
sub-interface on the provider bridge.
3. The provider bridge forwards the packet to the self-service router gateway port on the provider network
(5).
• For IPv4, the router performs DNAT on the packet which changes the destination IP address to
the instance IP address on the self-service network and sends it to the gateway IP address on the
self-service network via the self-service interface (6).
• For IPv6, the router sends the packet to the next-hop IP address, typically the gateway IP address
on the self-service network, via the self-service interface (6).
4. The router forwards the packet to the self-service bridge router port (7).
5. The self-service bridge forwards the packet to the VXLAN interface (8) which wraps the packet using
VNI 101.
6. The underlying physical interface (9) for the VXLAN interface forwards the packet to the network node
via the overlay network (10).
The following steps involve the compute node:
1. The underlying physical interface (11) for the VXLAN interface forwards the packet to the VXLAN
interface (12) which unwraps the packet.
2. Security group rules (13) on the self-service bridge handle firewalling and connection tracking for the
packet.
3. The self-service bridge instance port (14) forwards the packet to the instance interface (15) via veth pair.
Note: Egress instance traffic flows similar to north-south scenario 1, except SNAT changes the source IP
address of the packet to the floating IPv4 address rather than the router IP address on the provider network.
200 Deployment examples
Networking Guide (Release Version: 15.0.0)
East-west scenario 1: Instances on the same network
Instances with a fixed IPv4/IPv6 or floating IPv4 address on the same network communicate directly between
compute nodes containing those instances.
By default, the VXLAN protocol lacks knowledge of target location and uses multicast to discover it. After
discovery, it stores the location in the local forwarding database. In large deployments, the discovery process
can generate a significant amount of network that all nodes must process. To eliminate the latter and generally
increase efficiency, the Networking service includes the layer-2 population mechanism driver that automatically
populates the forwarding database for VXLAN interfaces. The example configuration enables this driver. For
more information, see ML2 plug-in.
• Instance 1 resides on compute node 1 and uses self-service network 1.
• Instance 2 resides on compute node 2 and uses self-service network 1.
• Instance 1 sends a packet to instance 2.
The following steps involve compute node 1:
Deployment examples 201
Networking Guide (Release Version: 15.0.0)
1. The instance 1 interface (1) forwards the packet to the self-service bridge instance port (2) via veth pair.
2. Security group rules (3) on the self-service bridge handle firewalling and connection tracking for the
packet.
3. The self-service bridge forwards the packet to the VXLAN interface (4) which wraps the packet using
VNI 101.
4. The underlying physical interface (5) for the VXLAN interface forwards the packet to compute node 2
via the overlay network (6).
The following steps involve compute node 2:
1. The underlying physical interface (7) for the VXLAN interface forwards the packet to the VXLAN
interface (8) which unwraps the packet.
2. Security group rules (9) on the self-service bridge handle firewalling and connection tracking for the
packet.
3. The self-service bridge instance port (10) forwards the packet to the instance 1 interface (11) via veth
pair.
Note: Return traffic follows similar steps in reverse.
202 Deployment examples
Networking Guide (Release Version: 15.0.0)
East-west scenario 2: Instances on different networks
Instances using a fixed IPv4/IPv6 address or floating IPv4 address communicate via router on the network node.
The self-service networks must reside on the same router.
• Instance 1 resides on compute node 1 and uses self-service network 1.
• Instance 2 resides on compute node 1 and uses self-service network 2.
• Instance 1 sends a packet to instance 2.
Note: Both instances reside on the same compute node to illustrate how VXLAN enables multiple overlays to
use the same layer-3 network.
The following steps involve the compute node:
1. The instance 1 interface (1) forwards the packet to the self-service bridge instance port (2) via veth pair.
2. Security group rules (3) on the self-service bridge handle firewalling and connection tracking for the
packet.
3. The self-service bridge forwards the packet to the VXLAN interface (4) which wraps the packet using
VNI 101.
4. The underlying physical interface (5) for the VXLAN interface forwards the packet to the network node
via the overlay network (6).
The following steps involve the network node:
1. The underlying physical interface (7) for the VXLAN interface forwards the packet to the VXLAN
interface (8) which unwraps the packet.
2. The self-service bridge router port (9) forwards the packet to the self-service network 1 interface (10) in
the router namespace.
3. The router sends the packet to the next-hop IP address, typically the gateway IP address on self-service
network 2, via the self-service network 2 interface (11).
4. The router forwards the packet to the self-service network 2 bridge router port (12).
5. The self-service network 2 bridge forwards the packet to the VXLAN interface (13) which wraps the
packet using VNI 102.
6. The physical network interface (14) for the VXLAN interface sends the packet to the compute node via
the overlay network (15).
The following steps involve the compute node:
1. The underlying physical interface (16) for the VXLAN interface sends the packet to the VXLAN interface
(17) which unwraps the packet.
2. Security group rules (18) on the self-service bridge handle firewalling and connection tracking for the
packet.
3. The self-service bridge instance port (19) forwards the packet to the instance 2 interface (20) via veth
pair.
Deployment examples 203
Networking Guide (Release Version: 15.0.0)
Note: Return traffic follows similar steps in reverse.
Linux bridge: High availability using VRRP
This architecture example augments the self-service deployment example with a high-availability mechanism
using the Virtual Router Redundancy Protocol (VRRP) via keepalived and provides failover of routing for
self-service networks. It requires a minimum of two network nodes because VRRP creates one master (active)
instance and at least one backup instance of each router.
204 Deployment examples
Networking Guide (Release Version: 15.0.0)
During normal operation, keepalived on the master router periodically transmits heartbeat packets over a
hidden network that connects all VRRP routers for a particular project. Each project with VRRP routers uses
a separate hidden network. By default this network uses the first value in the tenant_network_types option
in the ml2_conf.ini file. For additional control, you can specify the self-service network type and physical
network name for the hidden network using the l3_ha_network_type and l3_ha_network_name options in
the neutron.conf file.
If keepalived on the backup router stops receiving heartbeat packets, it assumes failure of the master router
and promotes the backup router to master router by configuring IP addresses on the interfaces in the qrouter
namespace. In environments with more than one backup router, keepalived on the backup router with the
next highest priority promotes that backup router to master router.
Note: This high-availability mechanism configures VRRP using the same priority for all routers. Therefore,
VRRP promotes the backup router with the highest IP address to the master router.
Warning: There is a known bug with keepalived v1.2.15 and earlier which can cause packet loss
when max_l3_agents_per_router is set to 3 or more. Therefore, we recommend that you upgrade to
keepalived v1.2.16 or greater when using this feature.
Interruption of VRRP heartbeat traffic between network nodes, typically due to a network interface or physical
network infrastructure failure, triggers a failover. Restarting the layer-3 agent, or failure of it, does not trigger
a failover providing keepalived continues to operate.
Consider the following attributes of this high-availability mechanism to determine practicality in your environment:
• Instance network traffic on self-service networks using a particular router only traverses the master instance
of that router. Thus, resource limitations of a particular network node can impact all master
instances of routers on that network node without triggering failover to another network node. However,
you can configure the scheduler to distribute the master instance of each router uniformly across a pool
of network nodes to reduce the chance of resource contention on any particular network node.
• Only supports self-service networks using a router. Provider networks operate at layer-2 and rely on
physical network infrastructure for redundancy.
• For instances with a floating IPv4 address, maintains state of network connections during failover as a
side effect of 1:1 static NAT. The mechanism does not actually implement connection tracking.
For production deployments, we recommend at least three network nodes with sufficient resources to handle
network traffic for the entire environment if one network node fails. Also, the remaining two nodes can continue
to provide redundancy.
Warning: This high-availability mechanism is not compatible with the layer-2 population mechanism.
You must disable layer-2 population in the linuxbridge_agent.ini file and restart the Linux bridge
agent on all existing network and compute nodes prior to deploying the example configuration.
Prerequisites
Add one network node with the following components:
Deployment examples 205
Networking Guide (Release Version: 15.0.0)
• Three network interfaces: management, provider, and overlay.
• OpenStack Networking layer-2 agent, layer-3 agent, and any dependencies.
Note: You can keep the DHCP and metadata agents on each compute node or move them to the network nodes.
206 Deployment examples
Networking Guide (Release Version: 15.0.0)
Architecture
The following figure shows components and connectivity for one self-service network and one untagged (flat)
network. The master router resides on network node 1. In this particular case, the instance resides on the same
Deployment examples 207
Networking Guide (Release Version: 15.0.0)
compute node as the DHCP agent for the network. If the DHCP agent resides on another compute node, the
latter only contains a DHCP namespace and Linux bridge with a port on the overlay physical network interface.
Example configuration
Use the following example configuration as a template to add support for high-availability using VRRP to an
existing operational environment that supports self-service networks.
208 Deployment examples
Networking Guide (Release Version: 15.0.0)
Controller node
1. In the neutron.conf file:
• Enable VRRP.
[DEFAULT]
l3_ha = True
2. Restart the following services:
• Server
Network node 1
No changes.
Network node 2
1. Install the Networking service Linux bridge layer-2 agent and layer-3 agent.
2. In the neutron.conf file, configure common options:
[DEFAULT]
core_plugin = ml2
auth_strategy = keystone
[database]
# ...
[keystone_authtoken]
# ...
[nova]
# ...
[agent]
# ...
See the Installation Tutorials and Guides and Configuration Reference for your OpenStack release
to obtain the appropriate additional configuration for the [DEFAULT], [database], [keystone_authtoken],
[nova], and [agent] sections.
3. In the linuxbridge_agent.ini file, configure the layer-2 agent.
[linux_bridge]
physical_interface_mappings = provider:PROVIDER_INTERFACE
[vxlan]
enable_vxlan = True
local_ip = OVERLAY_INTERFACE_IP_ADDRESS
[securitygroup]
firewall_driver = iptables
Deployment examples 209
Networking Guide (Release Version: 15.0.0)
Replace PROVIDER_INTERFACE with the name of the underlying interface that handles provider networks.
For example, eth1.
Replace OVERLAY_INTERFACE_IP_ADDRESS with the IP address of the interface that handles VXLAN
overlays for self-service networks.
4. In the l3_agent.ini file, configure the layer-3 agent.
[DEFAULT]
interface_driver = linuxbridge
external_network_bridge =
Note: The external_network_bridge option intentionally contains no value.
5. Start the following services:
• Linux bridge agent
• Layer-3 agent
Compute nodes
No changes.
Verify service operation
1. Source the administrative project credentials.
2. Verify presence and operation of the agents.
$ openstack network agent list
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| ID | Agent Type | Host | Availability
,→Zone | Alive | State | Binary |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| 09de6af6-c5f1-4548-8b09-18801f068c57 | Linux bridge agent | compute2 |
,→ | True | UP | neutron-linuxbridge-agent |
| 188945d1-9e70-4803-a276-df924e0788a4 | Linux bridge agent | compute1 |
,→ | True | UP | neutron-linuxbridge-agent |
| e76c440d-d5f6-4316-a674-d689630b629e | DHCP agent | compute1 | nova
,→ | True | UP | neutron-dhcp-agent |
| e67367de-6657-11e6-86a4-931cd04404bb | DHCP agent | compute2 | nova
,→ | True | UP | neutron-dhcp-agent |
| e8174cae-6657-11e6-89f0-534ac6d0cb5c | Metadata agent | compute1 |
,→ | True | UP | neutron-metadata-agent |
| ece49ec6-6657-11e6-bafb-c7560f19197d | Metadata agent | compute2 |
,→ | True | UP | neutron-metadata-agent |
| 598f6357-4331-4da5-a420-0f5be000bec9 | L3 agent | network1 | nova
,→ | True | UP | neutron-l3-agent |
| f4734e0f-bcd5-4922-a19d-e31d56b0a7ae | Linux bridge agent | network1 |
,→ | True | UP | neutron-linuxbridge-agent |
| 670e5805-340b-4182-9825-fa8319c99f23 | Linux bridge agent | network2 |
,→ | True | UP | neutron-linuxbridge-agent |
210 Deployment examples
Networking Guide (Release Version: 15.0.0)
| 96224e89-7c15-42e9-89c4-8caac7abdd54 | L3 agent | network2 | nova
,→ | True | UP | neutron-l3-agent |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
Create initial networks
Similar to the self-service deployment example, this configuration supports multiple VXLAN self-service networks.
After enabling high-availability, all additional routers use VRRP. The following procedure creates an
additional self-service network and router. The Networking service also supports adding high-availability to
existing routers. However, the procedure requires administratively disabling and enabling each router which
temporarily interrupts network connectivity for self-service networks with interfaces on that router.
1. Source a regular (non-administrative) project credentials.
2. Create a self-service network.
$ openstack network create selfservice2
+-------------------------+--------------+
| Field | Value |
+-------------------------+--------------+
| admin_state_up | UP |
| mtu | 1450 |
| name | selfservice2 |
| port_security_enabled | True |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
+-------------------------+--------------+
3. Create a IPv4 subnet on the self-service network.
$ openstack subnet create --subnet-range 198.51.100.0/24 \
--network selfservice2 --dns-nameserver 8.8.4.4 selfservice2-v4
+-------------------+------------------------------+
| Field | Value |
+-------------------+------------------------------+
| allocation_pools | 198.51.100.2-198.51.100.254 |
| cidr | 198.51.100.0/24 |
| dns_nameservers | 8.8.4.4 |
| enable_dhcp | True |
| gateway_ip | 198.51.100.1 |
| ip_version | 4 |
| name | selfservice2-v4 |
+-------------------+------------------------------+
4. Create a IPv6 subnet on the self-service network.
$ openstack subnet create --subnet-range fd00:198:51:100::/64 --ip-version 6 \
--ipv6-ra-mode slaac --ipv6-address-mode slaac --network selfservice2 \
--dns-nameserver 2001:4860:4860::8844 selfservice2-v6
+-------------------+--------------------------------------------------------+
| Field | Value |
+-------------------+--------------------------------------------------------+
| allocation_pools | fd00:198:51:100::2-fd00:198:51:100:ffff:ffff:ffff:ffff |
Deployment examples 211
Networking Guide (Release Version: 15.0.0)
| cidr | fd00:198:51:100::/64 |
| dns_nameservers | 2001:4860:4860::8844 |
| enable_dhcp | True |
| gateway_ip | fd00:198:51:100::1 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | slaac |
| name | selfservice2-v6 |
+-------------------+--------------------------------------------------------+
5. Create a router.
$ openstack router create router2
+-----------------------+---------+
| Field | Value |
+-----------------------+---------+
| admin_state_up | UP |
| name | router2 |
| status | ACTIVE |
+-----------------------+---------+
6. Add the IPv4 and IPv6 subnets as interfaces on the router.
$ openstack router add subnet router2 selfservice2-v4
$ openstack router add subnet router2 selfservice2-v6
Note: These commands provide no output.
7. Add the provider network as a gateway on the router.
$ neutron router-gateway-set router2 provider1
Set gateway for router router2
Verify network operation
1. Source the administrative project credentials.
2. Verify creation of the internal high-availability network that handles VRRP heartbeat traffic.
$ openstack network list
+--------------------------------------+----------------------------------------------
,→------+--------------------------------------+
| ID | Name
,→ | Subnets |
+--------------------------------------+----------------------------------------------
,→------+--------------------------------------+
| 1b8519c1-59c4-415c-9da2-a67d53c68455 | HA network tenant
,→f986edf55ae945e2bef3cb4bfd589928 | 6843314a-1e76-4cc9-94f5-c64b7a39364a |
+--------------------------------------+----------------------------------------------
,→------+--------------------------------------+
3. On each network node, verify creation of a qrouter namespace with the same ID.
Network node 1:
212 Deployment examples
Networking Guide (Release Version: 15.0.0)
# ip netns
qrouter-b6206312-878e-497c-8ef7-eb384f8add96
Network node 2:
# ip netns
qrouter-b6206312-878e-497c-8ef7-eb384f8add96
Note: The namespace for router 1 from Linux bridge: Self-service networks should only appear on
network node 1 because of creation prior to enabling VRRP.
4. On each network node, show the IP address of interfaces in the qrouter namespace. With the exception
of the VRRP interface, only one namespace belonging to the master router instance contains IP addresses
on the interfaces.
Network node 1:
# ip netns exec qrouter-b6206312-878e-497c-8ef7-eb384f8add96 ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default
,→qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
2: ha-eb820380-40@if21: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:78:ba:99 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 169.254.192.1/18 brd 169.254.255.255 scope global ha-eb820380-40
valid_lft forever preferred_lft forever
inet 169.254.0.1/24 scope global ha-eb820380-40
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe78:ba99/64 scope link
valid_lft forever preferred_lft forever
3: qr-da3504ad-ba@if24: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:dc:8e:a8 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 198.51.100.1/24 scope global qr-da3504ad-ba
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fedc:8ea8/64 scope link
valid_lft forever preferred_lft forever
4: qr-442e36eb-fc@if27: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:ee:c8:41 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet6 fd00:198:51:100::1/64 scope global nodad
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:feee:c841/64 scope link
valid_lft forever preferred_lft forever
5: qg-33fedbc5-43@if28: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:03:1a:f6 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 203.0.113.21/24 scope global qg-33fedbc5-43
valid_lft forever preferred_lft forever
inet6 fd00:203:0:113::21/64 scope global nodad
valid_lft forever preferred_lft forever
Deployment examples 213
Networking Guide (Release Version: 15.0.0)
inet6 fe80::f816:3eff:fe03:1af6/64 scope link
valid_lft forever preferred_lft forever
Network node 2:
# ip netns exec qrouter-b6206312-878e-497c-8ef7-eb384f8add96 ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default
,→qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
2: ha-7a7ce184-36@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state
,→UP group default qlen 1000
link/ether fa:16:3e:16:59:84 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 169.254.192.2/18 brd 169.254.255.255 scope global ha-7a7ce184-36
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe16:5984/64 scope link
valid_lft forever preferred_lft forever
3: qr-da3504ad-ba@if11: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:dc:8e:a8 brd ff:ff:ff:ff:ff:ff link-netnsid 0
4: qr-442e36eb-fc@if14: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue
,→state UP group default qlen 1000
5: qg-33fedbc5-43@if15: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:03:1a:f6 brd ff:ff:ff:ff:ff:ff link-netnsid 0
Note: The master router may reside on network node 2.
5. Launch an instance with an interface on the addtional self-service network. For example, a CirrOS image
using flavor ID 1.
$ openstack server create --flavor 1 --image cirros --nic net-id=NETWORK_ID
,→selfservice-instance2
Replace NETWORK_ID with the ID of the additional self-service network.
6. Determine the IPv4 and IPv6 addresses of the instance.
$ openstack server list
+--------------------------------------+-----------------------+--------+-------------
,→--------------------------------------------------------------+
| ID | Name | Status | Networks
,→ |
+--------------------------------------+-----------------------+--------+-------------
,→--------------------------------------------------------------+
| bde64b00-77ae-41b9-b19a-cd8e378d9f8b | selfservice-instance2 | ACTIVE |
,→selfservice2=fd00:198:51:100:f816:3eff:fe71:e93e, 198.51.100.4 |
+--------------------------------------+-----------------------+--------+-------------
,→--------------------------------------------------------------+
7. Create a floating IPv4 address on the provider network.
214 Deployment examples
Networking Guide (Release Version: 15.0.0)
$ openstack floating ip create provider1
+-------------+--------------------------------------+
| Field | Value |
+-------------+--------------------------------------+
| fixed_ip | None |
| id | 0174056a-fa56-4403-b1ea-b5151a31191f |
| instance_id | None |
| ip | 203.0.113.17 |
| pool | provider1 |
+-------------+--------------------------------------+
8. Associate the floating IPv4 address with the instance.
$ openstack server add floating ip selfservice-instance2 203.0.113.17
Note: This command provides no output.
Verify failover operation
1. Begin a continuous ping of both the floating IPv4 address and IPv6 address of the instance. While
performing the next three steps, you should see a minimal, if any, interruption of connectivity to the
instance.
2. On the network node with the master router, administratively disable the overlay network interface.
3. On the other network node, verify promotion of the backup router to master router by noting addition of
IP addresses to the interfaces in the qrouter namespace.
4. On the original network node in step 2, administratively enable the overlay network interface. Note that
the master router remains on the network node in step 3.
Keepalived VRRP health check
The health of your keepalived instances can be automatically monitored via a bash script that verifies connectivity
to all available and configured gateway addresses. In the event that connectivity is lost, the master
router is rescheduled to another node.
If all routers lose connectivity simultaneously, the process of selecting a new master router will be repeated in
a round-robin fashion until one or more routers have their connectivity restored.
To enable this feature, edit the l3_agent.ini file:
ha_vrrp_health_check_interval = 30
Where ha_vrrp_health_check_interval indicates how often in seconds the health check should run. The
default value is 0, which indicates that the check should not run at all.
Network traffic flow
This high-availability mechanism simply augments Linux bridge: Self-service networks with failover of layer-3
services to another router if the master router fails. Thus, you can reference Self-service network traffic flow
Deployment examples 215
Networking Guide (Release Version: 15.0.0)
for normal operation.
Open vSwitch mechanism driver
The Open vSwitch (OVS) mechanism driver uses a combination of OVS and Linux bridges as interconnection
devices. However, optionally enabling the OVS native implementation of security groups removes the
dependency on Linux bridges.
We recommend using Open vSwitch version 2.4 or higher. Optional features may require a higher minimum
version.
Open vSwitch: Provider networks
This architecture example provides layer-2 connectivity between instances and the physical network infrastructure
using VLAN (802.1q) tagging. It supports one untagged (flat) network and up to 4095 tagged (VLAN)
networks. The actual quantity of VLAN networks depends on the physical network infrastructure. For more
information on provider networks, see Provider networks.
Warning: Linux distributions often package older releases of Open vSwitch that can introduce issues
during operation with the Networking service. We recommend using at least the latest long-term stable
(LTS) release of Open vSwitch for the best experience and support from Open vSwitch. See http://www.
openvswitch.org for available releases and the installation instructions for
Prerequisites
One controller node with the following components:
• Two network interfaces: management and provider.
• OpenStack Networking server service and ML2 plug-in.
Two compute nodes with the following components:
• Two network interfaces: management and provider.
• OpenStack Networking Open vSwitch (OVS) layer-2 agent, DHCP agent, metadata agent, and any dependencies
including OVS.
Note: Larger deployments typically deploy the DHCP and metadata agents on a subset of compute nodes
to increase performance and redundancy. However, too many agents can overwhelm the message bus. Also,
to further simplify any deployment, you can omit the metadata agent and use a configuration drive to provide
metadata to instances.
216 Deployment examples
Networking Guide (Release Version: 15.0.0)
Architecture
The following figure shows components and connectivity for one untagged (flat) network. In this particular
case, the instance resides on the same compute node as the DHCP agent for the network. If the DHCP agent
resides on another compute node, the latter only contains a DHCP namespace with a port on the OVS integration
bridge.
Deployment examples 217
Networking Guide (Release Version: 15.0.0)
The following figure describes virtual connectivity among components for two tagged (VLAN) networks. Essentially,
all networks use a single OVS integration bridge with different internal VLAN tags. The internal
VLAN tags almost always differ from the network VLAN assignment in the Networking service. Similar to
the untagged network case, the DHCP agent may reside on a different compute node.
218 Deployment examples
Networking Guide (Release Version: 15.0.0)
Note: These figures omit the controller node because it does not handle instance network traffic.
Example configuration
Use the following example configuration as a template to deploy provider networks in your environment.
Controller node
1. Install the Networking service components that provide the neutron-server service and ML2 plug-in.
2. In the neutron.conf file:
• Configure common options:
[DEFAULT]
core_plugin = ml2
auth_strategy = keystone
Deployment examples 219
Networking Guide (Release Version: 15.0.0)
[database]
# ...
[keystone_authtoken]
# ...
[nova]
# ...
[agent]
# ...
See the Installation Tutorials and Guides and Configuration Reference for your OpenStack release
to obtain the appropriate additional configuration for the [DEFAULT], [database], [keystone_authtoken],
[nova], and [agent] sections.
• Disable service plug-ins because provider networks do not require any. However, this breaks portions
of the dashboard that manage the Networking service. See the Ocata Install Tutorials and
Guides for more information.
[DEFAULT]
service_plugins =
• Enable two DHCP agents per network so both compute nodes can provide DHCP service provider
networks.
[DEFAULT]
dhcp_agents_per_network = 2
• If necessary, configure MTU.
3. In the ml2_conf.ini file:
• Configure drivers and network types:
[ml2]
type_drivers = flat,vlan
tenant_network_types =
mechanism_drivers = openvswitch
extension_drivers = port_security
• Configure network mappings:
[ml2_type_flat]
flat_networks = provider
[ml2_type_vlan]
network_vlan_ranges = provider
Note: The tenant_network_types option contains no value because the architecture does not
support self-service networks.
Note: The provider value in the network_vlan_ranges option lacks VLAN ID ranges to
220 Deployment examples
Networking Guide (Release Version: 15.0.0)
support use of arbitrary VLAN IDs.
4. Populate the database.
# su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf \
--config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
5. Start the following services:
• Server
Compute nodes
1. Install the Networking service OVS layer-2 agent, DHCP agent, and metadata agent.
2. Install OVS.
3. In the neutron.conf file, configure common options:
[DEFAULT]
core_plugin = ml2
auth_strategy = keystone
[database]
# ...
[keystone_authtoken]
# ...
[nova]
# ...
[agent]
# ...
See the Installation Tutorials and Guides and Configuration Reference for your OpenStack release
to obtain the appropriate additional configuration for the [DEFAULT], [database], [keystone_authtoken],
[nova], and [agent] sections.
4. In the openvswitch_agent.ini file, configure the OVS agent:
[ovs]
bridge_mappings = provider:br-provider
[securitygroup]
firewall_driver = iptables_hybrid
5. In the dhcp_agent.ini file, configure the DHCP agent:
[DEFAULT]
interface_driver = openvswitch
enable_isolated_metadata = True
force_metadata = True
Note: The force_metadata option forces the DHCP agent to provide a host route to the metadata
Deployment examples 221
Networking Guide (Release Version: 15.0.0)
service on 169.254.169.254 regardless of whether the subnet contains an interface on a router, thus
maintaining similar and predictable metadata behavior among subnets.
6. In the metadata_agent.ini file, configure the metadata agent:
[DEFAULT]
nova_metadata_ip = controller
metadata_proxy_shared_secret = METADATA_SECRET
The value of METADATA_SECRET must match the value of the same option in the [neutron] section of
the nova.conf file.
7. Start the following services:
• OVS
8. Create the OVS provider bridge br-provider:
$ ovs-vsctl add-br br-provider
9. Add the provider network interface as a port on the OVS provider bridge br-provider:
$ ovs-vsctl add-port br-provider PROVIDER_INTERFACE
Replace PROVIDER_INTERFACE with the name of the underlying interface that handles provider networks.
For example, eth1.
10. Start the following services:
• OVS agent
• DHCP agent
• Metadata agent
Verify service operation
1. Source the administrative project credentials.
2. Verify presence and operation of the agents:
$ openstack network agent list
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| ID | Agent Type | Host | Availability
,→Zone | Alive | State | Binary |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| 1236bbcb-e0ba-48a9-80fc-81202ca4fa51 | Metadata agent | compute2 |
,→ | True | UP | neutron-metadata-agent |
| 457d6898-b373-4bb3-b41f-59345dcfb5c5 | Open vSwitch agent | compute2 |
,→ | True | UP | neutron-openvswitch-agent |
| 71f15e84-bc47-4c2a-b9fb-317840b2d753 | DHCP agent | compute2 | nova
,→ | True | UP | neutron-dhcp-agent |
| a6c69690-e7f7-4e56-9831-1282753e5007 | Metadata agent | compute1 |
,→ | True | UP | neutron-metadata-agent |
| af11f22f-a9f4-404f-9fd8-cd7ad55c0f68 | DHCP agent | compute1 | nova
,→ | True | UP | neutron-dhcp-agent |
222 Deployment examples
Networking Guide (Release Version: 15.0.0)
| bcfc977b-ec0e-4ba9-be62-9489b4b0e6f1 | Open vSwitch agent | compute1 |
,→ | True | UP | neutron-openvswitch-agent |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
Create initial networks
The configuration supports one flat or multiple VLAN provider networks. For simplicity, the following procedure
creates one flat provider network.
1. Source the administrative project credentials.
2. Create a flat network.
$ openstack network create --share --provider-physical-network provider \
--provider-network-type flat provider1
+---------------------------+-----------+-
| Field | Value |
+---------------------------+-----------+
| admin_state_up | UP |
| mtu | 1500 |
| name | provider1 |
| port_security_enabled | True |
| provider:network_type | flat |
| provider:physical_network | provider |
| provider:segmentation_id | None |
| router:external | Internal |
| shared | True |
| status | ACTIVE |
+---------------------------+-----------+
Note: The share option allows any project to use this network. To limit access to provider networks,
see Role-Based Access Control (RBAC).
Note: To create a VLAN network instead of a flat network, change --provider:network_type
flat to --provider-network-type vlan and add --provider-segment with a value referencing
the VLAN ID.
3. Create a IPv4 subnet on the provider network.
$ openstack subnet create --subnet-range 203.0.113.0/24 --gateway 203.0.113.1 \
--network provider1 --allocation-pool start=203.0.113.11,end=203.0.113.250 \
--dns-nameserver 8.8.4.4 provider1-v4
+-------------------+----------------------------+
| Field | Value |
+-------------------+----------------------------+
| allocation_pools | 203.0.113.11-203.0.113.250 |
| cidr | 203.0.113.0/24 |
| dns_nameservers | 8.8.4.4 |
| enable_dhcp | True |
| gateway_ip | 203.0.113.1 |
Deployment examples 223
Networking Guide (Release Version: 15.0.0)
| ip_version | 4 |
| name | provider1-v4 |
+-------------------+----------------------------+
Note: Enabling DHCP causes the Networking service to provide DHCP which can interfere with existing
DHCP services on the physical network infrastructure.
4. Create a IPv6 subnet on the provider network.
$ openstack subnet create --subnet-range fd00:203:0:113::/64 --gateway
,→fd00:203:0:113::1 \
--ip-version 6 --ipv6-address-mode slaac --network provider1 \
--dns-nameserver 2001:4860:4860::8844 provider1-v6
+-------------------+------------------------------------------------------+
| Field | Value |
+-------------------+------------------------------------------------------+
| allocation_pools | fd00:203:0:113::2-fd00:203:0:113:ffff:ffff:ffff:ffff |
| cidr | fd00:203:0:113::/64 |
| dns_nameservers | 2001:4860:4860::8844 |
| enable_dhcp | True |
| gateway_ip | fd00:203:0:113::1 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | None |
| name | provider1-v6 |
+-------------------+------------------------------------------------------+
Note: The Networking service uses the layer-3 agent to provide router advertisement. Provider networks
rely on physical network infrastructure for layer-3 services rather than the layer-3 agent. Thus,
the physical network infrastructure must provide router advertisement on provider networks for proper
operation of IPv6.
Verify network operation
1. On each compute node, verify creation of the qdhcp namespace.
# ip netns
qdhcp-8b868082-e312-4110-8627-298109d4401c
2. Source a regular (non-administrative) project credentials.
3. Create the appropriate security group rules to allow ping and SSH access instances using the network.
$ openstack security group rule create --proto icmp default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv4 |
| protocol | icmp |
| remote_ip_prefix | 0.0.0.0/0 |
224 Deployment examples
Networking Guide (Release Version: 15.0.0)
+------------------+-----------+
$ openstack security group rule create --ethertype IPv6 --proto ipv6-icmp default
+-----------+-----------+
| Field | Value |
+-----------+-----------+
| direction | ingress |
| ethertype | IPv6 |
| protocol | ipv6-icmp |
+-----------+-----------+
$ openstack security group rule create --proto tcp --dst-port 22 default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv4 |
| port_range_max | 22 |
| port_range_min | 22 |
| protocol | tcp |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+
$ openstack security group rule create --ethertype IPv6 --proto tcp --dst-port 22
,→default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv6 |
| port_range_max | 22 |
| port_range_min | 22 |
| protocol | tcp |
+------------------+-----------+
4. Launch an instance with an interface on the provider network. For example, a CirrOS image using flavor
ID 1.
$ openstack server create --flavor 1 --image cirros \
--nic net-id=NETWORK_ID provider-instance1
Replace NETWORK_ID with the ID of the provider network.
5. Determine the IPv4 and IPv6 addresses of the instance.
$ openstack server list
+--------------------------------------+--------------------+--------+----------------
,→--------------------------------------------+------------+
| ID | Name | Status | Networks
,→ | Image Name |
+--------------------------------------+--------------------+--------+----------------
,→--------------------------------------------+------------+
| 018e0ae2-b43c-4271-a78d-62653dd03285 | provider-instance1 | ACTIVE | provider1=203.
,→0.113.13, fd00:203:0:113:f816:3eff:fe58:be4e | cirros |
+--------------------------------------+--------------------+--------+----------------
,→--------------------------------------------+------------+
Deployment examples 225
Networking Guide (Release Version: 15.0.0)
6. On the controller node or any host with access to the provider network, ping the IPv4 and IPv6 addresses
of the instance.
$ ping -c 4 203.0.113.13
PING 203.0.113.13 (203.0.113.13) 56(84) bytes of data.
64 bytes from 203.0.113.13: icmp_req=1 ttl=63 time=3.18 ms
64 bytes from 203.0.113.13: icmp_req=2 ttl=63 time=0.981 ms
64 bytes from 203.0.113.13: icmp_req=3 ttl=63 time=1.06 ms
64 bytes from 203.0.113.13: icmp_req=4 ttl=63 time=0.929 ms
--- 203.0.113.13 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3002ms
rtt min/avg/max/mdev = 0.929/1.539/3.183/0.951 ms
$ ping6 -c 4 fd00:203:0:113:f816:3eff:fe58:be4e
PING fd00:203:0:113:f816:3eff:fe58:be4e(fd00:203:0:113:f816:3eff:fe58:be4e) 56 data
,→bytes
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=1 ttl=64 time=1.25 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=2 ttl=64 time=0.683 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=3 ttl=64 time=0.762 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=4 ttl=64 time=0.486 ms
--- fd00:203:0:113:f816:3eff:fe58:be4e ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 2999ms
rtt min/avg/max/mdev = 0.486/0.796/1.253/0.282 ms
7. Obtain access to the instance.
8. Test IPv4 and IPv6 connectivity to the Internet or other external network.
Network traffic flow
The following sections describe the flow of network traffic in several common scenarios. North-south network
traffic travels between an instance and external network such as the Internet. East-west network traffic travels
between instances on the same or different networks. In all scenarios, the physical network infrastructure
handles switching and routing among provider networks and external networks such as the Internet. Each case
references one or more of the following components:
• Provider network 1 (VLAN)
– VLAN ID 101 (tagged)
– IP address ranges 203.0.113.0/24 and fd00:203:0:113::/64
– Gateway (via physical network infrastructure)
* IP addresses 203.0.113.1 and fd00:203:0:113:0::1
• Provider network 2 (VLAN)
– VLAN ID 102 (tagged)
– IP address range 192.0.2.0/24 and fd00:192:0:2::/64
– Gateway
* IP addresses 192.0.2.1 and fd00:192:0:2::1
• Instance 1
226 Deployment examples
Networking Guide (Release Version: 15.0.0)
– IP addresses 203.0.113.101 and fd00:203:0:113:0::101
• Instance 2
– IP addresses 192.0.2.101 and fd00:192:0:2:0::101
North-south
• The instance resides on compute node 1 and uses provider network 1.
• The instance sends a packet to a host on the Internet.
The following steps involve compute node 1.
1. The instance interface (1) forwards the packet to the security group bridge instance port (2) via veth
pair.
2. Security group rules (3) on the security group bridge handle firewalling and connection tracking for the
packet.
3. The security group bridge OVS port (4) forwards the packet to the OVS integration bridge security group
port (5) via veth pair.
4. The OVS integration bridge adds an internal VLAN tag to the packet.
5. The OVS integration bridge int-br-provider patch port (6) forwards the packet to the OVS provider
bridge phy-br-provider patch port (7).
6. The OVS provider bridge swaps the internal VLAN tag with actual VLAN tag 101.
7. The OVS provider bridge provider network port (8) forwards the packet to the physical network interface
(9).
8. The physical network interface forwards the packet to the physical network infrastructure switch (10).
The following steps involve the physical network infrastructure:
1. The switch removes VLAN tag 101 from the packet and forwards it to the router (11).
2. The router routes the packet from the provider network (12) to the external network (13) and forwards
the packet to the switch (14).
3. The switch forwards the packet to the external network (15).
4. The external network (16) receives the packet.
Deployment examples 227
Networking Guide (Release Version: 15.0.0)
Note: Return traffic follows similar steps in reverse.
East-west scenario 1: Instances on the same network
Instances on the same network communicate directly between compute nodes containing those instances.
• Instance 1 resides on compute node 1 and uses provider network 1.
• Instance 2 resides on compute node 2 and uses provider network 1.
228 Deployment examples
Networking Guide (Release Version: 15.0.0)
• Instance 1 sends a packet to instance 2.
The following steps involve compute node 1:
1. The instance 1 interface (1) forwards the packet to the security group bridge instance port (2) via veth
pair.
2. Security group rules (3) on the security group bridge handle firewalling and connection tracking for the
packet.
3. The security group bridge OVS port (4) forwards the packet to the OVS integration bridge security group
port (5) via veth pair.
4. The OVS integration bridge adds an internal VLAN tag to the packet.
5. The OVS integration bridge int-br-provider patch port (6) forwards the packet to the OVS provider
bridge phy-br-provider patch port (7).
6. The OVS provider bridge swaps the internal VLAN tag with actual VLAN tag 101.
7. The OVS provider bridge provider network port (8) forwards the packet to the physical network interface
(9).
8. The physical network interface forwards the packet to the physical network infrastructure switch (10).
The following steps involve the physical network infrastructure:
1. The switch forwards the packet from compute node 1 to compute node 2 (11).
The following steps involve compute node 2:
1. The physical network interface (12) forwards the packet to the OVS provider bridge provider network
port (13).
2. The OVS provider bridge phy-br-provider patch port (14) forwards the packet to the OVS integration
bridge int-br-provider patch port (15).
3. The OVS integration bridge swaps the actual VLAN tag 101 with the internal VLAN tag.
4. The OVS integration bridge security group port (16) forwards the packet to the security group bridge
OVS port (17).
5. Security group rules (18) on the security group bridge handle firewalling and connection tracking for the
packet.
6. The security group bridge instance port (19) forwards the packet to the instance 2 interface (20) via veth
pair.
Deployment examples 229
Networking Guide (Release Version: 15.0.0)
230 Deployment examples
Networking Guide (Release Version: 15.0.0)
Note: Return traffic follows similar steps in reverse.
East-west scenario 2: Instances on different networks
Instances communicate via router on the physical network infrastructure.
• Instance 1 resides on compute node 1 and uses provider network 1.
• Instance 2 resides on compute node 1 and uses provider network 2.
• Instance 1 sends a packet to instance 2.
Note: Both instances reside on the same compute node to illustrate how VLAN tagging enables multiple
logical layer-2 networks to use the same physical layer-2 network.
The following steps involve the compute node:
1. The instance 1 interface (1) forwards the packet to the security group bridge instance port (2) via veth
pair.
2. Security group rules (3) on the security group bridge handle firewalling and connection tracking for the
packet.
3. The security group bridge OVS port (4) forwards the packet to the OVS integration bridge security group
port (5) via veth pair.
4. The OVS integration bridge adds an internal VLAN tag to the packet.
5. The OVS integration bridge int-br-provider patch port (6) forwards the packet to the OVS provider
bridge phy-br-provider patch port (7).
6. The OVS provider bridge swaps the internal VLAN tag with actual VLAN tag 101.
7. The OVS provider bridge provider network port (8) forwards the packet to the physical network interface
(9).
8. The physical network interface forwards the packet to the physical network infrastructure switch (10).
The following steps involve the physical network infrastructure:
1. The switch removes VLAN tag 101 from the packet and forwards it to the router (11).
2. The router routes the packet from provider network 1 (12) to provider network 2 (13).
3. The router forwards the packet to the switch (14).
4. The switch adds VLAN tag 102 to the packet and forwards it to compute node 1 (15).
The following steps involve the compute node:
1. The physical network interface (16) forwards the packet to the OVS provider bridge provider network
port (17).
2. The OVS provider bridge phy-br-provider patch port (18) forwards the packet to the OVS integration
bridge int-br-provider patch port (19).
3. The OVS integration bridge swaps the actual VLAN tag 102 with the internal VLAN tag.
Deployment examples 231
Networking Guide (Release Version: 15.0.0)
4. The OVS integration bridge security group port (20) removes the internal VLAN tag and forwards the
packet to the security group bridge OVS port (21).
5. Security group rules (22) on the security group bridge handle firewalling and connection tracking for the
packet.
6. The security group bridge instance port (23) forwards the packet to the instance 2 interface (24) via veth
pair.
Note: Return traffic follows similar steps in reverse.
Open vSwitch: Self-service networks
This architecture example augments Open vSwitch: Provider networks to support a nearly limitless quantity of
entirely virtual networks. Although the Networking service supports VLAN self-service networks, this example
focuses on VXLAN self-service networks. For more information on self-service networks, see Self-service
networks.
232 Deployment examples
Networking Guide (Release Version: 15.0.0)
Prerequisites
Add one network node with the following components:
• Three network interfaces: management, provider, and overlay.
• OpenStack Networking Open vSwitch (OVS) layer-2 agent, layer-3 agent, and any including OVS.
Modify the compute nodes with the following components:
• Add one network interface: overlay.
Note: You can keep the DHCP and metadata agents on each compute node or move them to the network node.
Deployment examples 233
Networking Guide (Release Version: 15.0.0)
Architecture
234 Deployment examples
Networking Guide (Release Version: 15.0.0)
The following figure shows components and connectivity for one self-service network and one untagged (flat)
provider network. In this particular case, the instance resides on the same compute node as the DHCP agent for
the network. If the DHCP agent resides on another compute node, the latter only contains a DHCP namespace
and with a port on the OVS integration bridge.
Example configuration
Use the following example configuration as a template to add support for self-service networks to an existing
operational environment that supports provider networks.
Controller node
1. In the neutron.conf file:
• Enable routing and allow overlapping IP address ranges.
Deployment examples 235
Networking Guide (Release Version: 15.0.0)
[DEFAULT]
service_plugins = router
allow_overlapping_ips = True
2. In the ml2_conf.ini file:
• Add vxlan to type drivers and project network types.
[ml2]
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
• Enable the layer-2 population mechanism driver.
[ml2]
mechanism_drivers = openvswitch,l2population
• Configure the VXLAN network ID (VNI) range.
[ml2_type_vxlan]
vni_ranges = VNI_START:VNI_END
Replace VNI_START and VNI_END with appropriate numerical values.
3. Restart the following services:
• Neutron Server
• Open vSwitch agent
Network node
1. Install the Networking service OVS layer-2 agent and layer-3 agent.
2. Install OVS.
3. In the neutron.conf file, configure common options:
[DEFAULT]
core_plugin = ml2
auth_strategy = keystone
[database]
# ...
[keystone_authtoken]
# ...
[nova]
# ...
[agent]
# ...
See the Installation Tutorials and Guides and Configuration Reference for your OpenStack release
to obtain the appropriate additional configuration for the [DEFAULT], [database], [keystone_authtoken],
[nova], and [agent] sections.
236 Deployment examples
Networking Guide (Release Version: 15.0.0)
4. Start the following services:
• OVS
5. Create the OVS provider bridge br-provider:
$ ovs-vsctl add-br br-provider
6. In the openvswitch_agent.ini file, configure the layer-2 agent.
[ovs]
bridge_mappings = provider:br-provider
local_ip = OVERLAY_INTERFACE_IP_ADDRESS
[agent]
tunnel_types = vxlan
l2_population = True
[securitygroup]
firewall_driver = iptables_hybrid
Replace OVERLAY_INTERFACE_IP_ADDRESS with the IP address of the interface that handles VXLAN
overlays for self-service networks.
7. In the l3_agent.ini file, configure the layer-3 agent.
[DEFAULT]
interface_driver = openvswitch
external_network_bridge =
Note: The external_network_bridge option intentionally contains no value.
8. Start the following services:
• Open vSwitch agent
• Layer-3 agent
Compute nodes
1. In the openvswitch_agent.ini file, enable VXLAN support including layer-2 population.
[ovs]
local_ip = OVERLAY_INTERFACE_IP_ADDRESS
[agent]
tunnel_types = vxlan
l2_population = True
Replace OVERLAY_INTERFACE_IP_ADDRESS with the IP address of the interface that handles VXLAN
overlays for self-service networks.
2. Restart the following services:
• Open vSwitch agent
Deployment examples 237
Networking Guide (Release Version: 15.0.0)
Verify service operation
1. Source the administrative project credentials.
2. Verify presence and operation of the agents.
$ openstack network agent list
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| ID | Agent Type | Host | Availability
,→Zone | Alive | State | Binary |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| 1236bbcb-e0ba-48a9-80fc-81202ca4fa51 | Metadata agent | compute2 |
,→ | True | UP | neutron-metadata-agent |
| 457d6898-b373-4bb3-b41f-59345dcfb5c5 | Open vSwitch agent | compute2 |
,→ | True | UP | neutron-openvswitch-agent |
| 71f15e84-bc47-4c2a-b9fb-317840b2d753 | DHCP agent | compute2 | nova
,→ | True | UP | neutron-dhcp-agent |
| 8805b962-de95-4e40-bdc2-7a0add7521e8 | L3 agent | network1 | nova
,→ | True | UP | neutron-l3-agent |
| a33cac5a-0266-48f6-9cac-4cef4f8b0358 | Open vSwitch agent | network1 |
,→ | True | UP | neutron-openvswitch-agent |
| a6c69690-e7f7-4e56-9831-1282753e5007 | Metadata agent | compute1 |
,→ | True | UP | neutron-metadata-agent |
| af11f22f-a9f4-404f-9fd8-cd7ad55c0f68 | DHCP agent | compute1 | nova
,→ | True | UP | neutron-dhcp-agent |
| bcfc977b-ec0e-4ba9-be62-9489b4b0e6f1 | Open vSwitch agent | compute1 |
,→ | True | UP | neutron-openvswitch-agent |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
Create initial networks
The configuration supports multiple VXLAN self-service networks. For simplicity, the following procedure
creates one self-service network and a router with a gateway on the flat provider network. The router uses NAT
for IPv4 network traffic and directly routes IPv6 network traffic.
Note: IPv6 connectivity with self-service networks often requires addition of static routes to nodes and physical
network infrastructure.
1. Source the administrative project credentials.
2. Update the provider network to support external connectivity for self-service networks.
$ openstack network set --external provider1
Note: This command provides no output.
3. Source a regular (non-administrative) project credentials.
4. Create a self-service network.
238 Deployment examples
Networking Guide (Release Version: 15.0.0)
$ openstack network create selfservice1
+-------------------------+--------------+
| Field | Value |
+-------------------------+--------------+
| admin_state_up | UP |
| mtu | 1450 |
| name | selfservice1 |
| port_security_enabled | True |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
+-------------------------+--------------+
5. Create a IPv4 subnet on the self-service network.
$ openstack subnet create --subnet-range 192.0.2.0/24 \
--network selfservice1 --dns-nameserver 8.8.4.4 selfservice1-v4
+-------------------+---------------------------+
| Field | Value |
+-------------------+---------------------------+
| allocation_pools | 192.0.2.2-192.0.2.254 |
| cidr | 192.0.2.0/24 |
| dns_nameservers | 8.8.4.4 |
| enable_dhcp | True |
| gateway_ip | 192.0.2.1 |
| ip_version | 4 |
| name | selfservice1-v4 |
+-------------------+---------------------------+
6. Create a IPv6 subnet on the self-service network.
$ openstack subnet create --subnet-range fd00:192:0:2::/64 --ip-version 6 \
--ipv6-ra-mode slaac --ipv6-address-mode slaac --network selfservice1 \
--dns-nameserver 2001:4860:4860::8844 selfservice1-v6
+-------------------+------------------------------------------------------+
| Field | Value |
+-------------------+------------------------------------------------------+
| allocation_pools | fd00:192:0:2::2-fd00:192:0:2:ffff:ffff:ffff:ffff |
| cidr | fd00:192:0:2::/64 |
| dns_nameservers | 2001:4860:4860::8844 |
| enable_dhcp | True |
| gateway_ip | fd00:192:0:2::1 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | slaac |
| name | selfservice1-v6 |
+-------------------+------------------------------------------------------+
7. Create a router.
$ openstack router create router1
+-----------------------+---------+
| Field | Value |
+-----------------------+---------+
| admin_state_up | UP |
| name | router1 |
| status | ACTIVE |
Deployment examples 239
Networking Guide (Release Version: 15.0.0)
+-----------------------+---------+
8. Add the IPv4 and IPv6 subnets as interfaces on the router.
$ openstack router add subnet router1 selfservice1-v4
$ openstack router add subnet router1 selfservice1-v6
Note: These commands provide no output.
9. Add the provider network as the gateway on the router.
$ neutron router-gateway-set router1 provider1
Set gateway for router router1
Verify network operation
1. On each compute node, verify creation of a second qdhcp namespace.
# ip netns
qdhcp-8b868082-e312-4110-8627-298109d4401c
qdhcp-8fbc13ca-cfe0-4b8a-993b-e33f37ba66d1
2. On the network node, verify creation of the qrouter namespace.
# ip netns
qrouter-17db2a15-e024-46d0-9250-4cd4d336a2cc
3. Source a regular (non-administrative) project credentials.
4. Create the appropriate security group rules to allow ping and SSH access instances using the network.
$ openstack security group rule create --proto icmp default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv4 |
| protocol | icmp |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+
$ openstack security group rule create --ethertype IPv6 --proto ipv6-icmp default
+-----------+-----------+
| Field | Value |
+-----------+-----------+
| direction | ingress |
| ethertype | IPv6 |
| protocol | ipv6-icmp |
+-----------+-----------+
$ openstack security group rule create --proto tcp --dst-port 22 default
+------------------+-----------+
| Field | Value |
240 Deployment examples
Networking Guide (Release Version: 15.0.0)
+------------------+-----------+
| direction | ingress |
| ethertype | IPv4 |
| port_range_max | 22 |
| port_range_min | 22 |
| protocol | tcp |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+
$ openstack security group rule create --ethertype IPv6 --proto tcp --dst-port 22
,→default
+------------------+-----------+
| Field | Value |
+------------------+-----------+
| direction | ingress |
| ethertype | IPv6 |
| port_range_max | 22 |
| port_range_min | 22 |
| protocol | tcp |
+------------------+-----------+
5. Launch an instance with an interface on the self-service network. For example, a CirrOS image using
flavor ID 1.
$ openstack server create --flavor 1 --image cirros --nic net-id=NETWORK_ID
,→selfservice-instance1
Replace NETWORK_ID with the ID of the self-service network.
6. Determine the IPv4 and IPv6 addresses of the instance.
$ openstack server list
+--------------------------------------+-----------------------+--------+-------------
,→-------------------------------------------------+
| ID | Name | Status | Networks
,→ |
+--------------------------------------+-----------------------+--------+-------------
,→-------------------------------------------------+
| c055cdb0-ebb4-4d65-957c-35cbdbd59306 | selfservice-instance1 | ACTIVE |
,→selfservice1=192.0.2.4, fd00:192:0:2:f816:3eff:fe30:9cb0 |
+--------------------------------------+-----------------------+--------+-------------
,→-------------------------------------------------+
Warning: The IPv4 address resides in a private IP address range (RFC1918). Thus, the Networking
service performs source network address translation (SNAT) for the instance to access external networks
such as the Internet. Access from external networks such as the Internet to the instance requires
a floating IPv4 address. The Networking service performs destination network address translation
(DNAT) from the floating IPv4 address to the instance IPv4 address on the self-service network. On
the other hand, the Networking service architecture for IPv6 lacks support for NAT due to the significantly
larger address space and complexity of NAT. Thus, floating IP addresses do not exist for
IPv6 and the Networking service only performs routing for IPv6 subnets on self-service networks.
In other words, you cannot rely on NAT to “hide” instances with IPv4 and IPv6 addresses or only
IPv6 addresses and must properly implement security groups to restrict access.
Deployment examples 241
Networking Guide (Release Version: 15.0.0)
7. On the controller node or any host with access to the provider network, ping the IPv6 address of the
instance.
$ ping6 -c 4 fd00:192:0:2:f816:3eff:fe30:9cb0
PING fd00:192:0:2:f816:3eff:fe30:9cb0(fd00:192:0:2:f816:3eff:fe30:9cb0) 56 data bytes
64 bytes from fd00:192:0:2:f816:3eff:fe30:9cb0: icmp_seq=1 ttl=63 time=2.08 ms
64 bytes from fd00:192:0:2:f816:3eff:fe30:9cb0: icmp_seq=2 ttl=63 time=1.88 ms
64 bytes from fd00:192:0:2:f816:3eff:fe30:9cb0: icmp_seq=3 ttl=63 time=1.55 ms
64 bytes from fd00:192:0:2:f816:3eff:fe30:9cb0: icmp_seq=4 ttl=63 time=1.62 ms
--- fd00:192:0:2:f816:3eff:fe30:9cb0 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3004ms
rtt min/avg/max/mdev = 1.557/1.788/2.085/0.217 ms
8. Optionally, enable IPv4 access from external networks such as the Internet to the instance.
(a) Create a floating IPv4 address on the provider network.
$ openstack floating ip create provider1
+-------------+--------------------------------------+
| Field | Value |
+-------------+--------------------------------------+
| fixed_ip | None |
| id | 22a1b088-5c9b-43b4-97f3-970ce5df77f2 |
| instance_id | None |
| ip | 203.0.113.16 |
| pool | provider1 |
+-------------+--------------------------------------+
(b) Associate the floating IPv4 address with the instance.
$ openstack server add floating ip selfservice-instance1 203.0.113.16
Note: This command provides no output.
(c) On the controller node or any host with access to the provider network, ping the floating IPv4
address of the instance.
$ ping -c 4 203.0.113.16
PING 203.0.113.16 (203.0.113.16) 56(84) bytes of data.
64 bytes from 203.0.113.16: icmp_seq=1 ttl=63 time=3.41 ms
64 bytes from 203.0.113.16: icmp_seq=2 ttl=63 time=1.67 ms
64 bytes from 203.0.113.16: icmp_seq=3 ttl=63 time=1.47 ms
64 bytes from 203.0.113.16: icmp_seq=4 ttl=63 time=1.59 ms
--- 203.0.113.16 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3005ms
rtt min/avg/max/mdev = 1.473/2.040/3.414/0.798 ms
9. Obtain access to the instance.
10. Test IPv4 and IPv6 connectivity to the Internet or other external network.
242 Deployment examples
Networking Guide (Release Version: 15.0.0)
Network traffic flow
The following sections describe the flow of network traffic in several common scenarios. North-south network
traffic travels between an instance and external network such as the Internet. East-west network traffic travels
between instances on the same or different networks. In all scenarios, the physical network infrastructure
handles switching and routing among provider networks and external networks such as the Internet. Each case
references one or more of the following components:
• Provider network (VLAN)
– VLAN ID 101 (tagged)
• Self-service network 1 (VXLAN)
– VXLAN ID (VNI) 101
• Self-service network 2 (VXLAN)
– VXLAN ID (VNI) 102
• Self-service router
– Gateway on the provider network
– Interface on self-service network 1
– Interface on self-service network 2
• Instance 1
• Instance 2
North-south scenario 1: Instance with a fixed IP address
For instances with a fixed IPv4 address, the network node performs SNAT on north-south traffic passing from
self-service to external networks such as the Internet. For instances with a fixed IPv6 address, the network node
performs conventional routing of traffic between self-service and external networks.
• The instance resides on compute node 1 and uses self-service network 1.
• The instance sends a packet to a host on the Internet.
The following steps involve compute node 1:
1. The instance interface (1) forwards the packet to the security group bridge instance port (2) via veth
pair.
2. Security group rules (3) on the security group bridge handle firewalling and connection tracking for the
packet.
3. The security group bridge OVS port (4) forwards the packet to the OVS integration bridge security group
port (5) via veth pair.
4. The OVS integration bridge adds an internal VLAN tag to the packet.
5. The OVS integration bridge exchanges the internal VLAN tag for an internal tunnel ID.
6. The OVS integration bridge patch port (6) forwards the packet to the OVS tunnel bridge patch port (7).
7. The OVS tunnel bridge (8) wraps the packet using VNI 101.
Deployment examples 243
Networking Guide (Release Version: 15.0.0)
8. The underlying physical interface (9) for overlay networks forwards the packet to the network node via
the overlay network (10).
The following steps involve the network node:
1. The underlying physical interface (11) for overlay networks forwards the packet to the OVS tunnel bridge
(12).
2. The OVS tunnel bridge unwraps the packet and adds an internal tunnel ID to it.
3. The OVS tunnel bridge exchanges the internal tunnel ID for an internal VLAN tag.
4. The OVS tunnel bridge patch port (13) forwards the packet to the OVS integration bridge patch port (14).
5. The OVS integration bridge port for the self-service network (15) removes the internal VLAN tag and
forwards the packet to the self-service network interface (16) in the router namespace.
• For IPv4, the router performs SNAT on the packet which changes the source IP address to the router
IP address on the provider network and sends it to the gateway IP address on the provider network
via the gateway interface on the provider network (17).
• For IPv6, the router sends the packet to the next-hop IP address, typically the gateway IP address
on the provider network, via the provider gateway interface (17).
6. The router forwards the packet to the OVS integration bridge port for the provider network (18).
7. The OVS integration bridge adds the internal VLAN tag to the packet.
8. The OVS integration bridge int-br-provider patch port (19) forwards the packet to the OVS provider
bridge phy-br-provider patch port (20).
9. The OVS provider bridge swaps the internal VLAN tag with actual VLAN tag 101.
10. The OVS provider bridge provider network port (21) forwards the packet to the physical network interface
(22).
11. The physical network interface forwards the packet to the Internet via physical network infrastructure
(23).
Note: Return traffic follows similar steps in reverse. However, without a floating IPv4 address, hosts on the
provider or external networks cannot originate connections to instances on the self-service network.
244 Deployment examples
Networking Guide (Release Version: 15.0.0)
Deployment examples 245
Networking Guide (Release Version: 15.0.0)
North-south scenario 2: Instance with a floating IPv4 address
For instances with a floating IPv4 address, the network node performs SNAT on north-south traffic passing from
the instance to external networks such as the Internet and DNAT on north-south traffic passing from external
networks to the instance. Floating IP addresses and NAT do not apply to IPv6. Thus, the network node routes
IPv6 traffic in this scenario.
• The instance resides on compute node 1 and uses self-service network 1.
• A host on the Internet sends a packet to the instance.
The following steps involve the network node:
1. The physical network infrastructure (1) forwards the packet to the provider physical network interface
(2).
2. The provider physical network interface forwards the packet to the OVS provider bridge provider network
port (3).
3. The OVS provider bridge swaps actual VLAN tag 101 with the internal VLAN tag.
4. The OVS provider bridge phy-br-provider port (4) forwards the packet to the OVS integration bridge
int-br-provider port (5).
5. The OVS integration bridge port for the provider network (6) removes the internal VLAN tag and forwards
the packet to the provider network interface (6) in the router namespace.
• For IPv4, the router performs DNAT on the packet which changes the destination IP address to
the instance IP address on the self-service network and sends it to the gateway IP address on the
self-service network via the self-service interface (7).
• For IPv6, the router sends the packet to the next-hop IP address, typically the gateway IP address
on the self-service network, via the self-service interface (8).
6. The router forwards the packet to the OVS integration bridge port for the self-service network (9).
7. The OVS integration bridge adds an internal VLAN tag to the packet.
8. The OVS integration bridge exchanges the internal VLAN tag for an internal tunnel ID.
9. The OVS integration bridge patch-tun patch port (10) forwards the packet to the OVS tunnel bridge
patch-int patch port (11).
10. The OVS tunnel bridge (12) wraps the packet using VNI 101.
11. The underlying physical interface (13) for overlay networks forwards the packet to the network node via
the overlay network (14).
The following steps involve the compute node:
1. The underlying physical interface (15) for overlay networks forwards the packet to the OVS tunnel bridge
(16).
2. The OVS tunnel bridge unwraps the packet and adds an internal tunnel ID to it.
3. The OVS tunnel bridge exchanges the internal tunnel ID for an internal VLAN tag.
4. The OVS tunnel bridge patch-int patch port (17) forwards the packet to the OVS integration bridge
patch-tun patch port (18).
5. The OVS integration bridge removes the internal VLAN tag from the packet.
246 Deployment examples
Networking Guide (Release Version: 15.0.0)
6. The OVS integration bridge security group port (19) forwards the packet to the security group bridge
OVS port (20) via veth pair.
7. Security group rules (21) on the security group bridge handle firewalling and connection tracking for the
packet.
8. The security group bridge instance port (22) forwards the packet to the instance interface (23) via veth
pair.
Deployment examples 247
Networking Guide (Release Version: 15.0.0)
248 Deployment examples
Networking Guide (Release Version: 15.0.0)
Note: Egress instance traffic flows similar to north-south scenario 1, except SNAT changes the source IP
address of the packet to the floating IPv4 address rather than the router IP address on the provider network.
East-west scenario 1: Instances on the same network
Instances with a fixed IPv4/IPv6 address or floating IPv4 address on the same network communicate directly
between compute nodes containing those instances.
By default, the VXLAN protocol lacks knowledge of target location and uses multicast to discover it. After
discovery, it stores the location in the local forwarding database. In large deployments, the discovery process
can generate a significant amount of network that all nodes must process. To eliminate the latter and generally
increase efficiency, the Networking service includes the layer-2 population mechanism driver that automatically
populates the forwarding database for VXLAN interfaces. The example configuration enables this driver. For
more information, see ML2 plug-in.
• Instance 1 resides on compute node 1 and uses self-service network 1.
• Instance 2 resides on compute node 2 and uses self-service network 1.
• Instance 1 sends a packet to instance 2.
The following steps involve compute node 1:
1. The instance 1 interface (1) forwards the packet to the security group bridge instance port (2) via veth
pair.
2. Security group rules (3) on the security group bridge handle firewalling and connection tracking for the
packet.
3. The security group bridge OVS port (4) forwards the packet to the OVS integration bridge security group
port (5) via veth pair.
4. The OVS integration bridge adds an internal VLAN tag to the packet.
5. The OVS integration bridge exchanges the internal VLAN tag for an internal tunnel ID.
6. The OVS integration bridge patch port (6) forwards the packet to the OVS tunnel bridge patch port (7).
7. The OVS tunnel bridge (8) wraps the packet using VNI 101.
8. The underlying physical interface (9) for overlay networks forwards the packet to compute node 2 via
the overlay network (10).
The following steps involve compute node 2:
1. The underlying physical interface (11) for overlay networks forwards the packet to the OVS tunnel bridge
(12).
2. The OVS tunnel bridge unwraps the packet and adds an internal tunnel ID to it.
3. The OVS tunnel bridge exchanges the internal tunnel ID for an internal VLAN tag.
4. The OVS tunnel bridge patch-int patch port (13) forwards the packet to the OVS integration bridge
patch-tun patch port (14).
5. The OVS integration bridge removes the internal VLAN tag from the packet.
Deployment examples 249
Networking Guide (Release Version: 15.0.0)
6. The OVS integration bridge security group port (15) forwards the packet to the security group bridge
OVS port (16) via veth pair.
7. Security group rules (17) on the security group bridge handle firewalling and connection tracking for the
packet.
8. The security group bridge instance port (18) forwards the packet to the instance 2 interface (19) via veth
pair.
250 Deployment examples
Networking Guide (Release Version: 15.0.0)
Note: Return traffic follows similar steps in reverse.
Deployment examples 251
Networking Guide (Release Version: 15.0.0)
East-west scenario 2: Instances on different networks
Instances using a fixed IPv4/IPv6 address or floating IPv4 address communicate via router on the network node.
The self-service networks must reside on the same router.
• Instance 1 resides on compute node 1 and uses self-service network 1.
• Instance 2 resides on compute node 1 and uses self-service network 2.
• Instance 1 sends a packet to instance 2.
Note: Both instances reside on the same compute node to illustrate how VXLAN enables multiple overlays to
use the same layer-3 network.
The following steps involve the compute node:
1. The instance interface (1) forwards the packet to the security group bridge instance port (2) via veth
pair.
2. Security group rules (3) on the security group bridge handle firewalling and connection tracking for the
packet.
3. The security group bridge OVS port (4) forwards the packet to the OVS integration bridge security group
port (5) via veth pair.
4. The OVS integration bridge adds an internal VLAN tag to the packet.
5. The OVS integration bridge exchanges the internal VLAN tag for an internal tunnel ID.
6. The OVS integration bridge patch-tun patch port (6) forwards the packet to the OVS tunnel bridge
patch-int patch port (7).
7. The OVS tunnel bridge (8) wraps the packet using VNI 101.
8. The underlying physical interface (9) for overlay networks forwards the packet to the network node via
the overlay network (10).
The following steps involve the network node:
1. The underlying physical interface (11) for overlay networks forwards the packet to the OVS tunnel bridge
(12).
2. The OVS tunnel bridge unwraps the packet and adds an internal tunnel ID to it.
3. The OVS tunnel bridge exchanges the internal tunnel ID for an internal VLAN tag.
4. The OVS tunnel bridge patch-int patch port (13) forwards the packet to the OVS integration bridge
patch-tun patch port (14).
5. The OVS integration bridge port for self-service network 1 (15) removes the internal VLAN tag and
forwards the packet to the self-service network 1 interface (16) in the router namespace.
6. The router sends the packet to the next-hop IP address, typically the gateway IP address on self-service
network 2, via the self-service network 2 interface (17).
7. The router forwards the packet to the OVS integration bridge port for self-service network 2 (18).
8. The OVS integration bridge adds the internal VLAN tag to the packet.
9. The OVS integration bridge exchanges the internal VLAN tag for an internal tunnel ID.
252 Deployment examples
Networking Guide (Release Version: 15.0.0)
10. The OVS integration bridge patch-tun patch port (19) forwards the packet to the OVS tunnel bridge
patch-int patch port (20).
11. The OVS tunnel bridge (21) wraps the packet using VNI 102.
12. The underlying physical interface (22) for overlay networks forwards the packet to the compute node via
the overlay network (23).
The following steps involve the compute node:
1. The underlying physical interface (24) for overlay networks forwards the packet to the OVS tunnel bridge
(25).
2. The OVS tunnel bridge unwraps the packet and adds an internal tunnel ID to it.
3. The OVS tunnel bridge exchanges the internal tunnel ID for an internal VLAN tag.
4. The OVS tunnel bridge patch-int patch port (26) forwards the packet to the OVS integration bridge
patch-tun patch port (27).
5. The OVS integration bridge removes the internal VLAN tag from the packet.
6. The OVS integration bridge security group port (28) forwards the packet to the security group bridge
OVS port (29) via veth pair.
7. Security group rules (30) on the security group bridge handle firewalling and connection tracking for the
packet.
8. The security group bridge instance port (31) forwards the packet to the instance interface (32) via veth
pair.
Note: Return traffic follows similar steps in reverse.
Deployment examples 253
Networking Guide (Release Version: 15.0.0)
254 Deployment examples
Networking Guide (Release Version: 15.0.0)
Open vSwitch: High availability using VRRP
This architecture example augments the self-service deployment example with a high-availability mechanism
using the Virtual Router Redundancy Protocol (VRRP) via keepalived and provides failover of routing for
self-service networks. It requires a minimum of two network nodes because VRRP creates one master (active)
instance and at least one backup instance of each router.
During normal operation, keepalived on the master router periodically transmits heartbeat packets over a
hidden network that connects all VRRP routers for a particular project. Each project with VRRP routers uses
a separate hidden network. By default this network uses the first value in the tenant_network_types option
in the ml2_conf.ini file. For additional control, you can specify the self-service network type and physical
network name for the hidden network using the l3_ha_network_type and l3_ha_network_name options in
the neutron.conf file.
If keepalived on the backup router stops receiving heartbeat packets, it assumes failure of the master router
and promotes the backup router to master router by configuring IP addresses on the interfaces in the qrouter
namespace. In environments with more than one backup router, keepalived on the backup router with the
next highest priority promotes that backup router to master router.
Note: This high-availability mechanism configures VRRP using the same priority for all routers. Therefore,
VRRP promotes the backup router with the highest IP address to the master router.
Warning: There is a known bug with keepalived v1.2.15 and earlier which can cause packet loss
when max_l3_agents_per_router is set to 3 or more. Therefore, we recommend that you upgrade to
keepalived v1.2.16 or greater when using this feature.
Interruption of VRRP heartbeat traffic between network nodes, typically due to a network interface or physical
network infrastructure failure, triggers a failover. Restarting the layer-3 agent, or failure of it, does not trigger
a failover providing keepalived continues to operate.
Consider the following attributes of this high-availability mechanism to determine practicality in your environment:
• Instance network traffic on self-service networks using a particular router only traverses the master instance
of that router. Thus, resource limitations of a particular network node can impact all master
instances of routers on that network node without triggering failover to another network node. However,
you can configure the scheduler to distribute the master instance of each router uniformly across a pool
of network nodes to reduce the chance of resource contention on any particular network node.
• Only supports self-service networks using a router. Provider networks operate at layer-2 and rely on
physical network infrastructure for redundancy.
• For instances with a floating IPv4 address, maintains state of network connections during failover as a
side effect of 1:1 static NAT. The mechanism does not actually implement connection tracking.
For production deployments, we recommend at least three network nodes with sufficient resources to handle
network traffic for the entire environment if one network node fails. Also, the remaining two nodes can continue
to provide redundancy.
Deployment examples 255
Networking Guide (Release Version: 15.0.0)
Prerequisites
Add one network node with the following components:
• Three network interfaces: management, provider, and overlay.
• OpenStack Networking layer-2 agent, layer-3 agent, and any dependencies.
Note: You can keep the DHCP and metadata agents on each compute node or move them to the network nodes.
256 Deployment examples
Networking Guide (Release Version: 15.0.0)
Architecture
Deployment examples 257
Networking Guide (Release Version: 15.0.0)
The following figure shows components and connectivity for one self-service network and one untagged (flat)
network. The master router resides on network node 1. In this particular case, the instance resides on the same
compute node as the DHCP agent for the network. If the DHCP agent resides on another compute node, the
latter only contains a DHCP namespace and Linux bridge with a port on the overlay physical network interface.
258 Deployment examples
Networking Guide (Release Version: 15.0.0)
Deployment examples 259
Networking Guide (Release Version: 15.0.0)
Example configuration
Use the following example configuration as a template to add support for high-availability using VRRP to an
existing operational environment that supports self-service networks.
Controller node
1. In the neutron.conf file:
• Enable VRRP.
[DEFAULT]
l3_ha = True
2. Restart the following services:
• Server
Network node 1
No changes.
Network node 2
1. Install the Networking service OVS layer-2 agent and layer-3 agent.
2. Install OVS.
3. In the neutron.conf file, configure common options:
[DEFAULT]
core_plugin = ml2
auth_strategy = keystone
[database]
# ...
[keystone_authtoken]
# ...
[nova]
# ...
[agent]
# ...
See the Installation Tutorials and Guides and Configuration Reference for your OpenStack release
to obtain the appropriate additional configuration for the [DEFAULT], [database], [keystone_authtoken],
[nova], and [agent] sections.
4. Start the following services:
• OVS
260 Deployment examples
Networking Guide (Release Version: 15.0.0)
5. Create the OVS provider bridge br-provider:
$ ovs-vsctl add-br br-provider
6. In the openvswitch_agent.ini file, configure the layer-2 agent.
[ovs]
bridge_mappings = provider:br-provider
local_ip = OVERLAY_INTERFACE_IP_ADDRESS
[agent]
tunnel_types = vxlan
l2_population = true
[securitygroup]
firewall_driver = iptables_hybrid
Replace OVERLAY_INTERFACE_IP_ADDRESS with the IP address of the interface that handles VXLAN
overlays for self-service networks.
7. In the l3_agent.ini file, configure the layer-3 agent.
[DEFAULT]
interface_driver = openvswitch
external_network_bridge =
Note: The external_network_bridge option intentionally contains no value.
8. Start the following services:
• Open vSwitch agent
• Layer-3 agent
Compute nodes
No changes.
Verify service operation
1. Source the administrative project credentials.
2. Verify presence and operation of the agents.
$ openstack network agent list
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| ID | Agent Type | Host | Availability
,→Zone | Alive | State | Binary |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| 1236bbcb-e0ba-48a9-80fc-81202ca4fa51 | Metadata agent | compute2 |
,→ | True | UP | neutron-metadata-agent |
| 457d6898-b373-4bb3-b41f-59345dcfb5c5 | Open vSwitch agent | compute2 |
,→ | True | UP | neutron-openvswitch-agent |
Deployment examples 261
Networking Guide (Release Version: 15.0.0)
| 71f15e84-bc47-4c2a-b9fb-317840b2d753 | DHCP agent | compute2 | nova
,→ | True | UP | neutron-dhcp-agent |
| 8805b962-de95-4e40-bdc2-7a0add7521e8 | L3 agent | network1 | nova
,→ | True | UP | neutron-l3-agent |
| a33cac5a-0266-48f6-9cac-4cef4f8b0358 | Open vSwitch agent | network1 |
,→ | True | UP | neutron-openvswitch-agent |
| a6c69690-e7f7-4e56-9831-1282753e5007 | Metadata agent | compute1 |
,→ | True | UP | neutron-metadata-agent |
| af11f22f-a9f4-404f-9fd8-cd7ad55c0f68 | DHCP agent | compute1 | nova
,→ | True | UP | neutron-dhcp-agent |
| bcfc977b-ec0e-4ba9-be62-9489b4b0e6f1 | Open vSwitch agent | compute1 |
,→ | True | UP | neutron-openvswitch-agent |
| 7f00d759-f2c9-494a-9fbf-fd9118104d03 | Open vSwitch agent | network2 |
,→ | True | UP | neutron-openvswitch-agent |
| b28d8818-9e32-4888-930b-29addbdd2ef9 | L3 agent | network2 | nova
,→ | True | UP | neutron-l3-agent |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
Create initial networks
Similar to the self-service deployment example, this configuration supports multiple VXLAN self-service networks.
After enabling high-availability, all additional routers use VRRP. The following procedure creates an
additional self-service network and router. The Networking service also supports adding high-availability to
existing routers. However, the procedure requires administratively disabling and enabling each router which
temporarily interrupts network connectivity for self-service networks with interfaces on that router.
1. Source a regular (non-administrative) project credentials.
2. Create a self-service network.
$ openstack network create selfservice2
+-------------------------+--------------+
| Field | Value |
+-------------------------+--------------+
| admin_state_up | UP |
| mtu | 1450 |
| name | selfservice2 |
| port_security_enabled | True |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
+-------------------------+--------------+
3. Create a IPv4 subnet on the self-service network.
$ openstack subnet create --subnet-range 198.51.100.0/24 \
--network selfservice2 --dns-nameserver 8.8.4.4 selfservice2-v4
+-------------------+------------------------------+
| Field | Value |
+-------------------+------------------------------+
| allocation_pools | 198.51.100.2-198.51.100.254 |
| cidr | 198.51.100.0/24 |
| dns_nameservers | 8.8.4.4 |
| enable_dhcp | True |
262 Deployment examples
Networking Guide (Release Version: 15.0.0)
| gateway_ip | 198.51.100.1 |
| ip_version | 4 |
| name | selfservice2-v4 |
+-------------------+------------------------------+
4. Create a IPv6 subnet on the self-service network.
$ openstack subnet create --subnet-range fd00:198:51:100::/64 --ip-version 6 \
--ipv6-ra-mode slaac --ipv6-address-mode slaac --network selfservice2 \
--dns-nameserver 2001:4860:4860::8844 selfservice2-v6
+-------------------+--------------------------------------------------------+
| Field | Value |
+-------------------+--------------------------------------------------------+
| allocation_pools | fd00:198:51:100::2-fd00:198:51:100:ffff:ffff:ffff:ffff |
| cidr | fd00:198:51:100::/64 |
| dns_nameservers | 2001:4860:4860::8844 |
| enable_dhcp | True |
| gateway_ip | fd00:198:51:100::1 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | slaac |
| name | selfservice2-v6 |
+-------------------+--------------------------------------------------------+
5. Create a router.
$ openstack router create router2
+-----------------------+---------+
| Field | Value |
+-----------------------+---------+
| admin_state_up | UP |
| name | router2 |
| status | ACTIVE |
+-----------------------+---------+
6. Add the IPv4 and IPv6 subnets as interfaces on the router.
$ openstack router add subnet router2 selfservice2-v4
$ openstack router add subnet router2 selfservice2-v6
Note: These commands provide no output.
7. Add the provider network as a gateway on the router.
$ neutron router-gateway-set router2 provider1
Set gateway for router router2
Verify network operation
1. Source the administrative project credentials.
2. Verify creation of the internal high-availability network that handles VRRP heartbeat traffic.
Deployment examples 263
Networking Guide (Release Version: 15.0.0)
$ openstack network list
+--------------------------------------+----------------------------------------------
,→------+--------------------------------------+
| ID | Name
,→ | Subnets |
+--------------------------------------+----------------------------------------------
,→------+--------------------------------------+
| 1b8519c1-59c4-415c-9da2-a67d53c68455 | HA network tenant
,→f986edf55ae945e2bef3cb4bfd589928 | 6843314a-1e76-4cc9-94f5-c64b7a39364a |
+--------------------------------------+----------------------------------------------
,→------+--------------------------------------+
3. On each network node, verify creation of a qrouter namespace with the same ID.
Network node 1:
# ip netns
qrouter-b6206312-878e-497c-8ef7-eb384f8add96
Network node 2:
# ip netns
qrouter-b6206312-878e-497c-8ef7-eb384f8add96
Note: The namespace for router 1 from Linux bridge: Self-service networks should only appear on
network node 1 because of creation prior to enabling VRRP.
4. On each network node, show the IP address of interfaces in the qrouter namespace. With the exception
of the VRRP interface, only one namespace belonging to the master router instance contains IP addresses
on the interfaces.
Network node 1:
# ip netns exec qrouter-b6206312-878e-497c-8ef7-eb384f8add96 ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default
,→qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
2: ha-eb820380-40@if21: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:78:ba:99 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 169.254.192.1/18 brd 169.254.255.255 scope global ha-eb820380-40
valid_lft forever preferred_lft forever
inet 169.254.0.1/24 scope global ha-eb820380-40
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe78:ba99/64 scope link
valid_lft forever preferred_lft forever
3: qr-da3504ad-ba@if24: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:dc:8e:a8 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 198.51.100.1/24 scope global qr-da3504ad-ba
valid_lft forever preferred_lft forever
264 Deployment examples
Networking Guide (Release Version: 15.0.0)
inet6 fe80::f816:3eff:fedc:8ea8/64 scope link
valid_lft forever preferred_lft forever
4: qr-442e36eb-fc@if27: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:ee:c8:41 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet6 fd00:198:51:100::1/64 scope global nodad
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:feee:c841/64 scope link
valid_lft forever preferred_lft forever
5: qg-33fedbc5-43@if28: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:03:1a:f6 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 203.0.113.21/24 scope global qg-33fedbc5-43
valid_lft forever preferred_lft forever
inet6 fd00:203:0:113::21/64 scope global nodad
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe03:1af6/64 scope link
valid_lft forever preferred_lft forever
Network node 2:
# ip netns exec qrouter-b6206312-878e-497c-8ef7-eb384f8add96 ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default
,→qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
2: ha-7a7ce184-36@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state
,→UP group default qlen 1000
link/ether fa:16:3e:16:59:84 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 169.254.192.2/18 brd 169.254.255.255 scope global ha-7a7ce184-36
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe16:5984/64 scope link
valid_lft forever preferred_lft forever
3: qr-da3504ad-ba@if11: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:dc:8e:a8 brd ff:ff:ff:ff:ff:ff link-netnsid 0
4: qr-442e36eb-fc@if14: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue
,→state UP group default qlen 1000
5: qg-33fedbc5-43@if15: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue
,→state UP group default qlen 1000
link/ether fa:16:3e:03:1a:f6 brd ff:ff:ff:ff:ff:ff link-netnsid 0
Note: The master router may reside on network node 2.
5. Launch an instance with an interface on the addtional self-service network. For example, a CirrOS image
using flavor ID 1.
$ openstack server create --flavor 1 --image cirros --nic net-id=NETWORK_ID
,→selfservice-instance2
Replace NETWORK_ID with the ID of the additional self-service network.
Deployment examples 265
Networking Guide (Release Version: 15.0.0)
6. Determine the IPv4 and IPv6 addresses of the instance.
$ openstack server list
+--------------------------------------+-----------------------+--------+-------------
,→--------------------------------------------------------------+
| ID | Name | Status | Networks
,→ |
+--------------------------------------+-----------------------+--------+-------------
,→--------------------------------------------------------------+
| bde64b00-77ae-41b9-b19a-cd8e378d9f8b | selfservice-instance2 | ACTIVE |
,→selfservice2=fd00:198:51:100:f816:3eff:fe71:e93e, 198.51.100.4 |
+--------------------------------------+-----------------------+--------+-------------
,→--------------------------------------------------------------+
7. Create a floating IPv4 address on the provider network.
$ openstack floating ip create provider1
+-------------+--------------------------------------+
| Field | Value |
+-------------+--------------------------------------+
| fixed_ip | None |
| id | 0174056a-fa56-4403-b1ea-b5151a31191f |
| instance_id | None |
| ip | 203.0.113.17 |
| pool | provider1 |
+-------------+--------------------------------------+
8. Associate the floating IPv4 address with the instance.
$ openstack server add floating ip selfservice-instance2 203.0.113.17
Note: This command provides no output.
Verify failover operation
1. Begin a continuous ping of both the floating IPv4 address and IPv6 address of the instance. While
performing the next three steps, you should see a minimal, if any, interruption of connectivity to the
instance.
2. On the network node with the master router, administratively disable the overlay network interface.
3. On the other network node, verify promotion of the backup router to master router by noting addition of
IP addresses to the interfaces in the qrouter namespace.
4. On the original network node in step 2, administratively enable the overlay network interface. Note that
the master router remains on the network node in step 3.
Keepalived VRRP health check
The health of your keepalived instances can be automatically monitored via a bash script that verifies connectivity
to all available and configured gateway addresses. In the event that connectivity is lost, the master
router is rescheduled to another node.
266 Deployment examples
Networking Guide (Release Version: 15.0.0)
If all routers lose connectivity simultaneously, the process of selecting a new master router will be repeated in
a round-robin fashion until one or more routers have their connectivity restored.
To enable this feature, edit the l3_agent.ini file:
ha_vrrp_health_check_interval = 30
Where ha_vrrp_health_check_interval indicates how often in seconds the health check should run. The
default value is 0, which indicates that the check should not run at all.
Network traffic flow
This high-availability mechanism simply augments Open vSwitch: Self-service networks with failover of layer-
3 services to another router if the master router fails. Thus, you can reference Self-service network traffic flow
for normal operation.
Open vSwitch: High availability using DVR
This architecture example augments the self-service deployment example with the Distributed Virtual Router
(DVR) high-availability mechanism that provides connectivity between self-service and provider networks on
compute nodes rather than network nodes for specific scenarios. For instances with a floating IPv4 address,
routing between self-service and provider networks resides completely on the compute nodes to eliminate single
point of failure and performance issues with network nodes. Routing also resides completely on the compute
nodes for instances with a fixed or floating IPv4 address using self-service networks on the same distributed
virtual router. However, instances with a fixed IP address still rely on the network node for routing and SNAT
services between self-service and provider networks.
Consider the following attributes of this high-availability mechanism to determine practicality in your environment:
• Only provides connectivity to an instance via the compute node on which the instance resides if the
instance resides on a self-service network with a floating IPv4 address. Instances on self-service networks
with only an IPv6 address or both IPv4 and IPv6 addresses rely on the network node for IPv6 connectivity.
• The instance of a router on each compute node consumes an IPv4 address on the provider network on
which it contains a gateway.
Prerequisites
Modify the compute nodes with the following components:
• Install the OpenStack Networking layer-3 agent.
Note: Consider adding at least one additional network node to provide high-availability for instances with a
fixed IP address. See See Distributed Virtual Routing with VRRP for more information.
Deployment examples 267
Networking Guide (Release Version: 15.0.0)
Architecture
268 Deployment examples
Networking Guide (Release Version: 15.0.0)
The following figure shows components and connectivity for one self-service network and one untagged (flat)
network. In this particular case, the instance resides on the same compute node as the DHCP agent for the
network. If the DHCP agent resides on another compute node, the latter only contains a DHCP namespace with
a port on the OVS integration bridge.
Deployment examples 269
Networking Guide (Release Version: 15.0.0)
Example configuration
Use the following example configuration as a template to add support for high-availability using DVR to an
existing operational environment that supports self-service networks.
Controller node
1. In the neutron.conf file:
• Enable distributed routing by default for all routers.
[DEFAULT]
router_distributed = True
2. Restart the following services:
• Server
Network node
1. In the openswitch_agent.ini file, enable distributed routing.
[DEFAULT]
enable_distributed_routing = True
2. In the l3_agent.ini file, configure the layer-3 agent to provide SNAT services.
[DEFAULT]
agent_mode = dvr_snat
Note: The external_network_bridge option intentionally contains no value.
3. Restart the following services:
• Open vSwitch agent
• Layer-3 agent
Compute nodes
1. Install the Networking service layer-3 agent.
2. In the openswitch_agent.ini file, enable distributed routing.
[DEFAULT]
enable_distributed_routing = True
3. In the l3_agent.ini file, configure the layer-3 agent.
270 Deployment examples
Networking Guide (Release Version: 15.0.0)
[DEFAULT]
interface_driver = openvswitch
external_network_bridge =
agent_mode = dvr
Note: The external_network_bridge option intentionally contains no value.
4. Restart the following services:
• Open vSwitch agent
• Layer-3 agent
Verify service operation
1. Source the administrative project credentials.
2. Verify presence and operation of the agents.
$ openstack network agent list
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| ID | Agent Type | Host | Availability
,→Zone | Alive | State | Binary |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
| 05d980f2-a4fc-4815-91e7-a7f7e118c0db | L3 agent | compute1 | nova
,→ | True | UP | neutron-l3-agent |
| 1236bbcb-e0ba-48a9-80fc-81202ca4fa51 | Metadata agent | compute2 |
,→ | True | UP | neutron-metadata-agent |
| 2a2e9a90-51b8-4163-a7d6-3e199ba2374b | L3 agent | compute2 | nova
,→ | True | UP | neutron-l3-agent |
| 457d6898-b373-4bb3-b41f-59345dcfb5c5 | Open vSwitch agent | compute2 |
,→ | True | UP | neutron-openvswitch-agent |
| 513caa68-0391-4e53-a530-082e2c23e819 | Linux bridge agent | compute1 |
,→ | True | UP | neutron-linuxbridge-agent |
| 71f15e84-bc47-4c2a-b9fb-317840b2d753 | DHCP agent | compute2 | nova
,→ | True | UP | neutron-dhcp-agent |
| 8805b962-de95-4e40-bdc2-7a0add7521e8 | L3 agent | network1 | nova
,→ | True | UP | neutron-l3-agent |
| a33cac5a-0266-48f6-9cac-4cef4f8b0358 | Open vSwitch agent | network1 |
,→ | True | UP | neutron-openvswitch-agent |
| a6c69690-e7f7-4e56-9831-1282753e5007 | Metadata agent | compute1 |
,→ | True | UP | neutron-metadata-agent |
| af11f22f-a9f4-404f-9fd8-cd7ad55c0f68 | DHCP agent | compute1 | nova
,→ | True | UP | neutron-dhcp-agent |
| bcfc977b-ec0e-4ba9-be62-9489b4b0e6f1 | Open vSwitch agent | compute1 |
,→ | True | UP | neutron-openvswitch-agent |
+--------------------------------------+--------------------+----------+--------------
,→-----+-------+-------+---------------------------+
Deployment examples 271
Networking Guide (Release Version: 15.0.0)
Create initial networks
Similar to the self-service deployment example, this configuration supports multiple VXLAN self-service networks.
After enabling high-availability, all additional routers use distributed routing. The following procedure
creates an additional self-service network and router. The Networking service also supports adding distributed
routing to existing routers.
1. Source a regular (non-administrative) project credentials.
2. Create a self-service network.
$ openstack network create selfservice2
+-------------------------+--------------+
| Field | Value |
+-------------------------+--------------+
| admin_state_up | UP |
| mtu | 1450 |
| name | selfservice2 |
| port_security_enabled | True |
| router:external | Internal |
| shared | False |
| status | ACTIVE |
+-------------------------+--------------+
3. Create a IPv4 subnet on the self-service network.
$ openstack subnet create --subnet-range 192.0.2.0/24 \
--network selfservice2 --dns-nameserver 8.8.4.4 selfservice2-v4
+-------------------+---------------------------+
| Field | Value |
+-------------------+---------------------------+
| allocation_pools | 192.0.2.2-192.0.2.254 |
| cidr | 192.0.2.0/24 |
| dns_nameservers | 8.8.4.4 |
| enable_dhcp | True |
| gateway_ip | 192.0.2.1 |
| ip_version | 4 |
| name | selfservice2-v4 |
+-------------------+---------------------------+
4. Create a IPv6 subnet on the self-service network.
$ openstack subnet create --subnet-range fd00:192:0:2::/64 --ip-version 6 \
--ipv6-ra-mode slaac --ipv6-address-mode slaac --network selfservice2 \
--dns-nameserver 2001:4860:4860::8844 selfservice2-v6
+-------------------+------------------------------------------------------+
| Field | Value |
+-------------------+------------------------------------------------------+
| allocation_pools | fd00:192:0:2::2-fd00:192:0:2:ffff:ffff:ffff:ffff |
| cidr | fd00:192:0:2::/64 |
| dns_nameservers | 2001:4860:4860::8844 |
| enable_dhcp | True |
| gateway_ip | fd00:192:0:2::1 |
| ip_version | 6 |
| ipv6_address_mode | slaac |
| ipv6_ra_mode | slaac |
| name | selfservice2-v6 |
272 Deployment examples
Networking Guide (Release Version: 15.0.0)
+-------------------+------------------------------------------------------+
5. Create a router.
$ openstack router create router2
+-----------------------+---------+
| Field | Value |
+-----------------------+---------+
| admin_state_up | UP |
| name | router2 |
| status | ACTIVE |
+-----------------------+---------+
6. Add the IPv4 and IPv6 subnets as interfaces on the router.
$ openstack router add subnet router2 selfservice2-v4
$ openstack router add subnet router2 selfservice2-v6
Note: These commands provide no output.
7. Add the provider network as a gateway on the router.
$ openstack router set router2 --external-gateway provider1
Verify network operation
1. Source the administrative project credentials.
2. Verify distributed routing on the router.
$ openstack router show router2
+-------------------------+---------+
| Field | Value |
+-------------------------+---------+
| admin_state_up | UP |
| distributed | True |
| ha | False |
| name | router2 |
| status | ACTIVE |
+-------------------------+---------+
3. On each compute node, verify creation of a qrouter namespace with the same ID.
Compute node 1:
# ip netns
qrouter-78d2f628-137c-4f26-a257-25fc20f203c1
Compute node 2:
# ip netns
qrouter-78d2f628-137c-4f26-a257-25fc20f203c1
Deployment examples 273
Networking Guide (Release Version: 15.0.0)
4. On the network node, verify creation of the snat and qrouter namespaces with the same ID.
# ip netns
snat-78d2f628-137c-4f26-a257-25fc20f203c1
qrouter-78d2f628-137c-4f26-a257-25fc20f203c1
Note: The namespace for router 1 from Open vSwitch: Self-service networks should also appear on
network node 1 because of creation prior to enabling distributed routing.
5. Launch an instance with an interface on the addtional self-service network. For example, a CirrOS image
using flavor ID 1.
$ openstack server create --flavor 1 --image cirros --nic net-id=NETWORK_ID
,→selfservice-instance2
Replace NETWORK_ID with the ID of the additional self-service network.
6. Determine the IPv4 and IPv6 addresses of the instance.
$ openstack server list
+--------------------------------------+-----------------------+--------+-------------
,→--------------------------------------------------------------+
| ID | Name | Status | Networks
,→ |
+--------------------------------------+-----------------------+--------+-------------
,→--------------------------------------------------------------+
| bde64b00-77ae-41b9-b19a-cd8e378d9f8b | selfservice-instance2 | ACTIVE |
,→selfservice2=fd00:192:0:2:f816:3eff:fe71:e93e, 192.0.2.4 |
+--------------------------------------+-----------------------+--------+-------------
,→--------------------------------------------------------------+
7. Create a floating IPv4 address on the provider network.
$ openstack floating ip create provider1
+-------------+--------------------------------------+
| Field | Value |
+-------------+--------------------------------------+
| fixed_ip | None |
| id | 0174056a-fa56-4403-b1ea-b5151a31191f |
| instance_id | None |
| ip | 203.0.113.17 |
| pool | provider1 |
+-------------+--------------------------------------+
8. Associate the floating IPv4 address with the instance.
$ openstack server add floating ip selfservice-instance2 203.0.113.17
Note: This command provides no output.
9. On the compute node containing the instance, verify creation of the fip namespace with the same ID as
the provider network.
274 Deployment examples
Networking Guide (Release Version: 15.0.0)
# ip netns
fip-4bfa3075-b4b2-4f7d-b88e-df1113942d43
Network traffic flow
The following sections describe the flow of network traffic in several common scenarios. North-south network
traffic travels between an instance and external network such as the Internet. East-west network traffic travels
between instances on the same or different networks. In all scenarios, the physical network infrastructure
handles switching and routing among provider networks and external networks such as the Internet. Each case
references one or more of the following components:
• Provider network (VLAN)
– VLAN ID 101 (tagged)
• Self-service network 1 (VXLAN)
– VXLAN ID (VNI) 101
• Self-service network 2 (VXLAN)
– VXLAN ID (VNI) 102
• Self-service router
– Gateway on the provider network
– Interface on self-service network 1
– Interface on self-service network 2
• Instance 1
• Instance 2
This section only contains flow scenarios that benefit from distributed virtual routing or that differ from conventional
operation. For other flow scenarios, see Network traffic flow.
North-south scenario 1: Instance with a fixed IP address
Similar to North-south scenario 1: Instance with a fixed IP address, except the router namespace on the network
node becomes the SNAT namespace. The network node still contains the router namespace, but it serves no
purpose in this case.
Deployment examples 275
Networking Guide (Release Version: 15.0.0)
276 Deployment examples
Networking Guide (Release Version: 15.0.0)
North-south scenario 2: Instance with a floating IPv4 address
For instances with a floating IPv4 address using a self-service network on a distributed router, the compute node
containing the instance performs SNAT on north-south traffic passing from the instance to external networks
such as the Internet and DNAT on north-south traffic passing from external networks to the instance. Floating IP
addresses and NAT do not apply to IPv6. Thus, the network node routes IPv6 traffic in this scenario. north-south
traffic passing between the instance and external networks such as the Internet.
• Instance 1 resides on compute node 1 and uses self-service network 1.
• A host on the Internet sends a packet to the instance.
The following steps involve the compute node:
1. The physical network infrastructure (1) forwards the packet to the provider physical network interface
(2).
2. The provider physical network interface forwards the packet to the OVS provider bridge provider network
port (3).
3. The OVS provider bridge swaps actual VLAN tag 101 with the internal VLAN tag.
4. The OVS provider bridge phy-br-provider port (4) forwards the packet to the OVS integration bridge
int-br-provider port (5).
5. The OVS integration bridge port for the provider network (6) removes the internal VLAN tag and forwards
the packet to the provider network interface (7) in the floating IP namespace. This interface
responds to any ARP requests for the instance floating IPv4 address.
6. The floating IP namespace routes the packet (8) to the distributed router namespace (9) using a pair of
IP addresses on the DVR internal network. This namespace contains the instance floating IPv4 address.
7. The router performs DNAT on the packet which changes the destination IP address to the instance IP
address on the self-service network via the self-service network interface (10).
8. The router forwards the packet to the OVS integration bridge port for the self-service network (11).
9. The OVS integration bridge adds an internal VLAN tag to the packet.
10. The OVS integration bridge removes the internal VLAN tag from the packet.
11. The OVS integration bridge security group port (12) forwards the packet to the security group bridge
OVS port (13) via veth pair.
12. Security group rules (14) on the security group bridge handle firewalling and connection tracking for the
packet.
13. The security group bridge instance port (15) forwards the packet to the instance interface (16) via veth
pair.
Deployment examples 277
Networking Guide (Release Version: 15.0.0)
Note: Egress traffic follows similar steps in reverse, except SNAT changes the source IPv4 address of the
packet to the floating IPv4 address.
East-west scenario 1: Instances on different networks on the same router
Instances with fixed IPv4/IPv6 address or floating IPv4 address on the same compute node communicate via
router on the compute node. Instances on different compute nodes communicate via an instance of the router
on each compute node.
Note: This scenario places the instances on different compute nodes to show the most complex situation.
The following steps involve compute node 1:
1. The instance interface (1) forwards the packet to the security group bridge instance port (2) via veth
pair.
2. Security group rules (3) on the security group bridge handle firewalling and connection tracking for the
packet.
3. The security group bridge OVS port (4) forwards the packet to the OVS integration bridge security group
278 Deployment examples
Networking Guide (Release Version: 15.0.0)
port (5) via veth pair.
4. The OVS integration bridge adds an internal VLAN tag to the packet.
5. The OVS integration bridge port for self-service network 1 (6) removes the internal VLAN tag and
forwards the packet to the self-service network 1 interface in the distributed router namespace (6).
6. The distributed router namespace routes the packet to self-service network 2.
7. The self-service network 2 interface in the distributed router namespace (8) forwards the packet to the
OVS integration bridge port for self-service network 2 (9).
8. The OVS integration bridge adds an internal VLAN tag to the packet.
9. The OVS integration bridge exchanges the internal VLAN tag for an internal tunnel ID.
10. The OVS integration bridge patch-tun port (10) forwards the packet to the OVS tunnel bridge patchint
port (11).
11. The OVS tunnel bridge (12) wraps the packet using VNI 101.
12. The underlying physical interface (13) for overlay networks forwards the packet to compute node 2 via
the overlay network (14).
The following steps involve compute node 2:
1. The underlying physical interface (15) for overlay networks forwards the packet to the OVS tunnel bridge
(16).
2. The OVS tunnel bridge unwraps the packet and adds an internal tunnel ID to it.
3. The OVS tunnel bridge exchanges the internal tunnel ID for an internal VLAN tag.
4. The OVS tunnel bridge patch-int patch port (17) forwards the packet to the OVS integration bridge
patch-tun patch port (18).
5. The OVS integration bridge removes the internal VLAN tag from the packet.
6. The OVS integration bridge security group port (19) forwards the packet to the security group bridge
OVS port (20) via veth pair.
7. Security group rules (21) on the security group bridge handle firewalling and connection tracking for the
packet.
8. The security group bridge instance port (22) forwards the packet to the instance 2 interface (23) via veth
pair.
Note: Routing between self-service networks occurs on the compute node containing the instance sending the
packet. In this scenario, routing occurs on compute node 1 for packets from instance 1 to instance 2 and on
compute node 2 for packets from instance 2 to instance 1.
Deployment examples 279
Networking Guide (Release Version: 15.0.0)
280 Deployment examples
Networking Guide (Release Version: 15.0.0)
Operations
IP availability metrics
Network IP Availability is an information-only API extension that allows a user or process to determine the
number of IP addresses that are consumed across networks and the allocation pools of their subnets. This
extension was added to neutron in the Mitaka release.
This section illustrates how you can get the Network IP address availability through the command-line interface.
Get Network IP address availability for all IPv4 networks:
$ openstack ip availability list
+--------------------------------------+--------------+-----------+----------+
| Network ID | Network Name | Total IPs | Used IPs |
+--------------------------------------+--------------+-----------+----------+
| 363a611a-b08b-4281-b64e-198d90cb94fd | private | 253 | 3 |
| c92d0605-caf2-4349-b1b8-8d5f9ac91df8 | public | 253 | 1 |
+--------------------------------------+--------------+-----------+----------+
Get Network IP address availability for all IPv6 networks:
$ openstack ip availability list --ip-version 6
+--------------------------------------+--------------+----------------------+----------+
| Network ID | Network Name | Total IPs | Used IPs |
+--------------------------------------+--------------+----------------------+----------+
| 363a611a-b08b-4281-b64e-198d90cb94fd | private | 18446744073709551614 | 3 |
| c92d0605-caf2-4349-b1b8-8d5f9ac91df8 | public | 18446744073709551614 | 1 |
+--------------------------------------+--------------+----------------------+----------+
Get Network IP address availability statistics for a specific network:
$ openstack ip availability show NETWORKUUID
+------------------------+--------------------------------------------------------------+
| Field | Value |
+------------------------+--------------------------------------------------------------+
| network_id | 0bf90de6-fc0f-4dba-b80d-96670dfb331a |
| network_name | public |
| project_id | 5669caad86a04256994cdf755df4d3c1 |
| subnet_ip_availability | cidr='192.0.2.224/28', ip_version='4', subnet_id='346806ee- |
| | a53e-44fd-968a-ddb2bcd2ba96', subnet_name='public_subnet', |
| | total_ips='13', used_ips='5' |
| total_ips | 13 |
| used_ips | 5 |
+------------------------+--------------------------------------------------------------+
Resource tags
Various virtual networking resources support tags for use by external systems or any other clients of the Networking
service API.
The currently supported resources are:
Operations 281
Networking Guide (Release Version: 15.0.0)
• networks
• subnets
• subnetpools
• ports
• routers
Use cases
The following use cases refer to adding tags to networks, but the same can be applicable to any other supported
Networking service resource:
1. Ability to map different networks in different OpenStack locations to one logically same network (for
multi-site OpenStack).
2. Ability to map IDs from different management/orchestration systems to OpenStack networks in mixed
environments. For example, in the Kuryr project, the Docker network ID is mapped to the Neutron
network ID.
3. Ability to leverage tags by deployment tools.
4. Ability to tag information about provider networks (for example, high-bandwidth, low-latency, and so
on).
Filtering with tags
The API allows searching/filtering of the GET /v2.0/networks API. The following query parameters are
supported:
• tags
• tags-any
• not-tags
• not-tags-any
To request the list of networks that have a single tag, tags argument should be set to the desired tag name.
Example:
GET /v2.0/networks?tags=red
To request the list of networks that have two or more tags, the tags argument should be set to the list of tags,
separated by commas. In this case, the tags given must all be present for a network to be included in the query
result. Example that returns networks that have the “red” and “blue” tags:
GET /v2.0/networks?tags=red,blue
To request the list of networks that have one or more of a list of given tags, the tags-any argument should be
set to the list of tags, separated by commas. In this case, as long as one of the given tags is present, the network
will be included in the query result. Example that returns the networks that have the “red” or the “blue” tag:
GET /v2.0/networks?tags-any=red,blue
282 Operations
Networking Guide (Release Version: 15.0.0)
To request the list of networks that do not have one or more tags, the not-tags argument should be set to the
list of tags, separated by commas. In this case, only the networks that do not have any of the given tags will be
included in the query results. Example that returns the networks that do not have either “red” or “blue” tag:
GET /v2.0/networks?not-tags=red,blue
To request the list of networks that do not have at least one of a list of tags, the not-tags-any argument should
be set to the list of tags, separated by commas. In this case, only the networks that do not have at least one of
the given tags will be included in the query result. Example that returns the networks that do not have the “red”
tag, or do not have the “blue” tag:
GET /v2.0/networks?not-tags-any=red,blue
The tags, tags-any, not-tags, and not-tags-any arguments can be combined to build more complex
queries. Example:
GET /v2.0/networks?tags=red,blue&tags-any=green,orange
The above example returns any networks that have the “red” and “blue” tags, plus at least one of “green” and
“orange”.
Complex queries may have contradictory parameters. Example:
GET /v2.0/networks?tags=blue&not-tags=blue
In this case, we should let the Networking service find these networks. Obviously, there are no such networks
and the service will return an empty list.
User workflow
Add a tag to a resource:
$ neutron tag-add --resource-type network --resource ab442634-1cc9-49e5-bd49-0dac9c811f69 --
,→tag red
$ neutron net-show net
+-------------------------+--------------------------------------+
| Field | Value |
+-------------------------+--------------------------------------+
| admin_state_up | True |
| availability_zone_hints | |
| availability_zones | |
| id | ab442634-1cc9-49e5-bd49-0dac9c811f69 |
| ipv4_address_scope | |
| ipv6_address_scope | |
| mtu | 1450 |
| name | net |
| port_security_enabled | True |
| router:external | False |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | red |
| tenant_id | e6710680bfd14555891f265644e1dd5c |
+-------------------------+--------------------------------------+
Operations 283
Networking Guide (Release Version: 15.0.0)
Remove a tag from a resource:
$ neutron tag-remove --resource-type network --resource ab442634-1cc9-49e5-bd49-
,→0dac9c811f69 --tag red
$ neutron net-show net
+-------------------------+--------------------------------------+
| Field | Value |
+-------------------------+--------------------------------------+
| admin_state_up | True |
| availability_zone_hints | |
| availability_zones | |
| id | ab442634-1cc9-49e5-bd49-0dac9c811f69 |
| ipv4_address_scope | |
| ipv6_address_scope | |
| mtu | 1450 |
| name | net |
| port_security_enabled | True |
| router:external | False |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | |
| tenant_id | e6710680bfd14555891f265644e1dd5c |
+-------------------------+--------------------------------------+
Replace all tags on the resource:
$ neutron tag-replace --resource-type network --resource ab442634-1cc9-49e5-bd49-
,→0dac9c811f69 --tag red --tag blue
$ neutron net-show net
+-------------------------+--------------------------------------+
| Field | Value |
+-------------------------+--------------------------------------+
| admin_state_up | True |
| availability_zone_hints | |
| availability_zones | |
| id | ab442634-1cc9-49e5-bd49-0dac9c811f69 |
| ipv4_address_scope | |
| ipv6_address_scope | |
| mtu | 1450 |
| name | net |
| port_security_enabled | True |
| router:external | False |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | red |
| | blue |
| tenant_id | e6710680bfd14555891f265644e1dd5c |
+-------------------------+--------------------------------------+
Clear tags from a resource:
$ neutron tag-remove --resource-type network --resource ab442634-1cc9-49e5-bd49-
,→0dac9c811f69 --all
$ neutron net-show net
+-------------------------+--------------------------------------+
| Field | Value |
284 Operations
Networking Guide (Release Version: 15.0.0)
+-------------------------+--------------------------------------+
| admin_state_up | True |
| availability_zone_hints | |
| availability_zones | |
| id | ab442634-1cc9-49e5-bd49-0dac9c811f69 |
| ipv4_address_scope | |
| ipv6_address_scope | |
| mtu | 1450 |
| name | net |
| port_security_enabled | True |
| router:external | False |
| shared | False |
| status | ACTIVE |
| subnets | |
| tags | |
| tenant_id | e6710680bfd14555891f265644e1dd5c |
+-------------------------+--------------------------------------+
Get list of resources with tag filters from networks. The networks are: test-net1 with “red” tag, test-net2 with
“red” and “blue” tags, test-net3 with “red”, “blue”, and “green” tags, and test-net4 with “green” tag.
Get list of resources with tags filter:
$ neutron net-list --tags red,blue
+--------------------------------------+-----------+---------+
| id | name | subnets |
+--------------------------------------+-----------+---------+
| 8ca3b9ed-f578-45fa-8c44-c53f13aec05a | test-net3 | |
| e736e63d-42e4-4f4c-836c-6ad286ffd68a | test-net2 | |
+--------------------------------------+-----------+---------+
Get list of resources with tags-any filter:
$ neutron net-list --tags-any red,blue
+--------------------------------------+-----------+---------+
| id | name | subnets |
+--------------------------------------+-----------+---------+
| 30491224-3855-431f-a688-fb29df004d82 | test-net1 | |
| 8ca3b9ed-f578-45fa-8c44-c53f13aec05a | test-net3 | |
| e736e63d-42e4-4f4c-836c-6ad286ffd68a | test-net2 | |
+--------------------------------------+-----------+---------+
Get list of resources with not-tags filter:
$ neutron net-list --not-tags red,blue
+--------------------------------------+-----------+---------+
| id | name | subnets |
+--------------------------------------+-----------+---------+
| 30491224-3855-431f-a688-fb29df004d82 | test-net1 | |
| cdb3ed08-ca63-4090-ba12-30b366372993 | test-net4 | |
+--------------------------------------+-----------+---------+
Get list of resources with not-tags-any filter:
$ neutron net-list --not-tags-any red,blue
+--------------------------------------+-----------+---------+
| id | name | subnets |
Operations 285
Networking Guide (Release Version: 15.0.0)
+--------------------------------------+-----------+---------+
| cdb3ed08-ca63-4090-ba12-30b366372993 | test-net4 | |
+--------------------------------------+-----------+---------+
Limitations
Filtering resources with a tag whose name contains a comma is not supported. Thus, do not put such a tag name
to resources.
Future support
In future releases, the Networking service may support setting tags for additional resources.
Resource purge
The Networking service provides a purge mechanism to delete the following network resources for a project:
• Networks
• Subnets
• Ports
• Router interfaces
• Routers
• Floating IP addresses
• Security groups
Typically, one uses this mechanism to delete networking resources for a defunct project regardless of its existence
in the Identity service.
Usage
1. Source the necessary project credentials. The administrative project can delete resources for all other
projects. A regular project can delete its own network resources and those belonging to other projects
for which it has sufficient access.
2. Delete the network resources for a particular project.
$ neutron purge PROJECT_ID
Replace PROJECT_ID with the project ID.
The command provides output that includes a completion percentage and the quantity of successful or unsuccessful
network resource deletions. An unsuccessful deletion usually indicates sharing of a resource with one
or more additional projects.
Purging resources: 100% complete.
Deleted 1 security_group, 2 ports, 1 router, 1 floatingip, 2 networks.
The following resources could not be deleted: 1 network.
286 Operations
Networking Guide (Release Version: 15.0.0)
The command also indicates if a project lacks network resources.
Tenant has no supported resources.
Migration
Database
The upgrade of the Networking service database is implemented with Alembic migration chains. The migrations
in the alembic/versions contain the changes needed to migrate from older Networking service releases to
newer ones.
Since Liberty, Networking maintains two parallel Alembic migration branches.
The first branch is called expand and is used to store expansion-only migration rules. These rules are strictly
additive and can be applied while the Neutron server is running.
The second branch is called contract and is used to store those migration rules that are not safe to apply while
Neutron server is running.
The intent of separate branches is to allow invoking those safe migrations from the expand branch while the
Neutron server is running and therefore reducing downtime needed to upgrade the service.
A database management command-line tool uses the Alembic library to manage the migration.
Database management command-line tool
The database management command-line tool is called neutron-db-manage. Pass the --help option to the
tool for usage information.
The tool takes some options followed by some commands:
$ neutron-db-manage <options> <commands>
The tool needs to access the database connection string, which is provided in the neutron.conf configuration
file in an installation. The tool automatically reads from /etc/neutron/neutron.conf if it is present. If the
configuration is in a different location, use the following command:
$ neutron-db-manage --config-file /path/to/neutron.conf <commands>
Multiple --config-file options can be passed if needed.
Instead of reading the DB connection from the configuration file(s), you can use the --database-connection
option:
$ neutron-db-manage --database-connection
mysql+pymysql://root:secret@127.0.0.1/neutron?charset=utf8 <commands>
The branches, current, and history commands all accept a --verbose option, which, when passed, will instruct
neutron-db-manage to display more verbose output for the specified command:
$ neutron-db-manage current --verbose
Migration 287
Networking Guide (Release Version: 15.0.0)
Note: The tool usage examples below do not show the options. It is assumed that you use the options that you
need for your environment.
In new deployments, you start with an empty database and then upgrade to the latest database version using the
following command:
$ neutron-db-manage upgrade heads
After installing a new version of the Neutron server, upgrade the database using the following command:
$ neutron-db-manage upgrade heads
In existing deployments, check the current database version using the following command:
$ neutron-db-manage current
To apply the expansion migration rules, use the following command:
$ neutron-db-manage upgrade --expand
To apply the non-expansive migration rules, use the following command:
$ neutron-db-manage upgrade --contract
To check if any contract migrations are pending and therefore if offline migration is required, use the following
command:
$ neutron-db-manage has_offline_migrations
Note: Offline migration requires all Neutron server instances in the cluster to be shutdown before you apply
any contract scripts.
To generate a script of the command instead of operating immediately on the database, use the following command:
$ neutron-db-manage upgrade heads --sql
.. note::
The `--sql` option causes the command to generate a script. The script
can be run later (online or offline), perhaps after verifying and/or
modifying it.
To migrate between specific migration versions, use the following command:
$ neutron-db-manage upgrade <start version>:<end version>
To upgrade the database incrementally, use the following command:
$ neutron-db-manage upgrade --delta <# of revs>
288 Migration
Networking Guide (Release Version: 15.0.0)
Note: Database downgrade is not supported.
To look for differences between the schema generated by the upgrade command and the schema defined by the
models, use the revision --autogenerate command:
neutron-db-manage revision -m REVISION_DESCRIPTION --autogenerate
Note: This generates a prepopulated template with the changes needed to match the database state with the
models.
Legacy nova-network to OpenStack Networking (neutron)
Two networking models exist in OpenStack. The first is called legacy networking (nova-network) and it is
a sub-process embedded in the Compute project (nova). This model has some limitations, such as creating
complex network topologies, extending its back-end implementation to vendor-specific technologies, and providing
project-specific networking elements. These limitations are the main reasons the OpenStack Networking
(neutron) model was created.
This section describes the process of migrating clouds based on the legacy networking model to the OpenStack
Networking model. This process requires additional changes to both compute and networking to support the
migration. This document describes the overall process and the features required in both Networking and
Compute.
The current process as designed is a minimally viable migration with the goal of deprecating and then removing
legacy networking. Both the Compute and Networking teams agree that a one-button migration process from
legacy networking to OpenStack Networking (neutron) is not an essential requirement for the deprecation and
removal of the legacy networking at a future date. This section includes a process and tools which are designed
to solve a simple use case migration.
Users are encouraged to take these tools, test them, provide feedback, and then expand on the feature set to suit
their own deployments; deployers that refrain from participating in this process intending to wait for a path that
better suits their use case are likely to be disappointed.
Impact and limitations
The migration process from the legacy nova-network networking service to OpenStack Networking (neutron)
has some limitations and impacts on the operational state of the cloud. It is critical to understand them in order
to decide whether or not this process is acceptable for your cloud and all users.
Management impact
The Networking REST API is publicly read-only until after the migration is complete. During the migration,
Networking REST API is read-write only to nova-api, and changes to Networking are only allowed via novaapi.
The Compute REST API is available throughout the entire process, although there is a brief period where it is
made read-only during a database migration. The Networking REST API will need to expose (to nova-api) all
details necessary for reconstructing the information previously held in the legacy networking database.
Migration 289
Networking Guide (Release Version: 15.0.0)
Compute needs a per-hypervisor “has_transitioned” boolean change in the data model to be used during the
migration process. This flag is no longer required once the process is complete.
Operations impact
In order to support a wide range of deployment options, the migration process described here requires a rolling
restart of hypervisors. The rate and timing of specific hypervisor restarts is under the control of the operator.
The migration may be paused, even for an extended period of time (for example, while testing or investigating
issues) with some hypervisors on legacy networking and some on Networking, and Compute API remains fully
functional. Individual hypervisors may be rolled back to legacy networking during this stage of the migration,
although this requires an additional restart.
In order to support the widest range of deployer needs, the process described here is easy to automate but is not
already automated. Deployers should expect to perform multiple manual steps or write some simple scripts in
order to perform this migration.
Performance impact
During the migration, nova-network API calls will go through an additional internal conversion to Networking
calls. This will have different and likely poorer performance characteristics compared with either the premigration
or post-migration APIs.
Migration process overview
1. Start neutron-server in intended final config, except with REST API restricted to read-write only by
nova-api.
2. Make the Compute REST API read-only.
3. Run a DB dump/restore tool that creates Networking data structures representing current legacy networking
config.
4. Enable a nova-api proxy that recreates internal Compute objects from Networking information (via the
Networking REST API).
5. Make Compute REST API read-write again. This means legacy networking DB is now unused, new
changes are now stored in the Networking DB, and no rollback is possible from here without losing
those new changes.
Note: At this moment the Networking DB is the source of truth, but nova-api is the only public read-write
API.
Next, you’ll need to migrate each hypervisor. To do that, follow these steps:
1. Disable the hypervisor. This would be a good time to live migrate or evacuate the compute node, if
supported.
2. Disable nova-compute.
3. Enable the Networking agent.
4. Set the “has_transitioned” flag in the Compute hypervisor database/config.
290 Migration
Networking Guide (Release Version: 15.0.0)
5. Reboot the hypervisor (or run “smart” live transition tool if available).
6. Re-enable the hypervisor.
At this point, all compute nodes have been migrated, but they are still using the nova-api API and Compute
gateways. Finally, enable OpenStack Networking by following these steps:
1. Bring up the Networking (l3) nodes. The new routers will have identical MAC+IPs as old Compute
gateways so some sort of immediate cutover is possible, except for stateful connections issues such as
NAT.
2. Make the Networking API read-write and disable legacy networking.
Migration Completed!
Add VRRP to an existing router
This section describes the process of migrating from a classic router to an L3 HA router, which is available
starting from the Mitaka release.
Similar to the classic scenario, all network traffic on a project network that requires routing actively traverses
only one network node regardless of the quantity of network nodes providing HA for the router. Therefore, this
high-availability implementation primarily addresses failure situations instead of bandwidth constraints that
limit performance. However, it supports random distribution of routers on different network nodes to reduce
the chances of bandwidth constraints and to improve scaling.
This section references parts of Linux bridge: High availability using VRRP and Open vSwitch: High availability
using VRRP. For details regarding needed infrastructure and configuration to allow actual L3 HA deployment,
read the relevant guide before continuing with the migration process.
Migration
The migration process is quite simple, it involves turning down the router by setting the router’s admin_state_up
attribute to False, upgrading the router to L3 HA and then setting the router’s admin_state_up
attribute back to True.
Warning: Once starting the migration, south-north connections (instances to internet) will be severed.
New connections will be able to start only when the migration is complete.
Here is the router we have used in our demonstration:
$ openstack router show router1
+-------------------------+-------------------------------------------+
| Field | Value |
+-------------------------+-------------------------------------------+
| admin_state_up | UP |
| distributed | False |
| external_gateway_info | |
| ha | False |
| id | 6b793b46-d082-4fd5-980f-a6f80cbb0f2a |
| name | router1 |
| project_id | bb8b84ab75be4e19bd0dfe02f6c3f5c1 |
| routes | |
Migration 291
Networking Guide (Release Version: 15.0.0)
| status | ACTIVE |
+-------------------------+-------------------------------------------+
1. Source the administrative project credentials.
2. Set the admin_state_up to False. This will severe south-north connections until admin_state_up is set
to True again.
$ openstack router set router1 --disable
3. Set the ha attribute of the router to True.
$ openstack router set router1 --ha
4. Set the admin_state_up to True. After this, south-north connections can start.
$ openstack router set router1 --enable
5. Make sure that the router’s ha attribute has changed to True.
$ openstack router show router1
+-------------------------+-------------------------------------------+
| Field | Value |
+-------------------------+-------------------------------------------+
| admin_state_up | UP |
| distributed | False |
| external_gateway_info | |
| ha | True |
| id | 6b793b46-d082-4fd5-980f-a6f80cbb0f2a |
| name | router1 |
| project_id | bb8b84ab75be4e19bd0dfe02f6c3f5c1 |
| routes | |
| status | ACTIVE |
+-------------------------+-------------------------------------------+
L3 HA to Legacy
To return to classic mode, turn down the router again, turning off L3 HA and starting the router again.
Warning: Once starting the migration, south-north connections (instances to internet) will be severed.
New connections will be able to start only when the migration is complete.
Here is the router we have used in our demonstration:
$ openstack router show router1
+-------------------------+-------------------------------------------+
| Field | Value |
+-------------------------+-------------------------------------------+
| admin_state_up | DOWN |
| distributed | False |
| external_gateway_info | |
| ha | True |
| id | 6b793b46-d082-4fd5-980f-a6f80cbb0f2a |
292 Migration
Networking Guide (Release Version: 15.0.0)
| name | router1 |
| project_id | bb8b84ab75be4e19bd0dfe02f6c3f5c1 |
| routes | |
| status | ACTIVE |
+-------------------------+-------------------------------------------+
1. Source the administrative project credentials.
2. Set the admin_state_up to False. This will severe south-north connections until admin_state_up is set
to True again.
$ openstack router set router1 --disable
3. Set the ha attribute of the router to True.
$ openstack router set router1 --no-ha
4. Set the admin_state_up to True. After this, south-north connections can start.
$ openstack router set router1 --enable
5. Make sure that the router’s ha attribute has changed to False.
$ openstack router show router1
+-------------------------+-------------------------------------------+
| Field | Value |
+-------------------------+-------------------------------------------+
| admin_state_up | UP |
| distributed | False |
| external_gateway_info | |
| ha | False |
| id | 6b793b46-d082-4fd5-980f-a6f80cbb0f2a |
| name | router1 |
| project_id | bb8b84ab75be4e19bd0dfe02f6c3f5c1 |
| routes | |
| status | ACTIVE |
+-------------------------+-------------------------------------------+
Miscellaneous
Firewall-as-a-Service (FWaaS) v2 scenario
Enable FWaaS v2
1. Enable the FWaaS plug-in in the /etc/neutron/neutron.conf file:
service_plugins = firewall_v2
[service_providers]
# ...
service_provider = FIREWALL:Iptables:neutron.agent.linux.iptables_firewall.
,→OVSHybridIptablesFirewallDriver:default
[fwaas]
Miscellaneous 293
Networking Guide (Release Version: 15.0.0)
driver = neutron_fwaas.services.firewall.drivers.linux.iptables_fwaas_v2.
,→IptablesFwaasDriver
enabled = True
Note: On Ubuntu, modify the [fwaas] section in the /etc/neutron/fwaas_driver.ini file instead
of /etc/neutron/neutron.conf.
2. Configure the FWaaS plugin for the L3 agent.
In the AGENT section of l3_agent.ini, make sure the FWaaS extension is loaded:
[AGENT]
extensions = fwaas
Edit the FWaaS section in the /etc/neutron/neutron.conf file to indicate the agent version and
driver:
[fwaas]
agent_version = v2
driver = iptables
enabled = True
3. Create the required tables in the database:
# neutron-db-manage --subproject neutron-fwaas upgrade head
4. Restart the neutron-l3-agent and neutron-server services to apply the settings.
Note: Firewall v2 is not supported by horizon yet.
Configure Firewall-as-a-Service v2
Create the firewall rules and create a policy that contains them. Then, create a firewall that applies the policy.
1. Create a firewall rule:
$ neutron firewall-rule-create --protocol {tcp,udp,icmp,any} \
--source-ip-address SOURCE_IP_ADDRESS \
--destination-ip-address DESTINATION_IP_ADDRESS \
--source-port SOURCE_PORT_RANGE --destination-port DEST_PORT_RANGE \
--action {allow,deny,reject}
The Networking client requires a protocol value. If the rule is protocol agnostic, you can use the any
value.
Note: When the source or destination IP address are not of the same IP version (for example, IPv6), the
command returns an error.
2. Create a firewall policy:
294 Miscellaneous
Networking Guide (Release Version: 15.0.0)
$ neutron firewall-policy-create --firewall-rules \
"FIREWALL_RULE_IDS_OR_NAMES" myfirewallpolicy
Separate firewall rule IDs or names with spaces. The order in which you specify the rules is important.
You can create a firewall policy without any rules and add rules later, as follows:
• To add multiple rules, use the update operation.
• To add a single rule, use the insert-rule operation.
For more details, see Networking command-line client in the OpenStack Command-Line Interface Reference.
Note: FWaaS always adds a default deny all rule at the lowest precedence of each policy. Consequently,
a firewall policy with no rules blocks all traffic by default.
3. Create a firewall:
$ neutron firewall-create FIREWALL_POLICY_UUID
Note: The firewall remains in PENDING_CREATE state until you create a Networking router and
attach an interface to it.
Firewall-as-a-Service (FWaaS) v1 scenario
Enable FWaaS v1
FWaaS management options are also available in the Dashboard.
1. Enable the FWaaS plug-in in the /etc/neutron/neutron.conf file:
service_plugins = firewall
[service_providers]
# ...
service_provider = FIREWALL:Iptables:neutron.agent.linux.iptables_firewall.
,→OVSHybridIptablesFirewallDriver:default
[fwaas]
driver = neutron_fwaas.services.firewall.drivers.linux.iptables_fwaas.
,→IptablesFwaasDriver
enabled = True
Note: On Ubuntu, modify the [fwaas] section in the /etc/neutron/fwaas_driver.ini file instead
of /etc/neutron/neutron.conf.
2. Configure the FWaaS plugin for the L3 agent.
In the AGENT section of l3_agent.ini, make sure the FWaaS extension is loaded:
Miscellaneous 295
Networking Guide (Release Version: 15.0.0)
[AGENT]
extensions = fwaas
Edit the FWaaS section in the /etc/neutron/neutron.conf file to indicate the agent version and
driver:
[fwaas]
agent_version = v1
driver = iptables
enabled = True
3. Create the required tables in the database:
# neutron-db-manage --subproject neutron-fwaas upgrade head
4. Enable the option in the local_settings.py file, which is typically located on the controller node:
OPENSTACK_NEUTRON_NETWORK = {
# ...
'enable_firewall' = True,
# ...
}
Note: By default, enable_firewall option value is True in local_settings.py file.
Apply the settings by restarting the web server.
5. Restart the neutron-l3-agent and neutron-server services to apply the settings.
Configure Firewall-as-a-Service v1
Create the firewall rules and create a policy that contains them. Then, create a firewall that applies the policy.
1. Create a firewall rule:
$ neutron firewall-rule-create --protocol {tcp,udp,icmp,any} \
--source-ip-address SOURCE_IP_ADDRESS \
--destination-ip-address DESTINATION_IP_ADDRESS \
--source-port SOURCE_PORT_RANGE --destination-port DEST_PORT_RANGE \
--action {allow,deny,reject}
The Networking client requires a protocol value. If the rule is protocol agnostic, you can use the any
value.
Note: When the source or destination IP address are not of the same IP version (for example, IPv6), the
command returns an error.
2. Create a firewall policy:
$ neutron firewall-policy-create --firewall-rules \
"FIREWALL_RULE_IDS_OR_NAMES" myfirewallpolicy
296 Miscellaneous
Networking Guide (Release Version: 15.0.0)
Separate firewall rule IDs or names with spaces. The order in which you specify the rules is important.
You can create a firewall policy without any rules and add rules later, as follows:
• To add multiple rules, use the update operation.
• To add a single rule, use the insert-rule operation.
For more details, see Networking command-line client in the OpenStack Command-Line Interface Reference.
Note: FWaaS always adds a default deny all rule at the lowest precedence of each policy. Consequently,
a firewall policy with no rules blocks all traffic by default.
3. Create a firewall:
$ neutron firewall-create FIREWALL_POLICY_UUID
Note: The firewall remains in PENDING_CREATE state until you create a Networking router and
attach an interface to it.
Disable libvirt networking
Most OpenStack deployments use the libvirt toolkit for interacting with the hypervisor. Specifically, OpenStack
Compute uses libvirt for tasks such as booting and terminating virtual machine instances. When OpenStack
Compute boots a new instance, libvirt provides OpenStack with the VIF associated with the instance, and
OpenStack Compute plugs the VIF into a virtual device provided by OpenStack Network. The libvirt toolkit
itself does not provide any networking functionality in OpenStack deployments.
However, libvirt is capable of providing networking services to the virtual machines that it manages. In particular,
libvirt can be configured to provide networking functionality akin to a simplified, single-node version
of OpenStack. Users can use libvirt to create layer 2 networks that are similar to OpenStack Networking’s
networks, confined to a single node.
libvirt network implementation
By default, libvirt’s networking functionality is enabled, and libvirt creates a network when the system boots.
To implement this network, libvirt leverages some of the same technologies that OpenStack Network does. In
particular, libvirt uses:
• Linux bridging for implementing a layer 2 network
• dnsmasq for providing IP addresses to virtual machines using DHCP
• iptables to implement SNAT so instances can connect out to the public internet, and to ensure that virtual
machines are permitted to communicate with dnsmasq using DHCP
By default, libvirt creates a network named default. The details of this network may vary by distribution; on
Ubuntu this network involves:
• a Linux bridge named virbr0 with an IP address of 192.0.2.1/24
Miscellaneous 297
Networking Guide (Release Version: 15.0.0)
• a dnsmasq process that listens on the virbr0 interface and hands out IP addresses in the range 192.0.
2.2-192.0.2.254
• a set of iptables rules
When libvirt boots a virtual machine, it places the machine’s VIF in the bridge virbr0 unless explicitly told
not to.
On Ubuntu, the iptables ruleset that libvirt creates includes the following rules:
*nat
-A POSTROUTING -s 192.0.2.0/24 -d 224.0.0.0/24 -j RETURN
-A POSTROUTING -s 192.0.2.0/24 -d 255.255.255.255/32 -j RETURN
-A POSTROUTING -s 192.0.2.0/24 ! -d 192.0.2.0/24 -p tcp -j MASQUERADE --to-ports 1024-65535
-A POSTROUTING -s 192.0.2.0/24 ! -d 192.0.2.0/24 -p udp -j MASQUERADE --to-ports 1024-65535
-A POSTROUTING -s 192.0.2.0/24 ! -d 192.0.2.0/24 -j MASQUERADE
*mangle
-A POSTROUTING -o virbr0 -p udp -m udp --dport 68 -j CHECKSUM --checksum-fill
*filter
-A INPUT -i virbr0 -p udp -m udp --dport 53 -j ACCEPT
-A INPUT -i virbr0 -p tcp -m tcp --dport 53 -j ACCEPT
-A INPUT -i virbr0 -p udp -m udp --dport 67 -j ACCEPT
-A INPUT -i virbr0 -p tcp -m tcp --dport 67 -j ACCEPT
-A FORWARD -d 192.0.2.0/24 -o virbr0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -s 192.0.2.0/24 -i virbr0 -j ACCEPT
-A FORWARD -i virbr0 -o virbr0 -j ACCEPT
-A FORWARD -o virbr0 -j REJECT --reject-with icmp-port-unreachable
-A FORWARD -i virbr0 -j REJECT --reject-with icmp-port-unreachable
-A OUTPUT -o virbr0 -p udp -m udp --dport 68 -j ACCEPT
The following shows the dnsmasq process that libvirt manages as it appears in the output of ps:
2881 ? S 0:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.
,→conf
How to disable libvirt networks
Although OpenStack does not make use of libvirt’s networking, this networking will not interfere with OpenStack’s
behavior, and can be safely left enabled. However, libvirt’s networking can be a nuisance when debugging
OpenStack networking issues. Because libvirt creates an additional bridge, dnsmasq process, and iptables
ruleset, these may distract an operator engaged in network troubleshooting. Unless you need to start up virtual
machines using libvirt directly, you can safely disable libvirt’s network.
To view the defined libvirt networks and their state:
# virsh net-list
Name State Autostart Persistent
----------------------------------------------------------
default active yes yes
To deactivate the libvirt network named default:
# virsh net-destroy default
Deactivating the network will remove the virbr0 bridge, terminate the dnsmasq process, and remove the iptables
rules.
298 Miscellaneous
Networking Guide (Release Version: 15.0.0)
To prevent the network from automatically starting on boot:
# virsh net-autostart --network default --disable
To activate the network after it has been deactivated:
# virsh net-start default
neutron-linuxbridge-cleanup utility
Description
Automated removal of empty bridges has been disabled to fix a race condition between the Compute (nova)
and Networking (neutron) services. Previously, it was possible for a bridge to be deleted during the time when
the only instance using it was rebooted.
Usage
Use this script to remove empty bridges on compute nodes by running the following command:
$ neutron-linuxbridge-cleanup
Important: Do not use this tool when creating or migrating an instance as it throws an error when the bridge
does not exist.
Note: Using this script can still trigger the original race condition. Only run this script if you have evacuated all
instances off a compute node and you want to clean up the bridges. In addition to evacuating all instances, you
should fence off the compute node where you are going to run this script so new instances do not get scheduled
on it.
Miscellaneous 299
APPENDIX
Community support
The following resources are available to help you run and use OpenStack. The OpenStack community constantly
improves and adds to the main features of OpenStack, but if you have any questions, do not hesitate to ask. Use
the following resources to get OpenStack support and troubleshoot your installations.
Documentation
For the available OpenStack documentation, see docs.openstack.org.
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at
OpenStack Documentation Mailing List, join our IRC channel #openstack-doc on the freenode IRC network,
or report a bug.
The following books explain how to install an OpenStack cloud and its associated components:
• Installation Tutorial for openSUSE Leap 42.2 and SUSE Linux Enterprise Server 12 SP2
• Installation Tutorial for Red Hat Enterprise Linux 7 and CentOS 7
• Installation Tutorial for Ubuntu 16.04 (LTS)
The following books explain how to configure and run an OpenStack cloud:
• Architecture Design Guide
• Administrator Guide
• Configuration Reference
• Operations Guide
• Networking Guide
• High Availability Guide
• Security Guide
• Virtual Machine Image Guide
The following books explain how to use the OpenStack Dashboard and command-line clients:
• End User Guide
• Command-Line Interface Reference
The following documentation provides reference and guidance information for the OpenStack APIs:
• API Guide
The following guide provides how to contribute to OpenStack documentation:
• Documentation Contributor Guide
300
Networking Guide (Release Version: 15.0.0)
ask.openstack.org
During the set up or testing of OpenStack, you might have questions about how a specific task is completed or
be in a situation where a feature does not work correctly. Use the ask.openstack.org site to ask questions and
get answers. When you visit the Ask OpenStack site, scan the recently asked questions to see whether your
question has already been answered. If not, ask a new question. Be sure to give a clear, concise summary in the
title and provide as much detail as possible in the description. Paste in your command output or stack traces,
links to screen shots, and any other information which might be useful.
OpenStack mailing lists
A great way to get answers and insights is to post your question or problematic scenario to the OpenStack
mailing list. You can learn from and help others who might have similar issues. To subscribe or view the
archives, go to the general OpenStack mailing list. If you are interested in the other mailing lists for specific
projects or development, refer to Mailing Lists.
The OpenStack wiki
The OpenStack wiki contains a broad range of topics but some of the information can be difficult to find or is a
few pages deep. Fortunately, the wiki search feature enables you to search by title or content. If you search for
specific information, such as about networking or OpenStack Compute, you can find a large amount of relevant
material. More is being added all the time, so be sure to check back often. You can find the search box in the
upper-right corner of any OpenStack wiki page.
The Launchpad Bugs area
The OpenStack community values your set up and testing efforts and wants your feedback. To log a bug, you
must sign up for a Launchpad account. You can view existing bugs and report bugs in the Launchpad Bugs
area. Use the search feature to determine whether the bug has already been reported or already been fixed. If it
still seems like your bug is unreported, fill out a bug report.
Some tips:
• Give a clear, concise summary.
• Provide as much detail as possible in the description. Paste in your command output or stack traces, links
to screen shots, and any other information which might be useful.
• Be sure to include the software and package versions that you are using, especially
if you are using a development branch, such as, "Kilo release" vs git commit
bc79c3ecc55929bac585d04a03475b72e06a3208.
• Any deployment-specific information is helpful, such as whether you are using Ubuntu 14.04 or are
performing a multi-node installation.
The following Launchpad Bugs areas are available:
• Bugs: OpenStack Block Storage (cinder)
• Bugs: OpenStack Compute (nova)
• Bugs: OpenStack Dashboard (horizon)
• Bugs: OpenStack Identity (keystone)
Community support 301
Networking Guide (Release Version: 15.0.0)
• Bugs: OpenStack Image service (glance)
• Bugs: OpenStack Networking (neutron)
• Bugs: OpenStack Object Storage (swift)
• Bugs: Application catalog (murano)
• Bugs: Bare metal service (ironic)
• Bugs: Clustering service (senlin)
• Bugs: Container Infrastructure Management service (magnum)
• Bugs: Data processing service (sahara)
• Bugs: Database service (trove)
• Bugs: Deployment service (fuel)
• Bugs: DNS service (designate)
• Bugs: Key Manager Service (barbican)
• Bugs: Monitoring (monasca)
• Bugs: Orchestration (heat)
• Bugs: Rating (cloudkitty)
• Bugs: Shared file systems (manila)
• Bugs: Telemetry (ceilometer)
• Bugs: Telemetry v3 (gnocchi)
• Bugs: Workflow service (mistral)
• Bugs: Messaging service (zaqar)
• Bugs: OpenStack API Documentation (developer.openstack.org)
• Bugs: OpenStack Documentation (docs.openstack.org)
The OpenStack IRC channel
The OpenStack community lives in the #openstack IRC channel on the Freenode network. You can hang out, ask
questions, or get immediate feedback for urgent and pressing issues. To install an IRC client or use a browserbased
client, go to https://webchat.freenode.net/. You can also use Colloquy (Mac OS X), mIRC (Windows),
or XChat (Linux). When you are in the IRC channel and want to share code or command output, the generally
accepted method is to use a Paste Bin. The OpenStack project has one at Paste. Just paste your longer amounts
of text or logs in the web form and you get a URL that you can paste into the channel. The OpenStack IRC
channel is #openstack on irc.freenode.net. You can find a list of all OpenStack IRC channels on the IRC
page on the wiki.
Documentation feedback
To provide feedback on documentation, join and use the openstack-docs@lists.openstack.org mailing list at
OpenStack Documentation Mailing List, or report a bug.
302 Community support
Networking Guide (Release Version: 15.0.0)
OpenStack distribution packages
The following Linux distributions provide community-supported packages for OpenStack:
• Debian: https://wiki.debian.org/OpenStack
• CentOS, Fedora, and Red Hat Enterprise Linux: https://www.rdoproject.org/
• openSUSE and SUSE Linux Enterprise Server: https://en.opensuse.org/Portal:OpenStack
• Ubuntu: https://wiki.ubuntu.com/ServerTeam/CloudArchive
Community support 303
GLOSSARY
Glossary
This glossary offers a list of terms and definitions to define a vocabulary for OpenStack-related concepts.
To add to OpenStack glossary, clone the openstack/openstack-manuals repository and update the source file
doc/common/glossary.rst through the OpenStack contribution process.
0-9
6to4 A mechanism that allows IPv6 packets to be transmitted over an IPv4 network, providing a strategy for
migrating to IPv6.
A
absolute limit Impassable limits for guest VMs. Settings include total RAM size, maximum number of vCPUs,
and maximum disk size.
access control list (ACL) A list of permissions attached to an object. An ACL specifies which users or system
processes have access to objects. It also defines which operations can be performed on specified objects.
Each entry in a typical ACL specifies a subject and an operation. For instance, the ACL entry (Alice,
delete) for a file gives Alice permission to delete the file.
access key Alternative term for an Amazon EC2 access key. See EC2 access key.
account The Object Storage context of an account. Do not confuse with a user account from an authentication
service, such as Active Directory, /etc/passwd, OpenLDAP, OpenStack Identity, and so on.
account auditor Checks for missing replicas and incorrect or corrupted objects in a specified Object Storage
account by running queries against the back-end SQLite database.
account database A SQLite database that contains Object Storage accounts and related metadata and that the
accounts server accesses.
account reaper An Object Storage worker that scans for and deletes account databases and that the account
server has marked for deletion.
account server Lists containers in Object Storage and stores container information in the account database.
account service An Object Storage component that provides account services such as list, create, modify, and
audit. Do not confuse with OpenStack Identity service, OpenLDAP, or similar user-account services.
accounting The Compute service provides accounting information through the event notification and system
usage data facilities.
Active Directory Authentication and identity service by Microsoft, based on LDAP. Supported in OpenStack.
active/active configuration In a high-availability setup with an active/active configuration, several systems
share the load together and if one fails, the load is distributed to the remaining systems.
active/passive configuration In a high-availability setup with an active/passive configuration, systems are set
up to bring additional resources online to replace those that have failed.
304
Networking Guide (Release Version: 15.0.0)
address pool A group of fixed and/or floating IP addresses that are assigned to a project and can be used by
or assigned to the VM instances in a project.
Address Resolution Protocol (ARP) The protocol by which layer-3 IP addresses are resolved into layer-2
link local addresses.
admin API A subset of API calls that are accessible to authorized administrators and are generally not accessible
to end users or the public Internet. They can exist as a separate service (keystone) or can be a
subset of another API (nova).
admin server In the context of the Identity service, the worker process that provides access to the admin API.
administrator The person responsible for installing, configuring, and managing an OpenStack cloud.
Advanced Message Queuing Protocol (AMQP) The open standard messaging protocol used by OpenStack
components for intra-service communications, provided by RabbitMQ, Qpid, or ZeroMQ.
Advanced RISC Machine (ARM) Lower power consumption CPU often found in mobile and embedded
devices. Supported by OpenStack.
alert The Compute service can send alerts through its notification system, which includes a facility to create
custom notification drivers. Alerts can be sent to and displayed on the dashboard.
allocate The process of taking a floating IP address from the address pool so it can be associated with a fixed
IP on a guest VM instance.
Amazon Kernel Image (AKI) Both a VM container format and disk format. Supported by Image service.
Amazon Machine Image (AMI) Both a VM container format and disk format. Supported by Image service.
Amazon Ramdisk Image (ARI) Both a VM container format and disk format. Supported by Image service.
Anvil A project that ports the shell script-based project named DevStack to Python.
aodh Part of the OpenStack Telemetry service; provides alarming functionality.
Apache The Apache Software Foundation supports the Apache community of open-source software projects.
These projects provide software products for the public good.
Apache License 2.0 All OpenStack core projects are provided under the terms of the Apache License 2.0
license.
Apache Web Server The most common web server software currently used on the Internet.
API endpoint The daemon, worker, or service that a client communicates with to access an API. API endpoints
can provide any number of services, such as authentication, sales data, performance meters, Compute
VM commands, census data, and so on.
API extension Custom modules that extend some OpenStack core APIs.
API extension plug-in Alternative term for a Networking plug-in or Networking API extension.
API key Alternative term for an API token.
API server Any node running a daemon or worker that provides an API endpoint.
API token Passed to API requests and used by OpenStack to verify that the client is authorized to run the
requested operation.
API version In OpenStack, the API version for a project is part of the URL. For example, example.com/
nova/v1/foobar.
applet A Java program that can be embedded into a web page.
Glossary 305
Networking Guide (Release Version: 15.0.0)
Application Catalog service (murano) The project that provides an application catalog service so that users
can compose and deploy composite environments on an application abstraction level while managing
the application lifecycle.
Application Programming Interface (API) A collection of specifications used to access a service, application,
or program. Includes service calls, required parameters for each call, and the expected return
values.
application server A piece of software that makes available another piece of software over a network.
Application Service Provider (ASP) Companies that rent specialized applications that help businesses and
organizations provide additional services with lower cost.
arptables Tool used for maintaining Address Resolution Protocol packet filter rules in the Linux kernel firewall
modules. Used along with iptables, ebtables, and ip6tables in Compute to provide firewall services
for VMs.
associate The process associating a Compute floating IP address with a fixed IP address.
Asynchronous JavaScript and XML (AJAX) A group of interrelated web development techniques used on
the client-side to create asynchronous web applications. Used extensively in horizon.
ATA over Ethernet (AoE) A disk storage protocol tunneled within Ethernet.
attach The process of connecting a VIF or vNIC to a L2 network in Networking. In the context of Compute,
this process connects a storage volume to an instance.
attachment (network) Association of an interface ID to a logical port. Plugs an interface into a port.
auditing Provided in Compute through the system usage data facility.
auditor A worker process that verifies the integrity of Object Storage objects, containers, and accounts. Auditors
is the collective term for the Object Storage account auditor, container auditor, and object auditor.
Austin The code name for the initial release of OpenStack. The first design summit took place in Austin,
Texas, US.
auth node Alternative term for an Object Storage authorization node.
authentication The process that confirms that the user, process, or client is really who they say they are
through private key, secret token, password, fingerprint, or similar method.
authentication token A string of text provided to the client after authentication. Must be provided by the user
or process in subsequent requests to the API endpoint.
AuthN The Identity service component that provides authentication services.
authorization The act of verifying that a user, process, or client is authorized to perform an action.
authorization node An Object Storage node that provides authorization services.
AuthZ The Identity component that provides high-level authorization services.
Auto ACK Configuration setting within RabbitMQ that enables or disables message acknowledgment. Enabled
by default.
auto declare A Compute RabbitMQ setting that determines whether a message exchange is automatically
created when the program starts.
availability zone An Amazon EC2 concept of an isolated area that is used for fault tolerance. Do not confuse
with an OpenStack Compute zone or cell.
306 Glossary
Networking Guide (Release Version: 15.0.0)
AWS CloudFormation template AWS CloudFormation allows Amazon Web Services (AWS) users to create
and manage a collection of related resources. The Orchestration service supports a CloudFormationcompatible
format (CFN).
B
back end Interactions and processes that are obfuscated from the user, such as Compute volume mount, data
transmission to an iSCSI target by a daemon, or Object Storage object integrity checks.
back-end catalog The storage method used by the Identity service catalog service to store and retrieve information
about API endpoints that are available to the client. Examples include an SQL database, LDAP
database, or KVS back end.
back-end store The persistent data store used to save and retrieve information for a service, such as lists of
Object Storage objects, current state of guest VMs, lists of user names, and so on. Also, the method that
the Image service uses to get and store VM images. Options include Object Storage, locally mounted
file system, RADOS block devices, VMware datastore, and HTTP.
Backup, Restore, and Disaster Recovery service (freezer) The project that provides integrated tooling for
backing up, restoring, and recovering file systems, instances, or database backups.
bandwidth The amount of available data used by communication resources, such as the Internet. Represents
the amount of data that is used to download things or the amount of data available to download.
barbican Code name of the Key Manager service.
bare An Image service container format that indicates that no container exists for the VM image.
Bare Metal service (ironic) The OpenStack service that provides a service and associated libraries capable
of managing and provisioning physical machines in a security-aware and fault-tolerant manner.
base image An OpenStack-provided image.
Bell-LaPadula model A security model that focuses on data confidentiality and controlled access to classified
information. This model divides the entities into subjects and objects. The clearance of a subject is
compared to the classification of the object to determine if the subject is authorized for the specific
access mode. The clearance or classification scheme is expressed in terms of a lattice.
Benchmark service (rally) OpenStack project that provides a framework for performance analysis and benchmarking
of individual OpenStack components as well as full production OpenStack cloud deployments.
Bexar A grouped release of projects related to OpenStack that came out in February of 2011. It included only
Compute (nova) and Object Storage (swift). Bexar is the code name for the second release of OpenStack.
The design summit took place in San Antonio, Texas, US, which is the county seat for Bexar county.
binary Information that consists solely of ones and zeroes, which is the language of computers.
bit A bit is a single digit number that is in base of 2 (either a zero or one). Bandwidth usage is measured in
bits per second.
bits per second (BPS) The universal measurement of how quickly data is transferred from place to place.
block device A device that moves data in the form of blocks. These device nodes interface the devices, such
as hard disks, CD-ROM drives, flash drives, and other addressable regions of memory.
block migration A method of VM live migration used by KVM to evacuate instances from one host to another
with very little downtime during a user-initiated switchover. Does not require shared storage. Supported
by Compute.
Glossary 307
Networking Guide (Release Version: 15.0.0)
Block Storage API An API on a separate endpoint for attaching, detaching, and creating block storage for
compute VMs.
Block Storage service (cinder) The OpenStack service that implement services and libraries to provide ondemand,
self-service access to Block Storage resources via abstraction and automation on top of other
block storage devices.
BMC (Baseboard Management Controller) The intelligence in the IPMI architecture, which is a specialized
micro-controller that is embedded on the motherboard of a computer and acts as a server. Manages the
interface between system management software and platform hardware.
bootable disk image A type of VM image that exists as a single, bootable file.
Bootstrap Protocol (BOOTP) A network protocol used by a network client to obtain an IP address from a
configuration server. Provided in Compute through the dnsmasq daemon when using either the FlatDHCP
manager or VLAN manager network manager.
Border Gateway Protocol (BGP) The Border Gateway Protocol is a dynamic routing protocol that connects
autonomous systems. Considered the backbone of the Internet, this protocol connects disparate networks
to form a larger network.
browser Any client software that enables a computer or device to access the Internet.
builder file Contains configuration information that Object Storage uses to reconfigure a ring or to re-create
it from scratch after a serious failure.
bursting The practice of utilizing a secondary environment to elastically build instances on-demand when the
primary environment is resource constrained.
button class A group of related button types within horizon. Buttons to start, stop, and suspend VMs are in
one class. Buttons to associate and disassociate floating IP addresses are in another class, and so on.
byte Set of bits that make up a single character; there are usually 8 bits to a byte.
C
cache pruner A program that keeps the Image service VM image cache at or below its configured maximum
size.
Cactus An OpenStack grouped release of projects that came out in the spring of 2011. It included Compute
(nova), Object Storage (swift), and the Image service (glance). Cactus is a city in Texas, US and is the
code name for the third release of OpenStack. When OpenStack releases went from three to six months
long, the code name of the release changed to match a geography nearest the previous summit.
CALL One of the RPC primitives used by the OpenStack message queue software. Sends a message and
waits for a response.
capability Defines resources for a cell, including CPU, storage, and networking. Can apply to the specific
services within a cell or a whole cell.
capacity cache A Compute back-end database table that contains the current workload, amount of free RAM,
and number of VMs running on each host. Used to determine on which host a VM starts.
capacity updater A notification driver that monitors VM instances and updates the capacity cache as needed.
CAST One of the RPC primitives used by the OpenStack message queue software. Sends a message and does
not wait for a response.
catalog A list of API endpoints that are available to a user after authentication with the Identity service.
308 Glossary
Networking Guide (Release Version: 15.0.0)
catalog service An Identity service that lists API endpoints that are available to a user after authentication
with the Identity service.
ceilometer Part of the OpenStack Telemetry service; gathers and stores metrics from other OpenStack services.
cell Provides logical partitioning of Compute resources in a child and parent relationship. Requests are passed
from parent cells to child cells if the parent cannot provide the requested resource.
cell forwarding A Compute option that enables parent cells to pass resource requests to child cells if the parent
cannot provide the requested resource.
cell manager The Compute component that contains a list of the current capabilities of each host within the
cell and routes requests as appropriate.
CentOS A Linux distribution that is compatible with OpenStack.
Ceph Massively scalable distributed storage system that consists of an object store, block store, and POSIXcompatible
distributed file system. Compatible with OpenStack.
CephFS The POSIX-compliant file system provided by Ceph.
certificate authority (CA) In cryptography, an entity that issues digital certificates. The digital certificate
certifies the ownership of a public key by the named subject of the certificate. This enables others
(relying parties) to rely upon signatures or assertions made by the private key that corresponds to the
certified public key. In this model of trust relationships, a CA is a trusted third party for both the subject
(owner) of the certificate and the party relying upon the certificate. CAs are characteristic of many public
key infrastructure (PKI) schemes. In OpenStack, a simple certificate authority is provided by Compute
for cloudpipe VPNs and VM image decryption.
Challenge-Handshake Authentication Protocol (CHAP) An iSCSI authentication method supported by
Compute.
chance scheduler A scheduling method used by Compute that randomly chooses an available host from the
pool.
changes since A Compute API parameter that downloads changes to the requested item since your last request,
instead of downloading a new, fresh set of data and comparing it against the old data.
Chef An operating system configuration management tool supporting OpenStack deployments.
child cell If a requested resource such as CPU time, disk storage, or memory is not available in the parent
cell, the request is forwarded to its associated child cells. If the child cell can fulfill the request, it does.
Otherwise, it attempts to pass the request to any of its children.
cinder Codename for Block Storage service.
CirrOS A minimal Linux distribution designed for use as a test image on clouds such as OpenStack.
Cisco neutron plug-in A Networking plug-in for Cisco devices and technologies, including UCS and Nexus.
cloud architect A person who plans, designs, and oversees the creation of clouds.
Cloud Auditing Data Federation (CADF) Cloud Auditing Data Federation (CADF) is a specification for
audit event data. CADF is supported by OpenStack Identity.
cloud computing A model that enables access to a shared pool of configurable computing resources, such as
networks, servers, storage, applications, and services, that can be rapidly provisioned and released with
minimal management effort or service provider interaction.
cloud controller Collection of Compute components that represent the global state of the cloud; talks to services,
such as Identity authentication, Object Storage, and node/storage workers through a queue.
Glossary 309
Networking Guide (Release Version: 15.0.0)
cloud controller node A node that runs network, volume, API, scheduler, and image services. Each service
may be broken out into separate nodes for scalability or availability.
Cloud Data Management Interface (CDMI) SINA standard that defines a RESTful API for managing objects
in the cloud, currently unsupported in OpenStack.
Cloud Infrastructure Management Interface (CIMI) An in-progress specification for cloud management.
Currently unsupported in OpenStack.
cloud-init A package commonly installed in VM images that performs initialization of an instance after boot
using information that it retrieves from the metadata service, such as the SSH public key and user data.
cloudadmin One of the default roles in the Compute RBAC system. Grants complete system access.
Cloudbase-Init A Windows project providing guest initialization features, similar to cloud-init.
cloudpipe A compute service that creates VPNs on a per-project basis.
cloudpipe image A pre-made VM image that serves as a cloudpipe server. Essentially, OpenVPN running on
Linux.
Clustering service (senlin) The project that implements clustering services and libraries for the management
of groups of homogeneous objects exposed by other OpenStack services.
command filter Lists allowed commands within the Compute rootwrap facility.
Common Internet File System (CIFS) A file sharing protocol. It is a public or open variation of the original
Server Message Block (SMB) protocol developed and used by Microsoft. Like the SMB protocol, CIFS
runs at a higher level and uses the TCP/IP protocol.
Common Libraries (oslo) The project that produces a set of python libraries containing code shared by OpenStack
projects. The APIs provided by these libraries should be high quality, stable, consistent, documented
and generally applicable.
community project A project that is not officially endorsed by the OpenStack Foundation. If the project is
successful enough, it might be elevated to an incubated project and then to a core project, or it might be
merged with the main code trunk.
compression Reducing the size of files by special encoding, the file can be decompressed again to its original
content. OpenStack supports compression at the Linux file system level but does not support compression
for things such as Object Storage objects or Image service VM images.
Compute API (Nova API) The nova-api daemon provides access to nova services. Can communicate with
other APIs, such as the Amazon EC2 API.
compute controller The Compute component that chooses suitable hosts on which to start VM instances.
compute host Physical host dedicated to running compute nodes.
compute node A node that runs the nova-compute daemon that manages VM instances that provide a wide
range of services, such as web applications and analytics.
Compute service (nova) The OpenStack core project that implements services and associated libraries to
provide massively-scalable, on-demand, self-service access to compute resources, including bare metal,
virtual machines, and containers.
compute worker The Compute component that runs on each compute node and manages the VM instance
lifecycle, including run, reboot, terminate, attach/detach volumes, and so on. Provided by the novacompute
daemon.
concatenated object A set of segment objects that Object Storage combines and sends to the client.
310 Glossary
Networking Guide (Release Version: 15.0.0)
conductor In Compute, conductor is the process that proxies database requests from the compute process.
Using conductor improves security because compute nodes do not need direct access to the database.
congress Code name for the Governance service.
consistency window The amount of time it takes for a new Object Storage object to become accessible to all
clients.
console log Contains the output from a Linux VM console in Compute.
container Organizes and stores objects in Object Storage. Similar to the concept of a Linux directory but
cannot be nested. Alternative term for an Image service container format.
container auditor Checks for missing replicas or incorrect objects in specified Object Storage containers
through queries to the SQLite back-end database.
container database A SQLite database that stores Object Storage containers and container metadata. The
container server accesses this database.
container format A wrapper used by the Image service that contains a VM image and its associated metadata,
such as machine state, OS disk size, and so on.
Container Infrastructure Management service (magnum) The project which provides a set of services for
provisioning, scaling, and managing container orchestration engines.
container server An Object Storage server that manages containers.
container service The Object Storage component that provides container services, such as create, delete, list,
and so on.
content delivery network (CDN) A content delivery network is a specialized network that is used to distribute
content to clients, typically located close to the client for increased performance.
controller node Alternative term for a cloud controller node.
core API Depending on context, the core API is either the OpenStack API or the main API of a specific core
project, such as Compute, Networking, Image service, and so on.
core service An official OpenStack service defined as core by DefCore Committee. Currently, consists of
Block Storage service (cinder), Compute service (nova), Identity service (keystone), Image service
(glance), Networking service (neutron), and Object Storage service (swift).
cost Under the Compute distributed scheduler, this is calculated by looking at the capabilities of each host
relative to the flavor of the VM instance being requested.
credentials Data that is only known to or accessible by a user and used to verify that the user is who he says
he is. Credentials are presented to the server during authentication. Examples include a password, secret
key, digital certificate, and fingerprint.
CRL A Certificate Revocation List (CRL) in a PKI model is a list of certificates that have been revoked. End
entities presenting these certificates should not be trusted.
Cross-Origin Resource Sharing (CORS) A mechanism that allows many resources (for example, fonts,
JavaScript) on a web page to be requested from another domain outside the domain from which the
resource originated. In particular, JavaScript’s AJAX calls can use the XMLHttpRequest mechanism.
Crowbar An open source community project by SUSE that aims to provide all necessary services to quickly
deploy and manage clouds.
current workload An element of the Compute capacity cache that is calculated based on the number of build,
snapshot, migrate, and resize operations currently in progress on a given host.
Glossary 311
Networking Guide (Release Version: 15.0.0)
customer Alternative term for project.
customization module A user-created Python module that is loaded by horizon to change the look and feel
of the dashboard.
D
daemon A process that runs in the background and waits for requests. May or may not listen on a TCP or
UDP port. Do not confuse with a worker.
Dashboard (horizon) OpenStack project which provides an extensible, unified, web-based user interface for
all OpenStack services.
data encryption Both Image service and Compute support encrypted virtual machine (VM) images (but not
instances). In-transit data encryption is supported in OpenStack using technologies such as HTTPS,
SSL, TLS, and SSH. Object Storage does not support object encryption at the application level but may
support storage that uses disk encryption.
Data loss prevention (DLP) software Software programs used to protect sensitive information and prevent
it from leaking outside a network boundary through the detection and denying of the data transportation.
Data Processing service (sahara) OpenStack project that provides a scalable data-processing stack and associated
management interfaces.
data store A database engine supported by the Database service.
database ID A unique ID given to each replica of an Object Storage database.
database replicator An Object Storage component that copies changes in the account, container, and object
databases to other nodes.
Database service (trove) An integrated project that provides scalable and reliable Cloud Database-as-aService
functionality for both relational and non-relational database engines.
deallocate The process of removing the association between a floating IP address and a fixed IP address. Once
this association is removed, the floating IP returns to the address pool.
Debian A Linux distribution that is compatible with OpenStack.
deduplication The process of finding duplicate data at the disk block, file, and/or object level to minimize
storage use—currently unsupported within OpenStack.
default panel The default panel that is displayed when a user accesses the dashboard.
default project New users are assigned to this project if no project is specified when a user is created.
default token An Identity service token that is not associated with a specific project and is exchanged for a
scoped token.
delayed delete An option within Image service so that an image is deleted after a predefined number of seconds
instead of immediately.
delivery mode Setting for the Compute RabbitMQ message delivery mode; can be set to either transient or
persistent.
denial of service (DoS) Denial of service (DoS) is a short form for denial-of-service attack. This is a malicious
attempt to prevent legitimate users from using a service.
deprecated auth An option within Compute that enables administrators to create and manage users through
the nova-manage command as opposed to using the Identity service.
312 Glossary
Networking Guide (Release Version: 15.0.0)
designate Code name for the DNS service.
Desktop-as-a-Service A platform that provides a suite of desktop environments that users access to receive
a desktop experience from any location. This may provide general use, development, or even homogeneous
testing environments.
developer One of the default roles in the Compute RBAC system and the default role assigned to a new user.
device ID Maps Object Storage partitions to physical storage devices.
device weight Distributes partitions proportionately across Object Storage devices based on the storage capacity
of each device.
DevStack Community project that uses shell scripts to quickly build complete OpenStack development environments.
DHCP agent OpenStack Networking agent that provides DHCP services for virtual networks.
Diablo A grouped release of projects related to OpenStack that came out in the fall of 2011, the fourth release
of OpenStack. It included Compute (nova 2011.3), Object Storage (swift 1.4.3), and the Image service
(glance). Diablo is the code name for the fourth release of OpenStack. The design summit took place in
the Bay Area near Santa Clara, California, US and Diablo is a nearby city.
direct consumer An element of the Compute RabbitMQ that comes to life when a RPC call is executed. It
connects to a direct exchange through a unique exclusive queue, sends the message, and terminates.
direct exchange A routing table that is created within the Compute RabbitMQ during RPC calls; one is created
for each RPC call that is invoked.
direct publisher Element of RabbitMQ that provides a response to an incoming MQ message.
disassociate The process of removing the association between a floating IP address and fixed IP and thus
returning the floating IP address to the address pool.
Discretionary Access Control (DAC) Governs the ability of subjects to access objects, while enabling users
to make policy decisions and assign security attributes. The traditional UNIX system of users, groups,
and read-write-execute permissions is an example of DAC.
disk encryption The ability to encrypt data at the file system, disk partition, or whole-disk level. Supported
within Compute VMs.
disk format The underlying format that a disk image for a VM is stored as within the Image service back-end
store. For example, AMI, ISO, QCOW2, VMDK, and so on.
dispersion In Object Storage, tools to test and ensure dispersion of objects and containers to ensure fault
tolerance.
distributed virtual router (DVR) Mechanism for highly available multi-host routing when using OpenStack
Networking (neutron).
Django A web framework used extensively in horizon.
DNS record A record that specifies information about a particular domain and belongs to the domain.
DNS service (designate) OpenStack project that provides scalable, on demand, self service access to authoritative
DNS services, in a technology-agnostic manner.
dnsmasq Daemon that provides DNS, DHCP, BOOTP, and TFTP services for virtual networks.
domain An Identity API v3 entity. Represents a collection of projects, groups and users that defines administrative
boundaries for managing OpenStack Identity entities. On the Internet, separates a website from
other sites. Often, the domain name has two or more parts that are separated by dots. For example,
Glossary 313
Networking Guide (Release Version: 15.0.0)
yahoo.com, usa.gov, harvard.edu, or mail.yahoo.com. Also, a domain is an entity or container of all
DNS-related information containing one or more records.
Domain Name System (DNS) A system by which Internet domain name-to-address and address-to-name resolutions
are determined. DNS helps navigate the Internet by translating the IP address into an address
that is easier to remember. For example, translating 111.111.111.1 into www.yahoo.com. All domains
and their components, such as mail servers, utilize DNS to resolve to the appropriate locations. DNS
servers are usually set up in a master-slave relationship such that failure of the master invokes the slave.
DNS servers might also be clustered or replicated such that changes made to one DNS server are automatically
propagated to other active servers. In Compute, the support that enables associating DNS
entries with floating IP addresses, nodes, or cells so that hostnames are consistent across reboots.
download The transfer of data, usually in the form of files, from one computer to another.
durable exchange The Compute RabbitMQ message exchange that remains active when the server restarts.
durable queue A Compute RabbitMQ message queue that remains active when the server restarts.
Dynamic Host Configuration Protocol (DHCP) A network protocol that configures devices that are connected
to a network so that they can communicate on that network by using the Internet Protocol (IP).
The protocol is implemented in a client-server model where DHCP clients request configuration data,
such as an IP address, a default route, and one or more DNS server addresses from a DHCP server. A
method to automatically configure networking for a host at boot time. Provided by both Networking and
Compute.
Dynamic HyperText Markup Language (DHTML) Pages that use HTML, JavaScript, and Cascading Style
Sheets to enable users to interact with a web page or show simple animation.
E
east-west traffic Network traffic between servers in the same cloud or data center. See also north-south traffic.
EBS boot volume An Amazon EBS storage volume that contains a bootable VM image, currently unsupported
in OpenStack.
ebtables Filtering tool for a Linux bridging firewall, enabling filtering of network traffic passing through
a Linux bridge. Used in Compute along with arptables, iptables, and ip6tables to ensure isolation of
network communications.
EC2 The Amazon commercial compute product, similar to Compute.
EC2 access key Used along with an EC2 secret key to access the Compute EC2 API.
EC2 API OpenStack supports accessing the Amazon EC2 API through Compute.
EC2 Compatibility API A Compute component that enables OpenStack to communicate with Amazon EC2.
EC2 secret key Used along with an EC2 access key when communicating with the Compute EC2 API; used
to digitally sign each request.
Elastic Block Storage (EBS) The Amazon commercial block storage product.
encapsulation The practice of placing one packet type within another for the purposes of abstracting or securing
data. Examples include GRE, MPLS, or IPsec.
encryption OpenStack supports encryption technologies such as HTTPS, SSH, SSL, TLS, digital certificates,
and data encryption.
endpoint See API endpoint.
314 Glossary
Networking Guide (Release Version: 15.0.0)
endpoint registry Alternative term for an Identity service catalog.
endpoint template A list of URL and port number endpoints that indicate where a service, such as Object
Storage, Compute, Identity, and so on, can be accessed.
entity Any piece of hardware or software that wants to connect to the network services provided by Networking,
the network connectivity service. An entity can make use of Networking by implementing a
VIF.
ephemeral image A VM image that does not save changes made to its volumes and reverts them to their
original state after the instance is terminated.
ephemeral volume Volume that does not save the changes made to it and reverts to its original state when the
current user relinquishes control.
Essex A grouped release of projects related to OpenStack that came out in April 2012, the fifth release of
OpenStack. It included Compute (nova 2012.1), Object Storage (swift 1.4.8), Image (glance), Identity
(keystone), and Dashboard (horizon). Essex is the code name for the fifth release of OpenStack. The
design summit took place in Boston, Massachusetts, US and Essex is a nearby city.
ESXi An OpenStack-supported hypervisor.
ETag MD5 hash of an object within Object Storage, used to ensure data integrity.
euca2ools A collection of command-line tools for administering VMs; most are compatible with OpenStack.
Eucalyptus Kernel Image (EKI) Used along with an ERI to create an EMI.
Eucalyptus Machine Image (EMI) VM image container format supported by Image service.
Eucalyptus Ramdisk Image (ERI) Used along with an EKI to create an EMI.
evacuate The process of migrating one or all virtual machine (VM) instances from one host to another, compatible
with both shared storage live migration and block migration.
exchange Alternative term for a RabbitMQ message exchange.
exchange type A routing algorithm in the Compute RabbitMQ.
exclusive queue Connected to by a direct consumer in RabbitMQ—Compute, the message can be consumed
only by the current connection.
extended attributes (xattr) File system option that enables storage of additional information beyond owner,
group, permissions, modification time, and so on. The underlying Object Storage file system must support
extended attributes.
extension Alternative term for an API extension or plug-in. In the context of Identity service, this is a call that
is specific to the implementation, such as adding support for OpenID.
external network A network segment typically used for instance Internet access.
extra specs Specifies additional requirements when Compute determines where to start a new instance. Examples
include a minimum amount of network bandwidth or a GPU.
F
FakeLDAP An easy method to create a local LDAP directory for testing Identity and Compute. Requires
Redis.
fan-out exchange Within RabbitMQ and Compute, it is the messaging interface that is used by the scheduler
service to receive capability messages from the compute, volume, and network nodes.
Glossary 315
Networking Guide (Release Version: 15.0.0)
federated identity A method to establish trusts between identity providers and the OpenStack cloud.
Fedora A Linux distribution compatible with OpenStack.
Fibre Channel Storage protocol similar in concept to TCP/IP; encapsulates SCSI commands and data.
Fibre Channel over Ethernet (FCoE) The fibre channel protocol tunneled within Ethernet.
fill-first scheduler The Compute scheduling method that attempts to fill a host with VMs rather than starting
new VMs on a variety of hosts.
filter The step in the Compute scheduling process when hosts that cannot run VMs are eliminated and not
chosen.
firewall Used to restrict communications between hosts and/or nodes, implemented in Compute using iptables,
arptables, ip6tables, and ebtables.
FireWall-as-a-Service (FWaaS) A Networking extension that provides perimeter firewall functionality.
fixed IP address An IP address that is associated with the same instance each time that instance boots, is
generally not accessible to end users or the public Internet, and is used for management of the instance.
Flat Manager The Compute component that gives IP addresses to authorized nodes and assumes DHCP, DNS,
and routing configuration and services are provided by something else.
flat mode injection A Compute networking method where the OS network configuration information is injected
into the VM image before the instance starts.
flat network Virtual network type that uses neither VLANs nor tunnels to segregate project traffic. Each
flat network typically requires a separate underlying physical interface defined by bridge mappings.
However, a flat network can contain multiple subnets.
FlatDHCP Manager The Compute component that provides dnsmasq (DHCP, DNS, BOOTP, TFTP) and
radvd (routing) services.
flavor Alternative term for a VM instance type.
flavor ID UUID for each Compute or Image service VM flavor or instance type.
floating IP address An IP address that a project can associate with a VM so that the instance has the same
public IP address each time that it boots. You create a pool of floating IP addresses and assign them to
instances as they are launched to maintain a consistent IP address for maintaining DNS assignment.
Folsom A grouped release of projects related to OpenStack that came out in the fall of 2012, the sixth release
of OpenStack. It includes Compute (nova), Object Storage (swift), Identity (keystone), Networking
(neutron), Image service (glance), and Volumes or Block Storage (cinder). Folsom is the code name
for the sixth release of OpenStack. The design summit took place in San Francisco, California, US and
Folsom is a nearby city.
FormPost Object Storage middleware that uploads (posts) an image through a form on a web page.
freezer Code name for the Backup, Restore, and Disaster Recovery service.
front end The point where a user interacts with a service; can be an API endpoint, the dashboard, or a
command-line tool.
G
gateway An IP address, typically assigned to a router, that passes network traffic between different networks.
316 Glossary
Networking Guide (Release Version: 15.0.0)
generic receive offload (GRO) Feature of certain network interface drivers that combines many smaller received
packets into a large packet before delivery to the kernel IP stack.
generic routing encapsulation (GRE) Protocol that encapsulates a wide variety of network layer protocols
inside virtual point-to-point links.
glance Codename for the Image service.
glance API server Alternative name for the Image API.
glance registry Alternative term for the Image service image registry.
global endpoint template The Identity service endpoint template that contains services available to all
projects.
GlusterFS A file system designed to aggregate NAS hosts, compatible with OpenStack.
gnocchi Part of the OpenStack Telemetry service; provides an indexer and time-series database.
golden image A method of operating system installation where a finalized disk image is created and then used
by all nodes without modification.
Governance service (congress) The project that provides Governance-as-a-Service across any collection of
cloud services in order to monitor, enforce, and audit policy over dynamic infrastructure.
Graphic Interchange Format (GIF) A type of image file that is commonly used for animated images on web
pages.
Graphics Processing Unit (GPU) Choosing a host based on the existence of a GPU is currently unsupported
in OpenStack.
Green Threads The cooperative threading model used by Python; reduces race conditions and only context
switches when specific library calls are made. Each OpenStack service is its own thread.
Grizzly The code name for the seventh release of OpenStack. The design summit took place in San Diego,
California, US and Grizzly is an element of the state flag of California.
Group An Identity v3 API entity. Represents a collection of users that is owned by a specific domain.
guest OS An operating system instance running under the control of a hypervisor.
H
Hadoop Apache Hadoop is an open source software framework that supports data-intensive distributed applications.
Hadoop Distributed File System (HDFS) A distributed, highly fault-tolerant file system designed to run on
low-cost commodity hardware.
handover An object state in Object Storage where a new replica of the object is automatically created due to
a drive failure.
HAProxy Provides a high availability load balancer and proxy server for TCP and HTTP-based applications
that spreads requests across multiple servers.
hard reboot A type of reboot where a physical or virtual power button is pressed as opposed to a graceful,
proper shutdown of the operating system.
Havana The code name for the eighth release of OpenStack. The design summit took place in Portland,
Oregon, US and Havana is an unincorporated community in Oregon.
Glossary 317
Networking Guide (Release Version: 15.0.0)
health monitor Determines whether back-end members of a VIP pool can process a request. A pool can
have several health monitors associated with it. When a pool has several monitors associated with it, all
monitors check each member of the pool. All monitors must declare a member to be healthy for it to stay
active.
heat Codename for the Orchestration service.
Heat Orchestration Template (HOT) Heat input in the format native to OpenStack.
high availability (HA) A high availability system design approach and associated service implementation
ensures that a prearranged level of operational performance will be met during a contractual measurement
period. High availability systems seek to minimize system downtime and data loss.
horizon Codename for the Dashboard.
horizon plug-in A plug-in for the OpenStack Dashboard (horizon).
host A physical computer, not a VM instance (node).
host aggregate A method to further subdivide availability zones into hypervisor pools, a collection of common
hosts.
Host Bus Adapter (HBA) Device plugged into a PCI slot, such as a fibre channel or network card.
hybrid cloud A hybrid cloud is a composition of two or more clouds (private, community or public) that
remain distinct entities but are bound together, offering the benefits of multiple deployment models.
Hybrid cloud can also mean the ability to connect colocation, managed and/or dedicated services with
cloud resources.
Hyper-V One of the hypervisors supported by OpenStack.
hyperlink Any kind of text that contains a link to some other site, commonly found in documents where
clicking on a word or words opens up a different website.
Hypertext Transfer Protocol (HTTP) An application protocol for distributed, collaborative, hypermedia information
systems. It is the foundation of data communication for the World Wide Web. Hypertext is
structured text that uses logical links (hyperlinks) between nodes containing text. HTTP is the protocol
to exchange or transfer hypertext.
Hypertext Transfer Protocol Secure (HTTPS) An encrypted communications protocol for secure communication
over a computer network, with especially wide deployment on the Internet. Technically, it is not a
protocol in and of itself; rather, it is the result of simply layering the Hypertext Transfer Protocol (HTTP)
on top of the TLS or SSL protocol, thus adding the security capabilities of TLS or SSL to standard HTTP
communications. Most OpenStack API endpoints and many inter-component communications support
HTTPS communication.
hypervisor Software that arbitrates and controls VM access to the actual underlying hardware.
hypervisor pool A collection of hypervisors grouped together through host aggregates.
I
Icehouse The code name for the ninth release of OpenStack. The design summit took place in Hong Kong
and Ice House is a street in that city.
ID number Unique numeric ID associated with each user in Identity, conceptually similar to a Linux or LDAP
UID.
Identity API Alternative term for the Identity service API.
318 Glossary
Networking Guide (Release Version: 15.0.0)
Identity back end The source used by Identity service to retrieve user information; an OpenLDAP server, for
example.
identity provider A directory service, which allows users to login with a user name and password. It is a
typical source of authentication tokens.
Identity service (keystone) The project that facilitates API client authentication, service discovery, distributed
multi-project authorization, and auditing. It provides a central directory of users mapped to
the OpenStack services they can access. It also registers endpoints for OpenStack services and acts as a
common authentication system.
Identity service API The API used to access the OpenStack Identity service provided through keystone.
IETF Internet Engineering Task Force (IETF) is an open standards organization that develops Internet standards,
particularly the standards pertaining to TCP/IP.
image A collection of files for a specific operating system (OS) that you use to create or rebuild a server.
OpenStack provides pre-built images. You can also create custom images, or snapshots, from servers
that you have launched. Custom images can be used for data backups or as “gold” images for additional
servers.
Image API The Image service API endpoint for management of VM images. Processes client requests for
VMs, updates Image service metadata on the registry server, and communicates with the store adapter to
upload VM images from the back-end store.
image cache Used by Image service to obtain images on the local host rather than re-downloading them from
the image server each time one is requested.
image ID Combination of a URI and UUID used to access Image service VM images through the image API.
image membership A list of projects that can access a given VM image within Image service.
image owner The project who owns an Image service virtual machine image.
image registry A list of VM images that are available through Image service.
Image service (glance) The OpenStack service that provide services and associated libraries to store, browse,
share, distribute and manage bootable disk images, other data closely associated with initializing compute
resources, and metadata definitions.
image status The current status of a VM image in Image service, not to be confused with the status of a running
instance.
image store The back-end store used by Image service to store VM images, options include Object Storage,
locally mounted file system, RADOS block devices, VMware datastore, or HTTP.
image UUID UUID used by Image service to uniquely identify each VM image.
incubated project A community project may be elevated to this status and is then promoted to a core project.
Infrastructure Optimization service (watcher) OpenStack project that aims to provide a flexible and scalable
resource optimization service for multi-project OpenStack-based clouds.
Infrastructure-as-a-Service (IaaS) IaaS is a provisioning model in which an organization outsources physical
components of a data center, such as storage, hardware, servers, and networking components. A service
provider owns the equipment and is responsible for housing, operating and maintaining it. The client
typically pays on a per-use basis. IaaS is a model for providing cloud services.
ingress filtering The process of filtering incoming network traffic. Supported by Compute.
Glossary 319
Networking Guide (Release Version: 15.0.0)
INI format The OpenStack configuration files use an INI format to describe options and their values. It
consists of sections and key value pairs.
injection The process of putting a file into a virtual machine image before the instance is started.
Input/Output Operations Per Second (IOPS) IOPS are a common performance measurement used to
benchmark computer storage devices like hard disk drives, solid state drives, and storage area networks.
instance A running VM, or a VM in a known state such as suspended, that can be used like a hardware server.
instance ID Alternative term for instance UUID.
instance state The current state of a guest VM image.
instance tunnels network A network segment used for instance traffic tunnels between compute nodes and
the network node.
instance type Describes the parameters of the various virtual machine images that are available to users;
includes parameters such as CPU, storage, and memory. Alternative term for flavor.
instance type ID Alternative term for a flavor ID.
instance UUID Unique ID assigned to each guest VM instance.
Intelligent Platform Management Interface (IPMI) IPMI is a standardized computer system interface used
by system administrators for out-of-band management of computer systems and monitoring of their operation.
In layman’s terms, it is a way to manage a computer using a direct network connection, whether
it is turned on or not; connecting to the hardware rather than an operating system or login shell.
interface A physical or virtual device that provides connectivity to another device or medium.
interface ID Unique ID for a Networking VIF or vNIC in the form of a UUID.
Internet Control Message Protocol (ICMP) A network protocol used by network devices for control messages.
For example, ping uses ICMP to test connectivity.
Internet protocol (IP) Principal communications protocol in the internet protocol suite for relaying datagrams
across network boundaries.
Internet Service Provider (ISP) Any business that provides Internet access to individuals or businesses.
Internet Small Computer System Interface (iSCSI) Storage protocol that encapsulates SCSI frames for
transport over IP networks. Supported by Compute, Object Storage, and Image service.
IP address Number that is unique to every computer system on the Internet. Two versions of the Internet
Protocol (IP) are in use for addresses: IPv4 and IPv6.
IP Address Management (IPAM) The process of automating IP address allocation, deallocation, and management.
Currently provided by Compute, melange, and Networking.
ip6tables Tool used to set up, maintain, and inspect the tables of IPv6 packet filter rules in the Linux kernel.
In OpenStack Compute, ip6tables is used along with arptables, ebtables, and iptables to create firewalls
for both nodes and VMs.
ipset Extension to iptables that allows creation of firewall rules that match entire “sets” of IP addresses simultaneously.
These sets reside in indexed data structures to increase efficiency, particularly on systems
with a large quantity of rules.
iptables Used along with arptables and ebtables, iptables create firewalls in Compute. iptables are the tables
provided by the Linux kernel firewall (implemented as different Netfilter modules) and the chains and
rules it stores. Different kernel modules and programs are currently used for different protocols: iptables
320 Glossary
Networking Guide (Release Version: 15.0.0)
applies to IPv4, ip6tables to IPv6, arptables to ARP, and ebtables to Ethernet frames. Requires root
privilege to manipulate.
ironic Codename for the Bare Metal service.
iSCSI Qualified Name (IQN) IQN is the format most commonly used for iSCSI names, which uniquely identify
nodes in an iSCSI network. All IQNs follow the pattern iqn.yyyy-mm.domain:identifier, where
‘yyyy-mm’ is the year and month in which the domain was registered, ‘domain’ is the reversed domain
name of the issuing organization, and ‘identifier’ is an optional string which makes each IQN under the
same domain unique. For example, ‘iqn.2015-10.org.openstack.408ae959bce1’.
ISO9660 One of the VM image disk formats supported by Image service.
itsec A default role in the Compute RBAC system that can quarantine an instance in any project.
J
Java A programming language that is used to create systems that involve more than one computer by way of
a network.
JavaScript A scripting language that is used to build web pages.
JavaScript Object Notation (JSON) One of the supported response formats in OpenStack.
jumbo frame Feature in modern Ethernet networks that supports frames up to approximately 9000 bytes.
Juno The code name for the tenth release of OpenStack. The design summit took place in Atlanta, Georgia,
US and Juno is an unincorporated community in Georgia.
K
Kerberos A network authentication protocol which works on the basis of tickets. Kerberos allows nodes
communication over a non-secure network, and allows nodes to prove their identity to one another in a
secure manner.
kernel-based VM (KVM) An OpenStack-supported hypervisor. KVM is a full virtualization solution for
Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V), ARM, IBM Power,
and IBM zSeries. It consists of a loadable kernel module, that provides the core virtualization infrastructure
and a processor specific module.
Key Manager service (barbican) The project that produces a secret storage and generation system capable
of providing key management for services wishing to enable encryption features.
keystone Codename of the Identity service.
Kickstart A tool to automate system configuration and installation on Red Hat, Fedora, and CentOS-based
Linux distributions.
Kilo The code name for the eleventh release of OpenStack. The design summit took place in Paris, France.
Due to delays in the name selection, the release was known only as K. Because k is the unit symbol
for kilo and the kilogram reference artifact is stored near Paris in the Pavillon de Breteuil in Sèvres, the
community chose Kilo as the release name.
L
large object An object within Object Storage that is larger than 5 GB.
Glossary 321
Networking Guide (Release Version: 15.0.0)
Launchpad The collaboration site for OpenStack.
Layer-2 (L2) agent OpenStack Networking agent that provides layer-2 connectivity for virtual networks.
Layer-2 network Term used in the OSI network architecture for the data link layer. The data link layer is
responsible for media access control, flow control and detecting and possibly correcting errors that may
occur in the physical layer.
Layer-3 (L3) agent OpenStack Networking agent that provides layer-3 (routing) services for virtual networks.
Layer-3 network Term used in the OSI network architecture for the network layer. The network layer is
responsible for packet forwarding including routing from one node to another.
Liberty The code name for the twelfth release of OpenStack. The design summit took place in Vancouver,
Canada and Liberty is the name of a village in the Canadian province of Saskatchewan.
libvirt Virtualization API library used by OpenStack to interact with many of its supported hypervisors.
Lightweight Directory Access Protocol (LDAP) An application protocol for accessing and maintaining distributed
directory information services over an IP network.
Linux Unix-like computer operating system assembled under the model of free and open-source software
development and distribution.
Linux bridge Software that enables multiple VMs to share a single physical NIC within Compute.
Linux Bridge neutron plug-in Enables a Linux bridge to understand a Networking port, interface attachment,
and other abstractions.
Linux containers (LXC) An OpenStack-supported hypervisor.
live migration The ability within Compute to move running virtual machine instances from one host to another
with only a small service interruption during switchover.
load balancer A load balancer is a logical device that belongs to a cloud account. It is used to distribute
workloads between multiple back-end systems or services, based on the criteria defined as part of its
configuration.
load balancing The process of spreading client requests between two or more nodes to improve performance
and availability.
Load-Balancer-as-a-Service (LBaaS) Enables Networking to distribute incoming requests evenly between
designated instances.
Load-balancing service (octavia) The project that aims to rovide scalable, on demand, self service access to
load-balancer services, in technology-agnostic manner.
Logical Volume Manager (LVM) Provides a method of allocating space on mass-storage devices that is more
flexible than conventional partitioning schemes.
M
magnum Code name for the Containers Infrastructure Management service.
management API Alternative term for an admin API.
management network A network segment used for administration, not accessible to the public Internet.
manager Logical groupings of related code, such as the Block Storage volume manager or network manager.
manifest Used to track segments of a large object within Object Storage.
322 Glossary
Networking Guide (Release Version: 15.0.0)
manifest object A special Object Storage object that contains the manifest for a large object.
manila Codename for OpenStack Shared File Systems service.
manila-share Responsible for managing Shared File System Service devices, specifically the back-end devices.
maximum transmission unit (MTU) Maximum frame or packet size for a particular network medium. Typically
1500 bytes for Ethernet networks.
mechanism driver A driver for the Modular Layer 2 (ML2) neutron plug-in that provides layer-2 connectivity
for virtual instances. A single OpenStack installation can use multiple mechanism drivers.
melange Project name for OpenStack Network Information Service. To be merged with Networking.
membership The association between an Image service VM image and a project. Enables images to be shared
with specified projects.
membership list A list of projects that can access a given VM image within Image service.
memcached A distributed memory object caching system that is used by Object Storage for caching.
memory overcommit The ability to start new VM instances based on the actual memory usage of a host, as
opposed to basing the decision on the amount of RAM each running instance thinks it has available. Also
known as RAM overcommit.
message broker The software package used to provide AMQP messaging capabilities within Compute. Default
package is RabbitMQ.
message bus The main virtual communication line used by all AMQP messages for inter-cloud communications
within Compute.
message queue Passes requests from clients to the appropriate workers and returns the output to the client
after the job completes.
Message service (zaqar) The project that provides a messaging service that affords a variety of distributed
application patterns in an efficient, scalable and highly available manner, and to create and maintain
associated Python libraries and documentation.
Meta-Data Server (MDS) Stores CephFS metadata.
Metadata agent OpenStack Networking agent that provides metadata services for instances.
migration The process of moving a VM instance from one host to another.
mistral Code name for Workflow service.
Mitaka The code name for the thirteenth release of OpenStack. The design summit took place in Tokyo,
Japan. Mitaka is a city in Tokyo.
Modular Layer 2 (ML2) neutron plug-in Can concurrently use multiple layer-2 networking technologies,
such as 802.1Q and VXLAN, in Networking.
monasca Codename for OpenStack Monitoring.
Monitor (LBaaS) LBaaS feature that provides availability monitoring using the ping command, TCP, and
HTTP/HTTPS GET.
Monitor (Mon) A Ceph component that communicates with external clients, checks data state and consistency,
and performs quorum functions.
Monitoring (monasca) The OpenStack service that provides a multi-project, highly scalable, performant,
fault-tolerant monitoring-as-a-service solution for metrics, complex event processing and logging. To
Glossary 323
Networking Guide (Release Version: 15.0.0)
build an extensible platform for advanced monitoring services that can be used by both operators and
projects to gain operational insight and visibility, ensuring availability and stability.
multi-factor authentication Authentication method that uses two or more credentials, such as a password
and a private key. Currently not supported in Identity.
multi-host High-availability mode for legacy (nova) networking. Each compute node handles NAT and DHCP
and acts as a gateway for all of the VMs on it. A networking failure on one compute node doesn’t affect
VMs on other compute nodes.
multinic Facility in Compute that allows each virtual machine instance to have more than one VIF connected
to it.
murano Codename for the Application Catalog service.
N
Nebula Released as open source by NASA in 2010 and is the basis for Compute.
netadmin One of the default roles in the Compute RBAC system. Enables the user to allocate publicly accessible
IP addresses to instances and change firewall rules.
NetApp volume driver Enables Compute to communicate with NetApp storage devices through the NetApp
OnCommand Provisioning Manager.
network A virtual network that provides connectivity between entities. For example, a collection of virtual
ports that share network connectivity. In Networking terminology, a network is always a layer-2 network.
Network Address Translation (NAT) Process of modifying IP address information while in transit. Supported
by Compute and Networking.
network controller A Compute daemon that orchestrates the network configuration of nodes, including IP
addresses, VLANs, and bridging. Also manages routing for both public and private networks.
Network File System (NFS) A method for making file systems available over the network. Supported by
OpenStack.
network ID Unique ID assigned to each network segment within Networking. Same as network UUID.
network manager The Compute component that manages various network components, such as firewall rules,
IP address allocation, and so on.
network namespace Linux kernel feature that provides independent virtual networking instances on a single
host with separate routing tables and interfaces. Similar to virtual routing and forwarding (VRF) services
on physical network equipment.
network node Any compute node that runs the network worker daemon.
network segment Represents a virtual, isolated OSI layer-2 subnet in Networking.
Network Service Header (NSH) Provides a mechanism for metadata exchange along the instantiated service
path.
Network Time Protocol (NTP) Method of keeping a clock for a host or node correct via communication with
a trusted, accurate time source.
network UUID Unique ID for a Networking network segment.
network worker The nova-network worker daemon; provides services such as giving an IP address to a
booting nova instance.
324 Glossary
Networking Guide (Release Version: 15.0.0)
Networking API (Neutron API) API used to access OpenStack Networking. Provides an extensible architecture
to enable custom plug-in creation.
Networking service (neutron) The OpenStack project which implements services and associated libraries to
provide on-demand, scalable, and technology-agnostic network abstraction.
neutron Codename for OpenStack Networking service.
neutron API An alternative name for Networking API.
neutron manager Enables Compute and Networking integration, which enables Networking to perform network
management for guest VMs.
neutron plug-in Interface within Networking that enables organizations to create custom plug-ins for advanced
features, such as QoS, ACLs, or IDS.
Newton The code name for the fourteenth release of OpenStack. The design summit took place in Austin,
Texas, US. The release is named after “Newton House” which is located at 1013 E. Ninth St., Austin,
TX. which is listed on the National Register of Historic Places.
Nexenta volume driver Provides support for NexentaStor devices in Compute.
NFV Orchestration Service (tacker) OpenStack service that aims to implement Network Function Virtualization
(NFV) orchestration services and libraries for end-to-end life-cycle management of network
services and Virtual Network Functions (VNFs).
Nginx An HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server.
No ACK Disables server-side message acknowledgment in the Compute RabbitMQ. Increases performance
but decreases reliability.
node A VM instance that runs on a host.
non-durable exchange Message exchange that is cleared when the service restarts. Its data is not written to
persistent storage.
non-durable queue Message queue that is cleared when the service restarts. Its data is not written to persistent
storage.
non-persistent volume Alternative term for an ephemeral volume.
north-south traffic Network traffic between a user or client (north) and a server (south), or traffic into the
cloud (south) and out of the cloud (north). See also east-west traffic.
nova Codename for OpenStack Compute service.
Nova API Alternative term for the Compute API.
nova-network A Compute component that manages IP address allocation, firewalls, and other network-related
tasks. This is the legacy networking option and an alternative to Networking.
O
object A BLOB of data held by Object Storage; can be in any format.
object auditor Opens all objects for an object server and verifies the MD5 hash, size, and metadata for each
object.
object expiration A configurable option within Object Storage to automatically delete objects after a specified
amount of time has passed or a certain date is reached.
Glossary 325
Networking Guide (Release Version: 15.0.0)
object hash Unique ID for an Object Storage object.
object path hash Used by Object Storage to determine the location of an object in the ring. Maps objects to
partitions.
object replicator An Object Storage component that copies an object to remote partitions for fault tolerance.
object server An Object Storage component that is responsible for managing objects.
Object Storage API API used to access OpenStack Object Storage.
Object Storage Device (OSD) The Ceph storage daemon.
Object Storage service (swift) The OpenStack core project that provides eventually consistent and redundant
storage and retrieval of fixed digital content.
object versioning Allows a user to set a flag on an Object Storage container so that all objects within the
container are versioned.
Ocata The code name for the fifteenth release of OpenStack. The design summit will take place in Barcelona,
Spain. Ocata is a beach north of Barcelona.
Octavia Code name for the Load-balancing service.
Oldie Term for an Object Storage process that runs for a long time. Can indicate a hung process.
Open Cloud Computing Interface (OCCI) A standardized interface for managing compute, data, and network
resources, currently unsupported in OpenStack.
Open Virtualization Format (OVF) Standard for packaging VM images. Supported in OpenStack.
Open vSwitch Open vSwitch is a production quality, multilayer virtual switch licensed under the open source
Apache 2.0 license. It is designed to enable massive network automation through programmatic extension,
while still supporting standard management interfaces and protocols (for example NetFlow, sFlow,
SPAN, RSPAN, CLI, LACP, 802.1ag).
Open vSwitch (OVS) agent Provides an interface to the underlying Open vSwitch service for the Networking
plug-in.
Open vSwitch neutron plug-in Provides support for Open vSwitch in Networking.
OpenLDAP An open source LDAP server. Supported by both Compute and Identity.
OpenStack OpenStack is a cloud operating system that controls large pools of compute, storage, and networking
resources throughout a data center, all managed through a dashboard that gives administrators
control while empowering their users to provision resources through a web interface. OpenStack is an
open source project licensed under the Apache License 2.0.
OpenStack code name Each OpenStack release has a code name. Code names ascend in alphabetical order:
Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, Icehouse, Juno, Kilo, Liberty, Mitaka,
Newton, Ocata, Pike, and Queens. Code names are cities or counties near where the corresponding
OpenStack design summit took place. An exception, called the Waldon exception, is granted to elements
of the state flag that sound especially cool. Code names are chosen by popular vote.
openSUSE A Linux distribution that is compatible with OpenStack.
operator The person responsible for planning and maintaining an OpenStack installation.
optional service An official OpenStack service defined as optional by DefCore Committee. Currently, consists
of Dashboard (horizon), Telemetry service (Telemetry), Orchestration service (heat), Database service
(trove), Bare Metal service (ironic), and so on.
326 Glossary
Networking Guide (Release Version: 15.0.0)
Orchestration service (heat) The OpenStack service which orchestrates composite cloud applications using
a declarative template format through an OpenStack-native REST API.
orphan In the context of Object Storage, this is a process that is not terminated after an upgrade, restart, or
reload of the service.
Oslo Codename for the Common Libraries project.
P
panko Part of the OpenStack Telemetry service; provides event storage.
parent cell If a requested resource, such as CPU time, disk storage, or memory, is not available in the parent
cell, the request is forwarded to associated child cells.
partition A unit of storage within Object Storage used to store objects. It exists on top of devices and is
replicated for fault tolerance.
partition index Contains the locations of all Object Storage partitions within the ring.
partition shift value Used by Object Storage to determine which partition data should reside on.
path MTU discovery (PMTUD) Mechanism in IP networks to detect end-to-end MTU and adjust packet size
accordingly.
pause A VM state where no changes occur (no changes in memory, network communications stop, etc); the
VM is frozen but not shut down.
PCI passthrough Gives guest VMs exclusive access to a PCI device. Currently supported in OpenStack
Havana and later releases.
persistent message A message that is stored both in memory and on disk. The message is not lost after a
failure or restart.
persistent volume Changes to these types of disk volumes are saved.
personality file A file used to customize a Compute instance. It can be used to inject SSH keys or a specific
network configuration.
Pike The code name for the sixteenth release of OpenStack. The design summit will take place in Boston,
Massachusetts, US. The release is named after the Massachusetts Turnpike, abbreviated commonly as
the Mass Pike, which is the easternmost stretch of Interstate 90.
Platform-as-a-Service (PaaS) Provides to the consumer the ability to deploy applications through a programming
language or tools supported by the cloud platform provider. An example of Platform-as-a-Service
is an Eclipse/Java programming platform provided with no downloads required.
plug-in Software component providing the actual implementation for Networking APIs, or for Compute APIs,
depending on the context.
policy service Component of Identity that provides a rule-management interface and a rule-based authorization
engine.
policy-based routing (PBR) Provides a mechanism to implement packet forwarding and routing according
to the policies defined by the network administrator.
pool A logical set of devices, such as web servers, that you group together to receive and process traffic.
The load balancing function chooses which member of the pool handles the new requests or connections
received on the VIP address. Each VIP has one pool.
Glossary 327
Networking Guide (Release Version: 15.0.0)
pool member An application that runs on the back-end server in a load-balancing system.
port A virtual network port within Networking; VIFs / vNICs are connected to a port.
port UUID Unique ID for a Networking port.
preseed A tool to automate system configuration and installation on Debian-based Linux distributions.
private image An Image service VM image that is only available to specified projects.
private IP address An IP address used for management and administration, not available to the public Internet.
private network The Network Controller provides virtual networks to enable compute servers to interact
with each other and with the public network. All machines must have a public and private network
interface. A private network interface can be a flat or VLAN network interface. A flat network interface
is controlled by the flat_interface with flat managers. A VLAN network interface is controlled by the
vlan_interface option with VLAN managers.
project Projects represent the base unit of “ownership” in OpenStack, in that all resources in OpenStack should
be owned by a specific project. In OpenStack Identity, a project must be owned by a specific domain.
project ID Unique ID assigned to each project by the Identity service.
project VPN Alternative term for a cloudpipe.
promiscuous mode Causes the network interface to pass all traffic it receives to the host rather than passing
only the frames addressed to it.
protected property Generally, extra properties on an Image service image to which only cloud administrators
have access. Limits which user roles can perform CRUD operations on that property. The cloud
administrator can configure any image property as protected.
provider An administrator who has access to all hosts and instances.
proxy node A node that provides the Object Storage proxy service.
proxy server Users of Object Storage interact with the service through the proxy server, which in turn looks
up the location of the requested data within the ring and returns the results to the user.
public API An API endpoint used for both service-to-service communication and end-user interactions.
public image An Image service VM image that is available to all projects.
public IP address An IP address that is accessible to end-users.
public key authentication Authentication method that uses keys rather than passwords.
public network The Network Controller provides virtual networks to enable compute servers to interact with
each other and with the public network. All machines must have a public and private network interface.
The public network interface is controlled by the public_interface option.
Puppet An operating system configuration-management tool supported by OpenStack.
Python Programming language used extensively in OpenStack.
Q
QEMU Copy On Write 2 (QCOW2) One of the VM image disk formats supported by Image service.
Qpid Message queue software supported by OpenStack; an alternative to RabbitMQ.
328 Glossary
Networking Guide (Release Version: 15.0.0)
Quality of Service (QoS) The ability to guarantee certain network or storage requirements to satisfy a Service
Level Agreement (SLA) between an application provider and end users. Typically includes performance
requirements like networking bandwidth, latency, jitter correction, and reliability as well as storage performance
in Input/Output Operations Per Second (IOPS), throttling agreements, and performance expectations
at peak load.
quarantine If Object Storage finds objects, containers, or accounts that are corrupt, they are placed in this
state, are not replicated, cannot be read by clients, and a correct copy is re-replicated.
Queens The code name for the seventeenth release of OpenStack. The design summit will take place in
Sydney, Australia. The release is named after the Queens Pound river in the South Coast region of New
South Wales.
Quick EMUlator (QEMU) QEMU is a generic and open source machine emulator and virtualizer. One of
the hypervisors supported by OpenStack, generally used for development purposes.
quota In Compute and Block Storage, the ability to set resource limits on a per-project basis.
R
RabbitMQ The default message queue software used by OpenStack.
Rackspace Cloud Files Released as open source by Rackspace in 2010; the basis for Object Storage.
RADOS Block Device (RBD) Ceph component that enables a Linux block device to be striped over multiple
distributed data stores.
radvd The router advertisement daemon, used by the Compute VLAN manager and FlatDHCP manager to
provide routing services for VM instances.
rally Codename for the Benchmark service.
RAM filter The Compute setting that enables or disables RAM overcommitment.
RAM overcommit The ability to start new VM instances based on the actual memory usage of a host, as
opposed to basing the decision on the amount of RAM each running instance thinks it has available.
Also known as memory overcommit.
rate limit Configurable option within Object Storage to limit database writes on a per-account and/or percontainer
basis.
raw One of the VM image disk formats supported by Image service; an unstructured disk image.
rebalance The process of distributing Object Storage partitions across all drives in the ring; used during initial
ring creation and after ring reconfiguration.
reboot Either a soft or hard reboot of a server. With a soft reboot, the operating system is signaled to restart,
which enables a graceful shutdown of all processes. A hard reboot is the equivalent of power cycling
the server. The virtualization platform should ensure that the reboot action has completed successfully,
even in cases in which the underlying domain/VM is paused or halted/stopped.
rebuild Removes all data on the server and replaces it with the specified image. Server ID and IP addresses
remain the same.
Recon An Object Storage component that collects meters.
record Belongs to a particular domain and is used to specify information about the domain. There are several
types of DNS records. Each record type contains particular information used to describe the purpose of
Glossary 329
Networking Guide (Release Version: 15.0.0)
that record. Examples include mail exchange (MX) records, which specify the mail server for a particular
domain; and name server (NS) records, which specify the authoritative name servers for a domain.
record ID A number within a database that is incremented each time a change is made. Used by Object
Storage when replicating.
Red Hat Enterprise Linux (RHEL) A Linux distribution that is compatible with OpenStack.
reference architecture A recommended architecture for an OpenStack cloud.
region A discrete OpenStack environment with dedicated API endpoints that typically shares only the Identity
(keystone) with other regions.
registry Alternative term for the Image service registry.
registry server An Image service that provides VM image metadata information to clients.
Reliable, Autonomic Distributed Object Store (RADOS)
A collection of components that provides object storage within Ceph. Similar to OpenStack Object
Storage.
Remote Procedure Call (RPC) The method used by the Compute RabbitMQ for intra-service communications.
replica Provides data redundancy and fault tolerance by creating copies of Object Storage objects, accounts,
and containers so that they are not lost when the underlying storage fails.
replica count The number of replicas of the data in an Object Storage ring.
replication The process of copying data to a separate physical device for fault tolerance and performance.
replicator The Object Storage back-end process that creates and manages object replicas.
request ID Unique ID assigned to each request sent to Compute.
rescue image A special type of VM image that is booted when an instance is placed into rescue mode. Allows
an administrator to mount the file systems for an instance to correct the problem.
resize Converts an existing server to a different flavor, which scales the server up or down. The original server
is saved to enable rollback if a problem occurs. All resizes must be tested and explicitly confirmed, at
which time the original server is removed.
RESTful A kind of web service API that uses REST, or Representational State Transfer. REST is the style of
architecture for hypermedia systems that is used for the World Wide Web.
ring An entity that maps Object Storage data to partitions. A separate ring exists for each service, such as
account, object, and container.
ring builder Builds and manages rings within Object Storage, assigns partitions to devices, and pushes the
configuration to other storage nodes.
role A personality that a user assumes to perform a specific set of operations. A role includes a set of rights
and privileges. A user assuming that role inherits those rights and privileges.
Role Based Access Control (RBAC) Provides a predefined list of actions that the user can perform, such
as start or stop VMs, reset passwords, and so on. Supported in both Identity and Compute and can be
configured using the dashboard.
role ID Alphanumeric ID assigned to each Identity service role.
330 Glossary
Networking Guide (Release Version: 15.0.0)
Root Cause Analysis (RCA) service (Vitrage) OpenStack project that aims to organize, analyze and visualize
OpenStack alarms and events, yield insights regarding the root cause of problems and deduce their
existence before they are directly detected.
rootwrap A feature of Compute that allows the unprivileged “nova” user to run a specified list of commands
as the Linux root user.
round-robin scheduler Type of Compute scheduler that evenly distributes instances among available hosts.
router A physical or virtual network device that passes network traffic between different networks.
routing key The Compute direct exchanges, fanout exchanges, and topic exchanges use this key to determine
how to process a message; processing varies depending on exchange type.
RPC driver Modular system that allows the underlying message queue software of Compute to be changed.
For example, from RabbitMQ to ZeroMQ or Qpid.
rsync Used by Object Storage to push object replicas.
RXTX cap Absolute limit on the amount of network traffic a Compute VM instance can send and receive.
RXTX quota Soft limit on the amount of network traffic a Compute VM instance can send and receive.
S
sahara Codename for the Data Processing service.
SAML assertion Contains information about a user as provided by the identity provider. It is an indication
that a user has been authenticated.
scheduler manager A Compute component that determines where VM instances should start. Uses modular
design to support a variety of scheduler types.
scoped token An Identity service API access token that is associated with a specific project.
scrubber Checks for and deletes unused VMs; the component of Image service that implements delayed
delete.
secret key String of text known only by the user; used along with an access key to make requests to the
Compute API.
secure boot Process whereby the system firmware validates the authenticity of the code involved in the boot
process.
secure shell (SSH) Open source tool used to access remote hosts through an encrypted communications channel,
SSH key injection is supported by Compute.
security group A set of network traffic filtering rules that are applied to a Compute instance.
segmented object An Object Storage large object that has been broken up into pieces. The re-assembled
object is called a concatenated object.
self-service For IaaS, ability for a regular (non-privileged) account to manage a virtual infrastructure component
such as networks without involving an administrator.
SELinux Linux kernel security module that provides the mechanism for supporting access control policies.
senlin Code name for the Clustering service.
Glossary 331
Networking Guide (Release Version: 15.0.0)
server Computer that provides explicit services to the client software running on that system, often managing
a variety of computer operations. A server is a VM instance in the Compute system. Flavor and image
are requisite elements when creating a server.
server image Alternative term for a VM image.
server UUID Unique ID assigned to each guest VM instance.
service An OpenStack service, such as Compute, Object Storage, or Image service. Provides one or more
endpoints through which users can access resources and perform operations.
service catalog Alternative term for the Identity service catalog.
Service Function Chain (SFC) For a given service, SFC is the abstracted view of the required service functions
and the order in which they are to be applied.
service ID Unique ID assigned to each service that is available in the Identity service catalog.
Service Level Agreement (SLA) Contractual obligations that ensure the availability of a service.
service project Special project that contains all services that are listed in the catalog.
service provider A system that provides services to other system entities. In case of federated identity, OpenStack
Identity is the service provider.
service registration An Identity service feature that enables services, such as Compute, to automatically register
with the catalog.
service token An administrator-defined token used by Compute to communicate securely with the Identity
service.
session back end The method of storage used by horizon to track client sessions, such as local memory, cookies,
a database, or memcached.
session persistence A feature of the load-balancing service. It attempts to force subsequent connections to a
service to be redirected to the same node as long as it is online.
session storage A horizon component that stores and tracks client session information. Implemented through
the Django sessions framework.
share A remote, mountable file system in the context of the Shared File Systems service. You can mount a
share to, and access a share from, several hosts by several users at a time.
share network An entity in the context of the Shared File Systems service that encapsulates interaction with
the Networking service. If the driver you selected runs in the mode requiring such kind of interaction,
you need to specify the share network to create a share.
Shared File Systems API A Shared File Systems service that provides a stable RESTful API. The service authenticates
and routes requests throughout the Shared File Systems service. There is python-manilaclient
to interact with the API.
Shared File Systems service (manila) The service that provides a set of services for management of shared
file systems in a multi-project cloud environment, similar to how OpenStack provides block-based storage
management through the OpenStack Block Storage service project. With the Shared File Systems
service, you can create a remote file system and mount the file system on your instances. You can also
read and write data from your instances to and from your file system.
shared IP address An IP address that can be assigned to a VM instance within the shared IP group. Public IP
addresses can be shared across multiple servers for use in various high-availability scenarios. When an
IP address is shared to another server, the cloud network restrictions are modified to enable each server
to listen to and respond on that IP address. You can optionally specify that the target server network
332 Glossary
Networking Guide (Release Version: 15.0.0)
configuration be modified. Shared IP addresses can be used with many standard heartbeat facilities,
such as keepalive, that monitor for failure and manage IP failover.
shared IP group A collection of servers that can share IPs with other members of the group. Any server in a
group can share one or more public IPs with any other server in the group. With the exception of the first
server in a shared IP group, servers must be launched into shared IP groups. A server may be a member
of only one shared IP group.
shared storage Block storage that is simultaneously accessible by multiple clients, for example, NFS.
Sheepdog Distributed block storage system for QEMU, supported by OpenStack.
Simple Cloud Identity Management (SCIM) Specification for managing identity in the cloud, currently unsupported
by OpenStack.
Simple Protocol for Independent Computing Environments (SPICE) SPICE provides remote desktop access
to guest virtual machines. It is an alternative to VNC. SPICE is supported by OpenStack.
Single-root I/O Virtualization (SR-IOV) A specification that, when implemented by a physical PCIe device,
enables it to appear as multiple separate PCIe devices. This enables multiple virtualized guests to share
direct access to the physical device, offering improved performance over an equivalent virtual device.
Currently supported in OpenStack Havana and later releases.
SmokeStack Runs automated tests against the core OpenStack API; written in Rails.
snapshot A point-in-time copy of an OpenStack storage volume or image. Use storage volume snapshots to
back up volumes. Use image snapshots to back up data, or as “gold” images for additional servers.
soft reboot A controlled reboot where a VM instance is properly restarted through operating system commands.
Software Development Lifecycle Automation service (solum) OpenStack project that aims to make cloud
services easier to consume and integrate with application development process by automating the sourceto-image
process, and simplifying app-centric deployment.
Software-defined networking (SDN) Provides an approach for network administrators to manage computer
network services through abstraction of lower-level functionality.
SolidFire Volume Driver The Block Storage driver for the SolidFire iSCSI storage appliance.
solum Code name for the Software Development Lifecycle Automation service.
spread-first scheduler The Compute VM scheduling algorithm that attempts to start a new VM on the host
with the least amount of load.
SQLAlchemy An open source SQL toolkit for Python, used in OpenStack.
SQLite A lightweight SQL database, used as the default persistent storage method in many OpenStack services.
stack A set of OpenStack resources created and managed by the Orchestration service according to a given
template (either an AWS CloudFormation template or a Heat Orchestration Template (HOT)).
StackTach Community project that captures Compute AMQP communications; useful for debugging.
static IP address Alternative term for a fixed IP address.
StaticWeb WSGI middleware component of Object Storage that serves container data as a static web page.
storage back end The method that a service uses for persistent storage, such as iSCSI, NFS, or local disk.
Glossary 333
Networking Guide (Release Version: 15.0.0)
storage manager A XenAPI component that provides a pluggable interface to support a wide variety of persistent
storage back ends.
storage manager back end A persistent storage method supported by XenAPI, such as iSCSI or NFS.
storage node An Object Storage node that provides container services, account services, and object services;
controls the account databases, container databases, and object storage.
storage services Collective name for the Object Storage object services, container services, and account services.
strategy Specifies the authentication source used by Image service or Identity. In the Database service, it
refers to the extensions implemented for a data store.
subdomain A domain within a parent domain. Subdomains cannot be registered. Subdomains enable you to
delegate domains. Subdomains can themselves have subdomains, so third-level, fourth-level, fifth-level,
and deeper levels of nesting are possible.
subnet Logical subdivision of an IP network.
SUSE Linux Enterprise Server (SLES) A Linux distribution that is compatible with OpenStack.
suspend Alternative term for a paused VM instance.
swap Disk-based virtual memory used by operating systems to provide more memory than is actually available
on the system.
swauth An authentication and authorization service for Object Storage, implemented through WSGI middleware;
uses Object Storage itself as the persistent backing store.
swift Codename for OpenStack Object Storage service.
swift All in One (SAIO) Creates a full Object Storage development environment within a single VM.
swift middleware Collective term for Object Storage components that provide additional functionality.
swift proxy server Acts as the gatekeeper to Object Storage and is responsible for authenticating the user.
swift storage node A node that runs Object Storage account, container, and object services.
sync point Point in time since the last container and accounts database sync among nodes within Object Storage.
sysadmin One of the default roles in the Compute RBAC system. Enables a user to add other users to a project,
interact with VM images that are associated with the project, and start and stop VM instances.
system usage A Compute component that, along with the notification system, collects meters and usage information.
This information can be used for billing.
T
tacker Code name for the NFV Orchestration service
Telemetry service (telemetry) The OpenStack project which collects measurements of the utilization of the
physical and virtual resources comprising deployed clouds, persists this data for subsequent retrieval and
analysis, and triggers actions when defined criteria are met.
TempAuth An authentication facility within Object Storage that enables Object Storage itself to perform
authentication and authorization. Frequently used in testing and development.
Tempest Automated software test suite designed to run against the trunk of the OpenStack core project.
334 Glossary
Networking Guide (Release Version: 15.0.0)
TempURL An Object Storage middleware component that enables creation of URLs for temporary object
access.
tenant A group of users; used to isolate access to Compute resources. An alternative term for a project.
Tenant API An API that is accessible to projects.
tenant endpoint An Identity service API endpoint that is associated with one or more projects.
tenant ID An alternative term for project ID.
token An alpha-numeric string of text used to access OpenStack APIs and resources.
token services An Identity service component that manages and validates tokens after a user or project has
been authenticated.
tombstone Used to mark Object Storage objects that have been deleted; ensures that the object is not updated
on another node after it has been deleted.
topic publisher A process that is created when a RPC call is executed; used to push the message to the topic
exchange.
Torpedo Community project used to run automated tests against the OpenStack API.
transaction ID Unique ID assigned to each Object Storage request; used for debugging and tracing.
transient Alternative term for non-durable.
transient exchange Alternative term for a non-durable exchange.
transient message A message that is stored in memory and is lost after the server is restarted.
transient queue Alternative term for a non-durable queue.
TripleO OpenStack-on-OpenStack program. The code name for the OpenStack Deployment program.
trove Codename for OpenStack Database service.
trusted platform module (TPM) Specialized microprocessor for incorporating cryptographic keys into devices
for authenticating and securing a hardware platform.
U
Ubuntu A Debian-based Linux distribution.
unscoped token Alternative term for an Identity service default token.
updater Collective term for a group of Object Storage components that processes queued and failed updates
for containers and objects.
user In OpenStack Identity, entities represent individual API consumers and are owned by a specific domain.
In OpenStack Compute, a user can be associated with roles, projects, or both.
user data A blob of data that the user can specify when they launch an instance. The instance can access this
data through the metadata service or config drive. Commonly used to pass a shell script that the instance
runs on boot.
User Mode Linux (UML) An OpenStack-supported hypervisor.
Glossary 335
Networking Guide (Release Version: 15.0.0)
V
VIF UUID Unique ID assigned to each Networking VIF.
Virtual Central Processing Unit (vCPU) Subdivides physical CPUs. Instances can then use those divisions.
Virtual Disk Image (VDI) One of the VM image disk formats supported by Image service.
Virtual Extensible LAN (VXLAN) A network virtualization technology that attempts to reduce the scalability
problems associated with large cloud computing deployments. It uses a VLAN-like encapsulation
technique to encapsulate Ethernet frames within UDP packets.
Virtual Hard Disk (VHD) One of the VM image disk formats supported by Image service.
virtual IP address (VIP) An Internet Protocol (IP) address configured on the load balancer for use by clients
connecting to a service that is load balanced. Incoming connections are distributed to back-end nodes
based on the configuration of the load balancer.
virtual machine (VM) An operating system instance that runs on top of a hypervisor. Multiple VMs can run
at the same time on the same physical host.
virtual network An L2 network segment within Networking.
Virtual Network Computing (VNC) Open source GUI and CLI tools used for remote console access to VMs.
Supported by Compute.
Virtual Network InterFace (VIF) An interface that is plugged into a port in a Networking network. Typically
a virtual network interface belonging to a VM.
virtual networking A generic term for virtualization of network functions such as switching, routing, load
balancing, and security using a combination of VMs and overlays on physical network infrastructure.
virtual port Attachment point where a virtual interface connects to a virtual network.
virtual private network (VPN) Provided by Compute in the form of cloudpipes, specialized instances that
are used to create VPNs on a per-project basis.
virtual server Alternative term for a VM or guest.
virtual switch (vSwitch) Software that runs on a host or node and provides the features and functions of a
hardware-based network switch.
virtual VLAN Alternative term for a virtual network.
VirtualBox An OpenStack-supported hypervisor.
Vitrage Code name for the Root Cause Analysis service.
VLAN manager A Compute component that provides dnsmasq and radvd and sets up forwarding to and from
cloudpipe instances.
VLAN network The Network Controller provides virtual networks to enable compute servers to interact with
each other and with the public network. All machines must have a public and private network interface.
A VLAN network is a private network interface, which is controlled by the vlan_interface option
with VLAN managers.
VM disk (VMDK) One of the VM image disk formats supported by Image service.
VM image Alternative term for an image.
VM Remote Control (VMRC) Method to access VM instance consoles using a web browser. Supported by
Compute.
336 Glossary
Networking Guide (Release Version: 15.0.0)
VMware API Supports interaction with VMware products in Compute.
VMware NSX Neutron plug-in Provides support for VMware NSX in Neutron.
VNC proxy A Compute component that provides users access to the consoles of their VM instances through
VNC or VMRC.
volume Disk-based data storage generally represented as an iSCSI target with a file system that supports
extended attributes; can be persistent or ephemeral.
Volume API Alternative name for the Block Storage API.
volume controller A Block Storage component that oversees and coordinates storage volume actions.
volume driver Alternative term for a volume plug-in.
volume ID Unique ID applied to each storage volume under the Block Storage control.
volume manager A Block Storage component that creates, attaches, and detaches persistent storage volumes.
volume node A Block Storage node that runs the cinder-volume daemon.
volume plug-in Provides support for new and specialized types of back-end storage for the Block Storage
volume manager.
volume worker A cinder component that interacts with back-end storage to manage the creation and deletion
of volumes and the creation of compute volumes, provided by the cinder-volume daemon.
vSphere An OpenStack-supported hypervisor.
W
Watcher Code name for the Infrastructure Optimization service.
weight Used by Object Storage devices to determine which storage devices are suitable for the job. Devices
are weighted by size.
weighted cost The sum of each cost used when deciding where to start a new VM instance in Compute.
weighting A Compute process that determines the suitability of the VM instances for a job for a particular
host. For example, not enough RAM on the host, too many CPUs on the host, and so on.
worker A daemon that listens to a queue and carries out tasks in response to messages. For example, the
cinder-volume worker manages volume creation and deletion on storage arrays.
Workflow service (mistral) The OpenStack service that provides a simple YAML-based language to write
workflows (tasks and transition rules) and a service that allows to upload them, modify, run them at scale
and in a highly available manner, manage and monitor workflow execution state and state of individual
tasks.
X
X.509 X.509 is the most widely used standard for defining digital certificates. It is a data structure that contains
the subject (entity) identifiable information such as its name along with its public key. The certificate can
contain a few other attributes as well depending upon the version. The most recent and standard version
of X.509 is v3.
Xen Xen is a hypervisor using a microkernel design, providing services that allow multiple computer operating
systems to execute on the same computer hardware concurrently.
Glossary 337
Networking Guide (Release Version: 15.0.0)
Xen API The Xen administrative API, which is supported by Compute.
Xen Cloud Platform (XCP) An OpenStack-supported hypervisor.
Xen Storage Manager Volume Driver A Block Storage volume plug-in that enables communication with
the Xen Storage Manager API.
XenServer An OpenStack-supported hypervisor.
XFS High-performance 64-bit file system created by Silicon Graphics. Excels in parallel I/O operations and
data consistency.
Z
zaqar Codename for the Message service.
ZeroMQ Message queue software supported by OpenStack. An alternative to RabbitMQ. Also spelled 0MQ.
Zuul Tool used in OpenStack development to ensure correctly ordered testing of changes in parallel.
338 Glossary
INDEX
Symbols
6to4, 304
A
absolute limit, 304
access control list (ACL), 304
access key, 304
account, 304
account auditor, 304
account database, 304
account reaper, 304
account server, 304
account service, 304
accounting, 304
Active Directory, 304
active/active configuration, 304
active/passive configuration, 304
address pool, 305
Address Resolution Protocol (ARP), 305
admin API, 305
admin server, 305
administrator, 305
Advanced Message Queuing Protocol (AMQP), 305
Advanced RISC Machine (ARM), 305
alert, 305
allocate, 305
Amazon Kernel Image (AKI), 305
Amazon Machine Image (AMI), 305
Amazon Ramdisk Image (ARI), 305
Anvil, 305
aodh, 305
Apache, 305
Apache License 2.0, 305
Apache Web Server, 305
API endpoint, 305
API extension, 305
API extension plug-in, 305
API key, 305
API server, 305
API token, 305
API version, 305
applet, 305
Application Catalog service (murano), 306
Application Programming Interface (API), 306
application server, 306
Application Service Provider (ASP), 306
arptables, 306
associate, 306
Asynchronous JavaScript and XML (AJAX), 306
ATA over Ethernet (AoE), 306
attach, 306
attachment (network), 306
auditing, 306
auditor, 306
Austin, 306
auth node, 306
authentication, 306
authentication token, 306
AuthN, 306
authorization, 306
authorization node, 306
AuthZ, 306
Auto ACK, 306
auto declare, 306
availability zone, 306
AWS CloudFormation template, 307
B
back end, 307
back-end catalog, 307
back-end store, 307
Backup, Restore, and Disaster Recovery service
(freezer), 307
bandwidth, 307
barbican, 307
bare, 307
Bare Metal service (ironic), 307
base image, 307
Bell-LaPadula model, 307
Benchmark service (rally), 307
Bexar, 307
binary, 307
339
Networking Guide (Release Version: 15.0.0)
bit, 307
bits per second (BPS), 307
block device, 307
block migration, 307
Block Storage API, 308
Block Storage service (cinder), 308
BMC (Baseboard Management Controller), 308
bootable disk image, 308
Bootstrap Protocol (BOOTP), 308
Border Gateway Protocol (BGP), 308
browser, 308
builder file, 308
bursting, 308
button class, 308
byte, 308
C
cache pruner, 308
Cactus, 308
CALL, 308
capability, 308
capacity cache, 308
capacity updater, 308
CAST, 308
catalog, 308
catalog service, 309
ceilometer, 309
cell, 309
cell forwarding, 309
cell manager, 309
CentOS, 309
Ceph, 309
CephFS, 309
certificate authority (CA), 309
Challenge-Handshake Authentication Protocol
(CHAP), 309
chance scheduler, 309
changes since, 309
Chef, 309
child cell, 309
cinder, 309
CirrOS, 309
Cisco neutron plug-in, 309
cloud architect, 309
Cloud Auditing Data Federation (CADF), 309
cloud computing, 309
cloud controller, 309
cloud controller node, 310
Cloud Data Management Interface (CDMI), 310
Cloud Infrastructure Management Interface (CIMI),
310
cloud-init, 310
cloudadmin, 310
Cloudbase-Init, 310
cloudpipe, 310
cloudpipe image, 310
Clustering service (senlin), 310
command filter, 310
Common Internet File System (CIFS), 310
Common Libraries (oslo), 310
community project, 310
compression, 310
Compute API (Nova API), 310
compute controller, 310
compute host, 310
compute node, 310
Compute service (nova), 310
compute worker, 310
concatenated object, 310
conductor, 311
congress, 311
consistency window, 311
console log, 311
container, 311
container auditor, 311
container database, 311
container format, 311
Container Infrastructure Management service (magnum),
311
container server, 311
container service, 311
content delivery network (CDN), 311
controller node, 311
core API, 311
core service, 311
cost, 311
credentials, 311
CRL, 311
Cross-Origin Resource Sharing (CORS), 311
Crowbar, 311
current workload, 311
customer, 312
customization module, 312
D
daemon, 312
Dashboard (horizon), 312
data encryption, 312
Data loss prevention (DLP) software, 312
Data Processing service (sahara), 312
data store, 312
database ID, 312
340 Index
Networking Guide (Release Version: 15.0.0)
database replicator, 312
Database service (trove), 312
deallocate, 312
Debian, 312
deduplication, 312
default panel, 312
default project, 312
default token, 312
delayed delete, 312
delivery mode, 312
denial of service (DoS), 312
deprecated auth, 312
designate, 313
Desktop-as-a-Service, 313
developer, 313
device ID, 313
device weight, 313
DevStack, 313
DHCP agent, 313
Diablo, 313
direct consumer, 313
direct exchange, 313
direct publisher, 313
disassociate, 313
Discretionary Access Control (DAC), 313
disk encryption, 313
disk format, 313
dispersion, 313
distributed virtual router (DVR), 313
Django, 313
DNS record, 313
DNS service (designate), 313
dnsmasq, 313
domain, 313
Domain Name System (DNS), 314
download, 314
durable exchange, 314
durable queue, 314
Dynamic Host Configuration Protocol (DHCP), 314
Dynamic HyperText Markup Language (DHTML),
314
E
east-west traffic, 314
EBS boot volume, 314
ebtables, 314
EC2, 314
EC2 access key, 314
EC2 API, 314
EC2 Compatibility API, 314
EC2 secret key, 314
Elastic Block Storage (EBS), 314
encapsulation, 314
encryption, 314
endpoint, 314
endpoint registry, 315
endpoint template, 315
entity, 315
ephemeral image, 315
ephemeral volume, 315
Essex, 315
ESXi, 315
ETag, 315
euca2ools, 315
Eucalyptus Kernel Image (EKI), 315
Eucalyptus Machine Image (EMI), 315
Eucalyptus Ramdisk Image (ERI), 315
evacuate, 315
exchange, 315
exchange type, 315
exclusive queue, 315
extended attributes (xattr), 315
extension, 315
external network, 315
extra specs, 315
F
FakeLDAP, 315
fan-out exchange, 315
federated identity, 316
Fedora, 316
Fibre Channel, 316
Fibre Channel over Ethernet (FCoE), 316
fill-first scheduler, 316
filter, 316
firewall, 316
FireWall-as-a-Service (FWaaS), 316
fixed IP address, 316
Flat Manager, 316
flat mode injection, 316
flat network, 316
FlatDHCP Manager, 316
flavor, 316
flavor ID, 316
floating IP address, 316
Folsom, 316
FormPost, 316
freezer, 316
front end, 316
G
gateway, 316
generic receive offload (GRO), 317
Index 341
Networking Guide (Release Version: 15.0.0)
generic routing encapsulation (GRE), 317
glance, 317
glance API server, 317
glance registry, 317
global endpoint template, 317
GlusterFS, 317
gnocchi, 317
golden image, 317
Governance service (congress), 317
Graphic Interchange Format (GIF), 317
Graphics Processing Unit (GPU), 317
Green Threads, 317
Grizzly, 317
Group, 317
guest OS, 317
H
Hadoop, 317
Hadoop Distributed File System (HDFS), 317
handover, 317
HAProxy, 317
hard reboot, 317
Havana, 317
health monitor, 318
heat, 318
Heat Orchestration Template (HOT), 318
high availability (HA), 318
horizon, 318
horizon plug-in, 318
host, 318
host aggregate, 318
Host Bus Adapter (HBA), 318
hybrid cloud, 318
Hyper-V, 318
hyperlink, 318
Hypertext Transfer Protocol (HTTP), 318
Hypertext Transfer Protocol Secure (HTTPS), 318
hypervisor, 318
hypervisor pool, 318
I
Icehouse, 318
ID number, 318
Identity API, 318
Identity back end, 319
identity provider, 319
Identity service (keystone), 319
Identity service API, 319
IETF, 319
image, 319
Image API, 319
image cache, 319
image ID, 319
image membership, 319
image owner, 319
image registry, 319
Image service (glance), 319
image status, 319
image store, 319
image UUID, 319
incubated project, 319
Infrastructure Optimization service (watcher), 319
Infrastructure-as-a-Service (IaaS), 319
ingress filtering, 319
INI format, 320
injection, 320
Input/Output Operations Per Second (IOPS), 320
instance, 320
instance ID, 320
instance state, 320
instance tunnels network, 320
instance type, 320
instance type ID, 320
instance UUID, 320
Intelligent Platform Management Interface (IPMI),
320
interface, 320
interface ID, 320
Internet Control Message Protocol (ICMP), 320
Internet protocol (IP), 320
Internet Service Provider (ISP), 320
Internet Small Computer System Interface (iSCSI),
320
IP address, 320
IP Address Management (IPAM), 320
ip6tables, 320
ipset, 320
iptables, 320
ironic, 321
iSCSI Qualified Name (IQN), 321
ISO9660, 321
itsec, 321
J
Java, 321
JavaScript, 321
JavaScript Object Notation (JSON), 321
jumbo frame, 321
Juno, 321
K
Kerberos, 321
kernel-based VM (KVM), 321
Key Manager service (barbican), 321
342 Index
Networking Guide (Release Version: 15.0.0)
keystone, 321
Kickstart, 321
Kilo, 321
L
large object, 321
Launchpad, 322
Layer-2 (L2) agent, 322
Layer-2 network, 322
Layer-3 (L3) agent, 322
Layer-3 network, 322
Liberty, 322
libvirt, 322
Lightweight Directory Access Protocol (LDAP), 322
Linux, 322
Linux bridge, 322
Linux Bridge neutron plug-in, 322
Linux containers (LXC), 322
live migration, 322
load balancer, 322
load balancing, 322
Load-Balancer-as-a-Service (LBaaS), 322
Load-balancing service (octavia), 322
Logical Volume Manager (LVM), 322
M
magnum, 322
management API, 322
management network, 322
manager, 322
manifest, 322
manifest object, 323
manila, 323
manila-share, 323
maximum transmission unit (MTU), 323
mechanism driver, 323
melange, 323
membership, 323
membership list, 323
memcached, 323
memory overcommit, 323
message broker, 323
message bus, 323
message queue, 323
Message service (zaqar), 323
Meta-Data Server (MDS), 323
Metadata agent, 323
migration, 323
mistral, 323
Mitaka, 323
Modular Layer 2 (ML2) neutron plug-in, 323
monasca, 323
Monitor (LBaaS), 323
Monitor (Mon), 323
Monitoring (monasca), 323
multi-factor authentication, 324
multi-host, 324
multinic, 324
murano, 324
N
Nebula, 324
netadmin, 324
NetApp volume driver, 324
network, 324
Network Address Translation (NAT), 324
network controller, 324
Network File System (NFS), 324
network ID, 324
network manager, 324
network namespace, 324
network node, 324
network segment, 324
Network Service Header (NSH), 324
Network Time Protocol (NTP), 324
network UUID, 324
network worker, 324
Networking API (Neutron API), 325
Networking service (neutron), 325
neutron, 325
neutron API, 325
neutron manager, 325
neutron plug-in, 325
Newton, 325
Nexenta volume driver, 325
NFV Orchestration Service (tacker), 325
Nginx, 325
No ACK, 325
node, 325
non-durable exchange, 325
non-durable queue, 325
non-persistent volume, 325
north-south traffic, 325
nova, 325
Nova API, 325
nova-network, 325
O
object, 325
object auditor, 325
object expiration, 325
object hash, 326
object path hash, 326
object replicator, 326
Index 343
Networking Guide (Release Version: 15.0.0)
object server, 326
Object Storage API, 326
Object Storage Device (OSD), 326
Object Storage service (swift), 326
object versioning, 326
Ocata, 326
Octavia, 326
Oldie, 326
Open Cloud Computing Interface (OCCI), 326
Open Virtualization Format (OVF), 326
Open vSwitch, 326
Open vSwitch (OVS) agent, 326
Open vSwitch neutron plug-in, 326
OpenLDAP, 326
OpenStack, 326
OpenStack code name, 326
openSUSE, 326
operator, 326
optional service, 326
Orchestration service (heat), 327
orphan, 327
Oslo, 327
P
panko, 327
parent cell, 327
partition, 327
partition index, 327
partition shift value, 327
path MTU discovery (PMTUD), 327
pause, 327
PCI passthrough, 327
persistent message, 327
persistent volume, 327
personality file, 327
Pike, 327
Platform-as-a-Service (PaaS), 327
plug-in, 327
policy service, 327
policy-based routing (PBR), 327
pool, 327
pool member, 328
port, 328
port UUID, 328
preseed, 328
private image, 328
private IP address, 328
private network, 328
project, 328
project ID, 328
project VPN, 328
promiscuous mode, 328
protected property, 328
provider, 328
proxy node, 328
proxy server, 328
public API, 328
public image, 328
public IP address, 328
public key authentication, 328
public network, 328
Puppet, 328
Python, 328
Q
QEMU Copy On Write 2 (QCOW2), 328
Qpid, 328
Quality of Service (QoS), 329
quarantine, 329
Queens, 329
Quick EMUlator (QEMU), 329
quota, 329
R
RabbitMQ, 329
Rackspace Cloud Files, 329
RADOS Block Device (RBD), 329
radvd, 329
rally, 329
RAM filter, 329
RAM overcommit, 329
rate limit, 329
raw, 329
rebalance, 329
reboot, 329
rebuild, 329
Recon, 329
record, 329
record ID, 330
Red Hat Enterprise Linux (RHEL), 330
reference architecture, 330
region, 330
registry, 330
registry server, 330
Reliable, Autonomic Distributed Object Store, 330
Remote Procedure Call (RPC), 330
replica, 330
replica count, 330
replication, 330
replicator, 330
request ID, 330
rescue image, 330
resize, 330
344 Index
Networking Guide (Release Version: 15.0.0)
RESTful, 330
ring, 330
ring builder, 330
role, 330
Role Based Access Control (RBAC), 330
role ID, 330
Root Cause Analysis (RCA) service (Vitrage), 331
rootwrap, 331
round-robin scheduler, 331
router, 331
routing key, 331
RPC driver, 331
rsync, 331
RXTX cap, 331
RXTX quota, 331
S
sahara, 331
SAML assertion, 331
scheduler manager, 331
scoped token, 331
scrubber, 331
secret key, 331
secure boot, 331
secure shell (SSH), 331
security group, 331
segmented object, 331
self-service, 331
SELinux, 331
senlin, 331
server, 332
server image, 332
server UUID, 332
service, 332
service catalog, 332
Service Function Chain (SFC), 332
service ID, 332
Service Level Agreement (SLA), 332
service project, 332
service provider, 332
service registration, 332
service token, 332
session back end, 332
session persistence, 332
session storage, 332
share, 332
share network, 332
Shared File Systems API, 332
Shared File Systems service (manila), 332
shared IP address, 332
shared IP group, 333
shared storage, 333
Sheepdog, 333
Simple Cloud Identity Management (SCIM), 333
Simple Protocol for Independent Computing Environments
(SPICE), 333
Single-root I/O Virtualization (SR-IOV), 333
SmokeStack, 333
snapshot, 333
soft reboot, 333
Software Development Lifecycle Automation service
(solum), 333
Software-defined networking (SDN), 333
SolidFire Volume Driver, 333
solum, 333
spread-first scheduler, 333
SQLAlchemy, 333
SQLite, 333
stack, 333
StackTach, 333
static IP address, 333
StaticWeb, 333
storage back end, 333
storage manager, 334
storage manager back end, 334
storage node, 334
storage services, 334
strategy, 334
subdomain, 334
subnet, 334
SUSE Linux Enterprise Server (SLES), 334
suspend, 334
swap, 334
swauth, 334
swift, 334
swift All in One (SAIO), 334
swift middleware, 334
swift proxy server, 334
swift storage node, 334
sync point, 334
sysadmin, 334
system usage, 334
T
tacker, 334
Telemetry service (telemetry), 334
TempAuth, 334
Tempest, 334
TempURL, 335
tenant, 335
Tenant API, 335
tenant endpoint, 335
Index 345
Networking Guide (Release Version: 15.0.0)
tenant ID, 335
token, 335
token services, 335
tombstone, 335
topic publisher, 335
Torpedo, 335
transaction ID, 335
transient, 335
transient exchange, 335
transient message, 335
transient queue, 335
TripleO, 335
trove, 335
trusted platform module (TPM), 335
U
Ubuntu, 335
unscoped token, 335
updater, 335
user, 335
user data, 335
User Mode Linux (UML), 335
V
VIF UUID, 336
Virtual Central Processing Unit (vCPU), 336
Virtual Disk Image (VDI), 336
Virtual Extensible LAN (VXLAN), 336
Virtual Hard Disk (VHD), 336
virtual IP address (VIP), 336
virtual machine (VM), 336
virtual network, 336
Virtual Network Computing (VNC), 336
Virtual Network InterFace (VIF), 336
virtual networking, 336
virtual port, 336
virtual private network (VPN), 336
virtual server, 336
virtual switch (vSwitch), 336
virtual VLAN, 336
VirtualBox, 336
Vitrage, 336
VLAN manager, 336
VLAN network, 336
VM disk (VMDK), 336
VM image, 336
VM Remote Control (VMRC), 336
VMware API, 337
VMware NSX Neutron plug-in, 337
VNC proxy, 337
volume, 337
Volume API, 337
volume controller, 337
volume driver, 337
volume ID, 337
volume manager, 337
volume node, 337
volume plug-in, 337
volume worker, 337
vSphere, 337
W
Watcher, 337
weight, 337
weighted cost, 337
weighting, 337
worker, 337
Workflow service (mistral), 337
X
X.509, 337
Xen, 337
Xen API, 338
Xen Cloud Platform (XCP), 338
Xen Storage Manager Volume Driver, 338
XenServer, 338
XFS, 338
Z
zaqar, 338
ZeroMQ, 338
Zuul, 338
346 Indexgood tutorials resources openstack closed 
issues working python generators openstack swift client 
relationship endpoints regions etc keystone openstack 
vagrant vms use manifests storeconfigs without puppet master 
distribution cirros based 
private cloud gpu virtualization similar amazon web services cluster gpu instances 
docker-machine behind corporate proxy 
re-run cloud-init without reboot 
jenkins slave went offline build 
authentication via config file 
ipv6 interface ip operations stopped floating ip ha failover 
openstack devstack installation stops connecting dashboard url 
minimal devstack nova floating ips 
installing openstack errors 
cloud platforms- sudo unable resolve host closed 
openstack devstack 
install swift openstack storage infrastructure openstack without installing nova 
advantages vm 's lightweight containers docker closed 
openstack hello world 
get openstack token validate 
get rackspace servers certain tag 
detect puppet run complete 
azure inter-operable amazon 
libvirterror internal error find suitable cpu model given data 
restarting service openstack installed using devstack 
api-paste.ini file openstack 
openstack python api download image glance using python api 
openstack “no valid host found” image cirros 
installing openstack mac 
images cloud-ready openstack 
error installing pbr 
launch openstack instances using python-boto 
create volume 2 gbs openstack closed 
openstack devstack fail installs keystoneauth1 2.12.2 demands 2.16.0 better 
tcp receives packets ignores 
storage objects created file object storage cloud 
instance creation devstack icehouse 
trouble installing openstack devstack script 
getting auth token keystone horizon 
get ip address launched instance boto 
error rabbit-mq server 
swift user 's authorization 
install packages cirros os 
devstack installation error directory 'opt/stack/nova ' installable 
php opencloud\rackspace problems 
retrieve cpu usage node kubernetes via api 
resolve error installing devstack kilo ubuntu 
fluent custom plugin registration throws “unknown output plugin” 
notifications openstack 
deploy cloudfoundry openstack 
openstack oslo_config nosuchopterror deprecated name - probably need define deprecated_opts 
kubernetes using openstack cinder one cloud provider nodes another 
openstack dashboard default password closed 
programmatically setting instance name openstack nova api 
openstack network working iptables turned back 
using lucene solr elasticsearch index amazon s3 rackspace cloud files openstack swift 
amazon s3/openstack swift api skeleton 
assign openstack floating ip making sure removed server 
login credentials ubuntu cloud server image 
openstack compute nova “error” 
difference virtualization private cloud computing closed 
one node openstack installation configuration closed 
openstack-swift clients closed 
glance store images openstack 
error sudoers list 
python script programming language scripts used restrict resources like ram cpu usage used applications 
add table nova database openstack 
cloud agnostic tool iaas based cloud 
urllib3 - failed establish new connection errno 111 
open source rails gem openstack api 
difference essex folsom release openstack 
rex identity files 
spark cluster master ip address binding floating ip 
pycadf error installing openstack liberty via devstack -ubuntu 14.04 lts 
debug openstack dashboard 
run cloud-init environment without using openstack ec2 
value/benefit openstack “network/storage” nodes 
enable kvm ubuntu 12.04 virtualbox imac'11 
append overwrite bytes existing object openstack swift 
openstack swift server waited long request sent client 
openstack/nova route host 'm trying ssh instance 
mac os x openstack 
configure vswitch openstack 
possible ge list tenants user associated openstack using keystoneclient 
integrating swift keystone 
convert ceilometer output python dataframe 
openstack rest api send file python openstack container 
authentication saio swift one using libcurl api c++ 
openstack neutron ca n't ping external network 
remote debugging openstack 
possible create ami openstack image 
handlers could found logger “oslo_config.cfg” 
nova boot baremetal select specific machine pool 'boot ' 
get cpu memory hdd info host openstack python api 
get usage quotas java implementation openstack 
get fixed ip address openstack instance using novaclient 
error message nova schedule 
advantages using openstack heat chef 
possible update several objects openstack swift atomically 
use openstack heat install setup mongodb cluster 
set image metadata python api 0.11.0 openstack grizzly 
openstack vm able access internet closed 
openstack nova-network start 
generate openstack compatible images 
completely remove openstack system installation devstack script 
openstack dashboard gives error “error unable retrieve usage information” 
python pep 440 - closed 
openstack-devstack ca n't create instance enough hosts available closed 
openstack - change admin password dashboard 
installing python packages without internet using source code .tar.gz .whl 
unable ping instance launched openstack 
minio cluster architecture minio.io object storage server 
getting user list openstack keystone api 
vagrant port forwarding work 
openstack “_member_” role “member” role 
manage users/passwords devstack 
upgrade ring file swift 
openstack .net api way combine object creation container creation one call 
get id name networks fields openstack nova list 
openstack dashboard error unable retrieve usage information instances 
rackspace cloudfile api - get object information 
nil turning null request fog 
create instance volume openstach python-novaclient 
openstack/devstack virtual machine installation error 
access hp helion using apache jclouds 
openstack.net sdk access services 
openstack images stored 
installing apache hadoop openstack 
uploading cloudfile rackspace cloud using openstack.net get net.openstack.core.exceptions.response.badservicerequestexception 
get cpu information openstack flavor using libcloud 
openstack - check availability container c .net 
changes local folder reflecting git openstack 
openstack horizon dashboard folsom release js builder 
possible create openstack image importing *.ovf file glance 
variable set settings.py redirect nova services url 
ca n't access openstack 's horizon dashboard 8000 outside 
migrating vm 's kvm openstack 
instead glances installed openstack-glance mistake 
terraform stalls trying get ip addresses multiple instances 
web server come play openstack - cloudfoundry stack 
hadoop java.net.unknownhostexception hadoop-slave-2 
docker container connecting https endpoints 
liberty openstack ubuntu install guide fails ping router 
give openstack server permissions call openstack apis 
configure cloudkitty devstack 
accessing openstack api bluemix 
openstack installation failed set rdo repo host 
check whether dhcp-server exists subnet creating subnet 
bad request error openstack 
wait condition heat/cloudformation instance startup order 
create temporary url swift object storage using rest api 
pass ip-address cloud-init metadata 
impliment auth_token ceilometer python client api 
jclouds - neutron specify nic associating floating ip 
reset password openstack instance using kvm libvirt 
openstack vm accessible lan 
openstack swift - alias symlink stored object 
get jpg file cloudfiles rackspace using openstack.net 
openstack-keystone failing create tenant 
docker driver openstack coexist libvirt.libvirtdriver 
openstack x-auth-token token 
rpc calls openstack 
change image memory openstack 
openstack dashboard login error devstack method 
openstack decide hardware capacity 
storage components openstack 
swift-open stack storage system manage file metadata 
“no module named memcache” error openstack newton keystone accessing murano dashboard 
heat autoscaling - scaleup - change flavor 
use ansible without ssh keys openstack instances 
openstack raspberry pi 2 closed 
openstack keystone nova-network inactive reboot 
recover deleted repository deleted branch 
jclouds openstack illegalstateexception expected begin_array string 
assign floating ip openstack instance using heat template 
openstack python-novaclient 
wilma pep proxy keystone - valid access token found 
installation openstack virtual machines multi-node architecture 
fiware keystone api create user 
keystone connection fail 
specify post-installation script creating instance openstack python-novaclient 
kubernetes unable connect pod master 
interaction components openstack 
configure object-store opscenter 
print python openstack object json 
recovering terminated instances openstack 
running devstack script ./stack.sh 
xargs command multiple parameters 
ubuntu 14.04 openstack installation failure 
novaclient - create cloud server using “boot” - error badrequest multiple possible networks found 
openstack glance working 
error occurred installing openstack setting force=yes 
set internal ip range devstack 
override handle method selfhandlingform openstack horizon 
openstack single node vs multi node 
jclouds openstack creating neutronapi object throws com.google.inject.configurationexception 
getting list block storage volumes rackspace using novaclient python api 
perform bulk-delete openstack swift 
ca n't find rackspace cloud server via openstacknetsdk 
openstack error puppet installing redhat 6.4 
set docker container 's ip 
create hybrid cloud aws ec2 local openstack instance 
couchbase xdcr openstack 
listening notification creation instance openstack 
installing cloudfoundry gives authentication error openstack debug opensource project namely bosh openstack 
use nova curl authentication required 
delete dynamic large objects openstack swift api 
openstack rackspace cloud files .net sdk 
localrc settings openstack 
openstack create volume via nova api 
amazon s3 bucket policy equivalent openstack swift world 
openstack 's virtual nodes permanently paused state 
spread number instances across availability zones heat 
passing segment already open filestream stream parameter 
configure openstack swift without openstack projects 
horizontal autoscaling nodes minions kubernetes openstack 
openstack novaclient python api working 
cloudidentity openstacksdknet could find user 
openstack neutron update port vif-model using update_port api 
unable connect jenkins plugin openstack 
openstack small image tiny flavor 
fiware object storage authorization error 
delete resource type openstack 
assigning floating ip load balancer heat template 
accesing instance ip lan openstack 
devstack installation swift enabled 
run openstack chef cookbook 
keystone ssl config intermediate ca cert 
filtering files container rackspace cdn 
openstack vm instance shutoff minute 
openstack api createinstance 
nova-api run different compute nodes 
remove duplicate service help nova-manage command 
openstack networking ca n't ping/ssh from/to vms 
update project running cluster 
able launch openstack instance terraform 
get uploaded object url using openstack4j 
commanderror installing openstack 
unable create alarm aodh alrming service using openstack4j 
got oslo-vmware install error 
openstack - iterate comma_delimited_list using index os :heat :resourcegroup 
delete objects folder/directory open stack swift using rest api 
create new role openstack4j 
integrate orion dbaas like trove openstack 
access service subdomain kubernetes 
http error 401 configuring keystone 
openstack get client 's ip python api 
rails using fog paperclip bluemix object storage v2 
network nic ordering heat template 
openstack4j list roles identity keystone v3 
authenticating restricted user cloud files rackspace using openstack.net 
enabled services devstack 
jclouds openstack > java.util.nosuchelementexception apitype compute found catalog 
unauthorized error using openstack api ca n't get authentication token keystone 
rdo openstack allinone add one mor compute node 
finding pid 's virtual machine openstack 
openstack create super user like admin 
get ip address computes nodes openstack network 
containers objects getting deleted file system swift 
ovf existing virtual machine openstack 
openstack keystoneclient get user name 
cloud infrastructure modeling capability 
list objects blocks network 
create volume/instance image error 
swift accessible openstack installed 
openstack “no hosts found map cell exiting” 
suds.client import client “import client” 
temporary public url ibm bluemix object storage service nodejs 
devstack installation failing tempest test-requirements 
difference cold hot reboot openstack 
python3 buildout error boss billing openstack software 
split huge code submit several patchsets gerrit 
fetching floating ip openstack vm 
swift list -p variable setting 
containerization openstack services kubernetes 
api get keystone notification events 
openstack port quota effecting 
delete branch using git review 
error deploying object storage bluemix using openstack4j 
error installation openstack kilo centos7 
jclouds + openstack nosuchelementexception key openstack-neutron list providers apis 
floating ip pool found 
installing solum openstack/devstack 
“getting started” issue joss javaswift 
difference ubuntu openstack openstack kilo build private cloud 
'cf app ' shows always 0.0 cpu - value measured 
openstack-ironic address 'ssh_address ' driver option referring ironic node pxe_ssh driver 
assign 2 fixed ips subnet server via heat 
openstack release name 
openstack login admin retrieve server data different tenants via python 
openstack ceilometer perform realtime instance monitoring 
authorized list projects keystone v3 
hyper-v openstack 
ansible module manage openstack load-balancers openstack lbaas 
cloudify “could determine type file ”sftp //root *** 192.168.10.xxx/root/gs-files“.” 
devstack services 
install openstack use openshift 
sahara service failed start devstack installation 
building cloud scratch using openstack 
test provider credentials valid apache libcloud 
openstack horizon rbac setting custom permissions 
devstack juno nova cells enabled failing run stack.sh database error unknown database 'nova_cell ' 
ssh key injection openstack 
devstack installation failing python update.py /opt/stack/keystone 
multiple kvm guests scipt using virt-install 
installation openstack ubuntu 12.04 
openstack django panel 
remove port external bridge openvswitch 
output script completely finishes vagrant provider=rackspace 
failure trying start devstack 
launch azure instance openstack 
create custom role openstack provide read-only access single tenant 
access hadoop remotely 
fabric fails name lookup failed celery run eventlet 
drop incoming packet openvswitch integration bridge specific ip 
jclouds create tenant openstack 
admin password create server api response working 
backing openstack swift data 
error using phpopencloud - openstack 
extract data < bound method server.interface_list < server ubuntu12_1 > > type object 
upload photos files openstack object storage swift 
wsgi post request 403 forbidden 
reload nova.conf without restarting openstack 
virtual interface virtual machine openstack 
openstack instance status “error” 
avoiding sensitive info openstack config files 
cloudfoundry grizzly keystone devstack install configparser error keystone.conf 
track functions used python command 
openstack hosted environment 
using aws-cloudformatting-resources heat & openstack local cloud 
get instances back reboot openstack 
open stack assign ip virtual machines 
neutron flat networking dhcp 
openstack delete instance dashboard 
differences vm image docker image duplicate 
unbound variable url printing bash/shell 
null value json interpreted python openstack api 
private cloud stack without hardware virtualization 
clone branch git 
x-storage-url x-auth-token 
bluemix - object storage - node.js - pkgcloud - openstack returns 401 
cloudfoundry compatible docker/coreos 
setup openstack havana docker driver 
create security group openstack cloud foundry 
openstack modify projects quotas 
force expiring openstack swift tokens 
rackspace cloud files openstack swift tempurl example vb.net 
setting pydev use openstack 
communicate openstack api using servlet 
automation networks using ansible openstack 
./stack.sh failed installing openstack 
best shared storage specifications openstack 
“zypper install python-mysqldb mysql-server” packages found 
backing openstack swift object store 
cinder creating volume 
new fog models missing gem 
cloudify 2.7 openstack icehouse 
openstack instance accessible lan 
post request /v1/ < account-name > openstack swift create account 
openstack-nova-volume dead make alive 
assign openstack network instance create fog 
install apache php mysql different servers 
guide writing new openstack module/project 
guidance using openstack launch instance via php automatically build instance depending request 
list available containers swift using jcloud 
unable run unit tests openstack nova 
openstack neutron command workflow 
devstack installation - floating ips 
partition drive automatically/set os-dcf diskconfig auto nova 
org.jclouds.openstack.nova.v2_0.domain.servercreated api 
openstack quantum vm able ping br-ex outside network 
run openstack-swift java api 
nova image-list error unauthorized http 401 
pass ssh key using jclouds openstack 
changing font openstack horizon 
python spider openstack login 
installing docker devstack 
connecting android openstack services 
openstack swift token expiration swauth large file upload 
vt set bios showing vmx cpu flag os 
start openstack machine guest without authentication 
http headers incorrectly treated case sensitive jclouds causing openstack x-storage-url fail 
django file download issue 
openstack secures token generated keystone 
trouble openstack ceilometer client 
failed update root certificat cacert.pem 
use keystone api extenstion send json format request “add global role user ” 
measuring cluster health openstack swift 
process close instance openstack 
nova compute network unable contact nova service restart manage services 
cloud object storage 
wso2 elb use cloud service cluster iaas internally 
use carrierwave openstack swift installation 
openstack exception adding new nova-compute node 
openstack first program 
openstack python novaclient require authentication 
modify live migration algorithm used openstack newton 
get users particular project openstack 
error “/opt/stack/logs/error.log file directory” - devstack deploy ubuntu 12.04 lts 
devstack - allow ssh guest vm devstack 
deleting openstack instance powershell 
mirantis fuel 9.2 “failed tasks task openstack-cinder/7 task openstack-cinder/6 task openstack-cinder/8 ” 
read current levels neutron quota usage openstack project 
kolla aio deploy fail hostname resolve ip address starting rabbitmq container 
scale up/down slurm cluster 
error configuring devstack compute nodes service n-net running 
detach volume server openstack python sdk 
kubernetes cinder volume plugin works 
solution multiple file upload using multer pkgcloud openstack 
kubernetes minion registering api server openstack hostname rejected 
nova instance reach metadata server 
openstack project list work - keystone authentication seems fail 
mysql error openstack database keystone 
get error “could find domain default” import core murano library manual installation 
openstack swift proxy-server malformed request s3 
enable permanently cors ovh object storage openstack swift 
query openstack api sr-iov 
map user domain federated openstack federation 
ruby openstack swift client works v3 identify version 
cinder volume create valid host weighed hosts available 
joss java client openstack swift determine proxy servers use 
openstack could delete security group says use 
possible stream file java server openstack swift 
multiple neutron nodes one node attached external network 
error “requires ruby release ” occured using packstack install openstack one 
pycharm remote debugging unable retrieve variables 
inject file instace linux running state 
openstack - keypair extension sometimes unavailable 
using https calling waitfor method setting openstack nova server 
openstack sdk create server one ip address 
openstack heatclient api query heat output-show 
openstack command-line clients macos - keystone command found 
api python neutron client common exceptions unauthorized 
basic salt-cloud config file openstack/nova mitaka 
openstack-packstack - mariadb error 
django openstack horizon - admin templates rendering properly 
creating clone vm openstack 
read objects openstack swift based object metadata 
swift could start 
openstack service find reply exchange rabbitmq 
curl received http code 404 proxy connect 
download object performance jclouds swift client 
make restful java client send get/post request openstack 
get stack status rest api without mentioning tenant/project id devstack 
openstack apis - error unable find matching endpoint requested service 
unable start corosync cluster engine 
paging folders files swift object storage 
openstack keystone project creation using hot fails 
perspective shell match instances full hostname 
copy binary file instance 
openstack heat dotnet sdk 
openstack sahara installation devstack juno 
openstack working instance connecting nova.clouds.ubuntu initiate connection 
openstack identity admin client find users roles services 
get instance usage data billing ceilometer client 
openstack devstack installation error openstackclient.shell exception 
upload openstack ubuntu cloud images aws ec2 
openstack instance network n't working 
get repomd.xml local yum install openstack-packstack 
running commands.getstatusoutput returning expected results apache 
able ping/ssh instances 
adding sign-up option openstack grizzly 
os-svc-daemon adding new service upstart 
openstack nova docker implement security group 
devstack metadata service - preset configuration 
want copy file host guest spinning vm .. pointers 
egress option rackspace openstack 
openstack create image 'connection aborted ' error 111 'connection refused ' 
could fetch remote template - tempest 
access instances openstack aws 
openstack specify behavior delete objectstore get download object progress 
generate config file oslo.config 
openstack auto-pilot - hardware/zone list available 
openstack4j error 400 authenticate got unexpected keyword argument 'username ' 
couchbase .net sdk connection issue “unable locate node” openstack 
openstack nova oslo error 
mysql error installing wso2 paas 
improve download/delete speed large chunked files openstack swift 
tempest installation openstack 
devstack unable ping vm 
re-use previously assigned floating ip openstack neutron using horizon 
large image upload horizon saving state forever 
errors use ./stack.sh devstack 
optimistic concurrency rackspace openstack cloud file 
turn openstack icehouse source code github .deb file installing 
installing novadocker openstack 
connecting vagrant consul server openstack 
keystone v2 - get token without using userid/password 
custom ceilometer metrics 
cpu stats captured docker containers 
launch openstack instance single nic 
cloud-installer ubuntu openstack 
openstack4j - maven unresolved dependencies 
cinder volume cant attach detach 
celery oslo config working together 
cloud servers identification 
framework cloud storage db simulation local setup 
openstack glace unknown command 
keystone returning is_admin 0 every case 
refreshing tab content openstack django webapp 
openstack swift module redirect client region location 
expecting authentication method via either service token 
operating sytem install openstack 
error occurred installing openstack ./stack.sh:137 die 
get devstack juno 
get `id` field output openstack creation commands shell script 
nonetype ' object attribute '__getitem__ ' 
docker driver openstack 
possible run openstack laptop/desktop 
openstack api implementations 
issue json.dumps adding extra \\ variables 
openstack devstack installation stalls git call failure 
error openstack dashboard authentication 
openstack-keystone failing start 
best tool deploying production-ready openstack cluster 
add security group vm via nova-client 
append data object openstack swift object storage 
objects openstack swift immediately available 
openstack - nova client - retrieve servers particular tenant 
openstack installation failed devstack stack.sh 
openstack heat & ansible vm spinup app deployment 
import pre-built vm images bluemix 
package openstack horizon dashboard plugin correctly 
python library used write rest service python closed 
couchdb cartridge responding docker image 
devstack/openstack create tenant 
python class method 
trace particular terminal command 
openstack installation 
change openstack horizon 's login page 
why/how certification fail auth_url/auth/tokens “no file” error 
“unauthorized request made requires authentication http 401 ” fiware horizon 
openstack project create exits error 
restart ceilometer service 
openstack docker 
packer ssh_private_key_file invalid 
install alluxio1.2 openstack 
terraform run provisioner existing resources 
sanitized input subprocess shell=true python 
create publicly readable object store containers ruby openstack gem 
openstack project quota modification using rest api 
fetching viewing rabbitmq results 
mq used openstack deployment java ee applications 
openstack novaclient error launching instance using servers.create 
using chef openstack would encourage use heat templates 
openstack swift add cache-control expires headers 
change default http port openstack dashboard 
openstack api token lifespan extension 
scale up/out mechanism works openstack cloudfoundry integration 
wso2 / mule vs openstack / cloudstack - differences similarities benefits 
python-swiftclient general usage procedure 
configure openstack packstack juno work external network centos 7 
possible use cloud-init heat-cfntools inside docker container 
failed add image got error openstack glance 
cloudfoundry installation 
importerror module named persistence.backends.sql 
mobicents jdiameter wrong example git 
openstack api services communicate services 
openstack cluster must machines processor architecture 
login username password vnc access 
authentication token issued identity service expired keystone openstack icehouse 
generating temporary uri object using blobstore 
nova error errno 111 connection refused 
emr ec2 openstack please clarify 
cant ssh instance created openstack cloud 
openstack harproxy configure file 
openstack permission denied generate picture python 
change ip address devstack installation 
swift response scenario file overwriting deletion happened simultaneously overwriting first deletion 
cloudfilesprovider updateobjectmetadata issue 
create object content-type “application/directory” inside rackspace cloudfiles container openstack.net 
installing openstack puppet 
error devstack script nova-api start 
devstack - changing ip address installation 
configuring openstack in-house test cloud 
retrieve list servers rackspace via openstack nova client 
unable locate package libvirt-clients error ubuntu 
disconnect openstack 
openstack nova api live update 
new openstack cli command performing router cleanup 
scope name use jclouds scriptbuilder 
packer deploying openstack images 
make nova service state test purpose without shutting node 
openstackcli access ` -- property` 's cloudinit script 
couchdb 2 run openstack 
disable authentication openstack swift 
openstack launching instance error 
“heat.stack.list ” list stack within openstack “admin” cred 
remove host ip address 
devstack failed create new centos instance 
copy swift object storage container new openstack service 
openstack add security rules failed due multiple security groups named “default” 
openstack training-labs - vboxmanage.exe error invalid machine group 
openstack cloud-init set hostname vm-name 
openstack sdk - create image kernel id ramdisk parameters 
install openstack importerror could import settings 'openstack_dashboard.settings ' *** module named angular_fileupload 
“shade required module” even though shade installed 
dns name virtual machine instance created ibm bluemix 
would like create single supe vm much processors possible openstack environment 
openstack mitaka command line client takes long time respond 
installed devstack vm reboot n't lose 
define keystone_user openstack/puppet-keystone via hiera 
perform local shell command openstack hot template file 
one undefined variables 'dict object ' attribute 'ansible_ssh_host ' 
root password centos image installed via openstack 
free nbd devices installing multiple instances openstack 
openstack installation - git pathspec error 
install openstack 
mtu 1500 fragment packets 1472 bytes 
anyway openstack attach physical disk directly 
ssh openstack instance ansible ca n't — differently 
open stack volume wo n't attach 
set floating ip vm using apache brooklyn - floating ips required options extension available error 
openstack swift url use use case 
upload large sqlite file object storage using openstack 's swift via python 
difference local_settings.py settings.py openstack_dashboard 
total size total number object ibm bluemix object store 
retrieve resource list openstack compute engine 
openstack heat webhooks up/down-scalling point internal storage ip 
vagrant virtualbox openstack - better way 
install swift without keystone 
setting multi-user virtual environment jupyter 
openstack permission denied publickey ssh'ing vms 
cloudify 3.3 - use existing network floating ip 
unable ssh devstack instance external computer 
glance image registration remote server 
network issuse scaling deployment cloudify 
hadoop top openstack extra features get 
create hdfs file using pywebhdfs get error errno -2 name service known 
urllib3 documentation telling 
sahara service failed start juno python error 
upload image trystack server using packer tool 
openstack horizon user-interface communicate openstack rest api 
exist openstack api implementation jclouds 
bosh-lite installation openstack 
python swift client object count mismatch actutal objects cloud 
get newly created machine details vagrant-openstack plugin 
restart nova services devstack 
kubernetes openstack integration 
openstack access token refresh 
mesos openstack vm 's public ip 
openstack extend nova host-describe command 
mysql cloud db api 's closed 
openstack - selectively deleting images based criteria 
integrate openstack instances local network 
openstack unable upload image image service 
conditionally check state openstack instance 
edit physical_interface_mappings openstack neutron network 
creating virtual machine bluemix giving error importing ssh key closed 
machine < ip_address > started enough memory 
list objects virtual directorys using rest api openstack object store swift 
live migration openstack 
ceilometer rest api recent results 
possible host meteor.js application hp helion openstack providers 
install devstack much memory use 
openstack api - nova vs ceilometer know instance state 
find complete list devstack services 
use response url recieved create server api response openstack 
get authorization token ceilometer api openstack 
relationship stack id stack name openstack heat 
accessing user password user table keystone database correct form string 
use jclouds openstack4j openstack 
openstack controller nodes deployed virtual machine 
ceilometer group metadata 
qpid amqp tcl 
openstack cloud identity service nova service swift service vs java application 
bad uri trying connect openstack 
configure cloud-controller.xml stratos 
new django need fill form intial data get_data 
custom openstack centos image set password use 
error installing icehouse openstack using devstack 
urlsplit error novaclient authentication 
kitchen open stack gives “excon.error.response” 
use openstack4j osgi platform 
want create open source cloud storing sharing system start 
creating glance image 403 forbidden access 
couldnt install openstack-devstack digitalocean 
garbage packet received needed prompt connection sudo password error 
connection neutron failed maximum attempts reached 
expecting auth url via either error thrown openstack 
neutron devstack 
creating instance virtual hp cloud openstack cloud using c 
find linux namespace network adapter 
dynamic storage deletion deleteonexit template option 
glance image create stuck saving status 
openstack ca n't locate log /var/log 
migrate form physical virtualized infrastructure 
get associated ip address openstack instance 
ca n't get network-id using quantum_network ansible ad-hoc command works neutron command 
openstack throws nullpointerexception connecting 
openstack nova cloudpipe-create fails 
pkgcloud gce google compute engine 
cant start instance openstack 
exits wadl file api 's openstack 
instance os find attached volume 
error import novaclient library python 
scilab/octave cloud parallel processing 
importerror module named pycurl - developing openstack horizon plugin 
get openstack json schema responses returns rest api 
attributeerror type object 'managerwithfind ' attribute 'client ' 
failed install openstack glance 
set default keypair openstack user 
make openstack image public available users 
integrate proprietary federated idp openstack 
setting private multi-project test cloud infrastructure openstack jenkins 
internal server error using php cloud files swift openstack 
openstack specific host vm launch 
collect stress test data openstack 
upload zip file openstack swift tsung 
error syncdb installing openstack 
errors openstack cli install windows 
instal devstack ubuntu14.04 faild 
openstack keystone service fails start configfilenotfound exception 
manually assign uuid openstack machine images created packer 
change openstack swift source code test research idea 
connection remote notebook openstack spark 
freeze package python program based openstack 's taskflow pyinstaller 
graphical way monitoring kvm instaces openstack 
installing openstack fuel mirantis ubuntu 16.04 xenial experiencing time using launch script 
networking openstack instances 
openstack rest api call returns forbidden 
openstack messagingtimeout timed waiting reply 
find operating system installed switch 
launching openstack instance bootable vm 
display vmware integrated openstack 
apply cloudbase-init windows .vhd file 
service know connection failure via 
userdata params got changed stack creation openstack 
use openstack api v3 list accessible volumes 
openstack critical keystonemiddleware.auth_token - unable validate 
openstack mitaka assign securitygroup lbaasv2 port using heat 
create self-authorized tls certificate ubuntu apply openstack 
dashboard login admin account 
public virtual ip responding - namespace 
custom goodness_function manila 
migrate vm image vmware openstack offline 
openstack kilo vmware vm console error console currently unavailable please try later 
import converted qcow2 openstack error 
usage variable role openstack-ansible 
parametrized terraform template 
devstack contextualversionconflict 
openstack telemetry aodh calculate time boundary right wrong 
deploying openstack instances end user 
get proxy node openstack swift cluster 
openstack-swift synchronize time different servers 
communicate instances openstack 
keystone database - credential table 
error installing openstack packstack 
necessary jenkins openstack system 
gui devstack instance working solution 
create instaces using jenkins openstack 
openstack kilo netrun dhcp reuse ip 
openstack install - create openstack project 
missing value auth-url required auth plugin password 
make devstack persist changes reboot system ubuntu 16.10 
tempest cleanup faild first tenant 
murano-test-runner connecte keystone 
openstack keystone reduce number workers 
saving instances openstack 
attach volume failed post `os-initialize_connection` 
openstack devstack unable retrieve volume list even running rejoin-stack.sh 
generate openstack swift access token another user 
missing entity id environment - keystone sp 
run multiple web applications ip using different ports 
openstack tempest cleanup delete tempest objects 
cloud foundry training/tutorials online 
message format openstack message queue 
load stressing web applications deployed openstack instances autoscaling group 
neutron agent list displayed 
python requests 2.13.0 could work oslo 1.19 
problems configuring mirantis openstack volume backups write external swift object storage 
browser-based upload openstack objectstorage 
configuring keystone service provider 
automate creation vm docker containers running inside 
optimize media transcode using cloud computing openstack 
enable/use upnp openstack neutron 
cloud foundry openstack private / on-premise cloud 
rezizing openstack volume remain old size 
ca n't launch openstack horizon dashboard ioerror errno 13 permission denied '/var/lib/openstack-dashboard/secret_key ' 
pycharm remote debug openstack nova-api errors 
kubeadm init -- cloud-provider=openstack 
openstack api querying whether enough resources available instantiate image 
accessing openstack swift deployment ec2 vm remotely 
openstack-ansible - error launching instance 
openstack-ansible - restart services 
openstack changing scheduler_max_attempts nova.conf affect anything 
error unable retrieve instances 
solve following error installing openstack using rdo 
tacker getting error resources.vdu1_flavor 'sessionclient ' object attribute 'set_management_url ' 
fiware health region configuration 
libreswan ipsec vpn fails establish phase 2 
keystone v3 api “401- unauthorized” error list users tenant default-admin credentials 
increase limit rdo stack volume creation size 
access openstack user-data error log automation 
openstack linux-bridge agent 
install openstack client windows 
query jenkins windows 
error creating sample service project openstack newton 
openstack heat-api heat-api-cfn fail start - error unable locate config file 
openstack magnum create stack times 
get rid content-disposition prefix added files uploading 
using repo pypi pip 
++./stack.sh main:1351 devstack ./stack.sh fails 
get remote ip openstack virtual machine accessed via nat 
execute command hosts physical machines via openstack code 
unable launch openstack instance jenkins 
build config-drive enabled image openstack 
convert jiffies percentages virt plugin 
error changes open stack horizon 
extend logical volume snapshot 
get right osclient token openstack4j java api v2 v3 
upload file open stack storage service android 
openstack managed services support 
unable deploy openstack vmware - datastore selected cluster 
os :heat :softwaredeployment staying stuck create_in_progress status 
output format unixbench 
create vm multiple nics terraform openstack 
run script vm 's first boot using openstack 
error openstack server list command 
low interval ceilometer meter sampling 
openstack installation using devstack - liberty 
programmatically obtain openstack resource usage metrics python 
drop mysql database 
installation openstack juno vmware workstation 12.0 
get information floating ip pool 
availability zone list order determined nova api openstack 
error creating vm openstack liberty using apache brooklyn 
accessing spark web ui remotely master running virtual machine top openstack 
openstack network namespace issue network 
resteasyproviderfactory classcastexception openstack4j 
waiting inputstream jclouds java 
check number floating ips available pool 
error codes bluemix/openstack object storage 
automatically update access token 
installing systemd inside ubuntu14.04 docker container - possible 
possible add 2 fixed internal ip assign 2 floating ip vm outside access 
node 1 ntp-check status error 
openstack newton dashboard unreachable centos 7 
openstack attributeerror 'module ' object attribute 'version ' 
kuberntes master starting openstack heat 
allocating resources mirantis 
oslo-config-generator example working 
running horizon python manage.py 
import error module named cmd.manage 
kube-up.sh failes openstack 
able bring kubernetes cluster openstack 
update metadata flavor openstack using python novaclient 
keystone command found 
getting error volumemanager ' object attribute 'list ' trying connect openstack via cbtool 
error status server created openstack compute api 
reuse openstack keystone component standalone identity component java based platforms 
'stack ' user openstack secure 
files 404 error extending volume size swift saio installation 
create open stack instance image .vmdk file volume 
pig jobs failed sahara 
skip test particular file 
use case/scenario stevedore driver/ extension/ hook 
get ip address instance use floating ip openstack 
configuring cloud minimum configuration oracle virtualbox using openstack software 
openstack newton installation using packstack error 
add segmentation_id ovf file 
add slave node openstack kubernetes cluster 
uninstalling openstackclient restore previous glance keystone version 
'module ' object attribute '__version__ ' openstack /devstack installation 
start vboxweb-service.service virtual ubuntu vm 
red hat osp10 deploy fails node profile tag even though configured 
devstack error running git remote update gerrit 
change swift ip address localhost 
openstack sahara error dashboard 
openstack monitoring 
using profiles keystone saltstack states 
oracle database openstack 
./launch_8gb.sh script disrupted even iso present required folder 
openstack horizon - “unable retrieve usage information” error vm shutdown 
openstack mitaka session timeout 
analysis cloud resources allocations 
cinder driver failed start 
extend storage swift openstack service 
openstack want map 1 1 instance network physical network interface 
cinder volume device_name honored cloudify orchestration 
openstack sahara error 
cinder-volume service going 
packstack installation root user 
openstack allow api access vm 
kubernetes external openstack load balancer working 
upgrade openstack fix plugin 
devstsck installation error failed creating password plugin ubuntu 14 
vary creation/not creation node instances “install” workflow 
saio swift one - access external ip 's 
install openstack neutron service multinode ubuntu 14.04 
rabbitmq working openstack 
ansible wait_for module non-unix systems 
openstack sahara image register error 
devstack - horizon dashboard admin user able login 
configure kubernetes trust openstack self signed certificate 
able access openstack-horizon 
openstack cache machanism 
unrecognized arguments floating-ips fixed-ips nova quota-class-update 
dynamically configure glassfish jms resources 
secure devstack public host 
python api change openstack glance using suds 
openstack multi-site versions difference 
getting error world dumping installing devstack 
systemctl services loaded failed centos 7 
private object public container openstack swift 
http 403 keystone /v3/users api 
nova-compute service state 
using terraform add openstack security group instance managed terraform 
maven build asks log file permission denied 
multi node devstack installation 
mirantis openstack installation physical machines 
puppet resource ordering working 
jenkins - openstack plugin retry launching instance different config first attempt fails 
nova server create & server affinity group create & map using python novaclient 1.1 
following tutorial openstack instal guide 
openstack error unable retrieve instances volume list snapshot list etc 
unable boot new instance openstack juno neutronclient issue 
openstack cli failed ssl verification 
make ovs seperate traffic different vms vms attach vlan 
file upload failure using swift storage 
api jclouds call “/os-cells/capacity” 
baremetal dhcp ip address tenant network 
error deploying vmware integrated openstack 3.0.0 vsphere 6.0 
openstack instances doesnt start 
python api failure setting openstack image metadata 'tags ' using glance 
map vm internal network 
dynamically change autoscaling instance names 
way provide run parameters docker container image starting container openstack nova 
devstack unable access dashboard fresh install 
conjure software works order send graphical symbols via ssh 
adding compute node…libvirterror internal error client socket closed 
glance n't work due authentication fail 
authorization scope federated user 
openstack-install command 
admin endpoint identity service found 
httpd.service start installing dashboard installing openstack centos 
heat fix dhcp port ip network 
manila multi-tenancy share 
adjust column width openstack horizon 
pass josn/list values template heat user_data 
mount openstack container docker container 
normal user create encrypted volume openstack 
add build task gulpfile 
create ovs bridge centos openstack ipv6 interface 
juno openstack dashboard working ha-proxy shows memcached service 's availabilty 
parameters sent auth_version_v1 auth_version_v2 auth_version_v3 swift client-openstack 
amqp server localhost:5672 unreachable errno 111 econnrefused 
authorization failed resource could found http 404 
openstack gerrit error git rebase merge 
installing devstack error creating password plugin keystone 
ping internet launch instance cirros mitaka 
autodiscovering hdfs nodes openstack instances 
openstack swift returns 500 non-existing object 
best way access openstack cli windows 
best way convert .iso win2010 & rhel7 qcow2 + add apps automatically 
openstack behind haproxy provide issues 
vagrant monasca vm mounting failed error device works sudo 
whats difference setup.py setup.cfg python projects 
storm version openstack 
segments ' count minimization openstack swift 
openstack swift ceph backend radosgw 
kubernetes- minion talk master 
nova supporting using keystone v3 
facing errors uninstalling openstack devstack 
unable establish connection http //controller:35357/v3/auth/tokens 
openstack devstack insatallation dashboard login default credential ca n't find username 
openstack one environment instance ca n't access service running docker container 
openstack instance fails spawn 
get token id keystoneclient v3 
facing errors openstack-devstack installation 
facing errors connecting openstack github repository 
error message “invalid flavorref provided.” trying create instance 
“connect time out” error jclouds via proxy 
vagrant checking vm created 
custom names resources cluster created using heat templates openstack 
openstack python sdk - glance return image md5 
centos httpd apache http server fails start 
unable launch instance openstack 
openstack access vm instance console script run dhclient reboot commands 
docker failed build openstack 
openstack authorization 
order use sql docker container deployed cloud 
choose pxe setup deploying opnfv arno via fuel master node 
get project id using project-scope authentication openstack4j 
error cloudify bootstrap openstack route host 
using openstack api access softlayer resources 
openstack swift org.javaswift.joss.exception.commandexception unable open input stream uploading 
softlayer - running jumpgate 
openstack unable start keystone installation step 
openstack keystone openid connect provider returned error 
add ironic db version file install n't copying db version file 
put method working openstack 
invalid user name password horizon 
stop openstack nova logging locally host 
live migration failure unable execute qemu command 'migrate ' migration disabled failed allocate shared memory 
handlers could found logger “keystoneclient.auth.identity.generic.base” 
configure server groups openstack ansible 
ceilometer api filters n't work multiple fields 
get auth-token openstack 
openstack cinder volume create valid host found weighed hosts available 
install devstack ubuntu 14.04 single node desktop environment 
bosh n't recognize deleted vm 
error oslo_messaging.rpc.dispatcher req-xxxxx exception message handling 'metadata ' 
“table 'nova.services ' exist” error openstack 
openstack devstack heat template create vm app 
unable connect open stack object storage java application 
openstack compute-node communicate/ping vms run 
jcloud nova unable find floating ip pool extension 
delete nova network - network address already use active instance 
openstack instance use entire hard disk 
openstack instances update 
starting openstack instances programmatically 
python module importation error 
create docker container openstack 
devstack installation fail python update.py /opt/stack//cinder 
openstack participating localization 
bluemix create vm public ip 
launch instance non-glance image openstack 
network block device - receive control failed result -32 - kernel 3.16.0-41 
's default timeout func 
meaning gettext 'some text ' openstack 
python-keystoneclient ' deprecationwarning 
failed launch instance 
openstack token get token curl ca n't urllib 
sequence setting keystone v3 
openstack make insstance acessible different machine 
choose specific network among multiple networks 
creating private cloud practice 
openstack allocate ram guest 
openstack swift cors configuration 
ceilometer identity.authenticate.failure 
error bootstrap management vm 
's possible customize timeout values 
dependency injection keystone middleware openstack 
migrate shutoff instance specific host 
adding node maas controller internet 
openstack juno jclouds api generating urisyntaxexception extra space response 
anyone know 's data source http //logstash.openstack.org 
unabe get openstack login prompt 
devstack default username pass showing console ubuntu cloud image 
ways install heat resource plug-in 
ovs internal port / veth pairs bandwidth limitation 
achieve high availability instance openstack 
multiple processes neutron-server juno 
password key salt-cloud use login newly created openstack vm minion 
deploy vagrant vm openstack image vagrant 1.4.3 provider virtualbox 
run openstack devstack asus chromebook 
juno glance like iso image 
pass parameters powershell script userdata 
able create alarms using openstack api 
iptables nat/masquerade allow openstack instances access sites external laptop 're running 
nosuchopterror running openstack functional tests 
libcloud upload image 
updating exisitng security group using heat 
ceilometer - meter getting active tenants 
fix invocationerror bug jenkins 
openstack router stuck build 
openstack getservers returning typeerror using pkgcloud nodejs 
partitioning instance openstack 
use existing lvm vg devstack 
resolve error 'you allowed start instance < instance name > ' openstack 
docker ubuntu container ping archive.ubuntu.com apt-get 
executing openstack-rally test cases 
arguments converted string formatting assign values 
keystone digest authentication 
openstack java programming 
ping failed second ip openstack instance 
openstack admin impersonate another user create vm 
possible use ssh tunnel openstack4j 
query ceilometer config 
openstack - could find resource admin error lauching multi-region 
openstack cinder error liberty 
open daylight project build issue 
solr slow response server connecting via external ip address 
openstack instance access external network closed 
openstack liberty keystone instalation unmet dependices 
openstack instance reachable due metadata issue liberty 
creating compute openstack cloud 
warning neutron_lbaas.services.loadbalancer.drivers.haproxy.namespace_driver - stats socket found pool 
cloudify 3.3 - openstack ssl3_get_server_certificate certificate verify failed 
pass user_data script python openstack heat-api client 
openstack swift pid files n't correspond running processes ' pids ca n't restart services - port already bound 
sync / update local volume data openstack server 
devstack unable create neutron network 
devstack error request made requires authentication http 401 
ceilometer alarm error - heat openstack 
port forwarding openstack instances 
specify disk size openstack create instance 
kernel panic - syncing fatal exception interrupt openstack vm 
ceilometer healthnmon measuring vm stats openstack 
openstack floating ip assocation work underneath 
use strings parameters openstack heat 
pycadf error installing devstack kilo ubuntu 
packstack fails - error unable connect mongodb server 
php server-to-server communication lan 
sharing resources two independent openstack cloud setups 
terraform error launching server openstack 
openstack api v2 tenants returns one tenant 
know type secret returned barbican get /v1/secrets/ call 
problems launching vm openstack 
openstack create new instance ubuntu 
devstack installation failing uploading fedora image 
update network subnet quota openstack installation 
openstack support dual-stack implementation 
retrieve id network openstack api shell script 
designate network id creating network 
openstack juno devstack - specify new neutron plugin configuration files 
trove guest agent starting 
rdo unable boot vm disk size specified 
fixtures openstack keystone 
rabbitmq start change config file 
connect android application openstack cloud 
horizon dashboard internally fetch data using angularjs 
swift balanced 
nova errno 111 connection refused 
correct paradigm manage client openstack environments 
aims cinder swift openstack 
django ioerror errno 13 permission denied 
devstack juno networking route networks 
error install trove openstack juno 
get nova client v1.1 use ssh tunnel retrieving server list 
installing libvirt virtual machine 
create private cloud using openstack file sharing 
end point mean terms ceilometer api python 
openstack horizon ui customizations 
openstack-neutron-lbaas 503 returned haproxy 
openstack swift requests blocked eventlet.green.httplib 
openstack neutron two physical nodes 
single sign openstack web app 
communication openstack vm 
docker cinder possible openstack 
openstack designate '_authtokenplugin ' attribute 'register_conf_options ' 
integrating openstack dashboard inside web app 
route host found - openstack 
dart lang app open stack / docker / vagrant 
find ip address virtual machine running 
logging jclouds still prints retry connection error console credentials 
automatic provisioning open stack vm docker containers 
issue starting openstack nova installing using devstack 
unable install nova compute error command python setup.py install 
list openstack services full info using python api 
cant reach internet instances icehouse 
integrate rhev 6.5 openstack juno 
rabbitmq consumes memory shuts 
setup public rule keystone policy file 
openstack relation controller & compute nodes 
swift juno complains 'account found ' 
openstack docker fails spawn instances 
correct routing table openstack controller 
devstack juno importerror module named persistence.backends.sql 
boot iso reboot installing iso icehouse 
prebuild ready iso openstack all-in-one node 
openstack - web console connection refused 
openstack prevent losing vms 
openstack sdk getobjectsavetofile headers 
rabbitmq openstack juno 
neutron namespaced networks connected physical interface 
reading json xhr.py angularjs using http.get 
need sync .conf file manually openstack multi-node environment 
enable os-ksadm admin extension openstack 
openstack vm network flow 
openstack slow performance compute nodes 
wso2 stratos openstack network id 
odl openstack issue 
deploy spark make highest resource utilization 
listing loadbalancers openstack 
jcloud openstack-neutron exception thread “main” com.google.inject.configurationexception 
get aws account name aws_access_key aws_secret_key 
“access denied user 'root ' 'localhost ' using password yes ” http 409 
create instance already uploaded vmdk image s3 bucket 
failed launch openstack instance 'authentication required ' trying create port 
routing openstack 
correct image spin vm openstack using vbox host centos 6.5 allinone “guest” 
openstack instance getting ip getting ip 
“unrecognized auth response” every heat stack-create request 
error get flavor list java.lang.nullpointerexception uri template newly created target must null 
openstack neutron specify network interface associating floating ip 
error failed attach interface http 500 request-id req-xxxx 
keystone get user email 
setup local.conf devstack 
error executing “to test glance upload “cirros cloud image” directly internet” 
apache mesos vs. apache cloudstack 
openstack icehouse dashboard authentication errors using multiple domains 
unable run openstack tempest testcases group 
connect instance directly ext-net external network 
keystone configuration file permissions 
deployment tools used hybrid cloud 
jclouds - specify network interface assigning floating ip 
python - openstack - pickle dump load 
openstack -keystone provide encrypted login 
creating network openstack using jclouds getting error “neutronerror” “unrecognized attribute 'networktype'” 
horizon vm working devices 
openstack ceiolometer throwing unknown error installing 
trying use openstack horizon started server got following error tried search getting anything anyone help 
openstack create multiple floating ip pools single public network 
getting list tenants open stack using keystone v2.0 python api 
way figure machines created specific user 
keystone configuration 
openstack nova modification 
fog openstack ca n't specify network name 
openstack-swift data writing mechanism 
openstack service failed creating password plugin 
fail launch vm xenserver openstack nova 
create trove backup mysqldump file 
“no valid host found” spawning hadoop cluster openstack sahara 
address space intersection networks admin pxe public 
openstack swift download storedobjects/directory local maintaining hierarchy 
packer-built openstack instance stuck “spawning” state 
packer build openstack/gce image docker directory second disk partition 
openstack ovh connection configuration 
docker “/bin/bash” could invoked mounting nfs file -v openstack 
error glance_image cirros change absent present failed command … running 20 seconds 
kubernetes ca n't start kubelet set openstack cloud-provider 
unable install python-openstackclient 
many kinds alarms ceilometer support 
metadata concepts 
openstack liberty compute memory dump 
creating different customer accounts keystone 
openstack instance connect external internet 
openstack lbaas v2 agent horizon menu 
crontab seems execute script script n't work 
foramt objects openstack api respose 
access instance openstack vm instance outside subnent 
openstack create bridge guest vm 
way load config file fly 
customize authentication openstack horizon 
password mentioned keystone authtoken 
include wrap file file content file object using softlayer object storage rest api 
issues installing openstack kilo rhel 7.2 
connect selfservice network directly physical network 
unable create keystone token v2 ldap 
openstack tripleo undercloud installation “could find class :ironic :drivers :deploy” 
qemu-img merge qcow2 delta images 
paramiko executing route add network command 
openstack token curl 
error trying install openstack using devstack 
rdo packstack losing ip connectivity installation 
openstack kilo cinder ca n't create snapshots lvm volume found 
find total number physical cores entire openstack based cloud 
php-opencloud openstack neutron call neutron api using publicurl try connect using internalurl 
valid host found enough hosts available 
sensu novaclient connection issues 
docker-machine openstack / ssh 
compute node cache image 
openstack heat - separate templates 
orchestration heat client 
openstak security monitoring 
openstack swift store rings 
action log file openstack web ui ubuntu 
typeerror 'openstacknetwork ' object support indexing 
installation admin actions extension openstack suspen/resume action 
difference global role tenant role openstack 
install docker using cloud-init 
create instances openstack using vagrant docker 
devstack installation error nova-api start 
invalid openstack identity credentials - glance 
softlayer authentication using openstacknet 
openstack.net sdk access service region 
openstack keystone 3.0 api 
run php process background send email finished 
openstack instances pinging different network 
ec2 openstack google app engine gae rest 
get openstack cinder volume size python bindings 
openshift scaling specific software condition 
ca n't ssh creating instance command line 
error creating container openstack swift 
get multi part object openstack swift using jclouds 
openstack notification service marconi 
creating rings regions openstack swift 
possible measure openstack load ganglia monitoring system 
fatal error class 'size ' found 
apply scheduling algorithms eucalyptus openstack instances 
keystone cert_required nova 
intergration docker openstack via docker heat plugin 
change flovour size vm using libvirt 
openstack service group softwares 
openstack ceilometer shell look detail data 
integrating phpmyadmin openstack trove database service solution 
possible get list database servers available openstack cloud using jclouds 
openflow nginx webserver 
mysql openstack need query advice 
create “something” service openstack cloud 
ubuntu cloud openstack single node - advantages disadvantages 
error distributions found prettytable > 0.7 
puppet unable run services chroot environment 
install sahara openstack 
error installing devstack 
jclouds able get list images 
openstack + chef + jenkins continuous delivery 
devstack — unable lauch instance due nova-rootwrap 
get hypervisor name obj_id table_action 
operation couldn’t completed kcferrordomaincfnetwork error 303 
trying use openstack create management interface manage already created vm 
openstack verify using kvm qemu 
swift + keystone user / password invalid 
dependency injection trouble keystone extensions 
automation create openstack havana script ubuntu server 12.04 lts 
“connection neutron failed” net-create 
create new server using openstack.net 
error running unstack.sh 
puppet openstack havana ha package dependency 
relocate devstack configuration files 
add column keystone model 
openstack cinder controller node make nfs share volumes available compute node 
select network new instance python-boto 
openstack havana installation 
managing openstack java 
efficient way get hadoop running private openstack cloud 
connectiong vms implementing devstack 
running openstack opensuse 13.1 
upload file openstack cloud using nsstream 
json object decoded 
failed launch instance timeout waiting rpc response - topic “network” 
openstack - controller node 
neutron port create 
openstack rdo ca n't connect mysql server '10.0.3.139 
object oriented ajax calls openstack environment 
testing openstack services devstack 
unable download object open stack object storage swift 
openstack swift ubuntu - unable create /var/run/swift 
capture reply rabbitmq message corresponding rpc.call request message openstack 
user creation cloud-init 
openstack error launching instance 
openstack-devstack ca n't successfully run stack.sh 
cloudable java app openstack 
openstack loadbalancer able tget_members filters 
adding new user keystone-openstack using android 
cloud foundry v2 openstack 
find cpu core host virtual machine running 
getting nova:628 die trying start openstack nova module 
configuring openstack mutiple servers running 
assign network vm using jclouds openstack list networks openstack using jclouds 
build openstack images 
instance gets multiple fixed ips 
openstack compute used process images back-end cloud front-end android app 
write rules openstack neutron environment 
python-novaclient source code explanation 
openstack create user without keystone authentication 
make openstack cloud read-only 
conductor api attribute “xxxxxx” 
advise clustering file system storage array fibre channel 
openstack vmware esxi hypervisor 
vpn environment non vlan netwoking openstack 
iam installing swift using link get error unauthorized 
web front-end swift written python？ 
rbac openstack via http verbs proxy 
changing disk size running instance openstack 
set vcpupin guest xml 
openstack overkill ha website stack 
dev stack/ media wiki apache server 
configure openstack nova remote bind server 
sslerror errno 1 _ssl.c:510 error:14090086 ssl routines ssl3_get_server_certificate certificate verify failed 
start openstack-services 
learn openstack cloud computing 
trying integrate ldap devstack ./stack.sh got localrc line 9 keystone_identity_backend command found 
injection keystone openstack 
apply new configuration devstack local.conf closed 
sed place variable replacment line 
printing class object python 
packer sample file openstack 
building private cloud research purposes 
stack openslack 
openstack ubuntu autopilot servers 
keystone client project list display projects 
check particular value class object get value another key 
difference provider network self-service network openstack 
'users ' table nova database 
openstack - hardware requirements 
deploy openstack directly bare metal 
vms able ping virtual gateway 
heat style auto-scaling openstack aws closed 
basic different openstack api & cli commands begineer 's perspective 
cloudify openstack ssl3_get_server_certificate certificate verify failed 
openstack installation without virtualbox 
openstack-cinder resize volume 
error creating neutron client 
aws find connected ephemeral storage object storage block storage 
horizon accessible unplugging ethernet host 
system n't configured run kvm properly vmware closed 
rest api calls done openstack 
openstack neutron - vm router two networks within tenant 
monitoring virtual block storage hold 
ca n't access openstack personal machine 
logs analysis cloud openstack iaas 
openstack first angularjs dashboard 
create vm multiple nics azure 
ca n't ping ssh vm openstack 
openstack swift region affinity account 
ceph set default stripe-count stripe-unit 
install openstack mitaka + ovs bridge + dvr 
openstack networking node mirantis fuel dashboard 
openstack horizon dashboard default password 
ubuntu user missing creating image diskimage-builder xenial cloud image base 
private ip address advertised public dns records federated services kubernetes 1.3 openstack cloud provider 
setting openstack devstack install soft layer bare metal 
devstack script error 
printing neutron ports 
devstack install openstack-mitaka ubuntu14.04 
openstack ping br-ex ca n't reach internet 
function libcloud upload image using openstack 
upload file instance openstack 
coding amqp listening user-facing daemon python 
able set cpuset atribute vcpu element instance xml 
openstack swift handles concurrent restful api request 
openstack nova switching cassandra — pros cons 
message queue openstack grizzly architecture 
find time taken register run vm euca commands using shell script 
change openstack ceilometer mysql reporting password 
openstack grizzly keystoe remove_user command removing users 
use aws :elasticloadbalancing :loadbalancer behind proxy 
error creating volume openstack dashboard 
ca n't use tempurl put method upload 
create image ovf file openstack using jcloud 
error re-running stack.sh 
sql api wrapper python swift object storage 
openstack_swift_installtion 
approaches build high-availability openstack cluster 
glance authenticate admin user 
openstack-folsom keystone script fail configure 
pip uninstall shows package uninstalled actually 
replicate modification devstack openstack production 
saas practical basics projects 
cloud shared memory management openstack 
package found try install python-glanceclient 
encryption algorithms vdi cloud 
add split vmdk glance openstack 
nova client image create 
install stratos 1.5.2 
unable launch windowsxp image openstack 
openstack api - curl request hypervisor information 
proper auth method keystone openstack 
horizon work keystone swauth 
object storage -swift restart problems 
openstack server validator tool 
glance index error openstack 
recover nova-compute crash “networknotfound” 
questions cloud experts - openstack hypertable 
openstack - nova-billing 
good setup distributed monitoring tracking latency/drops network 
fluentd replace rsyslog collecting server logs 
openstack sahara cdh cluster warning alerts 
mirantis fuel nailgun server 
devstack error deprecated attribute newton 
error installing openstack autopilot test drive vmware workstation juju ca n't connect websocket register service 
unable peer probe glusterfs transport endpoint connected 
openstack - windows images based qcow2 file starts 80 full resizing working 
start keystone service 
openstack installing glance python-greenlet install 
jclouds openstack create instance 
fix ` -- os-auth-token expected one argument` glance error 
openstack version better deploye vmware virtualization 
size qcow2 disk file grows rapidly 
make openstack mitaka services persists across reboot 
/usr/bin/systemctl start openstack-nova-api failed 
openstack recover orphaned instances 
add-type 0 metadata file 'system.linq.dll ' could found 
get gateway interface rdo openstack 
curl ca n't list tenants servers 
send requests directly bare metal machine openstack 
connect vm fiware lab ssh connection timed closed 
iptables command bridge openstack virtual network 
parse python module openstack 
openstack visibility availability zone 
difference “mechanism driver” “extension driver” openstack 
copy/paste html5 console openstack horizon 
bdf based pci-passthrough non sriov using openstack liberty 
configuring network chapter 2 basic environment openstack installation guide 
wrong openstack error 
restarting devstack services - safest way 
installing freepbx virtual machine 
glance image-list specific image name 
openstack deployment using kolla erroring 
ca n't create snapshots fallback swift cinder 
elaborate steps openstack installation 
ios swift listing files url 
cloudkitty devstack - setup automation 
httpinternalservererror http 500 unable delete bosh image 
best hypervisor nfv terms throughput 
juno neutron gre tunnel qrouter pinging vm vm getting ip dhcp 
cloudify network 
one try openstack cisco avos 
openstack login instance 
create glance image retrieve token 
could openshift deploy war diy tomcat switch openstack 
stop restart vm without reinstall service 
create vm windows server 2008 sun server sparc 
communication tenant network existing network openstack 
openstack failed launch instance “test” please try later error valid host found. 
openstack cloud billing software list closed 
openstack instance console returning error 1006 
add docker ubuntu 14 qcow2 image 
properly call __init__ horizon workflows.action 
devstack failed openstackclient.shell exception raised python-neutronclient 
glance failed upload image http 500 image status killed 
openstack rejoin-stack.sh script working reboot 
java api downloading openstack images 
nfv openstack 
create openstack using noveclient python api 
kitchen openstack steps/example 
unable share cache vms created openstack order create side channel 
error import xstatic.pkg.angular_cookies running manage.py 
convert video file another format openstack object storage server 
openstack keystone otp 
openstack ceilometer alarm creation error “missing argument \”data\“” 
dynamically add instances compute node openstack 
notifications external systems openstack 
issue swift container relative/absolute path 
place cloud-config file used cloud-init ubuntu image 
openstack python code **kwargs - > help=_ 'password connection nova admin context ' 
docker openstack benchmarking 
cdi deployment failure weld-001414 bean name ambiguous - jar issue 
devstack openstack-dashboard found /etc 
multinode installation swift using devstack 
route host c-class external ip 
kubernetes openstack cloud provider fails panic 
cloudkitty - rating display issue 
kafka slow/unresponsive openstack - troubleshoot 
neutron openvswitch br-int ca n't reach external network br-ex patch 
curl 7 failed connect connection refused openstack swift 
openstackdotnet sdk object storage object versioning 
openstack rally - possible check nova console logs 
install openstack identity service found “cliff.app found http 404 ” 
open stack enpoints api request os x 
getting vagrant synced folders work windows openstack 
use openstack components web app 
openstack - route host port 22 
establish authorized request keystone using python-openstack client api v3 
manage multiple architectures using source-repositories dependencies diskimage-builder 
cloud-init failed write hostname xxxx /var/lib/cloud/data/previous-hostname 
connection timeout openstack4j compute query 
swift authentification probelm try connect via api 
sync command openstack object storage like s3 sync 
openstack swift logging temp url cross domain 
vm unable connect virtual switch vif_type binding_failed 
openstack failed launch instances error valid host found 
neutron openvswitch cant bridge external nic profitbricks losing connection 
extend openstack nova api add new resource 
download specific openstack milestone 
find good reference configuring openstack nova cell environment using devstack 
sh check image exists openstack 
integrate opendaylight openstack opendaylight operate integrated openstack 
write file openstack container using joss 
openstack manila installation fails devstack 
devstack error submitting form please try launching instance 
openstack - stack.sh fails syntax errors 
openstack horizon django button action define 
confused suspend/resume vm openstack 
stats tab resource usage populated ceilometer 
bosh init gives error openstack creating bosh vm script 
fetch physical networks openstack using openstack4j api 
trying display list servers using openstack api 
invalid parameter domain keystone_tenant services 
make os tempest skip specific version apis 
opendaylight build particular project using maven 
openstack heat syntax multiple fixed_ips parameter 
n-cpu failure running ./stack.sh 
strstr working openstack instances 
high availability-odoo-server -openstack 
openstack ip address filter 
openstack opencontrail horizon login error 
xen vtpm 's integrated openstack cloud 
devstack error instance creation 
canonical maas multiple vlan 's 
openstack authentication always get 401 http response 
openstack api - difference dynamic large objects dlo & static large objects slo 
find guest vm usage details openstack 
make openstack keystone authentication request v3 v2 api 
create pecan project oslo rbac support 
curl “no json obeject could decoded” 
openstack nova-compute load multiple config 
openstack error neutron network service 
bare metal provisioning hypervisor installation 
networking api openstack work restclient -update quota tenant 
convert node.js file hybrid mobile app 
openstack instance snapshot creates empty image 
ssh cirros centos vm though ping ip using openstack 
neutron error “ rule create_port rule create_port fixed_ips performed” 
automation scripts using python api openstack swift 
use openstack identity api creating single sign web application 
add dynamically node existing cluster without restarting openstack 
expand single machine ubuntu cloud installation 
difference openvswitch_agent ml2_agent mitaka 
devstack installation fails aws ami instance 
deleted identity service openstack 
rpmdb corrupted 
integration openstack & opendaylight 
openstack able scan guest vm logical volumes 
openstack4j - list filter 
openstack installation single node 
create openstack vm instance specific port using jcloud api 
type object 'schemabasedmodel ' attribute 'in_scope ' 
way get ip assigned vm particular subnet embed heat orchestration template subnets 1 port 
setup barbican development environment centos 
failed floating-ips devstack installation 
real time deployment openstack 
openstack swift plugin jenkins 
heat set alarm configuration get alarm back ceilometer 
openstack view console connection times 
openstack neutron port unbound 
remote monitoring java visualvm jmx 
ssh openstack instance 
validate template using heat api client 
start stop script ubuntu 12.04 
file corrupted downloaded swift container 
make virtual machine assignment efficient 
tempest success single test fail batch test 
apache reports 500 errors mod_wsgi application exceptions django 
openstack api behind nginx proxy 
use 'x-bulk-delete true_value ' header openstack swift 
possible change ip docker container running 
crontab execute openstack 's commands 
deploying puppetlabs-grizzly 
“error errno 111 connection refused” try use nova image-list 
machine 's uptime openstack 
error configuration two object-store service keystone 
cli/api check openstack development version 
openstack assigning ips manually 
openstack nova list create problems 
sqlalchemy sometimes get data 
openstack- keystone connection android project 
django app deployment openstack 
openstack nova services 3 rabbitmq consumers associated 
openstack block-migration dhcp issue 
postfield look like 
deserialize keystone json response using newtonsoft json.net 
openstack horizon validators 
glance error pathspec 'requirements.txt ' match file known git 
setup common glance multiple openstack setups 
deploying cloud foundry openstack route host 
getting “cinder/cinder/service.py” line 249 _start_child trace cinder os._exit status trace cinder typeerror integer required `` 
upload data tornado.httpclient.asycnhttpclient object storage 
openstack instance name resolution 
grizzly instance launched ok console output 
django openstack authentication 
ceilometer launch error ceilometer-collector 
devstack multi node installation 
ceilometer http 500 
able access swift rest services remote machine 
keystone error devstack 
troubles installing openstack 
connect rabbitmq broker openstack 
openstack horizon fetch instances 
keystone returning token metadata is_admin 0 
classnotfound exception adding custom filter openstack 
rhel 6.x image openstack 
apply fix openstack 
debug keystone myeclipse raised exception 
internet connection openstack nova vm instance 
adding images glance 
upgrade libvirt 
nova boot work anything 
puppet diff without fqdn 
openstack identity extension v2 api missing 
devstack installation error 
openstack configured start certain instances hypervisor reboots 
write rest web service using apache jclouds 
changing root password virtual machine kvm 
partition accounts/users openstack tenant 
add security groups instance openstack 
one trove openstack database service solution virtual machine image database 
able connect openstack instance external machine 
openstack trove database instance status=error creating new instance 
perfect implementation database service private cloud 
ceilometer api use result parmeters 
make floating ip pools available openstack 
develop application openstack closed 
warning keystoneclient.middleware.auth_token - configuring auth_uri point public identity endpoint required 
migrate openstac one project another 
using jclouds list containers saio openstack swift server successful 
error communicating http //address:8777 errno 111 connection refused 
find keystone_authtoken ceilometer.conf setting 
authenticating openstack.net api cloud files using london account 
populate heat template openstack `user_data` without including script inline 
's best method let users access file openstack swift 
relation available ram ring size openstack swift 
ca n't display “flavors” interface openstack horizon dashboard 
openstack swift performance metrics collection api 
create dev/test cloud box two 
recommended setup “hom dev cloud” want run vm 's old desktop basically 
ceilometer api giving error 'str ' object callable 
amazon actually read code stored aws 
api specification openstack 
parse line bash using nova get line 
openstack .net api meta data getting added passed cloudfilesprovider.createobjectfromfile 
server disconnected code 1006 openstack 
using php curl store object open stack api file storage 
vms compute nodes still boot fast big image file needs downloaded storage nodes 
find unicode 
ssh openstack instance - error 
cloud foundry - installing micro bosh vm openstack 
upload cloud file rackspace set delete file expiry 
nodejs give error error initializing v8 cent os 
openstack nova api 
keystone log files always empty 
send data cloud instance opestack 
openstack run multiple glance instances 
openstack folsom server detail json response unmarshalled 
know os type image openstack 
set os.environ nova-api 
tests fails testing python-glanceclients 
devstack - edit nova.conf file using local.conf file 
openstack compute node performance 
openstack authentication issue using libcloud 
find logs openstack 
openstack load balancer vs nginx 
config file openstack poppy use & log debug info poppy running 
openstack odes nay flavours/images 
openstack juju yaml field explanations 
ssh connect host localmachine port 22 network unreachable error 
flat_interface local.conf devstack 
rabbitmq-server start failed exitcode 1 
installing openstack ubuntu vm 
openstack nova -- help “could load entrypoint.parse” error 
cloud computing hadoop 
openstack alarm engine 
kubernetes openstack integration - /cluster/openstack missing 
openstack kilo devstack install fails novnc issue 
register designate keystone 
openstack swift saio unittest error 
first steps jclouds 
filter floating ips neutron python api 
creating compute node '_allocate_network_async ' threw error 'permession dennied ' amqpdriver.py 
openstack instance syslog 
nova list fields 
deploy jar openstack cirros instance 
problems generating config files tox 
openstack cloud - boto get_all_buckets returns element found 
openstack heat template flat network 
write custom constraint heat template openstack 
runtime error either replica set mongos required guarantee message delivery 
vcpus mapped cpu multiple servers 
novnc console openstack dashboard rendering garbled ui 
nova-scheduler don`t rpc.cast nova-compute errors vm 'scheduling ' state 
openstack - assign floating ip pool specific tenant 
packet out-of-order open vswitch running multi-core 
ubuntu landscape openstack install kilo neutron-l3-agent unrecognized service 
install opencontrail without openstack 
openstack devstack - unable stop service using screen 
adding new compute nodes devstack installation 
openstack installation vmware 
devstack installation error - keystone 
virtio suitable type application 
instance creation openstack nova - logfile 
mirantis fuel 6.1 deployment using scripts 
unable resolve host hp cloud 
ovs-bridge cant pass physical interface neither ping neighbours 
ca n't get object swift tempurl 
rdo openstack ceilometer heat autoscaling instance 
openstack rdo ceilometer alarm action execute script 
build java/python paas platform-as-a-sevice openstack closed 
good tutorials resources openstack closed 
issues working python generators openstack swift client 
relationship endpoints regions etc keystone openstack 
vagrant vms use manifests storeconfigs without puppet master 
distribution cirros based 
private cloud gpu virtualization similar amazon web services cluster gpu instances 
docker-machine behind corporate proxy 
re-run cloud-init without reboot 
jenkins slave went offline build 
authentication via config file 
ipv6 interface ip operations stopped floating ip ha failover 
openstack devstack installation stops connecting dashboard url 
minimal devstack nova floating ips 
installing openstack errors 
cloud platforms- sudo unable resolve host closed 
openstack devstack 
install swift openstack storage infrastructure openstack without installing nova 
advantages vm 's lightweight containers docker closed 
openstack hello world 
get openstack token validate 
get rackspace servers certain tag 
detect puppet run complete 
azure inter-operable amazon 
libvirterror internal error find suitable cpu model given data 
restarting service openstack installed using devstack 
api-paste.ini file openstack 
openstack python api download image glance using python api 
openstack “no valid host found” image cirros 
installing openstack mac 
images cloud-ready openstack 
error installing pbr 
launch openstack instances using python-boto 
create volume 2 gbs openstack closed 
openstack devstack fail installs keystoneauth1 2.12.2 demands 2.16.0 better 
tcp receives packets ignores 
storage objects created file object storage cloud 
instance creation devstack icehouse 
trouble installing openstack devstack script 
getting auth token keystone horizon 
get ip address launched instance boto 
error rabbit-mq server 
swift user 's authorization 
install packages cirros os 
devstack installation error directory 'opt/stack/nova ' installable 
php opencloud\rackspace problems 
retrieve cpu usage node kubernetes via api 
resolve error installing devstack kilo ubuntu 
fluent custom plugin registration throws “unknown output plugin” 
notifications openstack 
deploy cloudfoundry openstack 
openstack oslo_config nosuchopterror deprecated name - probably need define deprecated_opts 
kubernetes using openstack cinder one cloud provider nodes another 
openstack dashboard default password closed 
programmatically setting instance name openstack nova api 
openstack network working iptables turned back 
using lucene solr elasticsearch index amazon s3 rackspace cloud files openstack swift 
amazon s3/openstack swift api skeleton 
assign openstack floating ip making sure removed server 
login credentials ubuntu cloud server image 
openstack compute nova “error” 
difference virtualization private cloud computing closed 
one node openstack installation configuration closed 
openstack-swift clients closed 
glance store images openstack 
error sudoers list 
python script programming language scripts used restrict resources like ram cpu usage used applications 
add table nova database openstack 
cloud agnostic tool iaas based cloud 
urllib3 - failed establish new connection errno 111 
open source rails gem openstack api 
difference essex folsom release openstack 
rex identity files 
spark cluster master ip address binding floating ip 
pycadf error installing openstack liberty via devstack -ubuntu 14.04 lts 
debug openstack dashboard 
run cloud-init environment without using openstack ec2 
value/benefit openstack “network/storage” nodes 
enable kvm ubuntu 12.04 virtualbox imac'11 
append overwrite bytes existing object openstack swift 
openstack swift server waited long request sent client 
openstack/nova route host 'm trying ssh instance 
mac os x openstack 
configure vswitch openstack 
possible ge list tenants user associated openstack using keystoneclient 
integrating swift keystone 
convert ceilometer output python dataframe 
openstack rest api send file python openstack container 
authentication saio swift one using libcurl api c++ 
openstack neutron ca n't ping external network 
remote debugging openstack 
possible create ami openstack image 
handlers could found logger “oslo_config.cfg” 
nova boot baremetal select specific machine pool 'boot ' 
get cpu memory hdd info host openstack python api 
get usage quotas java implementation openstack 
get fixed ip address openstack instance using novaclient 
error message nova schedule 
advantages using openstack heat chef 
possible update several objects openstack swift atomically 
use openstack heat install setup mongodb cluster 
set image metadata python api 0.11.0 openstack grizzly 
openstack vm able access internet closed 
openstack nova-network start 
generate openstack compatible images 
completely remove openstack system installation devstack script 
openstack dashboard gives error “error unable retrieve usage information” 
python pep 440 - closed 
openstack-devstack ca n't create instance enough hosts available closed 
openstack - change admin password dashboard 
installing python packages without internet using source code .tar.gz .whl 
unable ping instance launched openstack 
minio cluster architecture minio.io object storage server 
getting user list openstack keystone api 
vagrant port forwarding work 
openstack “_member_” role “member” role 
manage users/passwords devstack 
upgrade ring file swift 
openstack .net api way combine object creation container creation one call 
get id name networks fields openstack nova list 
openstack dashboard error unable retrieve usage information instances 
rackspace cloudfile api - get object information 
nil turning null request fog 
create instance volume openstach python-novaclient 
openstack/devstack virtual machine installation error 
access hp helion using apache jclouds 
openstack.net sdk access services 
openstack images stored 
installing apache hadoop openstack 
uploading cloudfile rackspace cloud using openstack.net get net.openstack.core.exceptions.response.badservicerequestexception 
get cpu information openstack flavor using libcloud 
openstack - check availability container c .net 
changes local folder reflecting git openstack 
openstack horizon dashboard folsom release js builder 
possible create openstack image importing *.ovf file glance 
variable set settings.py redirect nova services url 
ca n't access openstack 's horizon dashboard 8000 outside 
migrating vm 's kvm openstack 
instead glances installed openstack-glance mistake 
terraform stalls trying get ip addresses multiple instances 
web server come play openstack - cloudfoundry stack 
hadoop java.net.unknownhostexception hadoop-slave-2 
docker container connecting https endpoints 
liberty openstack ubuntu install guide fails ping router 
give openstack server permissions call openstack apis 
configure cloudkitty devstack 
accessing openstack api bluemix 
openstack installation failed set rdo repo host 
check whether dhcp-server exists subnet creating subnet 
bad request error openstack 
wait condition heat/cloudformation instance startup order 
create temporary url swift object storage using rest api 
pass ip-address cloud-init metadata 
impliment auth_token ceilometer python client api 
jclouds - neutron specify nic associating floating ip 
reset password openstack instance using kvm libvirt 
openstack vm accessible lan 
openstack swift - alias symlink stored object 
get jpg file cloudfiles rackspace using openstack.net 
openstack-keystone failing create tenant 
docker driver openstack coexist libvirt.libvirtdriver 
openstack x-auth-token token 
rpc calls openstack 
change image memory openstack 
openstack dashboard login error devstack method 
openstack decide hardware capacity 
storage components openstack 
swift-open stack storage system manage file metadata 
“no module named memcache” error openstack newton keystone accessing murano dashboard 
heat autoscaling - scaleup - change flavor 
use ansible without ssh keys openstack instances 
openstack raspberry pi 2 closed 
openstack keystone nova-network inactive reboot 
recover deleted repository deleted branch 
jclouds openstack illegalstateexception expected begin_array string 
assign floating ip openstack instance using heat template 
openstack python-novaclient 
wilma pep proxy keystone - valid access token found 
installation openstack virtual machines multi-node architecture 
fiware keystone api create user 
keystone connection fail 
specify post-installation script creating instance openstack python-novaclient 
kubernetes unable connect pod master 
interaction components openstack 
configure object-store opscenter 
print python openstack object json 
recovering terminated instances openstack 
running devstack script ./stack.sh 
xargs command multiple parameters 
ubuntu 14.04 openstack installation failure 
novaclient - create cloud server using “boot” - error badrequest multiple possible networks found 
openstack glance working 
error occurred installing openstack setting force=yes 
set internal ip range devstack 
override handle method selfhandlingform openstack horizon 
openstack single node vs multi node 
jclouds openstack creating neutronapi object throws com.google.inject.configurationexception 
getting list block storage volumes rackspace using novaclient python api 
perform bulk-delete openstack swift 
ca n't find rackspace cloud server via openstacknetsdk 
openstack error puppet installing redhat 6.4 
set docker container 's ip 
create hybrid cloud aws ec2 local openstack instance 
couchbase xdcr openstack 
listening notification creation instance openstack 
installing cloudfoundry gives authentication error openstack debug opensource project namely bosh openstack 
use nova curl authentication required 
delete dynamic large objects openstack swift api 
openstack rackspace cloud files .net sdk 
localrc settings openstack 
openstack create volume via nova api 
amazon s3 bucket policy equivalent openstack swift world 
openstack 's virtual nodes permanently paused state 
spread number instances across availability zones heat 
passing segment already open filestream stream parameter 
configure openstack swift without openstack projects 
horizontal autoscaling nodes minions kubernetes openstack 
openstack novaclient python api working 
cloudidentity openstacksdknet could find user 
openstack neutron update port vif-model using update_port api 
unable connect jenkins plugin openstack 
openstack small image tiny flavor 
fiware object storage authorization error 
delete resource type openstack 
assigning floating ip load balancer heat template 
accesing instance ip lan openstack 
devstack installation swift enabled 
run openstack chef cookbook 
keystone ssl config intermediate ca cert 
filtering files container rackspace cdn 
openstack vm instance shutoff minute 
openstack api createinstance 
nova-api run different compute nodes 
remove duplicate service help nova-manage command 
openstack networking ca n't ping/ssh from/to vms 
update project running cluster 
able launch openstack instance terraform 
get uploaded object url using openstack4j 
commanderror installing openstack 
unable create alarm aodh alrming service using openstack4j 
got oslo-vmware install error 
openstack - iterate comma_delimited_list using index os :heat :resourcegroup 
delete objects folder/directory open stack swift using rest api 
create new role openstack4j 
integrate orion dbaas like trove openstack 
access service subdomain kubernetes 
http error 401 configuring keystone 
openstack get client 's ip python api 
rails using fog paperclip bluemix object storage v2 
network nic ordering heat template 
openstack4j list roles identity keystone v3 
authenticating restricted user cloud files rackspace using openstack.net 
enabled services devstack 
jclouds openstack > java.util.nosuchelementexception apitype compute found catalog 
unauthorized error using openstack api ca n't get authentication token keystone 
rdo openstack allinone add one mor compute node 
finding pid 's virtual machine openstack 
openstack create super user like admin 
get ip address computes nodes openstack network 
containers objects getting deleted file system swift 
ovf existing virtual machine openstack 
openstack keystoneclient get user name 
cloud infrastructure modeling capability 
list objects blocks network 
create volume/instance image error 
swift accessible openstack installed 
openstack “no hosts found map cell exiting” 
suds.client import client “import client” 
temporary public url ibm bluemix object storage service nodejs 
devstack installation failing tempest test-requirements 
difference cold hot reboot openstack 
python3 buildout error boss billing openstack software 
split huge code submit several patchsets gerrit 
fetching floating ip openstack vm 
swift list -p variable setting 
containerization openstack services kubernetes 
api get keystone notification events 
openstack port quota effecting 
delete branch using git review 
error deploying object storage bluemix using openstack4j 
error installation openstack kilo centos7 
jclouds + openstack nosuchelementexception key openstack-neutron list providers apis 
floating ip pool found 
installing solum openstack/devstack 
“getting started” issue joss javaswift 
difference ubuntu openstack openstack kilo build private cloud 
'cf app ' shows always 0.0 cpu - value measured 
openstack-ironic address 'ssh_address ' driver option referring ironic node pxe_ssh driver 
assign 2 fixed ips subnet server via heat 
openstack release name 
openstack login admin retrieve server data different tenants via python 
openstack ceilometer perform realtime instance monitoring 
authorized list projects keystone v3 
hyper-v openstack 
ansible module manage openstack load-balancers openstack lbaas 
cloudify “could determine type file ”sftp //root *** 192.168.10.xxx/root/gs-files“.” 
devstack services 
install openstack use openshift 
sahara service failed start devstack installation 
building cloud scratch using openstack 
test provider credentials valid apache libcloud 
openstack horizon rbac setting custom permissions 
devstack juno nova cells enabled failing run stack.sh database error unknown database 'nova_cell ' 
ssh key injection openstack 
devstack installation failing python update.py /opt/stack/keystone 
multiple kvm guests scipt using virt-install 
installation openstack ubuntu 12.04 
openstack django panel 
remove port external bridge openvswitch 
output script completely finishes vagrant provider=rackspace 
failure trying start devstack 
launch azure instance openstack 
create custom role openstack provide read-only access single tenant 
access hadoop remotely 
fabric fails name lookup failed celery run eventlet 
drop incoming packet openvswitch integration bridge specific ip 
jclouds create tenant openstack 
admin password create server api response working 
backing openstack swift data 
error using phpopencloud - openstack 
extract data < bound method server.interface_list < server ubuntu12_1 > > type object 
upload photos files openstack object storage swift 
wsgi post request 403 forbidden 
reload nova.conf without restarting openstack 
virtual interface virtual machine openstack 
openstack instance status “error” 
avoiding sensitive info openstack config files 
cloudfoundry grizzly keystone devstack install configparser error keystone.conf 
track functions used python command 
openstack hosted environment 
using aws-cloudformatting-resources heat & openstack local cloud 
get instances back reboot openstack 
open stack assign ip virtual machines 
neutron flat networking dhcp 
openstack delete instance dashboard 
differences vm image docker image duplicate 
unbound variable url printing bash/shell 
null value json interpreted python openstack api 
private cloud stack without hardware virtualization 
clone branch git 
x-storage-url x-auth-token 
bluemix - object storage - node.js - pkgcloud - openstack returns 401 
cloudfoundry compatible docker/coreos 
setup openstack havana docker driver 
create security group openstack cloud foundry 
openstack modify projects quotas 
force expiring openstack swift tokens 
rackspace cloud files openstack swift tempurl example vb.net 
setting pydev use openstack 
communicate openstack api using servlet 
automation networks using ansible openstack 
./stack.sh failed installing openstack 
best shared storage specifications openstack 
“zypper install python-mysqldb mysql-server” packages found 
backing openstack swift object store 
cinder creating volume 
new fog models missing gem 
cloudify 2.7 openstack icehouse 
openstack instance accessible lan 
post request /v1/ < account-name > openstack swift create account 
openstack-nova-volume dead make alive 
assign openstack network instance create fog 
install apache php mysql different servers 
guide writing new openstack module/project 
guidance using openstack launch instance via php automatically build instance depending request 
list available containers swift using jcloud 
unable run unit tests openstack nova 
openstack neutron command workflow 
devstack installation - floating ips 
partition drive automatically/set os-dcf diskconfig auto nova 
org.jclouds.openstack.nova.v2_0.domain.servercreated api 
openstack quantum vm able ping br-ex outside network 
run openstack-swift java api 
nova image-list error unauthorized http 401 
pass ssh key using jclouds openstack 
changing font openstack horizon 
python spider openstack login 
installing docker devstack 
connecting android openstack services 
openstack swift token expiration swauth large file upload 
vt set bios showing vmx cpu flag os 
start openstack machine guest without authentication 
http headers incorrectly treated case sensitive jclouds causing openstack x-storage-url fail 
django file download issue 
openstack secures token generated keystone 
trouble openstack ceilometer client 
failed update root certificat cacert.pem 
use keystone api extenstion send json format request “add global role user ” 
measuring cluster health openstack swift 
process close instance openstack 
nova compute network unable contact nova service restart manage services 
cloud object storage 
wso2 elb use cloud service cluster iaas internally 
use carrierwave openstack swift installation 
openstack exception adding new nova-compute node 
openstack first program 
openstack python novaclient require authentication 
modify live migration algorithm used openstack newton 
get users particular project openstack 
error “/opt/stack/logs/error.log file directory” - devstack deploy ubuntu 12.04 lts 
devstack - allow ssh guest vm devstack 
deleting openstack instance powershell 
mirantis fuel 9.2 “failed tasks task openstack-cinder/7 task openstack-cinder/6 task openstack-cinder/8 ” 
read current levels neutron quota usage openstack project 
kolla aio deploy fail hostname resolve ip address starting rabbitmq container 
scale up/down slurm cluster 
error configuring devstack compute nodes service n-net running 
detach volume server openstack python sdk 
kubernetes cinder volume plugin works 
solution multiple file upload using multer pkgcloud openstack 
kubernetes minion registering api server openstack hostname rejected 
nova instance reach metadata server 
openstack project list work - keystone authentication seems fail 
mysql error openstack database keystone 
get error “could find domain default” import core murano library manual installation 
openstack swift proxy-server malformed request s3 
enable permanently cors ovh object storage openstack swift 
query openstack api sr-iov 
map user domain federated openstack federation 
ruby openstack swift client works v3 identify version 
cinder volume create valid host weighed hosts available 
joss java client openstack swift determine proxy servers use 
openstack could delete security group says use 
possible stream file java server openstack swift 
multiple neutron nodes one node attached external network 
error “requires ruby release ” occured using packstack install openstack one 
pycharm remote debugging unable retrieve variables 
inject file instace linux running state 
openstack - keypair extension sometimes unavailable 
using https calling waitfor method setting openstack nova server 
openstack sdk create server one ip address 
openstack heatclient api query heat output-show 
openstack command-line clients macos - keystone command found 
api python neutron client common exceptions unauthorized 
basic salt-cloud config file openstack/nova mitaka 
openstack-packstack - mariadb error 
django openstack horizon - admin templates rendering properly 
creating clone vm openstack 
read objects openstack swift based object metadata 
swift could start 
openstack service find reply exchange rabbitmq 
curl received http code 404 proxy connect 
download object performance jclouds swift client 
make restful java client send get/post request openstack 
get stack status rest api without mentioning tenant/project id devstack 
openstack apis - error unable find matching endpoint requested service 
unable start corosync cluster engine 
paging folders files swift object storage 
openstack keystone project creation using hot fails 
perspective shell match instances full hostname 
copy binary file instance 
openstack heat dotnet sdk 
openstack sahara installation devstack juno 
openstack working instance connecting nova.clouds.ubuntu initiate connection 
openstack identity admin client find users roles services 
get instance usage data billing ceilometer client 
openstack devstack installation error openstackclient.shell exception 
upload openstack ubuntu cloud images aws ec2 
openstack instance network n't working 
get repomd.xml local yum install openstack-packstack 
running commands.getstatusoutput returning expected results apache 
able ping/ssh instances 
adding sign-up option openstack grizzly 
os-svc-daemon adding new service upstart 
openstack nova docker implement security group 
devstack metadata service - preset configuration 
want copy file host guest spinning vm .. pointers 
egress option rackspace openstack 
openstack create image 'connection aborted ' error 111 'connection refused ' 
could fetch remote template - tempest 
access instances openstack aws 
openstack specify behavior delete objectstore get download object progress 
generate config file oslo.config 
openstack auto-pilot - hardware/zone list available 
openstack4j error 400 authenticate got unexpected keyword argument 'username ' 
couchbase .net sdk connection issue “unable locate node” openstack 
openstack nova oslo error 
mysql error installing wso2 paas 
improve download/delete speed large chunked files openstack swift 
tempest installation openstack 
devstack unable ping vm 
re-use previously assigned floating ip openstack neutron using horizon 
large image upload horizon saving state forever 
errors use ./stack.sh devstack 
optimistic concurrency rackspace openstack cloud file 
turn openstack icehouse source code github .deb file installing 
installing novadocker openstack 
connecting vagrant consul server openstack 
keystone v2 - get token without using userid/password 
custom ceilometer metrics 
cpu stats captured docker containers 
launch openstack instance single nic 
cloud-installer ubuntu openstack 
openstack4j - maven unresolved dependencies 
cinder volume cant attach detach 
celery oslo config working together 
cloud servers identification 
framework cloud storage db simulation local setup 
openstack glace unknown command 
keystone returning is_admin 0 every case 
refreshing tab content openstack django webapp 
openstack swift module redirect client region location 
expecting authentication method via either service token 
operating sytem install openstack 
error occurred installing openstack ./stack.sh:137 die 
get devstack juno 
get `id` field output openstack creation commands shell script 
nonetype ' object attribute '__getitem__ ' 
docker driver openstack 
possible run openstack laptop/desktop 
openstack api implementations 
issue json.dumps adding extra \\ variables 
openstack devstack installation stalls git call failure 
error openstack dashboard authentication 
openstack-keystone failing start 
best tool deploying production-ready openstack cluster 
add security group vm via nova-client 
append data object openstack swift object storage 
objects openstack swift immediately available 
openstack - nova client - retrieve servers particular tenant 
openstack installation failed devstack stack.sh 
openstack heat & ansible vm spinup app deployment 
import pre-built vm images bluemix 
package openstack horizon dashboard plugin correctly 
python library used write rest service python closed 
couchdb cartridge responding docker image 
devstack/openstack create tenant 
python class method 
trace particular terminal command 
openstack installation 
change openstack horizon 's login page 
why/how certification fail auth_url/auth/tokens “no file” error 
“unauthorized request made requires authentication http 401 ” fiware horizon 
openstack project create exits error 
restart ceilometer service 
openstack docker 
packer ssh_private_key_file invalid 
install alluxio1.2 openstack 
terraform run provisioner existing resources 
sanitized input subprocess shell=true python 
create publicly readable object store containers ruby openstack gem 
openstack project quota modification using rest api 
fetching viewing rabbitmq results 
mq used openstack deployment java ee applications 
openstack novaclient error launching instance using servers.create 
using chef openstack would encourage use heat templates 
openstack swift add cache-control expires headers 
change default http port openstack dashboard 
openstack api token lifespan extension 
scale up/out mechanism works openstack cloudfoundry integration 
wso2 / mule vs openstack / cloudstack - differences similarities benefits 
python-swiftclient general usage procedure 
configure openstack packstack juno work external network centos 7 
possible use cloud-init heat-cfntools inside docker container 
failed add image got error openstack glance 
cloudfoundry installation 
importerror module named persistence.backends.sql 
mobicents jdiameter wrong example git 
openstack api services communicate services 
openstack cluster must machines processor architecture 
login username password vnc access 
authentication token issued identity service expired keystone openstack icehouse 
generating temporary uri object using blobstore 
nova error errno 111 connection refused 
emr ec2 openstack please clarify 
cant ssh instance created openstack cloud 
openstack harproxy configure file 
openstack permission denied generate picture python 
change ip address devstack installation 
swift response scenario file overwriting deletion happened simultaneously overwriting first deletion 
cloudfilesprovider updateobjectmetadata issue 
create object content-type “application/directory” inside rackspace cloudfiles container openstack.net 
installing openstack puppet 
error devstack script nova-api start 
devstack - changing ip address installation 
configuring openstack in-house test cloud 
retrieve list servers rackspace via openstack nova client 
unable locate package libvirt-clients error ubuntu 
disconnect openstack 
openstack nova api live update 
new openstack cli command performing router cleanup 
scope name use jclouds scriptbuilder 
packer deploying openstack images 
make nova service state test purpose without shutting node 
openstackcli access ` -- property` 's cloudinit script 
couchdb 2 run openstack 
disable authentication openstack swift 
openstack launching instance error 
“heat.stack.list ” list stack within openstack “admin” cred 
remove host ip address 
devstack failed create new centos instance 
copy swift object storage container new openstack service 
openstack add security rules failed due multiple security groups named “default” 
openstack training-labs - vboxmanage.exe error invalid machine group 
openstack cloud-init set hostname vm-name 
openstack sdk - create image kernel id ramdisk parameters 
install openstack importerror could import settings 'openstack_dashboard.settings ' *** module named angular_fileupload 
“shade required module” even though shade installed 
dns name virtual machine instance created ibm bluemix 
would like create single supe vm much processors possible openstack environment 
openstack mitaka command line client takes long time respond 
installed devstack vm reboot n't lose 
define keystone_user openstack/puppet-keystone via hiera 
perform local shell command openstack hot template file 
one undefined variables 'dict object ' attribute 'ansible_ssh_host ' 
root password centos image installed via openstack 
free nbd devices installing multiple instances openstack 
openstack installation - git pathspec error 
install openstack 
mtu 1500 fragment packets 1472 bytes 
anyway openstack attach physical disk directly 
ssh openstack instance ansible ca n't — differently 
open stack volume wo n't attach 
set floating ip vm using apache brooklyn - floating ips required options extension available error 
openstack swift url use use case 
upload large sqlite file object storage using openstack 's swift via python 
difference local_settings.py settings.py openstack_dashboard 
total size total number object ibm bluemix object store 
retrieve resource list openstack compute engine 
openstack heat webhooks up/down-scalling point internal storage ip 
vagrant virtualbox openstack - better way 
install swift without keystone 
setting multi-user virtual environment jupyter 
openstack permission denied publickey ssh'ing vms 
cloudify 3.3 - use existing network floating ip 
unable ssh devstack instance external computer 
glance image registration remote server 
network issuse scaling deployment cloudify 
hadoop top openstack extra features get 
create hdfs file using pywebhdfs get error errno -2 name service known 
urllib3 documentation telling 
sahara service failed start juno python error 
upload image trystack server using packer tool 
openstack horizon user-interface communicate openstack rest api 
exist openstack api implementation jclouds 
bosh-lite installation openstack 
python swift client object count mismatch actutal objects cloud 
get newly created machine details vagrant-openstack plugin 
restart nova services devstack 
kubernetes openstack integration 
openstack access token refresh 
mesos openstack vm 's public ip 
openstack extend nova host-describe command 
mysql cloud db api 's closed 
openstack - selectively deleting images based criteria 
integrate openstack instances local network 
openstack unable upload image image service 
conditionally check state openstack instance 
edit physical_interface_mappings openstack neutron network 
creating virtual machine bluemix giving error importing ssh key closed 
machine < ip_address > started enough memory 
list objects virtual directorys using rest api openstack object store swift 
live migration openstack 
ceilometer rest api recent results 
possible host meteor.js application hp helion openstack providers 
install devstack much memory use 
openstack api - nova vs ceilometer know instance state 
find complete list devstack services 
use response url recieved create server api response openstack 
get authorization token ceilometer api openstack 
relationship stack id stack name openstack heat 
accessing user password user table keystone database correct form string 
use jclouds openstack4j openstack 
openstack controller nodes deployed virtual machine 
ceilometer group metadata 
qpid amqp tcl 
openstack cloud identity service nova service swift service vs java application 
bad uri trying connect openstack 
configure cloud-controller.xml stratos 
new django need fill form intial data get_data 
custom openstack centos image set password use 
error installing icehouse openstack using devstack 
urlsplit error novaclient authentication 
kitchen open stack gives “excon.error.response” 
use openstack4j osgi platform 
want create open source cloud storing sharing system start 
creating glance image 403 forbidden access 
couldnt install openstack-devstack digitalocean 
garbage packet received needed prompt connection sudo password error 
connection neutron failed maximum attempts reached 
expecting auth url via either error thrown openstack 
neutron devstack 
creating instance virtual hp cloud openstack cloud using c 
find linux namespace network adapter 
dynamic storage deletion deleteonexit template option 
glance image create stuck saving status 
openstack ca n't locate log /var/log 
migrate form physical virtualized infrastructure 
get associated ip address openstack instance 
ca n't get network-id using quantum_network ansible ad-hoc command works neutron command 
openstack throws nullpointerexception connecting 
openstack nova cloudpipe-create fails 
pkgcloud gce google compute engine 
cant start instance openstack 
exits wadl file api 's openstack 
instance os find attached volume 
error import novaclient library python 
scilab/octave cloud parallel processing 
importerror module named pycurl - developing openstack horizon plugin 
get openstack json schema responses returns rest api 
attributeerror type object 'managerwithfind ' attribute 'client ' 
failed install openstack glance 
set default keypair openstack user 
make openstack image public available users 
integrate proprietary federated idp openstack 
setting private multi-project test cloud infrastructure openstack jenkins 
internal server error using php cloud files swift openstack 
openstack specific host vm launch 
collect stress test data openstack 
upload zip file openstack swift tsung 
error syncdb installing openstack 
errors openstack cli install windows 
instal devstack ubuntu14.04 faild 
openstack keystone service fails start configfilenotfound exception 
manually assign uuid openstack machine images created packer 
change openstack swift source code test research idea 
connection remote notebook openstack spark 
freeze package python program based openstack 's taskflow pyinstaller 
graphical way monitoring kvm instaces openstack 
installing openstack fuel mirantis ubuntu 16.04 xenial experiencing time using launch script 
networking openstack instances 
openstack rest api call returns forbidden 
openstack messagingtimeout timed waiting reply 
find operating system installed switch 
launching openstack instance bootable vm 
display vmware integrated openstack 
apply cloudbase-init windows .vhd file 
service know connection failure via 
userdata params got changed stack creation openstack 
use openstack api v3 list accessible volumes 
openstack critical keystonemiddleware.auth_token - unable validate 
openstack mitaka assign securitygroup lbaasv2 port using heat 
create self-authorized tls certificate ubuntu apply openstack 
dashboard login admin account 
public virtual ip responding - namespace 
custom goodness_function manila 
migrate vm image vmware openstack offline 
openstack kilo vmware vm console error console currently unavailable please try later 
import converted qcow2 openstack error 
usage variable role openstack-ansible 
parametrized terraform template 
devstack contextualversionconflict 
openstack telemetry aodh calculate time boundary right wrong 
deploying openstack instances end user 
get proxy node openstack swift cluster 
openstack-swift synchronize time different servers 
communicate instances openstack 
keystone database - credential table 
error installing openstack packstack 
necessary jenkins openstack system 
gui devstack instance working solution 
create instaces using jenkins openstack 
openstack kilo netrun dhcp reuse ip 
openstack install - create openstack project 
missing value auth-url required auth plugin password 
make devstack persist changes reboot system ubuntu 16.10 
tempest cleanup faild first tenant 
murano-test-runner connecte keystone 
openstack keystone reduce number workers 
saving instances openstack 
attach volume failed post `os-initialize_connection` 
openstack devstack unable retrieve volume list even running rejoin-stack.sh 
generate openstack swift access token another user 
missing entity id environment - keystone sp 
run multiple web applications ip using different ports 
openstack tempest cleanup delete tempest objects 
cloud foundry training/tutorials online 
message format openstack message queue 
load stressing web applications deployed openstack instances autoscaling group 
neutron agent list displayed 
python requests 2.13.0 could work oslo 1.19 
problems configuring mirantis openstack volume backups write external swift object storage 
browser-based upload openstack objectstorage 
configuring keystone service provider 
automate creation vm docker containers running inside 
optimize media transcode using cloud computing openstack 
enable/use upnp openstack neutron 
cloud foundry openstack private / on-premise cloud 
rezizing openstack volume remain old size 
ca n't launch openstack horizon dashboard ioerror errno 13 permission denied '/var/lib/openstack-dashboard/secret_key ' 
pycharm remote debug openstack nova-api errors 
kubeadm init -- cloud-provider=openstack 
openstack api querying whether enough resources available instantiate image 
accessing openstack swift deployment ec2 vm remotely 
openstack-ansible - error launching instance 
openstack-ansible - restart services 
openstack changing scheduler_max_attempts nova.conf affect anything 
error unable retrieve instances 
solve following error installing openstack using rdo 
tacker getting error resources.vdu1_flavor 'sessionclient ' object attribute 'set_management_url ' 
fiware health region configuration 
libreswan ipsec vpn fails establish phase 2 
keystone v3 api “401- unauthorized” error list users tenant default-admin credentials 
increase limit rdo stack volume creation size 
access openstack user-data error log automation 
openstack linux-bridge agent 
install openstack client windows 
query jenkins windows 
error creating sample service project openstack newton 
openstack heat-api heat-api-cfn fail start - error unable locate config file 
openstack magnum create stack times 
get rid content-disposition prefix added files uploading 
using repo pypi pip 
++./stack.sh main:1351 devstack ./stack.sh fails 
get remote ip openstack virtual machine accessed via nat 
execute command hosts physical machines via openstack code 
unable launch openstack instance jenkins 
build config-drive enabled image openstack 
convert jiffies percentages virt plugin 
error changes open stack horizon 
extend logical volume snapshot 
get right osclient token openstack4j java api v2 v3 
upload file open stack storage service android 
openstack managed services support 
unable deploy openstack vmware - datastore selected cluster 
os :heat :softwaredeployment staying stuck create_in_progress status 
output format unixbench 
create vm multiple nics terraform openstack 
run script vm 's first boot using openstack 
error openstack server list command 
low interval ceilometer meter sampling 
openstack installation using devstack - liberty 
programmatically obtain openstack resource usage metrics python 
drop mysql database 
installation openstack juno vmware workstation 12.0 
get information floating ip pool 
availability zone list order determined nova api openstack 
error creating vm openstack liberty using apache brooklyn 
accessing spark web ui remotely master running virtual machine top openstack 
openstack network namespace issue network 
resteasyproviderfactory classcastexception openstack4j 
waiting inputstream jclouds java 
check number floating ips available pool 
error codes bluemix/openstack object storage 
automatically update access token 
installing systemd inside ubuntu14.04 docker container - possible 
possible add 2 fixed internal ip assign 2 floating ip vm outside access 
node 1 ntp-check status error 
openstack newton dashboard unreachable centos 7 
openstack attributeerror 'module ' object attribute 'version ' 
kuberntes master starting openstack heat 
allocating resources mirantis 
oslo-config-generator example working 
running horizon python manage.py 
import error module named cmd.manage 
kube-up.sh failes openstack 
able bring kubernetes cluster openstack 
update metadata flavor openstack using python novaclient 
keystone command found 
getting error volumemanager ' object attribute 'list ' trying connect openstack via cbtool 
error status server created openstack compute api 
reuse openstack keystone component standalone identity component java based platforms 
'stack ' user openstack secure 
files 404 error extending volume size swift saio installation 
create open stack instance image .vmdk file volume 
pig jobs failed sahara 
skip test particular file 
use case/scenario stevedore driver/ extension/ hook 
get ip address instance use floating ip openstack 
configuring cloud minimum configuration oracle virtualbox using openstack software 
openstack newton installation using packstack error 
add segmentation_id ovf file 
add slave node openstack kubernetes cluster 
uninstalling openstackclient restore previous glance keystone version 
'module ' object attribute '__version__ ' openstack /devstack installation 
start vboxweb-service.service virtual ubuntu vm 
red hat osp10 deploy fails node profile tag even though configured 
devstack error running git remote update gerrit 
change swift ip address localhost 
openstack sahara error dashboard 
openstack monitoring 
using profiles keystone saltstack states 
oracle database openstack 
./launch_8gb.sh script disrupted even iso present required folder 
openstack horizon - “unable retrieve usage information” error vm shutdown 
openstack mitaka session timeout 
analysis cloud resources allocations 
cinder driver failed start 
extend storage swift openstack service 
openstack want map 1 1 instance network physical network interface 
cinder volume device_name honored cloudify orchestration 
openstack sahara error 
cinder-volume service going 
packstack installation root user 
openstack allow api access vm 
kubernetes external openstack load balancer working 
upgrade openstack fix plugin 
devstsck installation error failed creating password plugin ubuntu 14 
vary creation/not creation node instances “install” workflow 
saio swift one - access external ip 's 
install openstack neutron service multinode ubuntu 14.04 
rabbitmq working openstack 
ansible wait_for module non-unix systems 
openstack sahara image register error 
devstack - horizon dashboard admin user able login 
configure kubernetes trust openstack self signed certificate 
able access openstack-horizon 
openstack cache machanism 
unrecognized arguments floating-ips fixed-ips nova quota-class-update 
dynamically configure glassfish jms resources 
secure devstack public host 
python api change openstack glance using suds 
openstack multi-site versions difference 
getting error world dumping installing devstack 
systemctl services loaded failed centos 7 
private object public container openstack swift 
http 403 keystone /v3/users api 
nova-compute service state 
using terraform add openstack security group instance managed terraform 
maven build asks log file permission denied 
multi node devstack installation 
mirantis openstack installation physical machines 
puppet resource ordering working 
jenkins - openstack plugin retry launching instance different config first attempt fails 
nova server create & server affinity group create & map using python novaclient 1.1 
following tutorial openstack instal guide 
openstack error unable retrieve instances volume list snapshot list etc 
unable boot new instance openstack juno neutronclient issue 
openstack cli failed ssl verification 
make ovs seperate traffic different vms vms attach vlan 
file upload failure using swift storage 
api jclouds call “/os-cells/capacity” 
baremetal dhcp ip address tenant network 
error deploying vmware integrated openstack 3.0.0 vsphere 6.0 
openstack instances doesnt start 
python api failure setting openstack image metadata 'tags ' using glance 
map vm internal network 
dynamically change autoscaling instance names 
way provide run parameters docker container image starting container openstack nova 
devstack unable access dashboard fresh install 
conjure software works order send graphical symbols via ssh 
adding compute node…libvirterror internal error client socket closed 
glance n't work due authentication fail 
authorization scope federated user 
openstack-install command 
admin endpoint identity service found 
httpd.service start installing dashboard installing openstack centos 
heat fix dhcp port ip network 
manila multi-tenancy share 
adjust column width openstack horizon 
pass josn/list values template heat user_data 
mount openstack container docker container 
normal user create encrypted volume openstack 
add build task gulpfile 
create ovs bridge centos openstack ipv6 interface 
juno openstack dashboard working ha-proxy shows memcached service 's availabilty 
parameters sent auth_version_v1 auth_version_v2 auth_version_v3 swift client-openstack 
amqp server localhost:5672 unreachable errno 111 econnrefused 
authorization failed resource could found http 404 
openstack gerrit error git rebase merge 
installing devstack error creating password plugin keystone 
ping internet launch instance cirros mitaka 
autodiscovering hdfs nodes openstack instances 
openstack swift returns 500 non-existing object 
best way access openstack cli windows 
best way convert .iso win2010 & rhel7 qcow2 + add apps automatically 
openstack behind haproxy provide issues 
vagrant monasca vm mounting failed error device works sudo 
whats difference setup.py setup.cfg python projects 
storm version openstack 
segments ' count minimization openstack swift 
openstack swift ceph backend radosgw 
kubernetes- minion talk master 
nova supporting using keystone v3 
facing errors uninstalling openstack devstack 
unable establish connection http //controller:35357/v3/auth/tokens 
openstack devstack insatallation dashboard login default credential ca n't find username 
openstack one environment instance ca n't access service running docker container 
openstack instance fails spawn 
get token id keystoneclient v3 
facing errors openstack-devstack installation 
facing errors connecting openstack github repository 
error message “invalid flavorref provided.” trying create instance 
“connect time out” error jclouds via proxy 
vagrant checking vm created 
custom names resources cluster created using heat templates openstack 
openstack python sdk - glance return image md5 
centos httpd apache http server fails start 
unable launch instance openstack 
openstack access vm instance console script run dhclient reboot commands 
docker failed build openstack 
openstack authorization 
order use sql docker container deployed cloud 
choose pxe setup deploying opnfv arno via fuel master node 
get project id using project-scope authentication openstack4j 
error cloudify bootstrap openstack route host 
using openstack api access softlayer resources 
openstack swift org.javaswift.joss.exception.commandexception unable open input stream uploading 
softlayer - running jumpgate 
openstack unable start keystone installation step 
openstack keystone openid connect provider returned error 
add ironic db version file install n't copying db version file 
put method working openstack 
invalid user name password horizon 
stop openstack nova logging locally host 
live migration failure unable execute qemu command 'migrate ' migration disabled failed allocate shared memory 
handlers could found logger “keystoneclient.auth.identity.generic.base” 
configure server groups openstack ansible 
ceilometer api filters n't work multiple fields 
get auth-token openstack 
openstack cinder volume create valid host found weighed hosts available 
install devstack ubuntu 14.04 single node desktop environment 
bosh n't recognize deleted vm 
error oslo_messaging.rpc.dispatcher req-xxxxx exception message handling 'metadata ' 
“table 'nova.services ' exist” error openstack 
openstack devstack heat template create vm app 
unable connect open stack object storage java application 
openstack compute-node communicate/ping vms run 
jcloud nova unable find floating ip pool extension 
delete nova network - network address already use active instance 
openstack instance use entire hard disk 
openstack instances update 
starting openstack instances programmatically 
python module importation error 
create docker container openstack 
devstack installation fail python update.py /opt/stack//cinder 
openstack participating localization 
bluemix create vm public ip 
launch instance non-glance image openstack 
network block device - receive control failed result -32 - kernel 3.16.0-41 
's default timeout func 
meaning gettext 'some text ' openstack 
python-keystoneclient ' deprecationwarning 
failed launch instance 
openstack token get token curl ca n't urllib 
sequence setting keystone v3 
openstack make insstance acessible different machine 
choose specific network among multiple networks 
creating private cloud practice 
openstack allocate ram guest 
openstack swift cors configuration 
ceilometer identity.authenticate.failure 
error bootstrap management vm 
's possible customize timeout values 
dependency injection keystone middleware openstack 
migrate shutoff instance specific host 
adding node maas controller internet 
openstack juno jclouds api generating urisyntaxexception extra space response 
anyone know 's data source http //logstash.openstack.org 
unabe get openstack login prompt 
devstack default username pass showing console ubuntu cloud image 
ways install heat resource plug-in 
ovs internal port / veth pairs bandwidth limitation 
achieve high availability instance openstack 
multiple processes neutron-server juno 
password key salt-cloud use login newly created openstack vm minion 
deploy vagrant vm openstack image vagrant 1.4.3 provider virtualbox 
run openstack devstack asus chromebook 
juno glance like iso image 
pass parameters powershell script userdata 
able create alarms using openstack api 
iptables nat/masquerade allow openstack instances access sites external laptop 're running 
nosuchopterror running openstack functional tests 
libcloud upload image 
updating exisitng security group using heat 
ceilometer - meter getting active tenants 
fix invocationerror bug jenkins 
openstack router stuck build 
openstack getservers returning typeerror using pkgcloud nodejs 
partitioning instance openstack 
use existing lvm vg devstack 
resolve error 'you allowed start instance < instance name > ' openstack 
docker ubuntu container ping archive.ubuntu.com apt-get 
executing openstack-rally test cases 
arguments converted string formatting assign values 
keystone digest authentication 
openstack java programming 
ping failed second ip openstack instance 
openstack admin impersonate another user create vm 
possible use ssh tunnel openstack4j 
query ceilometer config 
openstack - could find resource admin error lauching multi-region 
openstack cinder error liberty 
open daylight project build issue 
solr slow response server connecting via external ip address 
openstack instance access external network closed 
openstack liberty keystone instalation unmet dependices 
openstack instance reachable due metadata issue liberty 
creating compute openstack cloud 
warning neutron_lbaas.services.loadbalancer.drivers.haproxy.namespace_driver - stats socket found pool 
cloudify 3.3 - openstack ssl3_get_server_certificate certificate verify failed 
pass user_data script python openstack heat-api client 
openstack swift pid files n't correspond running processes ' pids ca n't restart services - port already bound 
sync / update local volume data openstack server 
devstack unable create neutron network 
devstack error request made requires authentication http 401 
ceilometer alarm error - heat openstack 
port forwarding openstack instances 
specify disk size openstack create instance 
kernel panic - syncing fatal exception interrupt openstack vm 
ceilometer healthnmon measuring vm stats openstack 
openstack floating ip assocation work underneath 
use strings parameters openstack heat 
pycadf error installing devstack kilo ubuntu 
packstack fails - error unable connect mongodb server 
php server-to-server communication lan 
sharing resources two independent openstack cloud setups 
terraform error launching server openstack 
openstack api v2 tenants returns one tenant 
know type secret returned barbican get /v1/secrets/ call 
problems launching vm openstack 
openstack create new instance ubuntu 
devstack installation failing uploading fedora image 
update network subnet quota openstack installation 
openstack support dual-stack implementation 
retrieve id network openstack api shell script 
designate network id creating network 
openstack juno devstack - specify new neutron plugin configuration files 
trove guest agent starting 
rdo unable boot vm disk size specified 
fixtures openstack keystone 
rabbitmq start change config file 
connect android application openstack cloud 
horizon dashboard internally fetch data using angularjs 
swift balanced 
nova errno 111 connection refused 
correct paradigm manage client openstack environments 
aims cinder swift openstack 
django ioerror errno 13 permission denied 
devstack juno networking route networks 
error install trove openstack juno 
get nova client v1.1 use ssh tunnel retrieving server list 
installing libvirt virtual machine 
create private cloud using openstack file sharing 
end point mean terms ceilometer api python 
openstack horizon ui customizations 
openstack-neutron-lbaas 503 returned haproxy 
openstack swift requests blocked eventlet.green.httplib 
openstack neutron two physical nodes 
single sign openstack web app 
communication openstack vm 
docker cinder possible openstack 
openstack designate '_authtokenplugin ' attribute 'register_conf_options ' 
integrating openstack dashboard inside web app 
route host found - openstack 
dart lang app open stack / docker / vagrant 
find ip address virtual machine running 
logging jclouds still prints retry connection error console credentials 
automatic provisioning open stack vm docker containers 
issue starting openstack nova installing using devstack 
unable install nova compute error command python setup.py install 
list openstack services full info using python api 
cant reach internet instances icehouse 
integrate rhev 6.5 openstack juno 
rabbitmq consumes memory shuts 
setup public rule keystone policy file 
openstack relation controller & compute nodes 
swift juno complains 'account found ' 
openstack docker fails spawn instances 
correct routing table openstack controller 
devstack juno importerror module named persistence.backends.sql 
boot iso reboot installing iso icehouse 
prebuild ready iso openstack all-in-one node 
openstack - web console connection refused 
openstack prevent losing vms 
openstack sdk getobjectsavetofile headers 
rabbitmq openstack juno 
neutron namespaced networks connected physical interface 
reading json xhr.py angularjs using http.get 
need sync .conf file manually openstack multi-node environment 
enable os-ksadm admin extension openstack 
openstack vm network flow 
openstack slow performance compute nodes 
wso2 stratos openstack network id 
odl openstack issue 
deploy spark make highest resource utilization 
listing loadbalancers openstack 
jcloud openstack-neutron exception thread “main” com.google.inject.configurationexception 
get aws account name aws_access_key aws_secret_key 
“access denied user 'root ' 'localhost ' using password yes ” http 409 
create instance already uploaded vmdk image s3 bucket 
failed launch openstack instance 'authentication required ' trying create port 
routing openstack 
correct image spin vm openstack using vbox host centos 6.5 allinone “guest” 
openstack instance getting ip getting ip 
“unrecognized auth response” every heat stack-create request 
error get flavor list java.lang.nullpointerexception uri template newly created target must null 
openstack neutron specify network interface associating floating ip 
error failed attach interface http 500 request-id req-xxxx 
keystone get user email 
setup local.conf devstack 
error executing “to test glance upload “cirros cloud image” directly internet” 
apache mesos vs. apache cloudstack 
openstack icehouse dashboard authentication errors using multiple domains 
unable run openstack tempest testcases group 
connect instance directly ext-net external network 
keystone configuration file permissions 
deployment tools used hybrid cloud 
jclouds - specify network interface assigning floating ip 
python - openstack - pickle dump load 
openstack -keystone provide encrypted login 
creating network openstack using jclouds getting error “neutronerror” “unrecognized attribute 'networktype'” 
horizon vm working devices 
openstack ceiolometer throwing unknown error installing 
trying use openstack horizon started server got following error tried search getting anything anyone help 
openstack create multiple floating ip pools single public network 
getting list tenants open stack using keystone v2.0 python api 
way figure machines created specific user 
keystone configuration 
openstack nova modification 
fog openstack ca n't specify network name 
openstack-swift data writing mechanism 
openstack service failed creating password plugin 
fail launch vm xenserver openstack nova 
create trove backup mysqldump file 
“no valid host found” spawning hadoop cluster openstack sahara 
address space intersection networks admin pxe public 
openstack swift download storedobjects/directory local maintaining hierarchy 
packer-built openstack instance stuck “spawning” state 
packer build openstack/gce image docker directory second disk partition 
openstack ovh connection configuration 
docker “/bin/bash” could invoked mounting nfs file -v openstack 
error glance_image cirros change absent present failed command … running 20 seconds 
kubernetes ca n't start kubelet set openstack cloud-provider 
unable install python-openstackclient 
many kinds alarms ceilometer support 
metadata concepts 
openstack liberty compute memory dump 
creating different customer accounts keystone 
openstack instance connect external internet 
openstack lbaas v2 agent horizon menu 
crontab seems execute script script n't work 
foramt objects openstack api respose 
access instance openstack vm instance outside subnent 
openstack create bridge guest vm 
way load config file fly 
customize authentication openstack horizon 
password mentioned keystone authtoken 
include wrap file file content file object using softlayer object storage rest api 
issues installing openstack kilo rhel 7.2 
connect selfservice network directly physical network 
unable create keystone token v2 ldap 
openstack tripleo undercloud installation “could find class :ironic :drivers :deploy” 
qemu-img merge qcow2 delta images 
paramiko executing route add network command 
openstack token curl 
error trying install openstack using devstack 
rdo packstack losing ip connectivity installation 
openstack kilo cinder ca n't create snapshots lvm volume found 
find total number physical cores entire openstack based cloud 
php-opencloud openstack neutron call neutron api using publicurl try connect using internalurl 
valid host found enough hosts available 
sensu novaclient connection issues 
docker-machine openstack / ssh 
compute node cache image 
openstack heat - separate templates 
orchestration heat client 
openstak security monitoring 
openstack swift store rings 
action log file openstack web ui ubuntu 
typeerror 'openstacknetwork ' object support indexing 
installation admin actions extension openstack suspen/resume action 
difference global role tenant role openstack 
install docker using cloud-init 
create instances openstack using vagrant docker 
devstack installation error nova-api start 
invalid openstack identity credentials - glance 
softlayer authentication using openstacknet 
openstack.net sdk access service region 
openstack keystone 3.0 api 
run php process background send email finished 
openstack instances pinging different network 
ec2 openstack google app engine gae rest 
get openstack cinder volume size python bindings 
openshift scaling specific software condition 
ca n't ssh creating instance command line 
error creating container openstack swift 
get multi part object openstack swift using jclouds 
openstack notification service marconi 
creating rings regions openstack swift 
possible measure openstack load ganglia monitoring system 
fatal error class 'size ' found 
apply scheduling algorithms eucalyptus openstack instances 
keystone cert_required nova 
intergration docker openstack via docker heat plugin 
change flovour size vm using libvirt 
openstack service group softwares 
openstack ceilometer shell look detail data 
integrating phpmyadmin openstack trove database service solution 
possible get list database servers available openstack cloud using jclouds 
openflow nginx webserver 
mysql openstack need query advice 
create “something” service openstack cloud 
ubuntu cloud openstack single node - advantages disadvantages 
error distributions found prettytable > 0.7 
puppet unable run services chroot environment 
install sahara openstack 
error installing devstack 
jclouds able get list images 
openstack + chef + jenkins continuous delivery 
devstack — unable lauch instance due nova-rootwrap 
get hypervisor name obj_id table_action 
operation couldn’t completed kcferrordomaincfnetwork error 303 
trying use openstack create management interface manage already created vm 
openstack verify using kvm qemu 
swift + keystone user / password invalid 
dependency injection trouble keystone extensions 
automation create openstack havana script ubuntu server 12.04 lts 
“connection neutron failed” net-create 
create new server using openstack.net 
error running unstack.sh 
puppet openstack havana ha package dependency 
relocate devstack configuration files 
add column keystone model 
openstack cinder controller node make nfs share volumes available compute node 
select network new instance python-boto 
openstack havana installation 
managing openstack java 
efficient way get hadoop running private openstack cloud 
connectiong vms implementing devstack 
running openstack opensuse 13.1 
upload file openstack cloud using nsstream 
json object decoded 
failed launch instance timeout waiting rpc response - topic “network” 
openstack - controller node 
neutron port create 
openstack rdo ca n't connect mysql server '10.0.3.139 
object oriented ajax calls openstack environment 
testing openstack services devstack 
unable download object open stack object storage swift 
openstack swift ubuntu - unable create /var/run/swift 
capture reply rabbitmq message corresponding rpc.call request message openstack 
user creation cloud-init 
openstack error launching instance 
openstack-devstack ca n't successfully run stack.sh 
cloudable java app openstack 
openstack loadbalancer able tget_members filters 
adding new user keystone-openstack using android 
cloud foundry v2 openstack 
find cpu core host virtual machine running 
getting nova:628 die trying start openstack nova module 
configuring openstack mutiple servers running 
assign network vm using jclouds openstack list networks openstack using jclouds 
build openstack images 
instance gets multiple fixed ips 
openstack compute used process images back-end cloud front-end android app 
write rules openstack neutron environment 
python-novaclient source code explanation 
openstack create user without keystone authentication 
make openstack cloud read-only 
conductor api attribute “xxxxxx” 
advise clustering file system storage array fibre channel 
openstack vmware esxi hypervisor 
vpn environment non vlan netwoking openstack 
iam installing swift using link get error unauthorized 
web front-end swift written python？ 
rbac openstack via http verbs proxy 
changing disk size running instance openstack 
set vcpupin guest xml 
openstack overkill ha website stack 
dev stack/ media wiki apache server 
configure openstack nova remote bind server 
sslerror errno 1 _ssl.c:510 error:14090086 ssl routines ssl3_get_server_certificate certificate verify failed 
start openstack-services 
learn openstack cloud computing 
trying integrate ldap devstack ./stack.sh got localrc line 9 keystone_identity_backend command found 
injection keystone openstack 
apply new configuration devstack local.conf closed 
sed place variable replacment line 
printing class object python 
packer sample file openstack 
building private cloud research purposes 
stack openslack 
openstack ubuntu autopilot servers 
keystone client project list display projects 
check particular value class object get value another key 
difference provider network self-service network openstack 
'users ' table nova database 
openstack - hardware requirements 
deploy openstack directly bare metal 
vms able ping virtual gateway 
heat style auto-scaling openstack aws closed 
basic different openstack api & cli commands begineer 's perspective 
cloudify openstack ssl3_get_server_certificate certificate verify failed 
openstack installation without virtualbox 
openstack-cinder resize volume 
error creating neutron client 
aws find connected ephemeral storage object storage block storage 
horizon accessible unplugging ethernet host 
system n't configured run kvm properly vmware closed 
rest api calls done openstack 
openstack neutron - vm router two networks within tenant 
monitoring virtual block storage hold 
ca n't access openstack personal machine 
logs analysis cloud openstack iaas 
openstack first angularjs dashboard 
create vm multiple nics azure 
ca n't ping ssh vm openstack 
openstack swift region affinity account 
ceph set default stripe-count stripe-unit 
install openstack mitaka + ovs bridge + dvr 
openstack networking node mirantis fuel dashboard 
openstack horizon dashboard default password 
ubuntu user missing creating image diskimage-builder xenial cloud image base 
private ip address advertised public dns records federated services kubernetes 1.3 openstack cloud provider 
setting openstack devstack install soft layer bare metal 
devstack script error 
printing neutron ports 
devstack install openstack-mitaka ubuntu14.04 
openstack ping br-ex ca n't reach internet 
function libcloud upload image using openstack 
upload file instance openstack 
coding amqp listening user-facing daemon python 
able set cpuset atribute vcpu element instance xml 
openstack swift handles concurrent restful api request 
openstack nova switching cassandra — pros cons 
message queue openstack grizzly architecture 
find time taken register run vm euca commands using shell script 
change openstack ceilometer mysql reporting password 
openstack grizzly keystoe remove_user command removing users 
use aws :elasticloadbalancing :loadbalancer behind proxy 
error creating volume openstack dashboard 
ca n't use tempurl put method upload 
create image ovf file openstack using jcloud 
error re-running stack.sh 
sql api wrapper python swift object storage 
openstack_swift_installtion 
approaches build high-availability openstack cluster 
glance authenticate admin user 
openstack-folsom keystone script fail configure 
pip uninstall shows package uninstalled actually 
replicate modification devstack openstack production 
saas practical basics projects 
cloud shared memory management openstack 
package found try install python-glanceclient 
encryption algorithms vdi cloud 
add split vmdk glance openstack 
nova client image create 
install stratos 1.5.2 
unable launch windowsxp image openstack 
openstack api - curl request hypervisor information 
proper auth method keystone openstack 
horizon work keystone swauth 
object storage -swift restart problems 
openstack server validator tool 
glance index error openstack 
recover nova-compute crash “networknotfound” 
questions cloud experts - openstack hypertable 
openstack - nova-billing 
good setup distributed monitoring tracking latency/drops network 
fluentd replace rsyslog collecting server logs 
openstack sahara cdh cluster warning alerts 
mirantis fuel nailgun server 
devstack error deprecated attribute newton 
error installing openstack autopilot test drive vmware workstation juju ca n't connect websocket register service 
unable peer probe glusterfs transport endpoint connected 
openstack - windows images based qcow2 file starts 80 full resizing working 
start keystone service 
openstack installing glance python-greenlet install 
jclouds openstack create instance 
fix ` -- os-auth-token expected one argument` glance error 
openstack version better deploye vmware virtualization 
size qcow2 disk file grows rapidly 
make openstack mitaka services persists across reboot 
/usr/bin/systemctl start openstack-nova-api failed 
openstack recover orphaned instances 
add-type 0 metadata file 'system.linq.dll ' could found 
get gateway interface rdo openstack 
curl ca n't list tenants servers 
send requests directly bare metal machine openstack 
connect vm fiware lab ssh connection timed closed 
iptables command bridge openstack virtual network 
parse python module openstack 
openstack visibility availability zone 
difference “mechanism driver” “extension driver” openstack 
copy/paste html5 console openstack horizon 
bdf based pci-passthrough non sriov using openstack liberty 
configuring network chapter 2 basic environment openstack installation guide 
wrong openstack error 
restarting devstack services - safest way 
installing freepbx virtual machine 
glance image-list specific image name 
openstack deployment using kolla erroring 
ca n't create snapshots fallback swift cinder 
elaborate steps openstack installation 
ios swift listing files url 
cloudkitty devstack - setup automation 
httpinternalservererror http 500 unable delete bosh image 
best hypervisor nfv terms throughput 
juno neutron gre tunnel qrouter pinging vm vm getting ip dhcp 
cloudify network 
one try openstack cisco avos 
openstack login instance 
create glance image retrieve token 
could openshift deploy war diy tomcat switch openstack 
stop restart vm without reinstall service 
create vm windows server 2008 sun server sparc 
communication tenant network existing network openstack 
openstack failed launch instance “test” please try later error valid host found. 
openstack cloud billing software list closed 
openstack instance console returning error 1006 
add docker ubuntu 14 qcow2 image 
properly call __init__ horizon workflows.action 
devstack failed openstackclient.shell exception raised python-neutronclient 
glance failed upload image http 500 image status killed 
openstack rejoin-stack.sh script working reboot 
java api downloading openstack images 
nfv openstack 
create openstack using noveclient python api 
kitchen openstack steps/example 
unable share cache vms created openstack order create side channel 
error import xstatic.pkg.angular_cookies running manage.py 
convert video file another format openstack object storage server 
openstack keystone otp 
openstack ceilometer alarm creation error “missing argument \”data\“” 
dynamically add instances compute node openstack 
notifications external systems openstack 
issue swift container relative/absolute path 
place cloud-config file used cloud-init ubuntu image 
openstack python code **kwargs - > help=_ 'password connection nova admin context ' 
docker openstack benchmarking 
cdi deployment failure weld-001414 bean name ambiguous - jar issue 
devstack openstack-dashboard found /etc 
multinode installation swift using devstack 
route host c-class external ip 
kubernetes openstack cloud provider fails panic 
cloudkitty - rating display issue 
kafka slow/unresponsive openstack - troubleshoot 
neutron openvswitch br-int ca n't reach external network br-ex patch 
curl 7 failed connect connection refused openstack swift 
openstackdotnet sdk object storage object versioning 
openstack rally - possible check nova console logs 
install openstack identity service found “cliff.app found http 404 ” 
open stack enpoints api request os x 
getting vagrant synced folders work windows openstack 
use openstack components web app 
openstack - route host port 22 
establish authorized request keystone using python-openstack client api v3 
manage multiple architectures using source-repositories dependencies diskimage-builder 
cloud-init failed write hostname xxxx /var/lib/cloud/data/previous-hostname 
connection timeout openstack4j compute query 
swift authentification probelm try connect via api 
sync command openstack object storage like s3 sync 
openstack swift logging temp url cross domain 
vm unable connect virtual switch vif_type binding_failed 
openstack failed launch instances error valid host found 
neutron openvswitch cant bridge external nic profitbricks losing connection 
extend openstack nova api add new resource 
download specific openstack milestone 
find good reference configuring openstack nova cell environment using devstack 
sh check image exists openstack 
integrate opendaylight openstack opendaylight operate integrated openstack 
write file openstack container using joss 
openstack manila installation fails devstack 
devstack error submitting form please try launching instance 
openstack - stack.sh fails syntax errors 
openstack horizon django button action define 
confused suspend/resume vm openstack 
stats tab resource usage populated ceilometer 
bosh init gives error openstack creating bosh vm script 
fetch physical networks openstack using openstack4j api 
trying display list servers using openstack api 
invalid parameter domain keystone_tenant services 
make os tempest skip specific version apis 
opendaylight build particular project using maven 
openstack heat syntax multiple fixed_ips parameter 
n-cpu failure running ./stack.sh 
strstr working openstack instances 
high availability-odoo-server -openstack 
openstack ip address filter 
openstack opencontrail horizon login error 
xen vtpm 's integrated openstack cloud 
devstack error instance creation 
canonical maas multiple vlan 's 
openstack authentication always get 401 http response 
openstack api - difference dynamic large objects dlo & static large objects slo 
find guest vm usage details openstack 
make openstack keystone authentication request v3 v2 api 
create pecan project oslo rbac support 
curl “no json obeject could decoded” 
openstack nova-compute load multiple config 
openstack error neutron network service 
bare metal provisioning hypervisor installation 
networking api openstack work restclient -update quota tenant 
convert node.js file hybrid mobile app 
openstack instance snapshot creates empty image 
ssh cirros centos vm though ping ip using openstack 
neutron error “ rule create_port rule create_port fixed_ips performed” 
automation scripts using python api openstack swift 
use openstack identity api creating single sign web application 
add dynamically node existing cluster without restarting openstack 
expand single machine ubuntu cloud installation 
difference openvswitch_agent ml2_agent mitaka 
devstack installation fails aws ami instance 
deleted identity service openstack 
rpmdb corrupted 
integration openstack & opendaylight 
openstack able scan guest vm logical volumes 
openstack4j - list filter 
openstack installation single node 
create openstack vm instance specific port using jcloud api 
type object 'schemabasedmodel ' attribute 'in_scope ' 
way get ip assigned vm particular subnet embed heat orchestration template subnets 1 port 
setup barbican development environment centos 
failed floating-ips devstack installation 
real time deployment openstack 
openstack swift plugin jenkins 
heat set alarm configuration get alarm back ceilometer 
openstack view console connection times 
openstack neutron port unbound 
remote monitoring java visualvm jmx 
